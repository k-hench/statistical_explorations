[["index.html", "Statistical Explorations A small notebook to keep track 1 Intro", " Statistical Explorations A small notebook to keep track Kosmas Hench 2022-03-21 1 Intro So far, the journey includes Statistical Rethinking (McElreath 2020) and Bayesian Statistics the Fun Way (Kurt 2019). × References "],["rethinking-chapter-1.html", "2 Rethinking: Chapter 1", " 2 Rethinking: Chapter 1 The Golem of Prague by Richard McElreath, building on the Summary by Solomon Kurz × "],["rethinking-chapter-2.html", "3 Rethinking: Chapter 2 3.1 Counting possibilities 3.2 Building a Model 3.3 Making the model go / Bayes’ Theorem 3.4 Motors: Grid Approximation 3.5 Quadratic Approximation 3.6 Marcov Chain Monte Carlo (MCMC) 3.7 Homework 3.8 {brms} section 3.9 pymc3 section", " 3 Rethinking: Chapter 2 Small Worlds and Large Worlds by Richard McElreath, building on the Summary by Solomon Kurz 3.1 Counting possibilities d &lt;- tibble(p1 = 0, p2 = rep(1:0, times = c(1, 3)), p3 = rep(1:0, times = c(2, 2)), p4 = rep(1:0, times = c(3, 1)), p5 = 1) p1 p2 p3 p4 p5 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 d %&gt;% mutate(turn = 1:4)%&gt;% pivot_longer(p1:p5, names_to = &quot;prob&quot;, values_to = &quot;realization&quot;) %&gt;% arrange(prob, turn) %&gt;% mutate(prob = factor(prob, levels = c(&quot;p5&quot;, &quot;p4&quot;, &quot;p3&quot;, &quot;p2&quot;, &quot;p1&quot;)), marble = c(&quot;white&quot;, &quot;dark&quot;)[realization + 1]) %&gt;% ggplot( aes( x = turn, y = prob ) ) + geom_point(shape = 21, size = 4, aes( fill = marble, color = after_scale(clr_darken(fill)))) + geom_text(data = tibble(x = rep(c(.65, 4.45), each = 5), y = rep(str_c(&quot;p&quot;,1:5), 2), label = rep(c(&quot;[&quot;, &quot;]&quot;), each = 5), vjust = .7), aes( x = x, y = y, label = label), family = fnt_sel, size = 6)+ scale_fill_manual(values = c(white = clr0, dark = clrd)) + theme(legend.position = &quot;bottom&quot;) tibble(draws = 1:3, marbles = 4) %&gt;% mutate(possibilities = marbles ^ draws) %&gt;% flextable::flextable() .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c39e49fa{}.cl-c398fb30{font-family:'DejaVu Sans';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c3990e04{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c3993974{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c3993992{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c39939a6{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} drawsmarblespossibilities14424163464 layout_round &lt;- function(round = 1, n = 4, angle = 360, start_angle = 0, p = .5, round_prefix = &quot;&quot;){ n_round &lt;- n ^ round tibble(idx_round = 1:n_round, idx_round_sacaled = scales::rescale(idx_round, from = c(.5, n_round+.5), to = c(0, 1) * angle/360 + start_angle/360), idx_draw = rep(1:n, n_round/n), idx_parent = ((idx_round - 1 ) %/% n) + 1, name_parent = str_c(round_prefix, round - 1, &quot;_&quot;, idx_parent), name = str_c(round_prefix, round, &quot;_&quot;, idx_round), x = sin(idx_round_sacaled * 2 * pi) * round, y = cos(idx_round_sacaled * 2 * pi) * round) %&gt;% mutate(marble = c(&quot;white&quot;, &quot;dark&quot;)[1 + ((idx_draw/n) &lt;= p)], round_prefix = round_prefix, round = round) } links_round &lt;- function(round = 1, n = 4, round_prefix = &quot;&quot;){ n_round &lt;- n ^ round n_prev &lt;- n ^ (round - 1) tibble(idx_round = 1:n_round, idx_parent = ((idx_round - 1 ) %/% n) + 1, from = str_c(round_prefix, round - 1, &quot;_&quot;, idx_parent), to = str_c(round_prefix,round, &quot;_&quot;, idx_round), round_prefix = round_prefix) } round_origin &lt;- origin_round &lt;- function(round_prefix = &quot;&quot;){ tibble(idx_round = 0, idx_round_sacaled = 0, idx_draw = 0, name = str_c(round_prefix, &quot;0_1&quot;), x = 0, y = 0, marble = NA) } marble_graph &lt;- function(n_rounds = 3, n_draws = 4, angle = 360,start_angle = 0, p = .5, round_prefix = &quot;&quot;){ tbl_graph(nodes = 1:n_rounds %&gt;% map_dfr(layout_round, n = n_draws, angle = angle, start_angle = start_angle, p = p, round_prefix = round_prefix) %&gt;% bind_rows(round_origin(round_prefix = round_prefix), .), edges = 1:n_rounds %&gt;% map_dfr(links_round, round_prefix = round_prefix)) %E&gt;% mutate(marble = .N()$marble[to], to_name = .N()$name[to], from_name = .N()$name[from]) } marble_graph(p = .25, n_rounds = 3, angle = 180, start_angle = -90) %&gt;% ggraph( layout = &quot;manual&quot;, x = . %N&gt;% as_tibble() %&gt;% .$x, y = . %N&gt;% as_tibble() %&gt;% .$y) + geom_node_point(aes(fill = marble, color = marble, size = -round), shape = 21) + geom_edge_link(aes(color = marble), start_cap = circle(2, &#39;mm&#39;), end_cap = circle(2, &#39;mm&#39;)) + coord_equal() + scale_color_manual(values = clr_darken(c(white = clr0, dark = clrd)), na.value = &quot;transparent&quot;, guide = &quot;none&quot;) + scale_fill_manual(values = c(white = clr0, dark = clrd), na.value = &quot;transparent&quot;) + scale_edge_color_manual(values = c(white = clr_darken(clr0, .2), dark = clrd), guide = &quot;none&quot;) + scale_size_continuous(range = c(1.5,3), guide = &quot;none&quot;) + guides(fill = guide_legend(override.aes = list(size = 4)), edge_alpha = &quot;none&quot;) + labs(title = &quot;p = 0.25&quot;) + theme(legend.position = &quot;bottom&quot;) n_deviders &lt;- 3 n_rounds &lt;- 3 dividers &lt;- tibble(x = rep(0,n_deviders), y = x, tau = seq(from = 0, to = 2*pi, length.out = n_deviders + 1)[2:(n_deviders+1)], xend = sin(tau) * (n_rounds + .1), yend = cos(tau) * (n_rounds + .1)) p_trials &lt;- c(.25, .5, .75) all_conjectures &lt;- tibble(start_angle = c(0, 120, 240), round_prefix = c(&quot;r1_&quot; ,&quot;r2_&quot;, &quot;r3_&quot;), p = p_trials) %&gt;% pmap(marble_graph, angle = 120) %&gt;% reduce(bind_graphs) na_to_false &lt;- function(x){x[is.na(x)] &lt;- FALSE; x} na_to_true &lt;- function(x){x[is.na(x)] &lt;- TRUE; x} tester &lt;- function(x){x$name[x$r1_right]} trial_sequence &lt;- c(&quot;white&quot;, &quot;dark&quot;)[c(2,1,2)] selectors &lt;- all_conjectures %N&gt;% mutate(r1_right = (round == 1 &amp; marble == trial_sequence[1]) %&gt;% na_to_true(), r2_still_in = name_parent %in% name[r1_right], r2_right = r2_still_in &amp; (round == 2 &amp; marble == trial_sequence[2]), r3_still_in = name_parent %in% name[r2_right], r3_right = r3_still_in &amp; (round == 3 &amp; marble == trial_sequence[3]), on_path = r1_right | r2_right |r3_right) %&gt;% as_tibble() %&gt;% filter(on_path) selector_results &lt;- selectors %&gt;% filter(round == n_rounds) %&gt;% group_by(round_prefix) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(tau = seq(from = 0, to = 2*pi, length.out = n_rounds + 1)[2:(n_rounds+1)] - (2*pi)/(n_rounds * 2), x = sin(tau) * (n_rounds + .5), y = cos(tau) * (n_rounds + .5)) all_conjectures %&gt;% ggraph( layout = &quot;manual&quot;, x = . %N&gt;% as_tibble() %&gt;% .$x, y = . %N&gt;% as_tibble() %&gt;% .$y) + geom_node_point(aes(fill = marble, color = marble, size = -round, alpha = name %in% selectors$name), shape = 21) + geom_edge_link(aes(color = marble, alpha = to_name %in% selectors$name), start_cap = circle(2, &#39;mm&#39;), end_cap = circle(2, &#39;mm&#39;)) + geom_segment(data = dividers, aes(x = x, y = y, xend = xend, yend = yend), color = clr_darken(&quot;white&quot;,.10)) + geom_text(data = selector_results, aes( x = x, y = y, label = n), family = fnt_sel, size = 6) + coord_equal() + scale_color_manual(values = clr_darken(c(white = clr0, dark = clrd)), na.value = &quot;transparent&quot;, guide = &quot;none&quot;) + scale_fill_manual(values = c(white = clr0, dark = clrd), na.value = &quot;transparent&quot;) + scale_edge_color_manual(values = c(white = clr_darken(clr0, .2), dark = clrd), guide = &quot;none&quot;) + scale_size_continuous(range = c(1.5,3), guide = &quot;none&quot;) + scale_alpha_manual(values = c(`TRUE` = 1, `FALSE` = .2), guide = &quot;none&quot;) + guides(fill = guide_legend(override.aes = list(size = 4)), edge_alpha = &quot;none&quot;) + labs(caption = str_c(trial_sequence,collapse = &quot;-&quot;)) + theme(legend.position = &quot;bottom&quot;) html_marbles &lt;- c( glue(&quot;&lt;span style=&#39;color:{clr0};filter:drop-shadow(0px 0px 1px black)&#39;&gt;\\U2B24&lt;/span&gt;&quot;), glue(&quot;&lt;span style=&#39;color:{clr1l};filter:drop-shadow(0px 0px 1px black)&#39;&gt;\\U2B24&lt;/span&gt;&quot;)) html_conjecture &lt;- function(x){ str_c(&quot;[ &quot;,str_c(html_marbles[c(x)+1], collapse = &quot; &quot;),&quot; ]&quot;) } tibble(conjectures = list(rep(0,4), rep(1:0, c(1,3)), rep(1:0, c(2,2)), rep(1:0, c(3,1)), rep(1,4)), conjecture = map_chr(conjectures, html_conjecture), ways = map_dbl(conjectures, sum), p = c(0, p_trials, 1), `ways data/prior counts` = c(0, selector_results$n, 0), `new count` = map2_chr( `ways data/prior counts`, ways, .f = function(x,y){glue(&quot;{x} $\\\\times$ {y} = {x * y}&quot;)}), plausibility = (`ways data/prior counts` / sum(`ways data/prior counts`)) %&gt;% round(digits = 2) ) %&gt;% rename(`ways to produce &lt;span style=&#39;color:#85769EFF;filter:drop-shadow(0px 0px 1px black)&#39;&gt;⬤&lt;/span&gt;` = &quot;ways&quot;) %&gt;% dplyr::select(-conjectures) %&gt;% knitr::kable() conjecture ways to produce ⬤ p ways data/prior counts new count plausibility [ ⬤ ⬤ ⬤ ⬤ ] 0 0.00 0 0 \\(\\times\\) 0 = 0 0.00 [ ⬤ ⬤ ⬤ ⬤ ] 1 0.25 3 3 \\(\\times\\) 1 = 3 0.15 [ ⬤ ⬤ ⬤ ⬤ ] 2 0.50 8 8 \\(\\times\\) 2 = 16 0.40 [ ⬤ ⬤ ⬤ ⬤ ] 3 0.75 9 9 \\(\\times\\) 3 = 27 0.45 [ ⬤ ⬤ ⬤ ⬤ ] 4 1.00 0 0 \\(\\times\\) 4 = 0 0.00 3.2 Building a Model d &lt;- tibble(toss = c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;), n_trials = 1:9, sequence = n_trials %&gt;% map_chr(.f = function(x, chr){str_c(chr[1:x], collapse = &quot;&quot;)}, chr = toss), n_success = cumsum(toss == &quot;w&quot;), lag_n_trials = lag(n_trials, default = 0), lag_n_success = lag(n_success, default = 0)) sequence_length &lt;- 50 d %&gt;% expand(nesting(n_trials, toss, n_success), p_water = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% group_by(p_water) %&gt;% # you can learn more about lagging here: https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/lag or here: https://dplyr.tidyverse.org/reference/lead-lag.html mutate(lagged_n_trials = lag(n_trials, k = 1), lagged_n_success = lag(n_success, k = 1)) #&gt; # A tibble: 450 × 6 #&gt; # Groups: p_water [50] #&gt; n_trials toss n_success p_water lagged_n_trials lagged_n_success #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 w 1 0 NA NA #&gt; 2 1 w 1 0.0204 NA NA #&gt; 3 1 w 1 0.0408 NA NA #&gt; 4 1 w 1 0.0612 NA NA #&gt; 5 1 w 1 0.0816 NA NA #&gt; 6 1 w 1 0.102 NA NA #&gt; 7 1 w 1 0.122 NA NA #&gt; 8 1 w 1 0.143 NA NA #&gt; 9 1 w 1 0.163 NA NA #&gt; 10 1 w 1 0.184 NA NA #&gt; # … with 440 more rows stat_binom &lt;- function(n_trials, n_success, lag_n_trials, lag_n_success, sequence, ...){ if(n_trials == 1) { g_lag &lt;- stat_function(data = tibble(n_trials = n_trials, sequence = sequence), fun = function(x){1}, xlim = c(0,1), linetype = 3) } else { g_lag &lt;- stat_function(data = tibble(n_trials = n_trials, sequence = sequence), fun = function(x){ f &lt;- function(x){ dbinom(x = lag_n_success, size = lag_n_trials, prob = x)} f(x)/ integrate(f = f,lower = 0, upper = 1)[[1]]}, xlim = c(0,1), n = 500, linetype = 3) } g_current &lt;- stat_function(data = tibble(n_trials = n_trials, sequence = sequence), fun = function(x){ f &lt;- function(x){ dbinom(x = n_success, size = n_trials, prob = x)} f(x)/ integrate(f = f,lower = 0,upper = 1)[[1]]}, xlim = c(0,1),n = 500) list( g_lag, g_current) } ggplot() + (d %&gt;% pmap(stat_binom) %&gt;% unlist()) + facet_wrap(str_c(n_trials,&quot;: &quot;, sequence) ~ .) 3.3 Making the model go / Bayes’ Theorem \\[ Pr(\\textit{p} | W, L) = \\frac{Pr(W, L | \\textit{p}) ~ Pr(\\textit{p})}{Pr(W,L)}\\\\ Posterior = \\frac{Probability~of~the~Data \\times Prior}{ Average~probability~of~the~Data} \\] f_posterior_unscaled &lt;- function(f_porior, f_like){ function(x){ f_porior(x) * f_like(x)} } f_parts &lt;- c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;) gg_posterior &lt;- function(f_porior, f_like, comp = 1){ list( stat_function(data = tibble(part = factor(&quot;prior&quot;, levels = f_parts), comp = comp), fun = f_porior, xlim = c(0,1), n = 500, geom = &quot;area&quot;, color = clr2, fill = fll2), stat_function(data = tibble(part = factor(&quot;likelihood&quot;, levels = f_parts), comp = comp), fun = f_like, xlim = c(0,1), n = 500, geom = &quot;area&quot;, color = clr2, fill = fll2), stat_function(data = tibble(part = factor(&quot;posterior&quot;, levels = f_parts), comp = comp), fun = f_posterior_unscaled(f_porior = f_porior, f_like = f_like), xlim = c(0,1), n = 500, geom = &quot;area&quot;, color = clr2, fill = fll2) ) } scale_fun &lt;- function(f){ # marginal likelihood function(x){f(x) / integrate(f = f, lower = 0, upper = 1)[[1]]} } f_like_in &lt;- function(x){dbeta(x = x, shape1 = 8, shape2 = 5)} f_uni &lt;- function(x){1} f_step &lt;- function(x){if_else(x &lt; .5, 0, 1)} f_peak &lt;- function(x){if_else(x &lt; .5, (x * 2)^3, ((1 - x) * 2)^3)} ggplot() + gg_posterior(f_porior = f_uni, f_like = f_like_in) + gg_posterior(f_porior = f_step, f_like = f_like_in, comp = 2) + gg_posterior(f_porior = f_peak, f_like = f_like_in, comp = 3) + facet_wrap(comp ~ part, scales = &quot;free_y&quot;) 3.4 Motors: Grid Approximation grid_approx &lt;- function(n_grid = 20, W = 6, L = 3, prior = function(x){rep(1, length(x))}){ tibble(p_grid = seq(0, 1, length.out = n_grid), prior = prior(p_grid), likelihood = dbinom(W, size = W + L, prob = p_grid), posterior_unstand = likelihood * prior, posterior = posterior_unstand / sum(posterior_unstand)) } plot_grid_approx &lt;- function(data){ data %&gt;% ggplot(aes(x = p_grid, y = posterior, color = posterior)) + ggforce::geom_link2() + geom_point(shape = 21, fill = &quot;white&quot;, size = 2.5) + scale_color_gradientn(colours = c(clr_darken(clr0), clr2), guide = &quot;none&quot;) + labs(title = glue(&quot;{length(data$p_grid)} grid points&quot;)) } c(4, 7, 15, 60) %&gt;% map(grid_approx) %&gt;% map(plot_grid_approx) %&gt;% wrap_plots(nrow = 1) &amp; scale_x_continuous(breaks = c(0, .5, 1)) Note how the y scale depends on the number of grid points: the peak reaches ~0.75 for 4 points, but only ~ 0.043 for 60 points. 3.5 Quadratic Approximation library(rethinking) map &lt;- purrr::map compare_qa &lt;- function(w_in, l_in){ globe_qa &lt;- quap( alist( W ~ dbinom( W + L, p ), # binomial likelihood p ~ dunif( 0, 1 ) # uniform prior ), data = list( W = w_in, L = l_in ) ) qa_results &lt;- precis(globe_qa) %&gt;% as_tibble() %&gt;% mutate(qa = glue(&quot;W: {w_in}, L: {l_in}&quot;)) ggplot() + stat_function(fun = function(x){ dbeta( shape1 = w_in + 1, shape2 = l_in + 1, x = x ) / integrate(f = function(x){ dbeta( shape1 = w_in + 1, shape2 = l_in + 1, x = x )}, lower = 0, upper = 1)[[1]] }, xlim = c(0,1), n = 500, geom = &quot;area&quot;, color = clr0d, fill = fll0)+ stat_function(fun = function(x){ dnorm(x = x, mean = qa_results$mean, sd = qa_results$sd )}, xlim = c(0,1), n = 500, geom = &quot;line&quot;, color = clr2, linetype = 3) + labs(title = glue(&quot;W: {w_in}, L: {l_in}, n = {w_in + l_in}&quot;), y = &quot;density&quot;, x = &quot;proportion water&quot;) } compare_qa(w_in = 6, l_in = 3) + compare_qa(w_in = 12, l_in = 6) + compare_qa(w_in = 24, l_in = 12) 3.6 Marcov Chain Monte Carlo (MCMC) n_samples &lt;- 1e4 p_init &lt;- rep( NA, n_samples ) p_init[1] &lt;- .5 manual_mcmc &lt;- function(p, W = 6, L = 3){ for ( i in 2:n_samples ) { p_new &lt;- rnorm( n = 1, mean = p[ i - 1], sd = 0.1) if ( p_new &lt; 0 ){ p_new &lt;- abs( p_new ) } if ( p_new &gt; 1 ){ p_new &lt;- 2 - p_new } q0 &lt;- dbinom( W, W + L, p[ i - 1 ] ) q1 &lt;- dbinom( W, W + L, p_new ) p[i] &lt;- if_else( runif(1) &lt; q1 / q0, p_new, p[i - 1] ) } p } p &lt;- manual_mcmc(p_init) p_36 &lt;- manual_mcmc(p_init, W = 24, L = 12) p_p &lt;- tibble(x = p) %&gt;% ggplot() + stat_function(fun = function(x){ dbeta( shape1 = 6 + 1, shape2 = 3 + 1, x = x ) / integrate(f = function(x){ dbeta( shape1 = 6 + 1, shape2 = 3 + 1, x = x )}, lower = 0, upper = 1)[[1]] }, xlim = c(0,1), n = 500, geom = &quot;area&quot;, aes(color = &quot;posterior&quot;, fill = after_scale(clr_alpha(color))))+ geom_density(aes(x = x, color = &quot;MCMC&quot;)) + scale_color_manual(&quot;approach&quot;, values = c(posterior = clr0, MCMC = clr2)) + labs(y = &quot;density&quot;, x = &quot;proportion water&quot;, title = &quot;n = 12&quot;) + theme(legend.position = &quot;bottom&quot;) p_p36 &lt;- tibble(x = p_36) %&gt;% ggplot() + stat_function(fun = function(x){ dbeta( shape1 = 24 + 1, shape2 = 12 + 1, x = x ) / integrate(f = function(x){ dbeta( shape1 = 24 + 1, shape2 = 12 + 1, x = x )}, lower = 0, upper = 1)[[1]] }, xlim = c(0,1), n = 500, geom = &quot;area&quot;, aes(color = &quot;posterior&quot;, fill = after_scale(clr_alpha(color))))+ geom_density(aes(x = x, color = &quot;MCMC&quot;)) + scale_color_manual(&quot;approach&quot;, values = c(posterior = clr0, MCMC = clr2)) + labs(y = &quot;density&quot;, x = &quot;proportion water&quot;, title = &quot;n = 36&quot;) + theme(legend.position = &quot;bottom&quot;) p_p + p_p36 library(rlang) chapter2_models &lt;- env( compare_qa = compare_qa ) write_rds(chapter2_models, &quot;envs/chapter2_models.rds&quot;) 3.7 Homework M1 plot_grid_approx &lt;- function(data){ data %&gt;% ggplot(aes(x = p_grid, y = posterior, color = posterior)) + ggforce::geom_link2() + geom_point(shape = 21, fill = &quot;white&quot;, size = 2.5) + scale_color_gradientn(colours = c(clr_darken(clr0), clr2), guide = &quot;none&quot;) + labs(title = glue(&quot;{length(data$p_grid)} grid points&quot;)) } tibble(n_grid = rep(c(4,7,20), 3), L = rep(c(0,1,2), each = 3), W = rep(c(3,3,5), each = 3)) %&gt;% pmap(grid_approx) %&gt;% map(plot_grid_approx) %&gt;% wrap_plots(nrow = 3) &amp; scale_x_continuous(breaks = c(0, .5, 1)) M2 tibble(n_grid = rep(c(4,7,20), 3), L = rep(c(0,1,2), each = 3), W = rep(c(3,3,5), each = 3)) %&gt;% pmap(grid_approx, prior = f_step ) %&gt;% map(plot_grid_approx) %&gt;% wrap_plots(nrow = 3) &amp; scale_x_continuous(breaks = c(0, .5, 1)) M3 \\[ Pr(Earth | Land) = \\frac{Pr(Land | Earth) \\times Pr(Earth)}{Pr(Land)} \\] p_l_on_earth &lt;- .3 p_l_on_mars &lt;- 1 p_earth &lt;- .5 average_p_l &lt;- .5 * (p_l_on_earth + p_l_on_mars) (p_earth_on_l &lt;- p_l_on_earth * p_earth / average_p_l) #&gt; [1] 0.2307692 M4 &amp; M5 cards &lt;- c(&quot;C1b1|C1b2&quot;, &quot;C2b1|C2w2&quot;, &quot;C3w1|C3w2&quot; ) conjectures &lt;- c(&quot;C1b1|C1b2&quot;, &quot;C1b2|C1b1&quot;, &quot;C2b1|C2w2&quot;, &quot;C2w2|C2b1&quot;, &quot;C3w1|C3w2&quot;, &quot;C3w2|C3w1&quot;) tibble(cards = c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;), ways = c(&quot;C1b1|C1b2 &amp; C1b2|C1b1&quot;, &quot;C2b1|C2w2&quot;, &quot;&quot;), ways_tor_produce_data = c(2, 1, 0), plausibility = (ways_tor_produce_data / sum(ways_tor_produce_data)) %&gt;% round(digits = 3), other_side = c(&quot;b&quot;, &quot;w&quot;, &quot;&quot;), prior = c(2,1,1), posterior = ((plausibility * prior) / sum(plausibility * prior)) %&gt;% round(digits = 3))%&gt;% knitr::kable() cards ways ways_tor_produce_data plausibility other_side prior posterior C1 C1b1|C1b2 &amp; C1b2|C1b1 2 0.667 b 2 0.8 C2 C2b1|C2w2 1 0.333 w 1 0.2 C3 0 0.000 1 0.0 M6 tibble(cards = c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;), ways = c(&quot;C1b1|C1b2 &amp; C1b2|C1b1&quot;, &quot;C2b1|C2w2&quot;, &quot;&quot;), ways_tor_produce_data = c(2, 1, 0), plausibility = (ways_tor_produce_data / sum(ways_tor_produce_data)) %&gt;% round(digits = 3), other_side = c(&quot;b&quot;, &quot;w&quot;, &quot;&quot;), prior = c(1,2,3), posterior = ((plausibility * prior) / sum(plausibility * prior)) %&gt;% round(digits = 3))%&gt;% knitr::kable() cards ways ways_tor_produce_data plausibility other_side prior posterior C1 C1b1|C1b2 &amp; C1b2|C1b1 2 0.667 b 1 0.5 C2 C2b1|C2w2 1 0.333 w 2 0.5 C3 0 0.000 3 0.0 M7 tibble(cards = c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;), ways = c(&quot;C1b1&gt;C2w2 &amp; C1b1&gt;C3w1 &amp; C1b1&gt;C3w2 &amp; C2b1&gt;C2w2 &amp; C2b1&gt;C3w1 &amp; C2b1&gt;C3w2&quot;, &quot;C2b1&gt;C3w1 &amp; C2b1&gt;C3w2&quot;, &quot;&quot;), ways_tor_produce_data = c(6, 2, 0), plausibility = (ways_tor_produce_data / sum(ways_tor_produce_data)) %&gt;% round(digits = 3))%&gt;% knitr::kable() cards ways ways_tor_produce_data plausibility C1 C1b1&gt;C2w2 &amp; C1b1&gt;C3w1 &amp; C1b1&gt;C3w2 &amp; C2b1&gt;C2w2 &amp; C2b1&gt;C3w1 &amp; C2b1&gt;C3w2 6 0.75 C2 C2b1&gt;C3w1 &amp; C2b1&gt;C3w2 2 0.25 C3 0 0.00 H1 \\[ Pr(twin | spec_a) = 0.2 \\\\ Pr(twin | spec_b) = 0.1 \\\\ Pr(twin) = 0.15 \\] \\[ Pr(spec_a | twin) = \\frac{Pr(spec_a) \\times Pr(twin | spec_a)}{Pr(twin)} \\\\ Pr(spec_b | twin) = \\frac{Pr(spec_b) \\times Pr(twin | spec_b)}{Pr(twin)} \\] pr_twn_on_a &lt;- .1 pr_twn_on_b &lt;- .2 pr_twn &lt;- (pr_twn_on_a + pr_twn_on_b) /2 prior_a &lt;- .5 pr_a_on_twn &lt;- (prior_a * pr_twn_on_a) / pr_twn pr_b_on_twn &lt;- ((1 - prior_a) * pr_twn_on_b) / pr_twn (p_next_twn &lt;- pr_a_on_twn * pr_twn_on_a + pr_b_on_twn * pr_twn_on_b) %&gt;% round(digits = 3) #&gt; [1] 0.167 H2 \\[ Pr(spec_a | twin) = \\frac{1}{3} \\] pr_a_on_twn #&gt; [1] 0.3333333 H3 \\[ Pr(single | spec_a) = Pr(\\neg twin | spec_a) = 1 - Pr(twin | spec_a) \\] \\[ Pr( spec_a | single) = \\frac{Pr(single|spec_a)Pr(spec_a)}{Pr(single)} \\] pr_sgl_on_a &lt;- 1 - pr_twn_on_a pr_sgl_on_b &lt;- 1 - pr_twn_on_b pr_sgl &lt;- weighted.mean(x = c(pr_sgl_on_a, pr_sgl_on_b), w = c(pr_a_on_twn, 1- pr_a_on_twn)) prior_a &lt;- pr_a_on_twn pr_a_on_sgl &lt;- (prior_a * pr_sgl_on_a) / pr_sgl pr_b_on_sgl &lt;- ((1 - prior_a) * pr_sgl_on_b) / pr_sgl tibble(pr_a_on_sgl = pr_a_on_sgl, pr_b_on_sgl = pr_b_on_sgl, control = pr_a_on_sgl + pr_b_on_sgl) %&gt;% round(digits = 4) %&gt;% knitr::kable() pr_a_on_sgl pr_b_on_sgl control 0.36 0.64 1 H4 \\[ Pr(spec_a | test ) = 0.8 \\\\ Pr(spec_b | test ) = 0.65 \\\\ Pr(spec_a | test ) = \\frac{Pr( test | spec_a ) \\times Pr(spec_a)}{Pr(test_positive)} \\] pr_testa_on_a &lt;- .8 pr_testb_on_b &lt;- .65 pr_testa_on_b &lt;- 1 - pr_testb_on_b prior_a &lt;- .5 pr_testa &lt;- (prior_a * pr_testa_on_a) + ((1- prior_a) * pr_testa_on_b) tibble(pr_on_testa = c(&quot;A&quot;,&quot;B&quot;), genetic_test = c((prior_a * pr_testa_on_a) / pr_testa, ((1 - prior_a) * pr_testa_on_b) / pr_testa)) #&gt; # A tibble: 2 × 2 #&gt; pr_on_testa genetic_test #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 A 0.696 #&gt; 2 B 0.304 prior_a_updated &lt;- pr_a_on_sgl pr_testa_updated &lt;- (prior_a_updated * pr_testa_on_a) + ((1- prior_a_updated) * pr_testa_on_b) tibble(pr_on_testa = c(&quot;A&quot;,&quot;B&quot;), genetic_test = c((prior_a_updated * pr_testa_on_a) / pr_testa_updated, ((1 - prior_a_updated) * pr_testa_on_b) / pr_testa_updated)) #&gt; # A tibble: 2 × 2 #&gt; pr_on_testa genetic_test #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 A 0.562 #&gt; 2 B 0.438 3.8 {brms} section brms_c2_36_tosses &lt;- brm( data = list(w = 24), family = binomial(link = &quot;identity&quot;), w | trials(36) ~ 0 + Intercept, prior(beta(1, 1), class = b, lb = 0, ub = 1), seed = 42, file = &quot;brms/brms_c2_36_tosses&quot; ) brms_c2_36_tosses %&gt;% summary() #&gt; Family: binomial #&gt; Links: mu = identity #&gt; Formula: w | trials(36) ~ 0 + Intercept #&gt; Data: list(w = 24) (Number of observations: 1) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; Intercept 0.66 0.08 0.49 0.80 1.00 1123 1813 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). posterior_summary(brms_c2_36_tosses) %&gt;% round(digits = 3) %&gt;% knitr::kable() Estimate Est.Error Q2.5 Q97.5 b_Intercept 0.657 0.078 0.492 0.798 lp__ -3.994 0.744 -6.055 -3.465 as_draws_df(brms_c2_36_tosses) %&gt;% as_tibble() %&gt;% ggplot(aes(x = b_Intercept)) + geom_density(color = clr1, fill = fll1) + scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) + labs(y = &quot;density&quot;, x = &quot;proportion water&quot;, title = &quot;{brms} posterior probability (n = 36)&quot;) 3.9 pymc3 section Taken from from pymc-devs… import arviz as az import matplotlib.pyplot as plt import numpy as np import pymc3 as pm import scipy.stats as stats import seaborn as sns import matplotlib from matplotlib.colors import ListedColormap import matplotlib.font_manager #matplotlib.font_manager.rebuild() ways = np.array([0, 3, 8, 9, 0]) ways / ways.sum() #&gt; array([0. , 0.15, 0.4 , 0.45, 0. ]) \\[ Pr(w | n, p) = \\frac{n!}{w! (n - w)!}p^{w}(1 - p)^{n-w} \\] stats.binom.pmf(6, n = 9, p = 0.5) #&gt; 0.16406250000000003 Computing the posterior using a grid approximation inside a python function. def posterior_grid_approx(grid_points = 5, success = 6, tosses = 9): &quot;&quot;&quot;&quot;&quot;&quot; # define grid p_grid = np.linspace(0, 1, grid_points) # define prior prior = np.repeat(5, grid_points) # uniform # prior = (p_grid &gt;= 0.5).astype(int) # truncated # prior = np.exp(- 5 * abs(p_grid - 0.5)) # double exp # compute likelihood at each point in the grid likelihood = stats.binom.pmf(success, tosses, p_grid) # compute product of likelihood and prior unstd_posterior = likelihood * prior # standardize the posterior, so it sums to 1 posterior = unstd_posterior / unstd_posterior.sum() return p_grid, posterior w, n = 6, 9 fig, ax = plt.subplots(1, 3, figsize = (12, 4)) points = (5, 10, 20) for idx, ps in enumerate(points): p_grid, posterior = posterior_grid_approx(ps, w, n) ax[idx].plot(p_grid, posterior, &quot;o-&quot;, color = r.clr3, label = f&quot;successes = {w}\\ntosses = {n}&quot;) ax[idx].set_xlabel(&quot;probability of water&quot;) ax[idx].set_ylabel(&quot;posterior probability&quot;) ax[idx].set_title(f&quot;{ps} points&quot;) ax[idx].legend(loc = 0) ax[idx].spines[&#39;left&#39;].set_visible(False) ax[idx].spines[&#39;right&#39;].set_visible(False) ax[idx].spines[&#39;top&#39;].set_visible(False) ax[idx].spines[&#39;bottom&#39;].set_visible(False) ax[idx].grid(linestyle = &#39;dotted&#39;) np.repeat((0, 1), (3, 6)) #&gt; array([0, 0, 0, 1, 1, 1, 1, 1, 1]) data = np.repeat((0, 1), (3, 6)) with pm.Model() as normal_approximation: p = pm.Uniform(&quot;p&quot;, 0, 1) # uniform priors w = pm.Binomial(&quot;w&quot;, n=len(data), p=p, observed=data.sum()) # binomial likelihood mean_q = pm.find_MAP() std_q = ((1 / pm.find_hessian(mean_q, vars=[p])) ** 0.5)[0] # display summary of quadratic approximation #&gt; █ print(&quot;Mean\\tStandard deviation\\np {:.2}\\t{:.2}&quot;.format(mean_q[&quot;p&quot;], std_q[0])) #&gt; Mean Standard deviation #&gt; p 0.67 0.16 # Compute the 89% percentile interval norm = stats.norm(mean_q, std_q) prob = 0.89 z = stats.norm.ppf([(1 - prob) / 2, (1 + prob) / 2]) pi = mean_q[&quot;p&quot;] + std_q * z print(&quot;5.5%, 94.5% \\n{:.2}, {:.2}&quot;.format(pi[0], pi[1])) #&gt; 5.5%, 94.5% #&gt; 0.42, 0.92 # analytical calculation w, n = 6, 9 x = np.linspace(0, 1, 100) plt.plot(x, stats.beta.pdf(x, w + 1, n - w + 1), label=&quot;True posterior&quot;, color = r.clr0d) # quadratic approximation plt.plot(x, stats.norm.pdf(x, mean_q[&quot;p&quot;], std_q), label=&quot;Quadratic approximation&quot;, color = r.clr3) plt.legend(loc = 0) plt.title(f&quot;n = {n}&quot;) plt.xlabel(&quot;Proportion water&quot;); plt.gca().spines[&#39;left&#39;].set_visible(False) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(False) plt.grid(linestyle = &#39;dotted&#39;) plt.show() x = np.linspace(0, 1, 100) w, n = [6, 12, 24], [9, 18, 36] fig, ax = plt.subplots(1, 3, figsize=(12, 4)) for idx, ps in enumerate(zip(w, n)): data = np.repeat((0, 1), (ps[1] - ps[0], ps[0])) with pm.Model() as normal_approximation: p = pm.Uniform(&quot;p&quot;, 0, 1) # uniform priors w = pm.Binomial(&quot;w&quot;, n=len(data), p=p, observed=data.sum()) # binomial likelihood mean_q = pm.find_MAP() std_q = ((1 / pm.find_hessian(mean_q, vars=[p])) ** 0.5)[0] ax[idx].plot(x, stats.beta.pdf(x, ps[0] + 1, ps[1] - ps[0] + 1), label = &quot;True posterior&quot;, color = r.clr0d) ax[idx].plot(x, stats.norm.pdf(x, mean_q[&quot;p&quot;], std_q), label = &quot;Quadratic approximation&quot;, color = r.clr3) ax[idx].set_xlabel(&quot;probability of water&quot;) ax[idx].set_ylabel(&quot;density&quot;) ax[idx].set_title(r&quot;$n={}$&quot;.format(ps[1])) ax[idx].legend(loc=&quot;upper left&quot;) ax[idx].spines[&#39;left&#39;].set_visible(False) ax[idx].spines[&#39;right&#39;].set_visible(False) ax[idx].spines[&#39;top&#39;].set_visible(False) ax[idx].spines[&#39;bottom&#39;].set_visible(False) ax[idx].grid(linestyle = &#39;dotted&#39;) n_samples = 1000 p = np.zeros(n_samples) p[0] = 0.5 W = 6 L = 3 for i in range(1, n_samples): p_new = stats.norm(p[i - 1], 0.1).rvs(1) if p_new &lt; 0: p_new = -p_new if p_new &gt; 1: p_new = 2 - p_new q0 = stats.binom.pmf(W, n = W + L, p = p[i - 1]) q1 = stats.binom.pmf(W, n = W + L, p = p_new) if stats.uniform.rvs(0, 1) &lt; q1 / q0: p[i] = p_new else: p[i] = p[i - 1] az.plot_kde(p, label = &quot;Metropolis approximation&quot;, plot_kwargs = {&quot;color&quot;: r.clr3}) x = np.linspace(0, 1, 100) plt.plot(x, stats.beta.pdf(x, W + 1, L + 1), &quot;C1&quot;, label = &quot;True posterior&quot;, color = r.clr0d) plt.legend() plt.gca().spines[&#39;left&#39;].set_visible(False) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(False) plt.grid(linestyle = &#39;dotted&#39;) plt.show() × "],["rethinking-chapter-3.html", "4 Rethinking: Chapter 3 4.1 Sampling from a grid approximate posterior 4.2 Sampling to Summarize 4.3 Point estimates 4.4 sample to simulate prediction 4.5 Homework 4.6 {brms} section 4.7 pymc3 section", " 4 Rethinking: Chapter 3 Sampling the Imaginary by Richard McElreath, building on the Summary by Solomon Kurz \\[ Pr(vampire|positive) = \\frac{Pr(positive|vampire) \\times Pr(vampire)}{Pr(positive)} \\] pr_positive_on_vamp &lt;- .95 pr_positive_on_mort &lt;- .01 pr_vamp &lt;- .001 pr_positive &lt;- pr_positive_on_vamp * pr_vamp + pr_positive_on_mort * (1 - pr_vamp) (pr_vamp_on_positive &lt;- pr_positive_on_vamp * pr_vamp /pr_positive) #&gt; [1] 0.08683729 4.1 Sampling from a grid approximate posterior posterior here means simply ‘the probability of p conditional on the data’: grid_approx &lt;- function(n_grid = 20, L = 6, W = 3, prior = function(x){rep(1, length(x))}){ tibble(p_grid = seq(0, 1, length.out = n_grid), prior = prior(p_grid), likelihood = dbinom(L, size = W + L, prob = p_grid), posterior_unstand = likelihood * prior, posterior = posterior_unstand / sum(posterior_unstand)) } grid_data &lt;- grid_approx(n_grid = 10^4) samples &lt;- tibble(sample = 1:(10^4), proportion_water = sample(x = grid_data$p_grid, size = length(sample), prob = grid_data$posterior, replace = TRUE)) p_scatter &lt;- samples %&gt;% ggplot(aes(x = sample, y = proportion_water)) + geom_point(size = .75, shape = 21, color = clr_alpha(clr2,.3), fill = clr_alpha(clr2,.1)) + scale_x_continuous(expand = c(0,0)) p_dens &lt;- samples %&gt;% ggplot(aes(x = proportion_water)) + geom_density( color = clr2, fill = fll2) + scale_x_continuous(limits = c(0,1), expand = c(0, 0)) p_scatter + p_dens 4.2 Sampling to Summarize Once the posterior distribution is created, the model is done. Typical targets / questions: intervals of defined boundaries intervals of defined probability mass point estimates 4.2.1 Intervals of devined boundaries sum(grid_data$posterior[grid_data$p_grid &lt; 0.5]) #&gt; [1] 0.171875 sum(samples$proportion_water &lt; .5) / length(samples$proportion_water) #&gt; [1] 0.1674 sum(samples$proportion_water &gt; .5 &amp; samples$proportion_water &lt; .75) / length(samples$proportion_water) #&gt; [1] 0.6038 # f_post &lt;- function(x){dbeta(x = x, shape1 = 9 +1 , shape2 = 6 +1)} f_post &lt;- function(x){dbinom(x = 6, size = 9, prob = x)} f_post_norm &lt;- function(x){ f_post(x) / integrate(f = f_post,lower = 0, upper = 1)[[1]]} plot_intervals &lt;- function(x_bounds = c(0, 1), x_line = as.numeric(NA), f_posterior = f_post_norm, data = samples, ylim = c(0, 3)){ p_d &lt;- ggplot() + stat_function(fun = f_posterior, xlim = c(0,1), geom = &quot;area&quot;, color = clr0, fill = fll0) + stat_function(fun = f_posterior, xlim = x_bounds, geom = &quot;area&quot;, color = clr2, fill = fll2) + labs(y = &quot;density&quot;, x = &quot;proportion_water&quot;) p_d_emp &lt;- data %&gt;% ggplot(aes(x = proportion_water)) + geom_density( color = clr0, fill = fll0) + stat_function(fun = function(x){demp(obs = data$proportion_water, x = x)}, xlim = x_bounds, geom = &quot;area&quot;, color = clr2, fill = fll2) + labs(y = &quot;empirical density&quot;) p_d + p_d_emp &amp; geom_vline(data = tibble(x = x_line), aes(xintercept = x), linetype = 3) &amp; scale_y_continuous(limits = ylim)&amp; scale_x_continuous(limits = c(0, 1)) } plot_intervals(x_bounds = c(0, .5), x_line = .5) / plot_intervals(x_bounds = c(.5, .75), x_line = c(.5, .75)) 4.2.2 Intervals of defined mass aka.: compatibility interval credible interval percentile interval special form: highest posterior density interval (HPDI) qnt_80 &lt;- quantile(samples$proportion_water, probs = .8) qnt_80_inner &lt;- quantile(samples$proportion_water, probs = c(.1, .9)) plot_intervals(x_bounds = c(0, qnt_80), x_line = qnt_80)/ plot_intervals(x_bounds = qnt_80_inner, x_line = qnt_80_inner) library(rethinking) map &lt;- purrr::map grid_data_skew &lt;- grid_approx(L = 3, W = 0, n_grid = 10^4) samples_skew &lt;- tibble(sample = 1:(10^4), proportion_water = sample(x = grid_data_skew$p_grid, size = length(sample), prob = grid_data_skew$posterior, replace = TRUE)) f_post_skew &lt;- function(x){dbinom(x = 3, size = 3, prob = x)} f_post_norm_skew &lt;- function(x){ f_post_skew(x) / integrate(f = f_post_skew, lower = 0, upper = 1)[[1]]} qnt_50_inner &lt;- PI(samples_skew$proportion_water, prob = .5) qnt_50_high_dens &lt;- HPDI(samples_skew$proportion_water, prob = .5) plot_intervals(x_bounds = qnt_50_inner, x_line = qnt_50_inner, f_posterior = f_post_norm_skew, data = samples_skew, ylim = c(0, 4)) / plot_intervals(x_bounds = qnt_50_high_dens, x_line = qnt_50_high_dens, f_posterior = f_post_norm_skew, data = samples_skew, ylim = c(0, 4)) 4.3 Point estimates point_estimates &lt;- tibble(proportion_water= list(mean, median, chainmode) %&gt;% map_dbl(.f = function(f, vals){ f(vals) }, vals = samples_skew$proportion_water), statistic = c(&quot;mean&quot;, &quot;median&quot;, &quot;mode&quot;)) p_point_estimates &lt;- ggplot() + stat_function(fun = f_post_norm_skew, xlim = c(0,1), geom = &quot;area&quot;, color = clr0, fill = fll0) + geom_vline(data = point_estimates, aes(xintercept = proportion_water, linetype = statistic), color = clr1) + labs(y = &quot;density&quot;) f_loss &lt;- function(x){ map_dbl(x, function(x){ sum( grid_data_skew$posterior * abs( x - grid_data_skew$p_grid)) })} f_loss_quad &lt;- function(x){ map_dbl(x, function(x){ sum( grid_data_skew$posterior * ( x - grid_data_skew$p_grid) ^ 2) })} p_loss &lt;- ggplot() + stat_function(fun = f_loss, xlim = c(0,1), geom = &quot;area&quot;, color = clr0, fill = fll0) + geom_point(data = tibble(x = point_estimates$proportion_water[2], y = f_loss(point_estimates$proportion_water[2])), aes(x = x, y = y), shape = 1, size = 3, color = clr1) + labs(x = &quot;poportion_water&quot;, y = &quot;expected proportional loss&quot;) p_loss_quad &lt;- ggplot() + stat_function(fun = f_loss_quad, xlim = c(0,1), geom = &quot;area&quot;, color = clr0, fill = fll0) + geom_point(data = tibble(x = point_estimates$proportion_water[2], y = f_loss_quad(point_estimates$proportion_water[2])), aes(x = x, y = y), shape = 1, size = 3, color = clr1) + labs(x = &quot;poportion_water&quot;, y = &quot;expected proportional loss&quot;) p_point_estimates + p_loss + p_loss_quad + plot_layout(guide = &quot;collect&quot;) &amp; theme(legend.position = &quot;bottom&quot;) 4.4 sample to simulate prediction binomial likelihood \\[ Pr(W | N ,p) = \\frac{N!}{W! (N -W)!} p^{W}(1 - p)^{N-W} \\] dbinom( 0:2, size = 2, prob = .7) #&gt; [1] 0.09 0.42 0.49 rbinom( 10, size = 2, prob = .7) #&gt; [1] 1 2 1 0 1 2 2 1 1 1 create_dummy_w &lt;- function(size, prob){ tibble(x = rbinom(10^5, size = size, prob = prob), size = size, prob = prob) } dummy_w &lt;- create_dummy_w(size = 9, prob = .7) dummy_w %&gt;% group_by(x) %&gt;% count() %&gt;% ungroup() %&gt;% ggplot(aes(x = factor(x), y = n)) + geom_bar(stat = &quot;identity&quot;, color = clr1, fill = fll1, width = .6) + labs(y = &quot;count&quot;, x = &quot;dummy water count&quot;) tibble(size = rep(c(3,6,9), each = 3), prob = rep(c(.3,.6,.9), 3)) %&gt;% pmap_dfr(create_dummy_w) %&gt;% group_by(x, size , prob) %&gt;% count() %&gt;% ungroup() %&gt;% ggplot(aes(x = factor(x), y = n)) + geom_bar(stat = &quot;identity&quot;, color = clr1, fill = fll1, width = .6) + facet_grid(prob ~ size, scales = &quot;free&quot;, space = &quot;free_x&quot;, labeller = label_both) + labs(y = &quot;count&quot;, x = &quot;dummy water count&quot;) + theme(panel.background = element_rect(color = clr0d, fill = clr_alpha(clr0d,.2))) grid_data &lt;- grid_approx(n_grid = 10^4 + 1, L = 6, W = 3, prior = function(x){rep(1, length(x))}) %&gt;% mutate(idx = 1:(10^4 + 1)) grid_data %&gt;% ggplot(aes(x = p_grid))+ geom_line(aes(y = posterior, color = &quot;posterior&quot;)) + scale_color_manual(values = c(posterior = clr1), guide = &quot;none&quot;) + theme(legend.position = &quot;bottom&quot;) samples &lt;- grid_data %&gt;% slice_sample(n = 10^5 , weight_by = posterior, replace = TRUE) %&gt;% mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 9), seq = map(w, .f = function(x){sample(x = rep(c(&quot;W&quot;,&quot;L&quot;), c(x, 9-x)), size = 9, replace = FALSE)}), max_run_length = map_dbl(seq,.f = function(x){rle(x)$lengths %&gt;% max()}), n_switches = map_dbl(seq,.f = function(x){(rle(x)$lengths %&gt;% length()) -1})) p_posterior &lt;- grid_data %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_area(color = clr0d, fill = fll0) + geom_segment(data = grid_data %&gt;% filter(idx %in% (1+ (1:9)*1000)), aes(xend = p_grid, yend = 0, size = posterior), color = fll1) + geom_point(data = grid_data %&gt;% filter(idx %in% (1+ (1:9)*1000)), color = clr1) + scale_size_continuous(range = c(.1, 1), guide = &quot;none&quot;) + labs(x = &quot;probability of water&quot;, y = &quot;density&quot;, title = &quot;Posterior Probability&quot;) simulate_binom &lt;- function(probability, n_draws = 10^5, size = 9) { rbinom(n_draws, size = size, prob = probability) } d_small &lt;- tibble(probability = seq(from = .1, to = .9, by = .1)) %&gt;% mutate(draws = purrr::map(probability, simulate_binom)) %&gt;% unnest(draws) p_small &lt;- d_small %&gt;% ggplot(aes(x = draws)) + geom_bar(stat = &quot;count&quot;, color = clr1, fill = fll1, width = .6) + facet_wrap(probability ~ ., nrow = 1)+ labs(title = &quot;Sampling Distributions&quot;) p_posterior_predictive &lt;- samples %&gt;% ggplot(aes(x = factor(w))) + geom_bar(stat = &quot;count&quot;, aes(color = w == 6 , fill = after_scale(clr_alpha(color, .3))), width = .6) + scale_color_manual(values = c(`TRUE` = clr1, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of water samples&quot;, title = &quot;Posterior Predictive Distribution&quot;) p_posterior / p_small / p_posterior_predictive sum( samples$w == 6 ) / length( samples$w ) #&gt; [1] 0.20034 globe_data &lt;- c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;) globe_run_length &lt;- rle(globe_data)$lengths %&gt;% max() globe_n_switches &lt;- (rle(globe_data)$lengths %&gt;% length()) -1 p_run_length &lt;- samples %&gt;% ggplot(aes(x = factor(max_run_length))) + geom_bar(stat = &quot;count&quot;, aes(color = max_run_length == globe_run_length, fill = after_scale(clr_alpha(color))), width = .6) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;longest run length&quot;) p_switches &lt;- samples %&gt;% ggplot(aes(x = factor(n_switches))) + geom_bar(stat = &quot;count&quot;, aes(color = n_switches == globe_n_switches, fill = after_scale(clr_alpha(color))), width = .6) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of switches&quot;) p_run_length + p_switches library(rlang) chapter3_models &lt;- env( grid_data = grid_data ) write_rds(chapter3_models, &quot;envs/chapter3_models.rds&quot;) 4.5 Homework n &lt;- 1e4 easy_data &lt;- tibble(p_grid = seq( from = 0, to = 1, length.out = n ), prior = rep(1 , n), likelihood = dbinom( 6, size = 9, prob = p_grid), posterior_unscaled = likelihood * prior, posterior = posterior_unscaled / sum(posterior_unscaled), cummulative_posterior = cumsum(posterior)) easy_data %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = prior / sum(prior), color = &quot;prior&quot;)) + geom_line(aes(y = likelihood / sum(likelihood), color = &quot;likelihood&quot;)) + geom_line(aes(y = posterior, color = &quot;posterior&quot;), linetype = 3) + geom_line(aes(y = cummulative_posterior / sum(cummulative_posterior), color = &quot;cummulative_posterior&quot;), linetype = 3) + scale_color_manual(values = c(prior = clr1, likelihood = clr0d, posterior = clr2, cummulative_posterior = &quot;black&quot;)) + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), legend.position = &quot;bottom&quot;) set.seed( 100 ) easy_samples &lt;- easy_data %&gt;% slice_sample(n = n, weight_by = posterior, replace = TRUE) easy_samples %&gt;% ggplot(aes(x = p_grid)) + geom_density(color = clr0d, fill = fll0) + scale_x_continuous(limits = c(0,1)) + geom_vline(data = tibble(x = c(.2, .8)), aes(xintercept = x), linetype = 3) E1 mean(easy_samples$p_grid &lt;= .2) #&gt; [1] 5e-04 E2 mean(easy_samples$p_grid &gt; .8) #&gt; [1] 0.1219 E3 mean( easy_samples$p_grid &gt; .2 &amp; easy_samples$p_grid &lt; .8) #&gt; [1] 0.8776 E4 quantile(easy_samples$p_grid , probs = .2) #&gt; 20% #&gt; 0.5145315 E5 quantile(easy_samples$p_grid , probs = .8) #&gt; 80% #&gt; 0.7618962 E6 HPDI(easy_samples$p_grid, prob = .66) #&gt; |0.66 0.66| #&gt; 0.5138514 0.7886789 p_e6 &lt;- easy_samples %&gt;% ggplot(aes(x = p_grid)) + geom_density(color = clr0d, fill = fll0) + stat_function(fun = function(x){demp(x = x, obs = easy_samples$p_grid)}, geom = &quot;area&quot;, xlim = HPDI(easy_samples$p_grid, prob = .66), color = clr2, fill = fll2) + scale_color_manual(values = c(posterior = clr2), guide = &quot;none&quot;) + theme(legend.position = &quot;bottom&quot;) E7 PI(easy_samples$p_grid, prob = .66) #&gt; 17% 83% #&gt; 0.4972327 0.7745775 p_e7 &lt;- easy_samples %&gt;% ggplot(aes(x = p_grid)) + geom_density(color = clr0d, fill = fll0) + stat_function(fun = function(x){demp(x = x, obs = easy_samples$p_grid)}, geom = &quot;area&quot;, xlim = PI(easy_samples$p_grid, prob = .66), color = clr2, fill = fll2) + scale_color_manual(values = c(posterior = clr2), guide = &quot;none&quot;) + theme(legend.position = &quot;bottom&quot;) p_e6 + p_e7 M1 grid_data &lt;- grid_approx(n_grid = 10^4 + 1, L = 8, W = 7, prior = function(x){rep(1, length(x))}) %&gt;% mutate(idx = 1:(10^4 + 1)) grid_data %&gt;% ggplot(aes(x = p_grid))+ geom_line(aes(y = posterior, color = &quot;posterior&quot;)) + scale_color_manual(values = c(posterior = clr2), guide = &quot;none&quot;) + theme(legend.position = &quot;bottom&quot;) M2 samples &lt;- grid_data %&gt;% slice_sample(n = 10^5 , weight_by = posterior, replace = TRUE) %&gt;% mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 15)) HPDI(samples$p_grid, prob = .9) #&gt; |0.9 0.9| #&gt; 0.3329 0.7218 M3 p_posterior &lt;- grid_data %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_area(color = clr0d, fill = fll0) + geom_segment(data = grid_data %&gt;% filter(idx %in% (1+ (1:9)*1000)), aes(xend = p_grid, yend = 0, size = posterior), color = fll2) + geom_point(data = grid_data %&gt;% filter(idx %in% (1+ (1:9)*1000)), color = clr2) + scale_size_continuous(range = c(.1, 1), guide = &quot;none&quot;) + labs(x = &quot;probability of water&quot;, y = &quot;density&quot;, title = &quot;Posterior Probability&quot;) d_small &lt;- tibble(probability = seq(from = .1, to = .9, by = .1)) %&gt;% mutate(draws = purrr::map(probability, simulate_binom, size = 15)) %&gt;% unnest(draws) p_small &lt;- d_small %&gt;% ggplot(aes(x = draws)) + geom_bar(stat = &quot;count&quot;, color = clr2, fill = fll2, width = .6) + facet_wrap(probability ~ ., nrow = 1)+ labs(title = &quot;Sampling Distributions&quot;) p_posterior_predictive &lt;- samples %&gt;% ggplot(aes(x = factor(w))) + geom_bar(stat = &quot;count&quot;, aes(color = w == 8 , fill = after_scale(clr_alpha(color, .3))), width = .6) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of water samples&quot;, title = &quot;Posterior Predictive Distribution&quot;) p_posterior / p_small / p_posterior_predictive sum( samples$w == 8 ) / length( samples$w ) #&gt; [1] 0.14738 M4 samples &lt;- grid_data %&gt;% slice_sample(n = 10^5 , weight_by = posterior, replace = TRUE) %&gt;% mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 9)) samples %&gt;% ggplot(aes(x = factor(w))) + geom_bar(stat = &quot;count&quot;, aes(color = w == 6 , fill = after_scale(clr_alpha(color, .3))), width = .6) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of water samples&quot;, title = &quot;Posterior Predictive Distribution&quot;) sum( samples$w == 6 ) / length( samples$w ) #&gt; [1] 0.17634 M5 grid_data &lt;- grid_approx(n_grid = 10^4 + 1, L = 8, W = 7, prior = function(x){if_else(x &lt; .5, 0, 1)}) %&gt;% mutate(idx = 1:(10^4 + 1)) samples &lt;- grid_data %&gt;% slice_sample(n = 10^5 , weight_by = posterior, replace = TRUE) %&gt;% mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 15)) HPDI(samples$p_grid, prob = .9) #&gt; |0.9 0.9| #&gt; 0.5000 0.7117 p_posterior &lt;- grid_data %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_area(color = clr0d, fill = fll0) + geom_segment(data = grid_data %&gt;% filter(idx %in% (1+ (1:9)*1000)), aes(xend = p_grid, yend = 0, size = posterior), color = fll2) + geom_point(data = grid_data %&gt;% filter(idx %in% (1+ (1:9)*1000)), color = clr2) + scale_size_continuous(range = c(.1, 1), guide = &quot;none&quot;) + labs(x = &quot;probability of water&quot;, y = &quot;density&quot;, title = &quot;Posterior Probability&quot;) d_small &lt;- tibble(probability = seq(from = .1, to = .9, by = .1)) %&gt;% mutate(draws = purrr::map(probability, simulate_binom, size = 15)) %&gt;% unnest(draws) p_small &lt;- d_small %&gt;% ggplot(aes(x = draws)) + geom_bar(stat = &quot;count&quot;, color = clr2, fill = fll2, width = .6) + facet_wrap(probability ~ ., nrow = 1)+ labs(title = &quot;Sampling Distributions&quot;) p_posterior_predictive &lt;- samples %&gt;% ggplot(aes(x = factor(w))) + geom_bar(stat = &quot;count&quot;, aes(color = w == 8 , fill = after_scale(clr_alpha(color, .3))), width = .6) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of water samples&quot;, title = &quot;Posterior Predictive Distribution&quot;) p_posterior / p_small / p_posterior_predictive sum( samples$w == 8 ) / length( samples$w ) #&gt; [1] 0.15846 M6 random_tosses &lt;- function(n, n_grid = 1e4, n_posterior_sample = 1e4, prior = function(x){rep(1, length(x))}){ grid_data &lt;- tibble(p_grid = seq(0, 1, length.out = n_grid), prior = prior(p_grid), likelihood = dbinom(rbinom(1, size = n, prob = .7), size = n, prob = p_grid), posterior_unstand = likelihood * prior, posterior = posterior_unstand / sum(posterior_unstand)) samples &lt;- grid_data %&gt;% slice_sample(n = n_posterior_sample, weight_by = posterior, replace = TRUE) tibble(n = n, grid_data = list(grid_data), samples = list(samples), hpdi = list(HPDI(samples[[1]]$p_grid, prob = .99)), hpdi_width = diff(hpdi[[1]])) } c(c(1:10),((1:100) *30)) %&gt;% map_dfr(random_tosses) %&gt;% ggplot(aes(x = n, y = hpdi_width)) + geom_point(aes(color = hpdi_width &lt; .05)) + geom_hline(yintercept = .05, color = &quot;black&quot;, linetype = 3) + scale_color_manual(values = c(`FALSE` = clr0d, `TRUE` = clr2), guide = &quot;none&quot;) H1 library(rethinking) data(homeworkch3) n_grid &lt;- 1e4 + 1 grid_data &lt;- tibble(p_grid = seq(0, 1, length.out = n_grid), prior = (function(x){rep(1, length(x))})(p_grid), likelihood = dbinom(sum(birth1 + birth2), size = length(c(birth1, birth2)), prob = p_grid), posterior_unstand = likelihood * prior, posterior = posterior_unstand / sum(posterior_unstand)) samples &lt;- grid_data %&gt;% slice_sample(n = 1e5, weight_by = posterior, replace = TRUE) (mode_posterior &lt;- chainmode(samples$p_grid)) #&gt; [1] 0.5547754 grid_data %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_area(color = clr0d, fill = fll0, size = .5) + geom_vline(xintercept = mode_posterior, color = clr2, linetype = 3) H2 (percentile_intervals &lt;- tibble(boundary = c(&quot;lower&quot;, &quot;upper&quot;), p50 = HPDI(samples$p_grid, prob = .5), p89 = HPDI(samples$p_grid, prob = .89), p97 = HPDI(samples$p_grid, prob = .97))) %&gt;% knitr::kable() boundary p50 p89 p97 lower 0.5306 0.4977 0.4775 upper 0.5778 0.6090 0.6274 grid_data %&gt;% ggplot(aes(x = p_grid, y = posterior)) + stat_function(fun = function(x){demp(x = x, obs = samples$p_grid)}, geom = &quot;line&quot;, color = clr0d, xlim = c(0,1), n = 501) + stat_function(fun = function(x){demp(x = x, obs = samples$p_grid)}, geom = &quot;area&quot;, color = clr2, fill = clr_alpha(fll2,.2), xlim = percentile_intervals$p50, n = 501) + stat_function(fun = function(x){demp(x = x, obs = samples$p_grid)}, geom = &quot;area&quot;, color = clr2, fill = clr_alpha(fll2,.2), xlim = percentile_intervals$p89, n = 501) + stat_function(fun = function(x){demp(x = x, obs = samples$p_grid)}, geom = &quot;area&quot;, color = clr2, fill = clr_alpha(fll2,.2), xlim = percentile_intervals$p97, n = 501) H3 random_births &lt;- grid_data %&gt;% slice_sample(n = 1e4, weight_by = posterior, replace = TRUE) %&gt;% mutate(births = map(p_grid, .f = function(x){rbinom(n = 200, size = 1, prob = x)}), n_boys = map_dbl(births, sum), n_girls = 200 - n_boys, n_boys_firstborn = map_dbl(births, function(x){ sum(x[1:100]) })) sum(random_births$n_boys &lt; sum(birth1 + birth2)) / sum(birth1 + birth2) #&gt; [1] 43.45946 p_all &lt;- random_births %&gt;% ggplot(aes(x = n_boys)) + geom_density(color = clr0d, fill = fll0) + geom_vline(xintercept = sum(birth1 + birth2), color = clr2, linetype = 3) + scale_x_continuous(limits = c(0, 200)) p_first &lt;- random_births %&gt;% ggplot(aes(x = n_boys_firstborn)) + geom_density(color = clr0d, fill = fll0) + geom_vline(xintercept = sum(birth1), color = clr2, linetype = 3) + scale_x_continuous(limits = c(0, 100)) p_all + p_first H5 births_first_girl &lt;- tibble(birth1 = birth1, birth2 = birth2) %&gt;% filter(birth1 == 0) n_first_girl &lt;- length(births_first_girl$birth2) random_births &lt;- grid_data %&gt;% slice_sample(n = 1e4, weight_by = posterior, replace = TRUE) %&gt;% mutate(births = map(p_grid, .f = function(x){rbinom(n = n_first_girl, size = 1, prob = x)}), n_boys = map_dbl(births, sum), n_girls = n_first_girl - n_boys) sum(random_births$n_boys &lt; sum(births_first_girl$birth2)) / 1e4 #&gt; [1] 0.9991 random_births %&gt;% ggplot(aes(x = n_boys)) + geom_density(color = clr0d, fill = fll0) + geom_vline(xintercept = sum(births_first_girl$birth2), color = clr2, linetype = 3) + scale_x_continuous(limits = c(0, n_first_girl), expand = c(0, 0)) 4.6 {brms} section brms_c3_6in9 &lt;- brm(data = list(w = 6), family = binomial(link = &quot;identity&quot;), w | trials(9) ~ 0 + Intercept, # this is a flat prior prior(beta(1, 1), class = b, lb = 0, ub = 1), iter = 5000, warmup = 1000, seed = 42, file = &quot;brms/brms_c3_6in9&quot;) posterior_summary(brms_c3_6in9) %&gt;% round(digits = 3) %&gt;% knitr::kable() Estimate Est.Error Q2.5 Q97.5 b_Intercept 0.637 0.140 0.346 0.879 lp__ -3.310 0.748 -5.392 -2.780 n_trials &lt;- 9 samples_brms &lt;- fitted(brms_c3_6in9, summary = FALSE, scale = &quot;linear&quot;) %&gt;% as_tibble() %&gt;% set_names(nm = &quot;p&quot;) %&gt;% mutate(w = rbinom(n(), size = n_trials, prob = p)) p_brms_posterior &lt;- samples_brms %&gt;% ggplot(aes(x = p)) + geom_density(color = clr1, fill = fll1) + scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) + labs(y = &quot;density&quot;, x = &quot;proportion water&quot;, title = &quot;{brms} posterior probability (6 in 9)&quot;) p_brms_posterior_predictive &lt;- samples_brms %&gt;% ggplot(aes(x = factor(w))) + geom_bar(stat = &quot;count&quot;, aes(color = w == 6 , fill = after_scale(clr_alpha(color, .3))), width = .6) + scale_color_manual(values = c(`TRUE` = clr1, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(y = &quot;count&quot;, x = &quot;number of water samples&quot;, title = &quot;posterior predictive distribution&quot;) p_brms_posterior + p_brms_posterior_predictive 4.7 pymc3 section Taken from pymc-devs… import arviz as az import matplotlib.pyplot as plt import numpy as np import pymc3 as pm import scipy.stats as stats import seaborn as sns import matplotlib from matplotlib.colors import ListedColormap import matplotlib.font_manager #matplotlib.font_manager._rebuild() def posterior_grid_approx(grid_points = 5, success = 6, tosses = 9): &quot;&quot;&quot;&quot;&quot;&quot; # define grid p_grid = np.linspace(0, 1, grid_points) # define prior prior = np.repeat(5, grid_points) # uniform # prior = (p_grid &gt;= 0.5).astype(int) # truncated # prior = np.exp(- 5 * abs(p_grid - 0.5)) # double exp # compute likelihood at each point in the grid likelihood = stats.binom.pmf(success, tosses, p_grid) # compute product of likelihood and prior unstd_posterior = likelihood * prior # standardize the posterior, so it sums to 1 posterior = unstd_posterior / unstd_posterior.sum() return p_grid, posterior PrPV = 0.95 PrPM = 0.01 PrV = 0.001 PrP = PrPV * PrV + PrPM * (1 - PrV) PrVP = PrPV * PrV / PrP PrVP #&gt; 0.08683729433272395 p_grid, posterior = posterior_grid_approx(grid_points = 100, success = 6, tosses = 9) samples = np.random.choice(p_grid, p = posterior, size = int(1e4), replace = True) fig, (ax0, ax1) = plt.subplots(1, 2, figsize = (12, 4)) ax0.plot(samples, &quot;o&quot;, alpha = 0.2, color = r.clr3) ax0.set_xlabel(&quot;sample number&quot;) ax0.set_ylabel(&quot;proportion water (p)&quot;) ax0.spines[&#39;left&#39;].set_visible(False) ax0.spines[&#39;right&#39;].set_visible(False) ax0.spines[&#39;top&#39;].set_visible(False) ax0.spines[&#39;bottom&#39;].set_visible(False) ax0.grid(linestyle = &#39;dotted&#39;) az.plot_kde(samples, ax = ax1, plot_kwargs = {&quot;color&quot;: r.clr3}) ax1.set_xlabel(&quot;proportion water (p)&quot;) ax1.set_ylabel(&quot;density&quot;) ax1.spines[&#39;left&#39;].set_visible(False) ax1.spines[&#39;right&#39;].set_visible(False) ax1.spines[&#39;top&#39;].set_visible(False) ax1.spines[&#39;bottom&#39;].set_visible(False) ax1.grid(linestyle = &#39;dotted&#39;) plt.show() sum(posterior[p_grid &lt; 0.5]) #&gt; 0.17183313110747475 sum(samples &lt; 0.5) / 1e4 #&gt; 0.1767 sum((samples &gt; 0.5) &amp; (samples &lt; 0.75)) / 1e4 #&gt; 0.6113 np.percentile(samples, 80) #&gt; 0.7575757575757577 np.percentile(samples, [10, 90]) #&gt; array([0.44444444, 0.80808081]) p_grid, posterior = posterior_grid_approx(grid_points = 100, success = 3, tosses = 3) plt.plot(p_grid, posterior, color = r.clr3) plt.xlabel(&quot;proportion water (p)&quot;) plt.ylabel(&quot;Density&quot;) plt.gca().spines[&#39;left&#39;].set_visible(False) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(False) plt.grid(linestyle = &#39;dotted&#39;) plt.show() samples = np.random.choice(p_grid, p = posterior, size = int(1e4), replace = True) np.percentile(samples, [25, 75]) #&gt; array([0.71717172, 0.93939394]) az.hdi(samples, hdi_prob = 0.5) #&gt; array([0.84848485, 1. ]) p_grid[posterior == max(posterior)] #&gt; array([1.]) stats.mode(samples)[0] #&gt; array([1.]) np.mean(samples), np.median(samples) #&gt; (0.8061979797979799, 0.8484848484848485) sum(posterior * abs(0.5 - p_grid)) #&gt; 0.31626874808692995 loss = [sum(posterior * abs(p - p_grid)) for p in p_grid] p_grid[loss == min(loss)] #&gt; array([0.84848485]) stats.binom.pmf(range(3), n = 2, p = 0.7) #&gt; array([0.09, 0.42, 0.49]) stats.binom.rvs(n = 2, p = 0.7, size = 1) #&gt; array([1]) stats.binom.rvs(n = 2, p = 0.7, size = 10) #&gt; array([2, 2, 1, 2, 1, 0, 2, 1, 2, 2]) dummy_w = stats.binom.rvs(n = 2, p = 0.7, size = int(1e5)) [(dummy_w == i).mean() for i in range(3)] #&gt; [0.08832, 0.42272, 0.48896] dummy_w = stats.binom.rvs(n = 9, p = 0.7, size = int(1e5)) # dummy_w = stats.binom.rvs(n=9, p=0.6, size=int(1e4)) # dummy_w = stats.binom.rvs(n=9, p=samples) bar_width = 0.7 plt.hist(dummy_w, bins = np.arange(0, 11) - bar_width / 2, width = bar_width, color = r.clr3) #&gt; (array([2.0000e+00, 4.2000e+01, 3.9200e+02, 2.1170e+03, 7.4230e+03, #&gt; 1.7153e+04, 2.6752e+04, 2.6557e+04, 1.5640e+04, 3.9220e+03]), array([-0.35, 0.65, 1.65, 2.65, 3.65, 4.65, 5.65, 6.65, 7.65, #&gt; 8.65, 9.65]), &lt;BarContainer object of 10 artists&gt;) plt.xlim(0, 9.5) #&gt; (0.0, 9.5) plt.xlabel(&quot;dummy water count&quot;) plt.ylabel(&quot;Frequency&quot;) plt.gca().spines[&#39;left&#39;].set_visible(False) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(False) plt.grid(linestyle = &#39;dotted&#39;) plt.show() p_grid, posterior = posterior_grid_approx(grid_points = 100, success = 6, tosses = 9) np.random.seed(100) samples = np.random.choice(p_grid, p = posterior, size = int(1e4), replace = True) birth1 = np.array([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1]) birth2 = np.array([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0]) sum(birth1) + sum(birth2) #&gt; 111 × "],["rethinking-chapter-4.html", "5 Rethinking: Chapter 4 5.1 Why normal distributions are normal 5.2 Normal by multiplication and by log-multiplication 5.3 A language for describing models 5.4 Linear Prediction 5.5 Curves from lines 5.6 Splines 5.7 Homework 5.8 {brms} section 5.9 pymc3 section", " 5 Rethinking: Chapter 4 Geocentric Models by Richard McElreath, building on the Summaries by Solomon Kurz and Jake Thompson. 5.1 Why normal distributions are normal 5.1.1 Normal by addition n_people &lt;- 1e3 position &lt;- crossing(person = 1:n_people, step = 0:16) %&gt;% mutate(deviation = map_dbl(step, ~if_else(. == 0, 0, runif(1, -1, 1)))) %&gt;% group_by(person) %&gt;% mutate(position = cumsum(deviation)) %&gt;% ungroup() p_all_steps &lt;- position %&gt;% ggplot(aes(x = step, y = position, group = person)) + geom_line(aes(color = person == n_people)) + geom_point(data = position %&gt;% filter(person == n_people), aes(color = &quot;TRUE&quot;), size = 1) + geom_vline(data = tibble(step = c(4, 8, 16)), aes(xintercept = step), linetype = 3, color = rgb(0,0,0,.5)) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr_alpha(clr0d, .05)), guide = &quot;none&quot;) + scale_x_continuous(breaks = c(0,4,8,16)) + theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) plot_steps &lt;- function(step_nr, data = position, add_ideal = FALSE){ data_step &lt;- data %&gt;% filter(step == step_nr) p &lt;- data_step %&gt;% ggplot(aes(x = position)) + geom_density(adjust = .2, color = clr0d, fill = fll0) + scale_x_continuous(limits = c(-6, 6)) + labs(title = glue(&quot;{step_nr} steps&quot;)) if(add_ideal){p &lt;- p + stat_function(fun = function(x){dnorm(x, mean = 0, sd = sd(data_step$position))}, n = 501, color = clr2, linetype = 3)} p } p_all_steps / (plot_steps(step_nr = 4) + plot_steps(step_nr = 8) + plot_steps(step_nr = 16, add_ideal = TRUE)) 5.2 Normal by multiplication and by log-multiplication normal_by_multiplication &lt;- function(effect_size = 0.1, x_scale = ggplot2::scale_x_continuous(), x_lab = &quot;normal&quot;){ tibble(person = 1:n_people, growth = replicate(length(person), prod(1 + runif(12, 0, effect_size)))) %&gt;% ggplot(aes(x = growth)) + geom_density(color = clr0d, fill = fll0) + labs(title = glue(&quot;effect size: {effect_size}&quot;), x = glue(&quot;growth ({x_lab})&quot;)) + x_scale } normal_by_multiplication(effect_size = .01) + normal_by_multiplication(effect_size = .1) + normal_by_multiplication(effect_size = .5)+ normal_by_multiplication(effect_size = .5, x_scale = scale_x_log10(), x_lab = &quot;log10&quot;) + plot_layout(nrow = 1) 5.2.1 using the Gaussian distribution part of the exponential family probability density function \\(\\mu\\): mean \\(\\sigma\\): standard deviation \\(\\tau\\): precision \\[ p( y | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp \\left( \\frac{(y-\\mu)^2}{2\\sigma^2} \\right)\\\\ \\tau = 1 / \\sigma^2 \\\\ p( y | \\mu, \\tau) = \\sqrt{\\frac{\\tau}{2\\pi}}exp(-\\tfrac{1}{2}\\tau(y - \\mu)^2) \\] 5.3 A language for describing models The first line defines the likelihood used in Bayes’ theorem, the other lines describe the priors used. The tilde means that the relationships are stochastic. re-describing the globe-toss model: The count \\(W\\) is distributed binomially with a sample size \\(N\\) and the probabiliy \\(p\\). The prior for \\(p\\) is assumed to be uniform between zero and one \\[ W \\sim Binomial(N, p)\\\\ p \\sim Uniform(0, 1) \\] Substituting in Bayes’ theorem: \\[ Pr(p | w, n) = \\frac{Binomial(w|n,p)~Uniform(p|0,1)}{\\int Binomial(w|n,p)~Uniform(p|0,1) dp} \\] w &lt;- 6 n &lt;- 9 grid_data &lt;- tibble(p_grid = seq(0,1, length.out = 101), likelihood = dbinom(w, n, p_grid), prior = dunif(p_grid, 0, 1), posterior_unstand = likelihood * prior, posterior = posterior_unstand / sum(posterior_unstand)) grid_data %&gt;% pivot_longer(cols = c(prior, likelihood, posterior), names_to = &quot;bayes_part&quot;, values_to = &quot;p&quot;) %&gt;% mutate(bayes_part = factor(bayes_part, levels = names(clr_bayes))) %&gt;% ggplot(aes(x = p_grid)) + geom_area(aes(y = p, color = bayes_part, fill = after_scale(clr_alpha(color)))) + scale_color_manual(values = clr_bayes, guide = &quot;none&quot;) + facet_wrap(bayes_part ~ ., scales = &quot;free_y&quot;) 5.3.1 Gaussian model of height 5.3.1.1 The data library(rethinking) data(Howell1) (data &lt;- as_tibble(Howell1)) %&gt;% precis() %&gt;% as_tibble(rownames = NA) %&gt;% knitr::kable() x 138.2635963 35.6106176 29.3443934 0.4724265 x 27.6024476 14.7191782 20.7468882 0.4996986 x 81.108550 9.360721 1.000000 0.000000 x 165.73500 54.50289 66.13500 1.00000 x ▁▁▁▁▁▁▁▂▁▇▇▅▁ ▁▂▃▂▂▂▂▅▇▇▃▂▁ ▇▅▅▃▅▂▂▁▁ ▇▁▁▁▁▁▁▁▁▇ (data_adults &lt;- data %&gt;% filter(age &gt;= 18)) %&gt;% precis() %&gt;% as_tibble(rownames = NA) %&gt;% knitr::kable() x 154.59709 44.99049 41.13849 0.46875 x 7.7423321 6.4567081 15.9678551 0.4997328 x 142.8750 35.1375 20.0000 0.0000 x 167.00500 55.76588 70.00000 1.00000 x ▁▃▇▇▅▇▂▁▁ ▁▅▇▇▃▂▁ ▂▅▇▅▃▇▃▃▂▂▂▁▁▁▁ ▇▁▁▁▁▁▁▁▁▇ data_adults %&gt;% ggplot(aes(x = height)) + geom_density(adjust = .5, color = clr0d, fill = fll0) + scale_x_continuous(limits = c(130,185)) 5.3.1.2 The model \\[ \\begin{array}{cccr} h_i &amp; \\stackrel{iid}{\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu &amp; \\sim &amp; Normal(178, 20) &amp; \\textrm{[$\\mu$ prior]}\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] where, \\(iid\\) means “independent and identically distributed”. Prior predictive simulation (‘what does the model think before seeing the data?’) n_samples &lt;- 1e4 prior_simulation &lt;- tibble( sample_mu = rnorm(n_samples, 178, 20), sample_sigma = runif(n_samples, 0, 50), prior_h = rnorm(n_samples, sample_mu, sample_sigma), bad_mu = rnorm(n_samples, 178, 100), bad_prior = rnorm(n_samples, bad_mu, sample_sigma) ) p_mu &lt;- ggplot() + stat_function(fun = function(x){dnorm(x = x, mean = 178, sd = 20)}, xlim = c(100,250), color = clr0d, fill = fll0, geom = &quot;area&quot;) + labs(title = glue(&quot;*\\U03BC* {mth(&#39;\\U007E&#39;)} dnorm( 178, 20 )&quot;), y = &quot;density&quot;, x = &quot;*\\U03BC*&quot;) p_sigma &lt;- ggplot() + stat_function(fun = function(x){dunif(x = x, min = 0, max = 50)}, xlim = c(-5, 55), color = clr1, fill = fll1, geom = &quot;area&quot;) + labs(title = glue(&quot;*{mth(&#39;\\U03C3&#39;)}* {mth(&#39;\\U007E&#39;)} dunif( 0, 50 )&quot;), y = &quot;density&quot;, x = glue(&quot;*{mth(&#39;\\U03C3&#39;)}*&quot;)) p_prior_sim &lt;- prior_simulation %&gt;% ggplot(aes(x = prior_h)) + geom_density(color = clr2, fill = fll2, adjust = .4) + scale_x_continuous(limits = c(0,356), breaks = c(0,73,178,283)) + labs(title = glue(&quot;*h&lt;sub&gt;i&lt;/sub&gt;* {mth(&#39;\\U007E&#39;)} dnorm( *\\U03BC*, {mth(&#39;\\U03C3&#39;)} )&quot;), x = &quot;height&quot;) p_bad_prior &lt;- prior_simulation %&gt;% ggplot(aes(x = bad_prior)) + geom_density(color = clr2, fill = fll2, adjust = .4) + scale_x_continuous(limits = c(-222,578), breaks = c(-128,0,178,484), expand = c(0,0)) + geom_vline(data = tibble(h = c(0,272)), aes(xintercept = h), linetype = 3)+ labs(title = glue(&quot;*h&lt;sub&gt;i&lt;/sub&gt;* {mth(&#39;\\U007E&#39;)} dnorm( *\\U03BC*, {mth(&#39;\\U03C3&#39;)} )&lt;br&gt;*\\U03BC* {mth(&#39;\\U007E&#39;)} dnorm( 178, 100 )&quot;), x = &quot;height&quot;) p_mu + p_sigma + p_prior_sim + p_bad_prior &amp; theme(plot.title = element_markdown(), axis.title.x = element_markdown()) 5.3.1.3 grid approximation of the posterior distribution n_grid &lt;- 101 grid_data &lt;- cross_df(list(mu = seq(from = 152, to = 157, length.out = n_grid), sigma = seq(from = 6.5, to = 9, length.out = n_grid))) %&gt;% mutate(log_likelihood = map2_dbl(.x = mu, .y = sigma, .f = function(x, y){ dnorm(x = data_adults$height, mean = x, sd = y, log = TRUE) %&gt;% sum() }), prior_mu = dnorm(mu, mean = 178, sd = 20, log = TRUE), prior_sigma = dunif(sigma, min = 0, max = 50, log = TRUE), product = log_likelihood + prior_mu + prior_sigma, probability = exp(product - max(product))) grid_data %&gt;% ggplot(aes(x = mu, y = sigma, z = probability)) + geom_raster(aes(fill = probability)) + geom_contour(color = rgb(1,1,1,.1)) + coord_cartesian(xlim = range(grid_data$mu), ylim = range(grid_data$sigma)) + scale_fill_gradientn(colours = clr_grd5 %&gt;% clr_alpha(alpha = .8), limits = c(0,1)) + coord_cartesian(xlim = range(grid_data$mu), ylim = range(grid_data$sigma), expand = 0) + guides(fill = guide_colorbar(title.position = &quot;top&quot;, barwidth = unit(.9,&quot;npc&quot;), barheight = unit(5, &quot;pt&quot;))) + labs(x = &quot; *\\U03BC*&quot;, y = glue(&quot;*{mth(&#39;\\U03C3&#39;)}*&quot;))+ theme(legend.position = &quot;bottom&quot;, axis.title.x = element_markdown(), axis.title.y = element_markdown()) Sampling from the posterior distribution n_posterior_sample &lt;- 1e4 samples &lt;- grid_data %&gt;% slice_sample(n = n_posterior_sample, weight_by = probability, replace = TRUE) p_samples &lt;- samples %&gt;% group_by(mu, sigma) %&gt;% count() %&gt;% ungroup() %&gt;% ggplot(aes(x = mu, y = sigma, color = n)) + geom_point(size = .4) + scale_color_gradientn(colours = clr_grd5 %&gt;% clr_alpha(alpha = .8)) + coord_cartesian(xlim = buffer_range(grid_data$mu), ylim = buffer_range(grid_data$sigma), expand = 0) + guides(color = guide_colorbar(title.position = &quot;top&quot;, barwidth = unit(.2,&quot;npc&quot;), barheight = unit(5, &quot;pt&quot;))) + labs(x = &quot; *\\U03BC*&quot;, y = glue(&quot;*{mth(&#39;\\U03C3&#39;)}*&quot;))+ theme(legend.position = &quot;bottom&quot;, axis.title.x = element_markdown(), axis.title.y = element_markdown()) p_mu_dens &lt;- samples %&gt;% ggplot(aes(x = mu)) + geom_density(color = clr0d, fill = fll0) + scale_x_continuous(limits = buffer_range(grid_data$mu), expand = c(0, 0)) + labs(y = &quot;marginal&lt;br&gt;density&quot;) + theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.title.y = element_markdown()) p_sigma_dens &lt;- samples %&gt;% ggplot(aes(x = sigma)) + geom_density(color = clr0d, fill = fll0) + scale_x_continuous(limits = buffer_range(grid_data$sigma), expand = c(0, 0)) + labs(y = &quot;marginal density&quot;) + coord_flip() + theme(axis.title.y = element_blank(), axis.text.y = element_blank()) p_mu_dens + patchwork::guide_area() + p_samples + p_sigma_dens + plot_layout(guides = &quot;collect&quot;, widths = c(1,.3), heights = c(.3,1)) Exploration of long tail for \\(\\sigma\\) when original sample size is small: heights_subset &lt;- sample(data_adults$height, size = 20) grid_data_subset &lt;- cross_df(list(mu = seq(from = 145, to = 165, length.out = n_grid), sigma = seq(from = 4.5, to = 16, length.out = n_grid))) %&gt;% mutate(log_likelihood = map2_dbl(.x = mu, .y = sigma, .f = function(x, y){ dnorm(x = heights_subset, mean = x, sd = y, log = TRUE) %&gt;% sum() }), prior_mu = dnorm(mu, mean = 178, sd = 20, log = TRUE), prior_sigma = dunif(sigma, min = 0, max = 50, log = TRUE), product = log_likelihood + prior_mu + prior_sigma, probability = exp(product - max(product))) samples_subset &lt;- grid_data_subset %&gt;% slice_sample(n = n_posterior_sample, weight_by = probability, replace = TRUE) p_samples &lt;- samples_subset %&gt;% group_by(mu, sigma) %&gt;% count() %&gt;% ungroup() %&gt;% ggplot(aes(x = mu, y = sigma, color = n)) + geom_point(size = .4) + scale_color_gradientn(colours = clr_grd4 %&gt;% clr_alpha(alpha = .8)) + coord_cartesian(xlim = buffer_range(grid_data_subset$mu), ylim = buffer_range(grid_data_subset$sigma), expand = 0) + guides(color = guide_colorbar(title.position = &quot;top&quot;, barwidth = unit(.2,&quot;npc&quot;), barheight = unit(5, &quot;pt&quot;))) + labs(x = &quot; *\\U03BC*&quot;, y = glue(&quot;*{mth(&#39;\\U03C3&#39;)}*&quot;))+ theme(legend.position = &quot;bottom&quot;, axis.title.x = element_markdown(), axis.title.y = element_markdown()) p_mu_dens &lt;- samples_subset %&gt;% ggplot(aes(x = mu)) + geom_density(color = clr0d, fill = fll0) + scale_x_continuous(limits = buffer_range(grid_data_subset$mu), expand = c(0, 0)) + labs(y = &quot;marginal&lt;br&gt;density&quot;) + theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.title.y = element_markdown()) p_sigma_dens &lt;- samples_subset %&gt;% ggplot(aes(x = sigma)) + geom_density(color = clr0d, fill = fll0) + scale_x_continuous(limits = buffer_range(grid_data_subset$sigma), expand = c(0, 0)) + labs(y = &quot;marginal density&quot;) + coord_flip() + theme(axis.title.y = element_blank(), axis.text.y = element_blank()) p_mu_dens + patchwork::guide_area() + p_samples + p_sigma_dens + plot_layout(guides = &quot;collect&quot;, widths = c(1,.3), heights = c(.3,1)) 5.3.1.4 Quadratic approximation of the posterior distribution \\[ \\begin{array}{cccr} h_i &amp; \\stackrel{iid}{\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\verb|height ~ dnorm(mu, sigma)|\\\\ \\mu &amp; \\sim &amp; Normal(178, 20) &amp; \\verb|mu ~ dnorm(178, 20)|\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; \\verb|sigma ~ dunif(0, 50)| \\end{array} \\] model_spec &lt;- alist( height ~ dnorm(mu, sigma), mu ~ dnorm(178, 20), sigma ~ dunif(0, 50) ) # &quot;maximum a priori estimate&quot; map_starting_points &lt;- list( mu = mean(data_adults$height), sigma = sd(data_adults$height) ) model_heights_quap_weak_prior &lt;- quap(flist = model_spec, data = data_adults, start = map_starting_points) precis(model_heights_quap_weak_prior) %&gt;% as_tibble(rownames = NA) %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% mu 154.61 0.41 153.95 155.27 sigma 7.73 0.29 7.27 8.20 Comparing how a stronger prior for \\(\\mu\\) (narrower distribution) forces a larger estimate of \\(\\sigma\\) to compensate for this. quap( flist = alist( height ~ dnorm( mu , sigma ), mu ~ dnorm( 178, 0.1 ), sigma ~ dunif( 0, 50 ) ), data = data_adults, start = map_starting_points) %&gt;% precis() %&gt;% as_tibble(rownames = NA) %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% mu 177.86 0.10 177.70 178.02 sigma 24.52 0.93 23.03 26.00 The variance-covariance matrix of the quadratic aprroximation for sampling the multi-dimensional gaussian distribution: vcov_mod_heights &lt;- vcov(model_heights_quap_weak_prior) vcov_mod_heights %&gt;% round(digits = 6) %&gt;% knitr::kable() mu sigma mu 0.169740 0.000218 sigma 0.000218 0.084906 diag(vcov_mod_heights) #&gt; mu sigma #&gt; 0.16973961 0.08490582 round(cov2cor(vcov_mod_heights), digits = 5) \\[\\begin{bmatrix} 1 &amp;0.00182 \\\\0.00182 &amp;1 \\\\ \\end{bmatrix}\\] sampling from the multi-dimensional posterior distribution posterior_sample &lt;- extract.samples(model_heights_quap_weak_prior, n = 1e4) %&gt;% as_tibble() precis(posterior_sample) %&gt;% as_tibble() %&gt;% knitr::kable() x 154.607630 7.731074 x 0.4189638 0.2895490 x 153.943389 7.263234 x 155.278781 8.191935 x ▁▁▅▇▂▁▁ ▁▁▂▅▇▇▃▁▁▁ 5.4 Linear Prediction ggplot(data_adults, aes(height, weight)) + geom_point(shape = 21, size = 1.5, color = clr1, fill = fll1) \\[ \\begin{array}{cccr} h_i &amp; {\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta (x_i - \\bar{x}) &amp; \\textrm{[linear model]}\\\\ % alternatively \\overline{x} \\alpha &amp; \\sim &amp; Normal(178, 20) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] The current prior for \\(\\beta\\) is a bad choice, because it allows negative as well as unreasonably high and low dependencies of \\(h\\) (height) on \\(x\\) (weight): set.seed(2971) N &lt;- 100 linear_priors &lt;- tibble(n = 1:N, alpha = rnorm( n = N, mean = 178, sd = 20 ), beta_1 = rnorm( n = N, mean = 0, sd = 10), beta_2 = rlnorm( n = N, mean = 0, sd = 1)) %&gt;% expand(nesting(n, alpha, beta_1, beta_2), weight = range(data_adults$weight)) %&gt;% mutate(height_1 = alpha + beta_1 * (weight - mean(data_adults$weight)), height_2 = alpha + beta_2 * (weight - mean(data_adults$weight))) p_lin_pr &lt;- ggplot(linear_priors, aes(x = weight, y = height_1, group = n)) + labs(title = glue(&quot;{mth(&#39;*\\U03B2* ~&#39;)} Normal(0, 10)&quot;), y = &quot;height&quot;) p_log_pr &lt;- ggplot(linear_priors, aes(x = weight, y = height_2, group = n)) + labs(title = glue(&quot;{mth(&#39;*\\U03B2* ~&#39;)} Log-Normal(0, 1)&quot;), y = &quot;height&quot;) p_lnorm &lt;- ggplot() + stat_function(fun = function(x){dlnorm(x = x, meanlog = 0, sdlog = 1)}, xlim = c(0,5), geom = &quot;area&quot;, color = clr2, fill = fll2, n = 501) + labs(title = &quot;Log-Norm(0, 0.1)&quot;, y = &quot;density&quot;) (p_lin_pr + p_log_pr &amp; geom_hline(data = tibble(height = c(0, 272), type = 1:2), aes(yintercept = height, linetype = factor(type)), size = .4) &amp; geom_line(color = clr2, alpha = .25) &amp; scale_linetype_manual(values = c(`1` = 3, `2` = 1), guide = &quot;none&quot;) &amp; coord_cartesian(xlim = range(data_adults$weight), ylim = c(-100, 400)) &amp; theme(plot.title = element_markdown())) + p_lnorm The log-normal prior seems more sensible, so we update the model priors as such: \\[ \\begin{array}{cccr} \\beta &amp; \\sim &amp; Log-Normal(0, 1) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\end{array} \\] 5.4.1 Finding the posterior Distribution xbar &lt;- mean(data_adults$weight) model_hight &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * ( weight - xbar ), alpha ~ dnorm( 178, 20 ), beta ~ dlnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_adults ) Table of marginal distributions of the parameters after training the model on the data centered_remember_hw &lt;- precis(model_hight) %&gt;% round(digits = 3) %&gt;% as_tibble(rownames = NA) centered_remember_hw %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 154.602 0.270 154.170 155.034 beta 0.903 0.042 0.836 0.970 sigma 5.072 0.191 4.766 5.377 We also need thevariance-covariance matrix to fully describe the audratic approximation completely: model_hight %&gt;% vcov() %&gt;% round(digits = 2) %&gt;% as.data.frame(row.names = row.names(.)) %&gt;% knitr::kable() alpha beta sigma alpha 0.07 0 0.00 beta 0.00 0 0.00 sigma 0.00 0 0.04 model_hight_smp &lt;- extract.samples(model_hight) %&gt;% as_tibble() model_hight_smp_mean &lt;- model_hight_smp %&gt;% summarise(across(.cols = everything(), mean)) model_hight_smp %&gt;% ggpairs( lower = list(continuous = wrap(ggally_points, colour = clr1, size = .2, alpha = .1)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll1, color = clr1, adjust = .5)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) Plotting the posterior distribution against the data ggplot(data_adults, aes(x = weight, y = height)) + geom_point(color = clr0d) + stat_function(fun = function(x){model_hight_smp_mean$alpha + model_hight_smp_mean$beta * (x - xbar)}, color = clr2, n = 2) A demonstration of the the effect of sample size on the uncertainty of the linear fit sub_model &lt;- function(N = 10){ data_inner &lt;- data_adults[1:N,] xbar &lt;- mean(data_inner$weight) model_hight_inner &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * ( weight - xbar ), alpha ~ dnorm( 178, 20 ), beta ~ dlnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_inner ) model_hight_smp_inner &lt;- extract.samples(model_hight_inner) %&gt;% as_tibble() %&gt;% sample_n(20) ggplot(data_inner, aes(x = weight, y = height)) + geom_point(color = clr0d) + (purrr::map(1:20, function(i){stat_function( fun = function(x){model_hight_smp_inner$alpha[i] + model_hight_smp_inner$beta[i] * (x - xbar)}, color = clr2, n = 2, alpha = .1)})) + labs(title = glue(&quot;N: {N}&quot;)) } sub_model(10) + sub_model(50) + sub_model(150) + sub_model(352) adding intervals mu_at_50 &lt;- model_hight_smp %&gt;% mutate(mu_at_50 = alpha + beta * (50 - xbar)) p_density &lt;- mu_at_50 %&gt;% ggplot(aes(x = mu_at_50)) + geom_density(adjust = .5, color = clr0d, fill = fll0) + stat_function(fun = function(x){demp(x, obs = mu_at_50$mu_at_50, density.arg.list = list(adjust = .5))}, xlim = mu_at_50$mu_at_50 %&gt;% PI(), geom = &quot;area&quot;, fill = fll2, color = clr2) + geom_vline(data = tibble(weights = mu_at_50$mu_at_50 %&gt;% PI()), aes(xintercept = weights), linetype = 3)+ scale_x_continuous(glue(&quot;{mth(&#39;*\\U03BC*&#39;)} | weight = 50&quot;), limits = c(157.7, 160.8)) + theme(axis.title.x = element_markdown()) mu_at_50$mu_at_50 %&gt;% PI() #&gt; 5% 94% #&gt; 158.5857 159.6717 weight_seq &lt;- seq(from = 25, to = 70, by = 1) model_hight_mu &lt;- link(model_hight, data = data.frame(weight = weight_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) p_dots &lt;- model_hight_mu %&gt;% ggplot(aes(x = weight, y = height)) + geom_point(aes(color = weight == 50), alpha = .1, size = .3) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) model_hight_mu_interval &lt;- model_hight_mu %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() p_interval &lt;- model_hight_mu_interval %&gt;% ggplot(aes(x = weight)) + geom_point(data = data_adults, aes(y = height), color = clr0, size = .3) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr2, alpha = .35) + geom_line(aes(y = mean)) p_density + p_dots + p_interval Prediction intervals model_hight_sd &lt;- sim(model_hight, data = data.frame(weight = weight_seq), n = 1e4) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq) %&gt;% pivot_longer(cols = `25`:`70`, names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) model_hight_sd %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = weight)) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data_adults, aes(y = height), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = model_hight_mu_interval, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_hight_mu_interval, aes(y = mean)) 5.5 Curves from lines The full data (including kids) is clearly curved in shape: ggplot(data = data, aes(x = weight, y = height)) + geom_point(color = clr0d) We will work on standardized \\(x\\) values to prevent “numerical glitches” by transforming \\(x\\) via \\(x_s = (\\frac{x - \\bar{x}}{sd(x)})\\): quadratic polynomial fit \\[ \\begin{array}{cccr} h_i &amp; {\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_1 x_i + \\beta_2 x_i ^ 2&amp; \\textrm{[linear model]}\\\\ % alternatively \\overline{x} \\alpha &amp; \\sim &amp; Normal(178, 20) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_1 &amp; \\sim &amp; Log-Normal(0, 1) &amp; \\textrm{[$\\beta_1$ prior]}\\\\ \\beta_2 &amp; \\sim &amp; Normal(0, 1) &amp; \\textrm{[$\\beta_2$ prior]}\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] cubic polynomial fit \\[ \\begin{array}{cccr} h_i &amp; {\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_1 x_i + \\beta_2 x_i ^ 2 + \\beta_3 x_i ^ 3 &amp; \\textrm{[linear model]}\\\\ % alternatively \\overline{x} \\alpha &amp; \\sim &amp; Normal(178, 20) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_1 &amp; \\sim &amp; Log-Normal(0, 1) &amp; \\textrm{[$\\beta_1$ prior]}\\\\ \\beta_2 &amp; \\sim &amp; Normal(0, 1) &amp; \\textrm{[$\\beta_2$ prior]}\\\\ \\beta_3 &amp; \\sim &amp; Normal(0, 1) &amp; \\textrm{[$\\beta_3$ prior]}\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] plot_model_intervals &lt;- function(mod, data, weight_seq = list(weight_s = seq(from = min(data_model$weight_s), to = max(data_model$weight_s), length.out = 70))){ model_hight_mu_interval &lt;- link(mod, data = weight_seq) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq$weight_s ) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight_s&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight_s = as.numeric(weight_s)) %&gt;% group_by(weight_s) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() model_hight_sd &lt;- sim(mod, data = weight_seq, n = 1e4) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq$weight_s) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight_s&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight_s = as.numeric(weight_s)) model_hight_sd %&gt;% group_by(weight_s) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = weight_s)) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data, aes(y = height), color = rgb(0,0,0,.25), size = .4) + geom_ribbon(data = model_hight_mu_interval, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_hight_mu_interval, aes(y = mean)) } data_model &lt;- data %&gt;% mutate(weight_s = (weight - mean(weight))/sd(weight), weight_s2 = weight_s ^ 2, weight_s3 = weight_s ^ 3) model_hight_s1 &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * weight_s , alpha ~ dnorm( 178, 20 ), beta ~ dlnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_model ) model_hight_s2 &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta1 * weight_s + beta2 * weight_s2, alpha ~ dnorm( 178, 20 ), beta1 ~ dlnorm( 0, 1 ), beta2 ~ dnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_model ) model_hight_s3 &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta1 * weight_s + beta2 * weight_s2 + beta3 * weight_s3, alpha ~ dnorm( 178, 20 ), beta1 ~ dlnorm( 0, 1 ), beta2 ~ dnorm( 0, 1 ), beta3 ~ dnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_model ) plot_model_intervals(model_hight_s1, data_model) + plot_model_intervals(model_hight_s2, data_model, weight_seq = tibble(weight_s = seq(from = min(data_model$weight_s), to = max(data_model$weight_s), length.out = 70), weight_s2 = weight_s ^ 2)) + plot_model_intervals(model_hight_s3, data_model, weight_seq = tibble(weight_s = seq(from = min(data_model$weight_s), to = max(data_model$weight_s), length.out = 70), weight_s2 = weight_s ^ 2, weight_s3 = weight_s ^ 3)) plot_model_intervals(model_hight_s3, data_model, weight_seq = tibble(weight_s = seq(from = min(data_model$weight_s), to = max(data_model$weight_s), length.out = 70), weight_s2 = weight_s ^ 2, weight_s3 = weight_s ^ 3)) + scale_x_continuous(&quot;weight [kg]&quot;, breaks = (seq(5,65, length.out = 5) - mean(data_model$weight)) / sd(data_model$weight), labels = seq(5,65, length.out = 5)) + labs(y = &quot;height [cm]&quot;) 5.6 Splines Loading the Hanami data (花見), containing the historical dates of first annual cherry tree blossom. data(cherry_blossoms) precis(cherry_blossoms) %&gt;% as_tibble() %&gt;% knitr::kable() x 1408.000000 104.540508 6.141886 7.185151 5.098941 x 350.8845964 6.4070362 0.6636479 0.9929206 0.8503496 x 867.77000 94.43000 5.15000 5.89765 3.78765 x 1948.23000 115.00000 7.29470 8.90235 6.37000 x ▇▇▇▇▇▇▇▇▇▇▇▇▁ ▁▂▅▇▇▃▁▁ ▁▃▅▇▃▂▁▁ ▁▂▅▇▇▅▂▂▁▁▁▁▁▁▁ ▁▁▁▁▁▁▁▃▅▇▃▂▁▁▁ cherry_blossoms %&gt;% ggplot(aes(x = year, y = doy)) + geom_point(color = clr2, alpha = .3) + labs(y = &quot;Day of first blossom&quot;) data_cherry &lt;- cherry_blossoms %&gt;% filter(complete.cases(doy)) %&gt;% as_tibble() n_knots &lt;- 15 knot_list &lt;- quantile(data_cherry$year, probs = seq(0, 1, length.out = n_knots)) library(splines) b_spline_cherry &lt;- bs(data_cherry$year, knots = knot_list[-c(1, n_knots)], degree = 3, intercept = TRUE) b_spline_tib &lt;- b_spline_cherry %&gt;% as_tibble() %&gt;% set_names(nm = str_pad(1:17, width = 2, pad = 0)) %&gt;% bind_cols(select(data_cherry, year)) %&gt;% pivot_longer(cols = -year, names_to = &quot;bias_function&quot;, values_to = &quot;bias&quot;) ggplot() + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = &quot;black&quot;) + geom_line(data = b_spline_tib, aes(x = year, y = bias, color = as.numeric(bias_function) , group = bias_function), size = 1, alpha = .75) + scale_color_gradientn(colours = c(&quot;black&quot;, clr0d, clr2), guide = &quot;none&quot;)+ theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) B-spline model: \\[ \\begin{array}{cccr} D_i &amp; \\sim &amp; Normal( \\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\sum_{k=1}^K w_k B_{k,i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(100, 10) &amp; \\textrm{[$\\alpha$ prior]}\\\\ w_i &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[w prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_cherry &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- a + B %*% w, a ~ dnorm(100, 10), w ~ dnorm(0, 10), sigma ~ dexp(1) ), data = list(D = data_cherry$doy, B = b_spline_cherry), start = list(w = rep(0, ncol(b_spline_cherry))) ) precis(model_cherry, depth = 2) %&gt;% round(digits = 2) %&gt;% as_tibble(rownames = NA) %&gt;% knitr::kable() mean sd 5.5% 94.5% w[1] -3.02 3.86 -9.19 3.15 w[2] -0.83 3.87 -7.01 5.36 w[3] -1.06 3.58 -6.79 4.67 w[4] 4.85 2.88 0.25 9.44 w[5] -0.84 2.87 -5.43 3.76 w[6] 4.32 2.91 -0.33 8.98 w[7] -5.32 2.80 -9.79 -0.84 w[8] 7.85 2.80 3.37 12.33 w[9] -1.00 2.88 -5.61 3.60 w[10] 3.04 2.91 -1.61 7.69 w[11] 4.67 2.89 0.05 9.29 w[12] -0.15 2.87 -4.74 4.43 w[13] 5.56 2.89 0.95 10.18 w[14] 0.72 3.00 -4.08 5.51 w[15] -0.80 3.29 -6.06 4.46 w[16] -6.96 3.38 -12.36 -1.57 w[17] -7.67 3.22 -12.82 -2.52 a 103.35 2.37 99.56 107.13 sigma 5.88 0.14 5.65 6.11 cherry_samples &lt;- extract.samples(model_cherry) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;a&quot;, &quot;sigma&quot;, str_pad(1:17, 2,pad = 0))) cherry_samples_mu &lt;- cherry_samples %&gt;% summarise(across(everything(), mean)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;bias_function&quot;, values_to =&quot;weight&quot;) ggplot() + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = &quot;black&quot;) + geom_line(data = b_spline_tib %&gt;% left_join(cherry_samples_mu), aes(x = year, y = bias * weight, color = as.numeric(bias_function) , group = bias_function), size = 1, alpha = .75) + scale_color_gradientn(colours = c(&quot;black&quot;, clr0d, clr2), guide = &quot;none&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) model_cherry_samples &lt;- link(model_cherry) %&gt;% as_tibble()%&gt;% set_names(nm = data_cherry$year) %&gt;% pivot_longer(cols = everything(), names_to = &quot;year&quot;, values_to = &quot;doy&quot;) %&gt;% mutate(year = as.numeric(year)) %&gt;% arrange(year) model_cherry_stats &lt;- model_cherry_samples %&gt;% group_by(year) %&gt;% nest() %&gt;% mutate(mean = map_dbl(data, function(data){mean(data$doy)}), PI_lower = map_dbl(data, function(data){PI(data$doy)[1]}), PI_upper = map_dbl(data, function(data){PI(data$doy)[2]})) model_cherry_stats %&gt;% ggplot(aes(x = year)) + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = &quot;black&quot;) + geom_point(data = cherry_blossoms, aes(y = doy), color = clr2, alpha = .3) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr2, alpha = .35) + geom_line(aes(y = mean)) + labs(y = &quot;Day of first blossom&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) library(rlang) chapter4_models &lt;- env( data_adults = data_adults, model_heights_quap_weak_prior = model_heights_quap_weak_prior, model_hight = model_hight, data_model = data_model, model_hight_s1 = model_hight_s1, model_hight_s2 = model_hight_s2, model_hight_s3 = model_hight_s3, data_cherry = data_cherry, b_spline_cherry = b_spline_cherry, model_cherry = model_cherry ) write_rds(chapter4_models, &quot;envs/chapter4_models.rds&quot;) 5.7 Homework E1 \\[ \\begin{array}{cccr} y_i &amp; \\sim &amp; Normal( \\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\mu$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] E2 There are two parameters: \\(\\mu\\) \\(\\sigma\\) E3 \\[ \\begin{array}{rcl} Pr( \\mu, \\sigma | y ) &amp; = &amp; \\frac{Normal( y | \\mu, \\sigma ) Pr(y)}{Pr(\\mu, \\sigma)} \\\\ Pr( \\mu, \\sigma | y ) &amp; = &amp; \\frac{\\prod_i Pr( y_i | \\mu, \\sigma) Normal( \\mu | 0, 10) Exponential(\\sigma | 1) }{ \\int\\int\\prod_i Pr( y_i | \\mu, \\sigma) Normal( \\mu | 0, 10) Exponential(\\sigma | 1)d\\mu d\\sigma} \\end{array} \\] E4 \\[ \\begin{array}{cccr} y_i &amp; \\sim &amp; Normal( \\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta x_i &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta &amp; \\sim &amp; Normal(0, 1) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(2) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] E5 There are three parameters \\(\\alpha\\) \\(\\beta\\) \\(\\sigma\\) M1 \\[ \\begin{array}{cccr} y_i &amp; \\sim &amp; Normal( \\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\mu$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] Using grid approximation n &lt;- 5e3 sample_data &lt;- tibble(y = rnorm(n = n, # sample size mean = rnorm(n = n, mean = 0, sd = 10), # mu prior sd = rexp(n = n, rate = 1))) # sigma prior sample_data %&gt;% ggplot(aes(x = y)) + geom_density(color = clr0d, fill = fll0) + stat_function(fun = function(x){dnorm(x = x, mean = 0, sd = 10 )}, geom = &quot;line&quot;, linetype = 3, color = clr1) + scale_x_continuous(limits = c(-50, 50)) + labs(y = &quot;density&quot;) M2 quap_formula &lt;- alist( y ~ dnorm(mu, sigma), # likelihood mu ~ dnorm(0, 10), # mu prior sigma ~ exp(1) # sigma prior ) M3 \\[ \\begin{array}{cccr} y_i &amp; \\sim &amp; Normal( \\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta x_i &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta &amp; \\sim &amp; Uniform(0, 1) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] M4 \\[ \\begin{array}{cccr} h_i &amp; \\sim &amp; Normal( \\mu, \\sigma) &amp; \\textrm{[likelihood]} \\\\ \\mu_i &amp; = &amp; \\alpha + \\beta h_i &amp; \\textrm{[linear model]} \\\\ \\alpha &amp; \\sim &amp; Normal(150, 5) &amp; \\textrm{[$\\alpha$ prior, starting size]} \\\\ \\beta &amp; \\sim &amp; Uniform(0, 10) &amp; \\textrm{[$\\beta$ prior, yearly growth]} \\\\ \\sigma &amp; \\sim &amp; Normal(0, 8) &amp; \\textrm{[$\\sigma$ prior, size variation]} \\end{array} \\] M5 No, the chosen prior for \\(\\beta\\) already covers this information: \\(\\beta \\sim Uniform(0, 10)\\) is always positive, forcing a positive growth per year. M6 Limiting the variance of height to 64cm could be done in different ways: by choosing a uniform prior with fixed boundaries [eg. \\(Uniform(0,64)\\)], or by limiting the variance of an unbound distribution [eg. \\(\\sigma\\) for a normal distribution. 99.7% of the mass is within \\(3 \\sigma\\), so \\(Normal(32, 10)\\) would do as well]. M7 model_uncentered &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * weight, alpha ~ dnorm( 178, 20), beta ~ dlnorm( 0, 1 ), sigma ~ dunif( 0, 50 ) ), data = data_adults ) precis(model_uncentered) %&gt;% round(digits = 3) %&gt;% as_tibble(rownames = NA) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 114.534 1.898 111.501 117.567 beta 0.891 0.042 0.824 0.957 sigma 5.073 0.191 4.767 5.378 model_uncentered %&gt;% vcov() %&gt;% round(digits = 2) %&gt;% as.data.frame(row.names = row.names(.)) %&gt;% knitr::kable() alpha beta sigma alpha 3.60 -0.08 0.01 beta -0.08 0.00 0.00 sigma 0.01 0.00 0.04 compare to the centered version: centered_remember_hw %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 154.602 0.270 154.170 155.034 beta 0.903 0.042 0.836 0.970 sigma 5.072 0.191 4.766 5.377 model_hight %&gt;% vcov() %&gt;% round(digits = 2) %&gt;% as.data.frame(row.names = row.names(.)) %&gt;% knitr::kable() alpha beta sigma alpha 0.07 0 0.00 beta 0.00 0 0.00 sigma 0.00 0 0.04 The un-centered model shows higher covariances between \\(\\alpha\\) and all other parameters. model_uncentered_mu &lt;- link(model_uncentered, data = data.frame(weight = weight_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) model_uncentered_mu_interval &lt;- model_uncentered_mu %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() model_uncentered_sd &lt;- sim(model_uncentered, data = data.frame(weight = weight_seq), n = 5e3) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq) %&gt;% pivot_longer(cols = `25`:`70`, names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) p_1 &lt;- model_uncentered_sd %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = weight)) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data_adults, aes(y = height), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = model_uncentered_mu_interval, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_uncentered_mu_interval, aes(y = mean))+ labs(title = &quot; uncentered&quot;) p_2 &lt;- model_hight_sd %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = weight)) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data_adults, aes(y = height), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = model_hight_mu_interval, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_hight_mu_interval, aes(y = mean))+ labs(title = &quot; centered&quot;) p_1 + p_2 Hmm 🤔`: I can’t see a difference - maybe that is the point? M8 spline_check &lt;- function(n_knots = 15, inner = TRUE){ knot_list &lt;- quantile(data_cherry$year, probs = seq(0, 1, length.out = n_knots)) b_spline_cherry &lt;- bs(data_cherry$year, knots = knot_list[-c(1, n_knots)], degree = 3, intercept = TRUE) b_spline_tib &lt;- b_spline_cherry %&gt;% as_tibble() %&gt;% set_names(nm = str_pad(1:(n_knots+2), width = 2, pad = 0)) %&gt;% bind_cols(select(data_cherry, year)) %&gt;% pivot_longer(cols = -year, names_to = &quot;bias_function&quot;, values_to = &quot;bias&quot;) model_cherry &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- a + B %*% w, a ~ dnorm(100, prior_sd_a), w ~ dnorm(0, prior_sd_w), sigma ~ dexp(1) ), data = list(D = data_cherry$doy, B = b_spline_cherry), start = list(w = rep(0, ncol(b_spline_cherry))) ) cherry_samples &lt;- extract.samples(model_cherry) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;a&quot;, &quot;sigma&quot;, str_pad(1:(n_knots+2), 2,pad = 0))) cherry_samples_mu &lt;- cherry_samples %&gt;% summarise(across(everything(), mean)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;bias_function&quot;, values_to =&quot;weight&quot;) model_cherry_samples &lt;- link(model_cherry) %&gt;% as_tibble()%&gt;% set_names(nm = data_cherry$year) %&gt;% pivot_longer(cols = everything(), names_to = &quot;year&quot;, values_to = &quot;doy&quot;) %&gt;% mutate(year = as.numeric(year)) %&gt;% arrange(year) model_cherry_stats &lt;- model_cherry_samples %&gt;% group_by(year) %&gt;% nest() %&gt;% mutate(mean = map_dbl(data, function(data){mean(data$doy)}), PI_lower = map_dbl(data, function(data){PI(data$doy)[1]}), PI_upper = map_dbl(data, function(data){PI(data$doy)[2]})) p_splines_pure &lt;- ggplot() + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = rgb(0,0,0,.5)) + geom_line(data = b_spline_tib %&gt;% left_join(cherry_samples_mu), aes(x = year, y = bias * weight, color = as.numeric(bias_function) , group = bias_function), size = .3, alpha = .75) + scale_color_gradientn(colours = c(&quot;black&quot;, clr0d, clr1), guide = &quot;none&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.title.x = element_blank(), axis.text.x = element_blank()) + labs(title = glue(&quot;{n_knots} kn, sd a: {prior_sd_a}, sd w: {prior_sd_w}&quot;)) p_splines_fitted &lt;- model_cherry_stats %&gt;% ggplot(aes(x = year)) + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = rgb(0,0,0,.5)) + geom_point(data = cherry_blossoms, aes(y = doy), color = clr1, alpha = .1, size = .2) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .65) + geom_line(aes(y = mean)) + labs(y = &quot;Day of first blossom&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.title.x = element_blank(), axis.text.x = element_blank()) if(inner){ p_splines_pure &lt;- p_splines_pure + theme(axis.title.y = element_blank()) p_splines_fitted &lt;- p_splines_fitted + theme(axis.title.y = element_blank()) } p_splines_pure + p_splines_fitted + plot_layout(ncol = 1, heights = c(.5, 1)) } set.seed(14) prior_sd_a = 10 prior_sd_w &lt;- 10 p1 &lt;- spline_check(n_knots = 3, inner = FALSE) prior_sd_w &lt;- 50 p2 &lt;- spline_check(n_knots = 3, inner = TRUE) set.seed(42) prior_sd_w &lt;- 100 p3 &lt;- spline_check(n_knots = 3, inner = TRUE) p1 | p2 | p3 set.seed(41) prior_sd_w &lt;- 10 p1 &lt;- spline_check(n_knots = 10, inner = FALSE) prior_sd_w &lt;- 50 p2 &lt;- spline_check(n_knots = 10, inner = TRUE) prior_sd_w &lt;- 100 p3 &lt;- spline_check(n_knots = 10, inner = TRUE) p1 | p2 | p3 set.seed(42) prior_sd_w &lt;- 10 p1 &lt;- spline_check(n_knots = 30, inner = FALSE) prior_sd_w &lt;- 50 p2 &lt;- spline_check(n_knots = 30, inner = TRUE) prior_sd_w &lt;- 100 p3 &lt;- spline_check(n_knots = 30, inner = TRUE) p1 | p2 | p3 They control the division of data and the initial scale for the weighting H1 model_hight_smp %&gt;% mutate(mu_at_50 = alpha + beta * (50 - xbar), `46.95` = alpha + beta * (46.95 - xbar), `43.72` = alpha + beta * (43.72 - xbar), `64.78` = alpha + beta * (64.78 - xbar), `32.59` = alpha + beta * (32.59 - xbar), `54.63` = alpha + beta * (54.63 - xbar)) %&gt;% dplyr::select(`46.95`:`54.63` ) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% group_by(weight) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(median_weight = map_dbl(data, function(x){median(x$height)}), mean_weight = map_dbl(data, function(x){mean(x$height)}), lower_89 = map_dbl(data, function(x){PI(x$height)[[1]]}), upper_89 = map_dbl(data, function(x){PI(x$height)[[2]]}), individual = 1:5) %&gt;% dplyr::select(individual, weight,median_weight:upper_89) %&gt;% mutate(across(everything(), .fns = ~ round(as.numeric(.x), digits = 5))) %&gt;% knitr::kable() individual weight median_weight mean_weight lower_89 upper_89 1 46.95 156.3742 156.3739 155.9302 156.8173 2 43.72 153.4580 153.4585 153.0147 153.8970 3 64.78 172.4641 172.4671 171.0703 173.8520 4 32.59 143.4179 143.4127 142.4567 144.3474 5 54.63 163.3039 163.3058 162.5306 164.0879 H2 data_children &lt;- data %&gt;% filter(age &lt; 18) ggplot(data_children, aes(weight, height)) + geom_point(shape = 21, size = 1.5, color = clr1, fill = fll1) xbar_children &lt;- mean(data_children$weight) model_hight_children &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * ( weight - xbar_children ), alpha ~ dnorm( 178, 20 ), beta ~ dlnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_children ) precis(model_hight_children) %&gt;% round(digits = 3) %&gt;% as_tibble(rownames = NA) #&gt; # A tibble: 3 × 4 #&gt; mean sd `5.5%` `94.5%` #&gt; * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 108. 0.609 107. 109. #&gt; 2 2.72 0.068 2.61 2.83 #&gt; 3 8.44 0.431 7.75 9.13 model_hight_children %&gt;% vcov() %&gt;% cov2cor() %&gt;% round(digits = 2) %&gt;% as.data.frame(row.names = row.names(.)) %&gt;% knitr::kable() alpha beta sigma alpha 1.00 0.00 0.01 beta 0.00 1.00 -0.01 sigma 0.01 -0.01 1.00 model_hight_smp_children &lt;- extract.samples(model_hight_children) %&gt;% as_tibble() model_hight_smp_mean_children &lt;- model_hight_smp_children %&gt;% summarise(across(.cols = everything(), mean)) ggplot(data_children, aes(x = weight, y = height)) + geom_point(color = clr0d) + stat_function(fun = function(x){model_hight_smp_mean_children$alpha + model_hight_smp_mean_children$beta * (x - xbar_children)}, color = clr2, n = 2) model_hight_smp_mean_children %&gt;% knitr::kable() alpha beta sigma 108.3763 2.716742 8.44416 A child get 27.1674225128912 cm taller per 10 kg weight. weight_seq_children &lt;- seq(from = 2, to = 45, by = 1) model_hight_mu_children &lt;- link(model_hight_children, data = data.frame(weight = weight_seq_children)) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq_children) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) model_hight_mu_interval_children &lt;- model_hight_mu_children %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() model_hight_sd_children &lt;- sim(model_hight_children, data = data.frame(weight = weight_seq_children), n = 1e4) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq_children) %&gt;% pivot_longer(cols = `2`:`45`, names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) model_hight_sd_children %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = weight)) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data_children, aes(y = height), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = model_hight_mu_interval_children, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_hight_mu_interval_children, aes(y = mean)) The model seems to be systematically overestimate the height for the more extreme weights (very light and rather heavy). The relationship does not appear to be linear in the first place, so a non-lnear fit would be better - ideally one that is biologically motivated. H3 data_log &lt;- data %&gt;% mutate(weight_log = log10(weight)) xbar_log &lt;- mean(data_log$weight_log) model_hight_log &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * ( weight_log - xbar_log ), alpha ~ dnorm( 179, 20 ), beta ~ dlnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_log ) precis(model_hight_log) %&gt;% round(digits = 3) %&gt;% as_tibble(rownames = NA) #&gt; # A tibble: 3 × 4 #&gt; mean sd `5.5%` `94.5%` #&gt; * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 138. 0.22 138. 139. #&gt; 2 108. 0.881 107. 110. #&gt; 3 5.14 0.156 4.89 5.38 model_hight_log %&gt;% vcov() %&gt;% cov2cor() %&gt;% round(digits = 2) %&gt;% as.data.frame(row.names = row.names(.)) %&gt;% knitr::kable() alpha beta sigma alpha 1 0 0 beta 0 1 0 sigma 0 0 1 model_hight_smp_log &lt;- extract.samples(model_hight_log) %&gt;% as_tibble() model_hight_smp_mean_log &lt;- model_hight_smp_log %&gt;% summarise(across(.cols = everything(), mean)) ggplot(data_log, aes(x = weight_log, y = height)) + geom_point(color = clr0d) + stat_function(fun = function(x){model_hight_smp_mean_log$alpha + model_hight_smp_mean_log$beta * (x - xbar_log)}, color = clr2, n = 2) weight_seq_log &lt;- log10(seq(from = 2, to = 70, by = 1)) model_hight_mu_log &lt;- link(model_hight_log, data = data.frame(weight_log = weight_seq_log)) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq_log) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight_log&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight_log = as.numeric(weight_log)) model_hight_mu_interval_log &lt;- model_hight_mu_log %&gt;% group_by(weight_log) %&gt;% summarise(mean = mean(height), PI_lower = PI(height, prob = .97)[1], PI_upper = PI(height, prob = .97)[2]) %&gt;% ungroup() model_hight_sd_log &lt;- sim(model_hight_log, data = data.frame(weight_log = weight_seq_log), n = 1e4) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq_log) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight_log&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight_log = as.numeric(weight_log)) model_hight_sd_log %&gt;% group_by(weight_log) %&gt;% summarise(mean = mean(height), PI_lower = PI(height, prob = .97)[1], PI_upper = PI(height, prob = .97)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = 10^(weight_log))) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data, aes(x = weight, y = height), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = model_hight_mu_interval_log, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_hight_mu_interval_log, aes(y = mean), linetype = 3) H4 N &lt;- 25 cubic_priors &lt;- tibble(n = 1:N, alpha = rnorm( n = N, mean = -128, sd = 20 ), beta_1 = rnorm( n = N, mean = 11, sd = .1), beta_2 = rnorm( n = N, mean = -.1, sd = .01)) %&gt;% expand(nesting(n, alpha, beta_1, beta_2), weight = range(data_adults$weight)) %&gt;% mutate(height = alpha + beta_1 * (weight - mean(data_adults$weight)) + beta_2 * (weight - mean(data_adults$weight))^2) ggplot(cubic_priors, aes(x = weight, y = height, group = n)) + pmap(cubic_priors, function(alpha, beta_1, beta_2, ...){ stat_function(fun = function(x){alpha + beta_1 * x + beta_2 * x^2}, color = fll1, alpha = .1, n = 100, lwd = .2, geom = &quot;line&quot;) }) + geom_hline(data = tibble(height = c(0, 272), type = 1:2), aes(yintercept = height, linetype = factor(type)), size = .4) + scale_linetype_manual(values = c(`1` = 3, `2` = 1), guide = &quot;none&quot;) + coord_cartesian(xlim = range(data_adults$weight), ylim = c(-100, 400)) + theme(plot.title = element_markdown()) H5 cherry_blossoms_tib &lt;- cherry_blossoms %&gt;% as_tibble() %&gt;% filter(!is.na(temp) &amp; !is.na(doy)) %&gt;% mutate(temp_s = (temp - mean(temp, na.rm = TRUE))/sd(temp, na.rm = TRUE)) temp_bar &lt;- mean(cherry_blossoms_tib$temp, na.rm = TRUE) temp_sd &lt;- sd(cherry_blossoms_tib$temp, na.rm = TRUE) cherry_blossoms_tib %&gt;% as_tibble() %&gt;% ggplot(aes(x = temp, y = doy)) + geom_point(size = 1.2, color = fll2) \\[ \\begin{array}{cccr} d_i &amp; \\sim &amp; Normal( \\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta x_i &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(105, 10) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_temp &lt;- quap( flist = alist( doy ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * temp_s, alpha ~ dnorm( 105, 10 ), beta ~ dnorm( 0, 10 ), sigma ~ dexp( 1 ) ), data = cherry_blossoms_tib ) temp_seq &lt;- (seq(from = 4.5, to = 8.4, by = .1) - temp_bar) / temp_sd model_temp_mu &lt;- link(model_temp, data = data.frame(temp_s = temp_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = temp_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;temp_s&quot;, values_to = &quot;doy&quot;) %&gt;% mutate(temp_s = as.numeric(temp_s), temp = temp_s * temp_sd + temp_bar) model_temp_mu_interval &lt;- model_temp_mu %&gt;% group_by(temp) %&gt;% summarise(mean = mean(doy), PI_lower = PI(doy)[1], PI_upper = PI(doy)[2]) %&gt;% ungroup() model_temp_sd &lt;- sim(model_temp, data = data.frame(temp_s = temp_seq), n = 1e4) %&gt;% as_tibble() %&gt;% set_names(nm = temp_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;temp_s&quot;, values_to = &quot;doy&quot;) %&gt;% mutate(temp_s = as.numeric(temp_s), temp = temp_s * temp_sd + temp_bar) model_temp_sd %&gt;% group_by(temp) %&gt;% summarise(mean = mean(doy), PI_lower = PI(doy)[1], PI_upper = PI(doy)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = temp)) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = cherry_blossoms_tib, aes(y = doy), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = model_temp_mu_interval, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_temp_mu_interval, aes(y = mean)) H6 n_knots &lt;- 15 knot_list &lt;- quantile(data_cherry$year, probs = seq(0, 1, length.out = n_knots)) b_spline_cherry &lt;- bs(data_cherry$year, knots = knot_list[-c(1, n_knots)], degree = 3, intercept = TRUE) prior_predictive &lt;- function(n = 100, prior_w = function(knots){rnorm(n = knots + 2, 0, 10)}){ tibble(.draw = 1:n, alpha = rnorm(n, 100, 10), w = purrr::map(seq_len(n), # random weighting of knots function(x, knots){ w &lt;- prior_w(knots) w }, knots = n_knots)) %&gt;% mutate(mu = purrr::map2(alpha, w, .f = function(alpha, w, b){ mu &lt;- alpha + b %*% w mu %&gt;% as_tibble(.name_repair = ~&quot;mu&quot;) %&gt;% mutate(year = data_cherry$year, .before = 1) }, b = b_spline_cherry)) %&gt;% unnest(cols = mu) } p1 &lt;- prior_predictive(n = 50) %&gt;% ggplot(aes(x = year, y = mu)) + labs(title = &quot;weight sd: 10&quot;) p2 &lt;- prior_predictive(n = 50, prior_w = function(knots){rnorm(n = knots + 2, mean = 0, sd = .3)}) %&gt;% ggplot(aes(x = year, y = mu)) + labs(title = &quot;weight sd: 0.3&quot;) p1 + p2 &amp; geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = &quot;black&quot;) &amp; geom_line(aes(group = .draw, color = .draw), alpha = .2) &amp; scale_color_gradientn(colours = c(&quot;black&quot;, clr0d, clr2), guide = &quot;none&quot;) &amp; theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) H7 This one is missing… H8 set.seed(42) model_cherry2 &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- B %*% w, w ~ dnorm(0, 10), sigma ~ dexp(1) ), data = list(D = data_cherry$doy, B = b_spline_cherry), start = list(w = rep(0, ncol(b_spline_cherry))), control = list(maxit = 5000) ) precis(model_cherry2, depth = 2) %&gt;% round(digits = 2) %&gt;% as_tibble(rownames = NA) %&gt;% knitr::kable() mean sd 5.5% 94.5% w[1] 92.84 3.22 87.69 97.98 w[2] 102.76 3.09 97.83 107.69 w[3] 100.43 2.75 96.03 104.83 w[4] 108.34 1.64 105.71 110.96 w[5] 101.56 1.68 98.88 104.24 w[6] 106.97 1.74 104.19 109.75 w[7] 97.56 1.53 95.12 99.99 w[8] 110.64 1.53 108.19 113.08 w[9] 101.68 1.68 99.00 104.36 w[10] 105.75 1.73 102.99 108.52 w[11] 107.37 1.70 104.66 110.08 w[12] 102.67 1.65 100.02 105.31 w[13] 108.15 1.69 105.44 110.86 w[14] 103.77 1.87 100.78 106.75 w[15] 101.31 2.34 97.57 105.05 w[16] 95.98 2.44 92.08 99.87 w[17] 92.12 2.30 88.45 95.80 sigma 5.95 0.15 5.71 6.18 cherry_samples &lt;- extract.samples(model_cherry2) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;sigma&quot;, str_pad(1:17, 2,pad = 0))) cherry_samples_mu &lt;- cherry_samples %&gt;% summarise(across(everything(), mean)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;bias_function&quot;, values_to =&quot;weight&quot;) model_cherry_samples &lt;- link(model_cherry2) %&gt;% as_tibble()%&gt;% set_names(nm = data_cherry$year) %&gt;% pivot_longer(cols = everything(), names_to = &quot;year&quot;, values_to = &quot;doy&quot;) %&gt;% mutate(year = as.numeric(year)) %&gt;% arrange(year) model_cherry_stats &lt;- model_cherry_samples %&gt;% group_by(year) %&gt;% nest() %&gt;% mutate(mean = map_dbl(data, function(data){mean(data$doy)}), PI_lower = map_dbl(data, function(data){PI(data$doy)[1]}), PI_upper = map_dbl(data, function(data){PI(data$doy)[2]})) model_cherry_stats %&gt;% ggplot(aes(x = year)) + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = &quot;black&quot;) + geom_point(data = cherry_blossoms, aes(y = doy), color = clr2, alpha = .3) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr2, alpha = .35) + geom_line(aes(y = mean)) + labs(y = &quot;Day of first blossom&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) 5.8 {brms} section 5.8.1 linear model of adult height finding the posterior with {brms} brms_c4_adult_heights &lt;- brm(data = data_adults, family = gaussian, height ~ 1, prior = c(prior(normal(178, 20), class = Intercept), prior(uniform(0, 50), class = sigma)), iter = 31000, warmup = 30000, chains = 7, cores = 7, seed = 4, file = &quot;brms/brms_c4_adult_heights&quot;) posterior_summary(brms_c4_adult_heights,probs = c(.055, .945)) %&gt;% round(digits = 3) %&gt;% knitr::kable() Estimate Est.Error Q5.5 Q94.5 b_Intercept 154.604 0.418 153.947 155.275 sigma 7.742 0.304 7.276 8.250 lp__ -1226.924 1.058 -1228.772 -1225.927 brms_summary_plot &lt;- function(mod, n_chains = 4){ bayes_data &lt;- bayesplot::mcmc_areas_data(mod, prob = .95) %&gt;% filter(parameter != &quot;lp__&quot;) bayes_chains_data &lt;- bayesplot::mcmc_trace_data(mod) %&gt;% filter(parameter != &quot;lp__&quot;) p_dens &lt;- bayes_data %&gt;% filter(interval == &quot;outer&quot;) %&gt;% ggplot(aes(x = x, y = scaled_density)) + geom_area(color = clr0d, fill = fll0) + geom_area(data = bayes_data %&gt;% filter(interval == &quot;inner&quot;), color = clr2, fill = fll2) + facet_wrap(parameter ~ . , scales = &quot;free&quot;, ncol = 1) p_chains &lt;- bayes_chains_data %&gt;% ggplot(aes(x = iteration, y = value, group = chain)) + geom_line(aes(color = chain), alpha = .6) + facet_wrap(parameter ~ . , scales = &quot;free&quot;, ncol = 1) + scale_color_manual(values = scales::colour_ramp(colors = c(&quot;black&quot;,clr0d,clr2))(seq(0,1,length.out = n_chains))) p_dens + p_chains } brms_summary_plot(brms_c4_adult_heights) sampling from the posterior # equivalent to `rethinking::extract.samples()` brms_post &lt;- as_draws_df(brms_c4_adult_heights) %&gt;% as_tibble() head(brms_post) #&gt; # A tibble: 6 × 6 #&gt; b_Intercept sigma lp__ .chain .iteration .draw #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 154. 8.07 -1227. 1 1 1 #&gt; 2 154. 7.84 -1227. 1 2 2 #&gt; 3 155. 8.21 -1228. 1 3 3 #&gt; 4 154. 7.99 -1227. 1 4 4 #&gt; 5 155. 7.77 -1226. 1 5 5 #&gt; 6 154. 7.93 -1227. 1 6 6 select(brms_post, b_Intercept:sigma) %&gt;% cov() %&gt;% cov2cor() \\[\\begin{bmatrix} 1 &amp;-0.0192241582730429 \\\\-0.0192241582730429 &amp;1 \\\\ \\end{bmatrix}\\] brms_post %&gt;% dplyr::select(-(lp__:.draw)) %&gt;% pivot_longer(cols = everything()) %&gt;% group_by(name) %&gt;% summarise(quantiles = list(tibble(quant = quantile(value, probs = c(.5, .025, .75)), perc = str_c(&quot;q&quot;,names(quant)))) )%&gt;% unnest(quantiles) %&gt;% pivot_wider(names_from = perc, values_from = quant) %&gt;% mutate(across(.cols = -name, ~ round(.x, digits = 2))) %&gt;% knitr::kable() name q50% q2.5% q75% b_Intercept 154.59 153.80 154.89 sigma 7.73 7.18 7.94 posterior_summary(brms_post) \\[\\begin{bmatrix} 154.603681717344 &amp;0.417698657902289 &amp;153.797143321477 &amp;155.411872045528 &amp;7.7417412324536 &amp;0.304409842943225 \\\\7.17548730321122 &amp;8.38470302045445 &amp;-1226.92435306886 &amp;1.05809371424477 &amp;-1229.68670608237 &amp;-1225.89312495532 \\\\2.5 &amp;1.11817376920787 &amp;1 &amp;4 &amp;500.5 &amp;288.711081398222 \\\\25.975 &amp;975.025 &amp;2000.5 &amp;1154.84486692658 &amp;100.975 &amp;3900.025 \\\\ \\end{bmatrix}\\] the height model with a predictor data_adults &lt;- data_adults %&gt;% mutate(weight_centered = weight - mean(weight)) brms_c4_heights_x &lt;- brm(data = data_adults, family = gaussian, height ~ 1 + weight_centered, prior = c(prior(normal(178, 20), class = Intercept), prior(lognormal(0, 1), class = b), prior(uniform(0, 50), class = sigma)), iter = 28000, warmup = 27000, chains = 7, cores = 7, seed = 42, file = &quot;brms/brms_c4_heights_x&quot;) brms_summary_plot(brms_c4_heights_x, n_chains = 12) Logs and exps (m4.3b) brms_c4_heights_x_log &lt;- brm(data = data_adults, family = gaussian, bf(height ~ alpha + exp(logbeta) * weight_centered, alpha ~ 1, logbeta ~ 1, nl = TRUE), prior = c(prior(normal(178, 20), class = b, nlpar = alpha), prior(normal(0, 1), class = b, nlpar = logbeta), prior(uniform(0, 50), class = sigma)), iter = 31000, warmup = 30000, chains = 7, cores = 7, seed = 42, file = &quot;brms/brms_c4_heights_x_log&quot;) posterior_summary(brms_c4_heights_x)[1:3, ] %&gt;% round(digits = 2)%&gt;% as.data.frame() #&gt; Estimate Est.Error Q2.5 Q97.5 #&gt; b_Intercept 154.60 0.27 154.08 155.12 #&gt; b_weight_centered 0.90 0.04 0.82 0.99 #&gt; sigma 5.11 0.20 4.74 5.52 vcov(brms_c4_heights_x) %&gt;% cov2cor() %&gt;% round(3) %&gt;% as.data.frame() #&gt; Intercept weight_centered #&gt; Intercept 1.000 -0.003 #&gt; weight_centered -0.003 1.000 brms_posterior_samples &lt;- as_draws_df(brms_c4_heights_x) %&gt;% as_tibble() %&gt;% select(-(lp__:.draw)) brms_posterior_samples %&gt;% cov() %&gt;% cov2cor() %&gt;% round(digits = 3)%&gt;% as.data.frame() #&gt; b_Intercept b_weight_centered sigma #&gt; b_Intercept 1.000 -0.003 0.006 #&gt; b_weight_centered -0.003 1.000 0.000 #&gt; sigma 0.006 0.000 1.000 ggpairs(brms_posterior_samples, lower = list(continuous = wrap(ggally_points, colour = clr1, size = .3, alpha = .1)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll1, color = clr1, adjust = .5)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) weight_seq &lt;- tibble(weight = 25:70) %&gt;% mutate(weight_centered = weight - mean(data_adults$weight)) brms_model_hight_mu &lt;- fitted(brms_c4_heights_x, summary = FALSE, newdata = weight_seq) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq$weight_centered) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight_centered&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight_centered = as.numeric(weight_centered)) brms_model_hight_mu_interval &lt;- brms_model_hight_mu %&gt;% group_by(weight_centered) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() brms_model_hight_samples &lt;- predict(brms_c4_heights_x, summary = FALSE, newdata = weight_seq) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq$weight_centered) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight_centered&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight_centered = as.numeric(weight_centered)) brms_model_hight_samples_interval &lt;- brms_model_hight_samples %&gt;% group_by(weight_centered) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() data_adults %&gt;% ggplot(aes(x = weight_centered, y = height)) + geom_point(shape = 21, size = 2, color = clr1, fill = fll1) + geom_abline(intercept = fixef(brms_c4_heights_x)[1], slope = fixef(brms_c4_heights_x)[2]) brms_model_hight_mu_interval %&gt;% ggplot(aes(x = weight_centered)) + geom_ribbon(data = brms_model_hight_samples_interval, aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data_adults, aes(y = height), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(aes(y = mean)) brms_c4_curve_x &lt;- brm(data = data_model, family = gaussian, height ~ 1 + weight_s + weight_s2, prior = c(prior(normal(178, 20), class = Intercept), prior(lognormal(0, 1), class = b, coef = &quot;weight_s&quot;), prior(normal(0, 1), class = b, coef = &quot;weight_s2&quot;), prior(uniform(0, 50), class = sigma)), iter = 30000, warmup = 29000, chains = 4, cores = 4, seed = 4, file = &quot;brms/brms_c4_curve_x&quot;) brms_summary_plot(brms_c4_curve_x, n_chains = 4) weight_seq &lt;- tibble(weight_s = seq(from = -2.5, to = 2.5, length.out = 30)) %&gt;% mutate(weight_s2 = weight_s^2) fitd_quad &lt;- fitted(brms_c4_curve_x, newdata = weight_seq, probs = c(.055, .955)) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) %&gt;% as_tibble() pred_quad &lt;- predict(brms_c4_curve_x, newdata = weight_seq, probs = c(.055, .955)) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) %&gt;% as_tibble() ggplot(data = data_model, aes(x = weight_s)) + geom_ribbon(data = pred_quad, aes(ymin = Q5.5, ymax = Q95.5), fill = clr0d, alpha = .35) + geom_smooth(data = fitd_quad, aes(y = Estimate, ymin = Q5.5, ymax = Q95.5), stat = &quot;identity&quot;, color = clr1, fill = fll1, alpha = .35, size = .2) + geom_point(aes(y = height), color = rgb(0,0,0,.5), size = .6) num_knots &lt;- 15 knot_list &lt;- quantile(data_cherry$year, probs = seq(from = 0, to = 1, length.out = num_knots)) B &lt;- bs(data_cherry$year, knots = knot_list[-c(1, num_knots)], degree = 3, intercept = TRUE) data_cherry_B &lt;- data_cherry %&gt;% mutate(B = B) data_cherry_B %&gt;% glimpse() #&gt; Rows: 827 #&gt; Columns: 6 #&gt; $ year &lt;int&gt; 812, 815, 831, 851, 853, 864, 866, 869, 889, 891, 892, 894,… #&gt; $ doy &lt;int&gt; 92, 105, 96, 108, 104, 100, 106, 95, 104, 109, 108, 106, 10… #&gt; $ temp &lt;dbl&gt; NA, NA, NA, 7.38, NA, 6.42, 6.44, NA, 6.83, 6.98, 7.11, 6.9… #&gt; $ temp_upper &lt;dbl&gt; NA, NA, NA, 12.10, NA, 8.69, 8.11, NA, 8.48, 8.96, 9.11, 8.… #&gt; $ temp_lower &lt;dbl&gt; NA, NA, NA, 2.66, NA, 4.14, 4.77, NA, 5.19, 5.00, 5.11, 5.5… #&gt; $ B &lt;bs[,17]&gt; &lt;bs[26 x 17]&gt; brms_c4_cherry_spline &lt;- brm(data = data_cherry_B, family = gaussian, doy ~ 1 + B, prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;brms/brms_c4_cherry_spline&quot;) brms_posterior_samples &lt;- as_draws_df(brms_c4_cherry_spline) years_seq &lt;- tibble(year = seq(from = min(data_cherry$year), to = max(data_cherry$year), by = 10)) fitd_quad &lt;- fitted(brms_c4_cherry_spline, probs = c(.055, .955)) %&gt;% data.frame() %&gt;% bind_cols(data_cherry_B) %&gt;% as_tibble() pred_quad &lt;- predict(brms_c4_cherry_spline, probs = c(.055, .955)) %&gt;% data.frame() %&gt;% bind_cols(data_cherry_B) %&gt;% as_tibble() ggplot(data = data_cherry, aes(x = year)) + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = rgb(0,0,0,.4)) + geom_point(aes(y = doy), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = pred_quad, aes(ymin = Q5.5, ymax = Q95.5), fill = clr0d, alpha = .35) + geom_smooth(data = fitd_quad, aes(y = Estimate, ymin = Q5.5, ymax = Q95.5), stat = &quot;identity&quot;, color = clr1, fill = fll1, alpha = .35, size = .2) + geom_hline(yintercept = fixef(brms_c4_cherry_spline)[1, 1], color = clr1, linetype = 2) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) get_prior(data = data_cherry, family = gaussian, doy ~ 1 + s(year)) #&gt; prior class coef group resp dpar nlpar bound #&gt; (flat) b #&gt; (flat) b syear_1 #&gt; student_t(3, 105, 5.9) Intercept #&gt; student_t(3, 0, 5.9) sds #&gt; student_t(3, 0, 5.9) sds s(year) #&gt; student_t(3, 0, 5.9) sigma #&gt; source #&gt; default #&gt; (vectorized) #&gt; default #&gt; default #&gt; (vectorized) #&gt; default Using a thin plate spline brms_c4_cherry_smooth &lt;- brm(data = data_cherry, family = gaussian, doy ~ 1 + s(year), prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = b), prior(student_t(3, 0, 5.9), class = sds), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, control = list(adapt_delta = .99), file = &quot;brms/brms_c4_cherry_smooth&quot;) fitted(brms_c4_cherry_smooth, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(select(data_cherry, year, doy)) %&gt;% as_tibble() %&gt;% ggplot(aes(x = year, y = doy, ymin = Q5.5, ymax = Q94.5)) + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = rgb(0,0,0,.4)) + geom_point(aes(y = doy), color = rgb(0,0,0,.5), size = .6) + geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5), stat = &quot;identity&quot;, color = clr1, fill = fll1, alpha = .35, size = .2) + geom_hline(yintercept = fixef(brms_c4_cherry_smooth)[1, 1], color = clr1, linetype = 2) + labs(subtitle = &quot;brms smooth using s(year) (thin plate)&quot;, y = &quot;day in year&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) brms_c4_cherry_smooth2 &lt;- brm(data = data_cherry, family = gaussian, doy ~ 1 + s(year, bs = &quot;bs&quot;, k = 19), prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = b), prior(student_t(3, 0, 5.9), class = sds), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, control = list(adapt_delta = .99), file = &quot;brms/brms_c4_cherry_smooth2&quot;) fitted(brms_c4_cherry_smooth2, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(select(data_cherry, year, doy)) %&gt;% as_tibble() %&gt;% ggplot(aes(x = year, y = doy, ymin = Q5.5, ymax = Q94.5)) + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = rgb(0,0,0,.4)) + geom_point(aes(y = doy), color = rgb(0,0,0,.5), size = .6) + geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5), stat = &quot;identity&quot;, color = clr1, fill = fll1, alpha = .35, size = .2) + geom_hline(yintercept = fixef(brms_c4_cherry_smooth2)[1, 1], color = clr1, linetype = 2) + labs(subtitle = &quot;brms smooth using s(year) (B-spline)&quot;, y = &quot;day in year&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) Add-on about matrix-columns (4.7 Second bonus) n &lt;- 100 # how many continuous x predictor variables would you like? k &lt;- 10 # simulate a dichotomous dummy variable for z # simulate an n by k array for X set.seed(4) data_matrix_columns &lt;- tibble(z = sample(0:1, size = n, replace = T), X = array(runif(n * k, min = 0, max = 1), dim = c(n, k))) # set the data-generating parameter values a &lt;- 1 theta &lt;- 5 b &lt;- 1:k sigma &lt;- 2 # simulate the criterion data_matrix_columns &lt;- data_matrix_columns %&gt;% mutate(y = as.vector(a + X %*% b + theta * z + rnorm(n, mean = 0, sd = sigma))) # data_matrix_columns %&gt;% glimpse() # data_matrix_columns$X[1,] brms_c4_matrix_column &lt;- brm(data = data_matrix_columns, family = gaussian, y ~ 1 + z + X, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;brms/brms_c4_matrix_column&quot;) summary(brms_c4_matrix_column) #&gt; Family: gaussian #&gt; Links: mu = identity; sigma = identity #&gt; Formula: y ~ 1 + z + X #&gt; Data: data_matrix_columns (Number of observations: 100) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; Intercept 0.95 1.19 -1.47 3.34 1.00 6928 3291 #&gt; z 4.74 0.42 3.91 5.56 1.00 7389 3032 #&gt; X1 0.57 0.75 -0.86 2.04 1.00 6798 3312 #&gt; X2 0.90 0.69 -0.47 2.26 1.00 6421 2949 #&gt; X3 3.41 0.75 1.96 4.90 1.00 7375 3214 #&gt; X4 2.81 0.73 1.36 4.25 1.00 6712 3469 #&gt; X5 5.74 0.72 4.32 7.12 1.00 6621 3588 #&gt; X6 6.40 0.73 4.97 7.82 1.00 6552 3228 #&gt; X7 8.49 0.73 7.06 9.90 1.00 7098 3335 #&gt; X8 8.40 0.69 7.04 9.76 1.00 8718 3363 #&gt; X9 8.82 0.81 7.27 10.39 1.00 8378 3273 #&gt; X10 9.32 0.73 7.88 10.78 1.00 8260 2894 #&gt; #&gt; Family Specific Parameters: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; sigma 1.99 0.16 1.71 2.32 1.00 5722 3433 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). 5.9 pymc3 section × "],["rethinking-chapter-5.html", "6 Rethinking: Chapter 5 6.1 Directed Acyclic Graphs 6.2 Multiple Regression notion 6.3 Masked relationship 6.4 Categorical Variables 6.5 Homework 6.6 {brms} section 6.7 pymc3 section", " 6 Rethinking: Chapter 5 Spurious waffles by Richard McElreath, building on the Summaries by Solomon Kurz and Jake Thompson. library(sf) library(rethinking) library(ggfx) data(WaffleDivorce) WaffleDivorce &lt;- WaffleDivorce %&gt;% as_tibble() usa &lt;- read_sf(&quot;~/work/geo_store/USA/usa_states_albers_revised.gpkg&quot;) %&gt;% left_join(WaffleDivorce, by = c(name = &quot;Location&quot; )) p_waffle &lt;- usa %&gt;% ggplot(aes(fill = WaffleHouses / Population)) + scale_fill_gradientn(colours = c(clr0d, clr2) %&gt;% clr_lighten(.3)) p_divorce &lt;- usa %&gt;% ggplot(aes(fill = Divorce))+ scale_fill_gradientn(colours = c(clr0d, clr1) %&gt;% clr_lighten(.3)) p_age &lt;- usa %&gt;% ggplot(aes(fill = MedianAgeMarriage))+ scale_fill_gradientn(colours = c(clr_lighten(clr0d, .3), clr3)) p_waffle + p_divorce + p_age + plot_layout(guides = &quot;collect&quot;) &amp; with_shadow(geom_sf(aes(color = after_scale(clr_darken(fill)))), x_offset = 0, y_offset = 0, sigma = 3) &amp; guides(fill = guide_colorbar(title.position = &quot;top&quot;, barheight = unit(5,&quot;pt&quot;))) &amp; theme(legend.position = &quot;bottom&quot;) Age Model: first model (divorce rate depends on age at marriage) \\[ \\begin{array}{cccr} D_i &amp; {\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{A} A_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.2) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{A} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] data_waffle &lt;- WaffleDivorce %&gt;% mutate(across(.cols = c(Divorce, Marriage, MedianAgeMarriage), .fns = standardize, .names = &quot;{str_to_lower(.col)}_std&quot;), waffle_pop = WaffleHouses / Population) %&gt;% rename(median_age_std = &quot;medianagemarriage_std&quot;) sd(data_waffle$MedianAgeMarriage) #&gt; [1] 1.24363 model_age &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_A * median_age_std , alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle ) set.seed(10) age_priors &lt;- extract.prior(model_age) %&gt;% as_tibble() prior_prediction_range &lt;- c(-2, 2) age_prior_predictions &lt;- link(model_age, post = age_priors, data = list(median_age_std = prior_prediction_range)) %&gt;% as_tibble() %&gt;% set_names(nm = as.character(prior_prediction_range)) %&gt;% mutate(.draw = row_number()) age_prior_predictions %&gt;% filter(.draw &lt; 51) %&gt;% ggplot() + geom_segment(aes(x = -2, xend = 2, y = `-2`, yend = `2`, group = .draw), color = clr2, alpha = .2) + labs(x = &quot;median age of marriage (std)&quot;, y = &quot;divorce rate (std)&quot;) age_seq &lt;- seq(min(data_waffle$median_age_std), max(data_waffle$median_age_std), length.out = 101) model_age_posterior_prediction_samples &lt;- link(model_age, data = data.frame(median_age_std = age_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = age_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;median_age_std&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(median_age_std = as.numeric(median_age_std), MedianAgeMarriage = median_age_std * sd(data_waffle$MedianAgeMarriage) + mean(data_waffle$MedianAgeMarriage), Divorce = divorce_std * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) model_age_posterior_prediction_pi &lt;- model_age_posterior_prediction_samples %&gt;% group_by(median_age_std, MedianAgeMarriage) %&gt;% summarise(mean = mean(Divorce), PI_lower = PI(Divorce)[1], PI_upper = PI(Divorce)[2]) %&gt;% ungroup() model_age_posterior_prediction_simulation &lt;- sim(model_age, data = data.frame(median_age_std = age_seq), n = 5e3) %&gt;% as_tibble() %&gt;% set_names(nm = age_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;median_age_std&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(median_age_std = as.numeric(median_age_std), MedianAgeMarriage = median_age_std * sd(data_waffle$MedianAgeMarriage) + mean(data_waffle$MedianAgeMarriage), Divorce = divorce_std * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) model_age_posterior_prediction_simulation_pi &lt;- model_age_posterior_prediction_simulation %&gt;% group_by(median_age_std, MedianAgeMarriage) %&gt;% summarise(mean = mean(Divorce), PI_lower = PI(Divorce)[1], PI_upper = PI(Divorce)[2]) %&gt;% ungroup() p_age &lt;- ggplot(mapping = aes(x = MedianAgeMarriage)) + geom_ribbon(data = model_age_posterior_prediction_simulation_pi, aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_smooth(data = model_age_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr3, fill = fll3, size = .4) + geom_point(data = data_waffle, aes(y = Divorce), color = rgb(0,0,0,.5), size = .6) + labs(y = &quot; divorce&quot;) Marriage Model: alternative model (divorce rate depends on marriage rate) \\[ \\begin{array}{cccr} D_i &amp; {\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{M} M_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.2) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{M} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_marriage &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std , alpha ~ dnorm( 0, 0.2 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle ) marriage_seq &lt;- seq(min(data_waffle$marriage_std), max(data_waffle$marriage_std), length.out = 101) model_marriage_posterior_prediction_samples &lt;- link(model_marriage, data = data.frame(marriage_std = marriage_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = marriage_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;marriage_std&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(marriage_std = as.numeric(marriage_std), Marriage = marriage_std * sd(data_waffle$Marriage) + mean(data_waffle$Marriage), Divorce = divorce_std * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) model_marriage_posterior_prediction_pi &lt;- model_marriage_posterior_prediction_samples %&gt;% group_by(marriage_std, Marriage) %&gt;% summarise(mean = mean(Divorce), PI_lower = PI(Divorce)[1], PI_upper = PI(Divorce)[2]) %&gt;% ungroup() model_marriage_posterior_prediction_simulation &lt;- sim(model_marriage, data = data.frame(marriage_std = marriage_seq), n = 5e3) %&gt;% as_tibble() %&gt;% set_names(nm = marriage_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;marriage_std&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(marriage_std = as.numeric(marriage_std), Marriage = marriage_std * sd(data_waffle$Marriage) + mean(data_waffle$Marriage), Divorce = divorce_std * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) model_marriage_posterior_prediction_simulation_pi &lt;- model_marriage_posterior_prediction_simulation %&gt;% group_by(marriage_std, Marriage) %&gt;% summarise(mean = mean(Divorce), PI_lower = PI(Divorce)[1], PI_upper = PI(Divorce)[2]) %&gt;% ungroup() p_marriage &lt;- ggplot(mapping = aes(x = Marriage)) + geom_ribbon(data = model_marriage_posterior_prediction_simulation_pi, aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_smooth(data = model_marriage_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr1, fill = fll1, size = .2) + geom_point(data = data_waffle, aes(y = Divorce), color = rgb(0,0,0,.5), size = .6) + labs(y = &quot; divorce&quot;) Waffle Model: model_waffle &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_W * waffle_pop , alpha ~ dnorm( 0, 0.2 ), beta_W ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle ) waffle_seq &lt;- seq(min(data_waffle$waffle_pop), max(data_waffle$waffle_pop), length.out = 101) model_waffle_posterior_prediction_samples &lt;- link(model_waffle, data = data.frame(waffle_pop = waffle_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = waffle_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;waffle_pop&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(waffle_pop = as.numeric(waffle_pop), Divorce = divorce_std * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) model_waffle_posterior_prediction_pi &lt;- model_waffle_posterior_prediction_samples %&gt;% group_by(waffle_pop) %&gt;% summarise(mean = mean(Divorce), PI_lower = PI(Divorce)[1], PI_upper = PI(Divorce)[2]) %&gt;% ungroup() p_waffle &lt;- ggplot(mapping = aes(x = waffle_pop)) + geom_smooth(data = model_waffle_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr2, fill = fll2, size = .2) + geom_point(data = data_waffle, aes(y = Divorce), color = rgb(0,0,0,.5), size = .6) + labs(y = &quot; divorce&quot;) p_waffle + p_marriage + theme(axis.title.y = element_blank(), axis.text.y = element_blank()) + p_age + theme(axis.title.y = element_blank(), axis.text.y = element_blank()) &amp; lims(y = c(4, 15)) 6.1 Directed Acyclic Graphs dag1 &lt;- dagify( D ~ A + M, M ~ A, exposure = &quot;A&quot;, outcome = &quot;M&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,1,.5), y = c(1,1, .4))) %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) dag2 &lt;- dagify( D ~ A, M ~ A, exposure = &quot;A&quot;, outcome = &quot;M&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,.5,1), y = c(1, .4, 1))) %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag1, clr_in = clr3) + plot_dag(dag2, clr_in = clr3) &amp; scale_y_continuous(limits = c(.35, 1.05)) &amp; coord_equal() DAG notation: \\(Y \\perp \\!\\!\\! \\perp X | Z\\): “\\(Y\\) is independent of \\(X\\) conditional on \\(Z\\)” \\(D \\not\\!\\perp\\!\\!\\!\\perp A\\): \"\\(D\\) is associated with \\(A\\)\" Check pair wise correlations with cor(): data_waffle %&gt;% dplyr::select(divorce_std,marriage_std, median_age_std) %&gt;% cor() %&gt;% as.data.frame(row.names = row.names(.)) %&gt;% round(digits = 2) %&gt;% knitr::kable() divorce_std marriage_std median_age_std divorce_std 1.00 0.37 -0.60 marriage_std 0.37 1.00 -0.72 median_age_std -0.60 -0.72 1.00 library(dagitty) dagitty(&#39;dag{ D &lt;- A -&gt; M -&gt; D}&#39;) %&gt;% impliedConditionalIndependencies() dagitty(&#39;dag{ D &lt;- A -&gt; M }&#39;) %&gt;% impliedConditionalIndependencies() #&gt; D _||_ M | A 6.2 Multiple Regression notion \\[ \\begin{array}{cccr} D_i &amp; {\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{M} M_{i} + \\beta_{A} A_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.2) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{M} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_M$ prior]}\\\\ \\beta_{A} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_A$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] or compact notion \\[ \\mu_i = \\alpha + \\sum_{j = 1}^{n} \\beta_jx_{ji} \\] or even matrix notion \\[ m = Xb \\] model_multiple &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle ) precis(model_multiple) %&gt;% round(digits = 2) %&gt;% as.matrix() %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.10 -0.16 0.16 beta_A -0.61 0.15 -0.85 -0.37 beta_M -0.07 0.15 -0.31 0.18 sigma 0.79 0.08 0.66 0.91 ct &lt;- coeftab(model_age, model_marriage, model_multiple,se = TRUE) plot_coeftab(ct) beta_A doesn’t really change, it only grows more uncertain, yet beta_M is only associated with divorce, when marriage rate is missing from the model. “Once we know the median age at marriage for a State, there is little to no additional predictive power in also knowing the rate of marriage at that State.” \\(\\rightarrow\\) \\(D \\perp \\!\\!\\! \\perp M | A\\) simulating the divorcee example n &lt;- 50 data_divorce_sim &lt;- tibble(median_age_std = rnorm(n), marriage_std = rnorm(n, mean = -median_age_std), divorce_std = rnorm(n, mean = median_age_std), divorce_codep = rnorm(n, mean = median_age_std + marriage_std)) p1 &lt;- ggpairs(data_divorce_sim %&gt;% dplyr::select(-divorce_codep), lower = list(continuous = wrap(ggally_points, colour = clr1, size = .9, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll1, color = clr1, adjust = 1)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) p2 &lt;- ggpairs(data_divorce_sim %&gt;% dplyr::select(-divorce_std), lower = list(continuous = wrap(ggally_points, colour = clr2, size = .9, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll2, color = clr2, adjust = 1)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) cowplot::plot_grid(ggmatrix_gtable(p1), ggmatrix_gtable(p2)) simulating the right DAG (\\(D \\perp \\!\\!\\! \\perp M | A\\)) model_multiple_sim &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_divorce_sim ) model_age_sim &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_divorce_sim ) model_marriage_sim &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std, alpha ~ dnorm( 0, 0.2 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_divorce_sim ) ct_sim &lt;- coeftab(model_age_sim, model_marriage_sim, model_multiple_sim, se = TRUE) plot_coeftab(ct_sim) simulating the left DAG (\\(D \\not\\!\\perp\\!\\!\\!\\perp M | A\\)) model_multiple_sim_codep &lt;- quap( flist = alist( divorce_codep ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_divorce_sim ) model_age_sim_codep &lt;- quap( flist = alist( divorce_codep ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_divorce_sim ) model_marriage_sim_codep &lt;- quap( flist = alist( divorce_codep ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std, alpha ~ dnorm( 0, 0.2 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_divorce_sim ) ct_sim_codep &lt;- coeftab(model_age_sim_codep, model_marriage_sim_codep, model_multiple_sim_codep, se = TRUE) plot_coeftab(ct_sim_codep) 6.2.1 Visualizations for multivariate regressions Predictor residual plots. useful for understanding the model, but not much else Posterior prediction plots. checking fit and assessing predictions Counterfactual plots. implied predictions for imaginary experiments 6.2.1.1 Predictor residual plots predictor residual plot for marriage rate pred_res_marriage &lt;- quap( flist = alist( marriage_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_AM * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_AM ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle ) residuals_marriage &lt;- link(pred_res_marriage) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(data_waffle$median_age_std)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;fit_marriage&quot;) %&gt;% group_by(row_idx) %&gt;% summarise(mean_marriage = mean(fit_marriage), lower_pi = PI(fit_marriage)[1], upper_pi = PI(fit_marriage)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx)) %&gt;% left_join(data_waffle %&gt;% mutate(row_idx = row_number()),. ) %&gt;% mutate(residual_marriage = marriage_std - mean_marriage) p_11 &lt;- residuals_marriage %&gt;% ggplot(aes(x = median_age_std)) + geom_segment(aes(xend = median_age_std, y = mean_marriage, yend = marriage_std), color = rgb(0,0,0,.6), linetype = 3) + geom_line(aes(y = mean_marriage), color = clr1) + geom_point(aes(y = marriage_std), color = clr1, fill = clr_lighten(clr1, .35), shape = 21) + geom_text(data = residuals_marriage %&gt;% filter(Loc %in% c(&quot;DC&quot;, &quot;HI&quot;, &quot;ND&quot;, &quot;ME&quot;, &quot;WY&quot;)), aes(x = median_age_std - .1, y = marriage_std, label = Loc), hjust = 1) pred_res_marriage_mu &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta * residual_marriage, alpha ~ dnorm( 0, 0.2 ), beta ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = residuals_marriage ) seq_res &lt;- seq(min(residuals_marriage$residual_marriage), max(residuals_marriage$residual_marriage), length.out = 101) residual_lm_posterior &lt;- link(pred_res_marriage_mu, data = data.frame(residual_marriage = seq_res)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_res) %&gt;% pivot_longer(cols = everything(), names_to = &quot;residual_marriage&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(residual_marriage = as.numeric(residual_marriage)) %&gt;% group_by(residual_marriage) %&gt;% summarise(mean = mean(divorce_std), PI_lower = PI(divorce_std)[1], PI_upper = PI(divorce_std)[2]) %&gt;% ungroup() p_12 &lt;- ggplot(mapping = aes(x = residual_marriage)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_smooth(data = residual_lm_posterior, aes(y = mean, ymin = PI_lower, ymax = PI_upper), stat = &quot;identity&quot;, color = clr1, fill = fll1, size = .4) + geom_point(data = residuals_marriage, aes(y = divorce_std), color = clr1, fill = clr_lighten(clr1,.35), shape = 21) + geom_text(data = residuals_marriage %&gt;% filter(Loc %in% c(&quot;DC&quot;, &quot;HI&quot;, &quot;ND&quot;, &quot;ME&quot;, &quot;WY&quot;)), aes(y = divorce_std - .4, label = Loc)) + labs(y = &quot;divorce_rate (std)&quot;) predictor residual plot for age at marriage pred_res_age &lt;- quap( flist = alist( median_age_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_MA * marriage_std, alpha ~ dnorm( 0, 0.2 ), beta_MA ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle ) residuals_age &lt;- link(pred_res_age) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(data_waffle$marriage_std)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;fit_age&quot;) %&gt;% group_by(row_idx) %&gt;% summarise(mean_age = mean(fit_age), lower_pi = PI(fit_age)[1], upper_pi = PI(fit_age)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx)) %&gt;% left_join(data_waffle %&gt;% mutate(row_idx = row_number()),. ) %&gt;% mutate(residual_age = median_age_std - mean_age) p_21 &lt;- residuals_age %&gt;% ggplot(aes(x = marriage_std)) + geom_segment(aes(xend = marriage_std, y = mean_age, yend = median_age_std), color = rgb(0,0,0,.6), linetype = 3) + geom_line(aes(y = mean_age), color = clr2) + geom_point(aes(y = median_age_std), color = clr2, fill = clr_lighten(clr2, .35), shape = 21) + geom_text(data = residuals_marriage %&gt;% filter(Loc %in% c(&quot;DC&quot;, &quot;HI&quot;, &quot;ID&quot;)), aes(x = marriage_std - .1, y = median_age_std, label = Loc), hjust = 1) pred_res_age_mu &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta * residual_age, alpha ~ dnorm( 0, 0.2 ), beta ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = residuals_age ) seq_res_age &lt;- seq(min(residuals_age$residual_age), max(residuals_age$residual_age), length.out = 101) residual_lm_posterior_age &lt;- link(pred_res_age_mu, data = data.frame(residual_age = seq_res_age)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_res_age) %&gt;% pivot_longer(cols = everything(), names_to = &quot;residual_age&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(residual_age = as.numeric(residual_age)) %&gt;% group_by(residual_age) %&gt;% summarise(mean = mean(divorce_std), PI_lower = PI(divorce_std)[1], PI_upper = PI(divorce_std)[2]) %&gt;% ungroup() p_22 &lt;- ggplot(mapping = aes(x = residual_age)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_smooth(data = residual_lm_posterior_age, aes(y = mean, ymin = PI_lower, ymax = PI_upper), stat = &quot;identity&quot;, color = clr2, fill = fll2, size = .4) + geom_point(data = residuals_age, aes(y = divorce_std), color = clr2, fill = clr_lighten(clr2,.35), shape = 21) + geom_text(data = residuals_age %&gt;% filter(Loc %in% c(&quot;DC&quot;, &quot;HI&quot;, &quot;ID&quot;)), aes(y = divorce_std - .4, label = Loc)) + labs(y = &quot;divorce_rate (std)&quot;) p_11 + p_21 + p_12 + p_22 6.2.1.2 Posterior Preediction Plots posterior_prediction &lt;- link(model_multiple) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(data_waffle$divorce_std)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;divorce_predicted&quot;) %&gt;% group_by(row_idx) %&gt;% summarise(divorce_predicted_mean = mean(divorce_predicted), lower_pi = PI(divorce_predicted)[1], upper_pi = PI(divorce_predicted)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx)) %&gt;% left_join(data_waffle %&gt;% mutate(row_idx = row_number()), . ) posterior_simmulation &lt;- sim(model_multiple) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(data_waffle$divorce_std)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;divorce_predicted&quot;) %&gt;% group_by(row_idx) %&gt;% summarise(lower_pi = PI(divorce_predicted)[1], upper_pi = PI(divorce_predicted)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx)) %&gt;% left_join(data_waffle %&gt;% mutate(row_idx = row_number()), . ) ggplot(mapping = aes(x = divorce_std)) + geom_abline(slope = 1, size = .7, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(data = posterior_prediction, aes(ymin = lower_pi, ymax = upper_pi, color = Loc %in% c(&quot;ID&quot;, &quot;UT&quot;)))+ geom_point(data = posterior_prediction, aes(y = divorce_predicted_mean, color = Loc %in% c(&quot;ID&quot;, &quot;UT&quot;), fill = after_scale(clr_lighten(color ,.5))), shape = 21, size = 1.5)+ geom_text(data = posterior_prediction %&gt;% filter(Loc %in% c(&quot;ID&quot;, &quot;ME&quot;, &quot;RI&quot;, &quot;UT&quot;)), aes(x = divorce_std - .15, y = divorce_predicted_mean, label = Loc)) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) Regressions tend to under-estimate variable in the high end of the range and over-estimate in the low end of the range. This is normal, they “pull towards the mean”. The labeled States however (ID, ME, RI, UT), are not well predicted by the Model (eg. due to additional social factors). Simulating spurious association N &lt;- 100 data_spurious &lt;- tibble(x_real = rnorm(N), x_spur = rnorm(N, x_real), y = rnorm(N, x_real)) ggpairs(data_spurious, lower = list(continuous = wrap(ggally_points, colour = clr3, size = .9, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll3, color = clr3, adjust = 1)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) model_spurious &lt;- quap( flist = alist( y ~ dnorm(mu, sigma), mu &lt;- alpha + beta_r * x_real + beta_s * x_spur, alpha ~ dnorm(0, .2), beta_r ~ dnorm(0, .5), beta_s ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_spurious ) precis(model_spurious) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.09 0.09 -0.06 0.24 beta_r 0.87 0.14 0.64 1.10 beta_s 0.09 0.11 -0.09 0.27 sigma 1.06 0.07 0.94 1.17 Note, how the estimated mean for beta_s is close to 0 (0.09) – despite the correlation shown above 🤔`. 6.2.1.3 Counterfactual Plots model_counterfactual &lt;- quap( flist = alist( # A -&gt; D &lt;- M divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ), # A -&gt; M marriage_std ~ dnorm( mu_M, sigma_M ), mu_M &lt;- alpha_M + beta_AM * median_age_std, alpha_M ~ dnorm( 0, 0.2 ), beta_AM ~ dnorm( 0, 0.5 ), sigma_M ~ dexp(1) ), data = data_waffle ) precis(model_counterfactual) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.10 -0.16 0.16 beta_A -0.61 0.15 -0.85 -0.37 beta_M -0.07 0.15 -0.31 0.18 sigma 0.79 0.08 0.66 0.91 alpha_M 0.00 0.09 -0.14 0.14 beta_AM -0.69 0.10 -0.85 -0.54 sigma_M 0.68 0.07 0.57 0.79 Note, that marriage_std and median_age_std are strongly negatively correlated (-0.69) A_seq &lt;- seq(-2, 2, length.out = 30) unpack_sim &lt;- function(x, seq = A_seq){ nms &lt;- names(x) purrr::map(.x = nms, .f = function(y, x, seq_in = seq){ x[[y]] %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(seq_in)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;value&quot;) %&gt;% mutate(parameter = y) }, x = x) %&gt;% purrr::reduce(bind_rows) } data_sim &lt;- sim(fit = model_counterfactual, data = tibble(median_age_std = A_seq), vars = c(&quot;marriage_std&quot;, &quot;divorce_std&quot;)) %&gt;% unpack_sim() data_sim_pi &lt;- data_sim %&gt;% group_by(row_idx, parameter) %&gt;% summarise(mean = mean(value), PI_lower = PI(value)[1], PI_upper = PI(value)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx), median_age_std = A_seq[row_idx]) %&gt;% arrange(parameter, median_age_std) data_sim_pi %&gt;% ggplot() + geom_smooth(aes(x = median_age_std, y = mean, ymin = PI_lower, ymax = PI_upper, color = parameter, fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(clr0d, clr3), guide = &quot;none&quot;) + labs(y = &quot;counterfactual value&quot;, title = &quot;Counterfactual effects of age at marriage on&quot;) + facet_wrap(parameter ~ .) Numerical operations (eg. simulating the causal effect of raising the median age of marriage from 20 to 30): A_seq2 &lt;- (c(20, 30) - mean(data_waffle$MedianAgeMarriage)) / sd(data_waffle$MedianAgeMarriage) data_sim_num &lt;- sim(fit = model_counterfactual, data = tibble(median_age_std = A_seq2), vars = c(&quot;marriage_std&quot;, &quot;divorce_std&quot;)) %&gt;% unpack_sim(seq = A_seq2) data_sim_num %&gt;% filter(parameter == &quot;divorce_std&quot;) %&gt;% dplyr::select(-parameter) %&gt;% mutate(pair = (row_number() + 1) %/% 2) %&gt;% pivot_wider(names_from = row_idx, values_from = value) %&gt;% mutate(effect = `2` - `1`) %&gt;% summarise(mean = mean(effect)) #&gt; # A tibble: 1 × 1 #&gt; mean #&gt; &lt;dbl&gt; #&gt; 1 -4.59 …A change of four and a half standard deviations is quite extreme! M_seq &lt;- A_seq data_sim_M &lt;- sim(fit = model_counterfactual, data = tibble(marriage_std = M_seq, median_age_std = 0), vars = c(&quot;divorce_std&quot;)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(M_seq)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;divorce_std&quot;) data_sim_M_pi &lt;- data_sim_M %&gt;% group_by(row_idx) %&gt;% summarise(mean = mean(divorce_std), PI_lower = PI(divorce_std)[1], PI_upper = PI(divorce_std)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx), marriage_std = M_seq[row_idx]) data_sim_M_pi %&gt;% ggplot() + geom_smooth(aes(x = marriage_std, y = mean, ymin = PI_lower, ymax = PI_upper), color = clr1, fill = fll1, stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(clr0d, clr3), guide = &quot;none&quot;) + labs(y = &quot;counterfactual value&quot;, title = &quot;Counterfactual effects of marriage rate on divorce rate&quot;) + lims(y = c(-2, 2)) 6.3 Masked relationship Loading the milk data data(milk) data_milk &lt;- milk %&gt;% filter(complete.cases(.)) %&gt;% as_tibble() %&gt;% mutate(`mass.log` = log(mass), across(.cols = c(`kcal.per.g`, `neocortex.perc`, `mass.log`), .fns = standardize, .names = &quot;{str_remove_all(.col, &#39;\\\\\\\\..*&#39;)}_std&quot;)) data_milk %&gt;% precis() %&gt;% as.matrix() %&gt;% as.data.frame() %&gt;% filter(!is.na(mean)) %&gt;% mutate(across(.cols = mean:`94.5%`, function(x){round(as.numeric(x), digits = 2)})) %&gt;% knitr::kable() mean sd 5.5% 94.5% histogram kcal.per.g 0.66 0.17 0.47 0.93 ▇▂▁▁▁▂▁▁▁▁▁ perc.fat 36.06 14.71 15.08 54.45 ▂▁▁▂▃▃▂▅▃▁▇▂ perc.protein 16.26 5.60 9.28 23.79 ▂▅▅▅▅▂▂▅▇▂ perc.lactose 47.68 13.59 30.35 68.31 ▂▇▅▅▂▇▅▁▅▂ mass 16.64 23.58 0.30 57.89 ▇▁▁▁▁▁▁▁ neocortex.perc 67.58 5.97 58.41 75.59 ▂▁▂▅▁▅▅▅▇▅▂▂ mass.log 1.50 1.93 -1.26 4.05 ▂▁▂▂▂▂▅▂▇▁▂▂▅▅ kcal_std 0.00 1.00 -1.09 1.55 ▃▇▁▃▁▂▂ neocortex_std 0.00 1.00 -1.54 1.34 ▁▁▂▃▁▇▃▂ mass_std 0.00 1.00 -1.43 1.32 ▁▂▂▃▃▁▇ 6.3.1 Bi-variate models Neocortex effect on caloric content of milk \\[ \\begin{array}{cccr} K_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{N} N_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.2) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{N} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_N$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] Mothers weight effect on caloric content of milk \\[ \\begin{array}{cccr} K_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{M} M_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.2) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{M} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_M$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] Model implementation (neocortex, draft) model_milk_draft &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_N * neocortex_std, alpha ~ dnorm(0, 1), beta_N ~ dnorm(0, 1), sigma ~ dexp(1) ), data = data_milk ) prior_milk_draft &lt;- extract.prior(model_milk_draft) %&gt;% as_tibble() seq_prior &lt;- c(-2, 2) prior_prediction_milk_draft &lt;- link(model_milk_draft, post = prior_milk_draft, data = tibble(neocortex_std = seq_prior)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_prior) p_draft &lt;- prior_prediction_milk_draft %&gt;% filter(row_number() &lt;= 50) %&gt;% ggplot() + geom_segment(aes(x = -2, xend = 2, y = `-2`, yend = `2`), alpha = .6, color = clr0d) Model implementation (neocortex) model_milk_cortex &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_N * neocortex_std, alpha ~ dnorm(0, .2), beta_N ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_milk ) precis(model_milk_cortex) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.15 -0.24 0.24 beta_N 0.13 0.21 -0.21 0.47 sigma 0.93 0.15 0.69 1.18 prior_milk_cortex &lt;- extract.prior(model_milk_cortex) %&gt;% as_tibble() prior_prediction_milk_cortex &lt;- link(model_milk_cortex, post = prior_milk_cortex, data = tibble(neocortex_std = seq_prior)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_prior) p_cortex &lt;- prior_prediction_milk_cortex %&gt;% filter(row_number() &lt;= 50) %&gt;% ggplot() + geom_segment(aes(x = -2, xend = 2, y = `-2`, yend = `2`), alpha = .6, color = clr0d) p_draft + p_cortex &amp; coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 2)) &amp; labs(x = &quot;neocortex_std&quot;, y = &quot;kcal_std&quot;) seq_cortex &lt;- seq(min(data_milk$neocortex_std) - .15, max(data_milk$neocortex_std) + .15, length.out = 51) model_milk_cortex_posterior_prediction_samples &lt;- link(model_milk_cortex, data = data.frame(neocortex_std = seq_cortex)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_cortex) %&gt;% pivot_longer(cols = everything(), names_to = &quot;neocortex_std&quot;, values_to = &quot;kcal_std&quot;) %&gt;% mutate(neocortex_std = as.numeric(neocortex_std)) model_milk_cortex_posterior_prediction_pi &lt;- model_milk_cortex_posterior_prediction_samples %&gt;% group_by(neocortex_std) %&gt;% summarise(mean = mean(kcal_std), PI_lower = PI(kcal_std)[1], PI_upper = PI(kcal_std)[2]) %&gt;% ungroup() p_cortex &lt;- ggplot(mapping = aes(x = neocortex_std)) + geom_smooth(data = model_milk_cortex_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr2, fill = fll2, size = .2) + geom_point(data = data_milk, aes(y = kcal_std), color = rgb(0,0,0,.5), size = 1.6) + labs(x = &quot;neocprtex_std&quot;, y = &quot;kcal_std&quot;) Model implementation (mothers weight) model_milk_weight &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_M * mass_std, alpha ~ dnorm(0, .2), beta_M ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_milk ) precis(model_milk_weight) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.15 -0.23 0.23 beta_M -0.30 0.20 -0.62 0.03 sigma 0.89 0.15 0.65 1.12 seq_weight &lt;- seq(min(data_milk$mass_std) - .15, max(data_milk$mass_std) + .15, length.out = 51) model_milk_weight_posterior_prediction_samples &lt;- link(model_milk_weight, data = data.frame(mass_std = seq_weight)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_weight) %&gt;% pivot_longer(cols = everything(), names_to = &quot;mass_std&quot;, values_to = &quot;kcal_std&quot;) %&gt;% mutate(mass_std = as.numeric(mass_std)) model_milk_weight_posterior_prediction_pi &lt;- model_milk_weight_posterior_prediction_samples %&gt;% group_by(mass_std) %&gt;% summarise(mean = mean(kcal_std), PI_lower = PI(kcal_std)[1], PI_upper = PI(kcal_std)[2]) %&gt;% ungroup() p_weight &lt;- ggplot(mapping = aes(x = mass_std)) + geom_smooth(data = model_milk_weight_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr2, fill = fll2, size = .2) + geom_point(data = data_milk, aes(y = kcal_std), color = rgb(0,0,0,.5), size = 1.6) + labs(x = &quot;mass_std&quot;, y = &quot;kcal_std&quot;) p_cortex + p_weight Model implementation (necocortex and mothers weight) \\[ \\begin{array}{cccr} K_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{N} N_{i} + \\beta_{M} M_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.2) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{N} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_N$ prior]}\\\\ \\beta_{M} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_M$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_milk_multi &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_N * neocortex_std + beta_M * mass_std, alpha ~ dnorm(0, .2), beta_N ~ dnorm(0, .5), beta_M ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_milk ) precis(model_milk_multi) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.13 -0.20 0.20 beta_N 0.64 0.23 0.27 1.01 beta_M -0.75 0.23 -1.12 -0.37 sigma 0.69 0.12 0.49 0.88 ct_milk &lt;- coeftab(model_milk_cortex, model_milk_weight, model_milk_multi, se = TRUE) plot_coeftab(ct_milk) data_milk %&gt;% dplyr::select(kcal_std, neocortex_std, mass_std) %&gt;% ggpairs(lower = list(continuous = wrap(ggally_points, colour = clr2, size = .9, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll2, color = clr2, adjust = 1)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) dag1 &lt;- dagify( K ~ M + N, N ~ M, exposure = &quot;M&quot;, outcome = &quot;K&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,1,.5), y = c(1,1, .4))) %&gt;% mutate(stage = if_else(name == &quot;K&quot;, &quot;response&quot;, if_else(name %in% c(&quot;M&quot;, &quot;N&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) dag2 &lt;- dagify( K ~ M + N, M ~ N, exposure = &quot;M&quot;, outcome = &quot;K&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,1,.5), y = c(1,1, .4))) %&gt;% mutate(stage = if_else(name == &quot;K&quot;, &quot;response&quot;, if_else(name %in% c(&quot;M&quot;, &quot;N&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) dag3 &lt;- dagify( K ~ M + N, M ~ U, N ~ U, exposure = &quot;M&quot;, outcome = &quot;K&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,1,.5, .5), y = c(1,1, 1,.4))) %&gt;% mutate(stage = if_else(name == &quot;K&quot;, &quot;response&quot;, if_else(name %in% c(&quot;M&quot;, &quot;N&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag1, clr_in = clr3) + plot_dag(dag2, clr_in = clr3) + plot_dag(dag3, clr_in = clr3) + plot_layout(nrow = 1) + plot_annotation(tag_levels = &quot;a&quot;) &amp; scale_y_continuous(limits = c(.35, 1.05)) &amp; coord_equal() &amp; theme(plot.tag = element_text(family = fnt_sel)) Counterfactual plots for DAG c) data_sim_mass &lt;- link(fit = model_milk_multi, data = tibble(mass_std = 0, neocortex_std = seq_cortex), vars = c(&quot;kcal_std&quot;)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(seq_cortex)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;kcal_std&quot;) data_sim_mass_pi &lt;- data_sim_mass %&gt;% group_by(row_idx) %&gt;% summarise(mean = mean(kcal_std), PI_lower = PI(kcal_std)[1], PI_upper = PI(kcal_std)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx), neocortex_std = seq_cortex[row_idx]) p_mass &lt;- data_sim_mass_pi %&gt;% ggplot() + geom_smooth(aes(x = neocortex_std, y = mean, ymin = PI_lower, ymax = PI_upper), color = clr2, fill = fll2, stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(clr0d, clr3), guide = &quot;none&quot;) + labs(y = &quot;counterfactual kcal&quot;, title = &quot;kcal at mass_std = 0&quot;) data_sim_cortex &lt;- link(fit = model_milk_multi, data = tibble(mass_std = seq_weight, neocortex_std = 0), vars = c(&quot;kcal_std&quot;)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(seq_weight)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;kcal_std&quot;) data_sim_cortex_pi &lt;- data_sim_cortex %&gt;% group_by(row_idx) %&gt;% summarise(mean = mean(kcal_std), PI_lower = PI(kcal_std)[1], PI_upper = PI(kcal_std)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx), mass_std = seq_weight[row_idx]) p_cortex &lt;- data_sim_cortex_pi %&gt;% ggplot() + geom_smooth(aes(x = mass_std, y = mean, ymin = PI_lower, ymax = PI_upper), color = clr2, fill = fll2, stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(clr0d, clr3), guide = &quot;none&quot;) + labs(y = &quot;counterfactual kcal&quot;, title = &quot;kcal at neocortex_std = 0&quot;) p_mass + p_cortex &amp; coord_cartesian(ylim = c(-1, 2)) 6.3.2 Simulate a masking relationship DAG a) (\\(M \\rightarrow K \\leftarrow N \\leftarrow M\\)) n &lt;- 100 data_milk_sim1 &lt;- tibble(mass_std = rnorm(n = n), neocortex_std = rnorm(n = n, mean = mass_std), kcal_std = rnorm(n = n, mean = neocortex_std - mass_std)) data_milk_sim1 %&gt;% dplyr::select(kcal_std, neocortex_std, mass_std) %&gt;% ggpairs(lower = list(continuous = wrap(ggally_points, colour = clr0d, size = .9, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll0, color = clr0d, adjust = 1)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) DAG b) (\\(N \\rightarrow M \\rightarrow K \\leftarrow N\\)) data_milk_sim2 &lt;- tibble(neocortex_std = rnorm(n = n), mass_std = rnorm(n = n, mean = neocortex_std), kcal_std = rnorm(n = n, mean = neocortex_std - mass_std)) DAG c) (\\(U \\rightarrow N \\rightarrow M \\rightarrow K \\leftarrow N \\leftarrow U\\)) data_milk_sim3 &lt;- tibble(unsampled = rnorm(n = n), neocortex_std = rnorm(n = n, mean = unsampled), mass_std = rnorm(n = n, mean = unsampled), kcal_std = rnorm(n = n, mean = neocortex_std - mass_std)) model_milk_cortex_sim &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_N * neocortex_std, alpha ~ dnorm(0, .2), beta_N ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_milk_sim1 ) model_milk_weight_sim &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_M * mass_std, alpha ~ dnorm(0, .2), beta_M ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_milk_sim1 ) model_milk_multi_sim &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_N * neocortex_std + beta_M * mass_std, alpha ~ dnorm(0, .2), beta_N ~ dnorm(0, .5), beta_M ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_milk_sim1 ) ct_milk_sim &lt;- coeftab(model_milk_cortex_sim, model_milk_weight_sim, model_milk_multi_sim, se = TRUE) plot_coeftab(ct_milk_sim) Computing the Marcov Equivalence Set dag_milk &lt;- dagitty(&quot;dag{ M -&gt; K &lt;- N M -&gt; N}&quot;) coordinates(dag_milk) &lt;- list( x = c( M = 0, N = 1, K = .5), y = c( M = 1, N = 1, K = .3)) dag_milk %&gt;% node_equivalent_dags() %&gt;% mutate(stage = &quot;predictor&quot;) %&gt;% plot_dag() + coord_cartesian(xlim = c(-.1, 1.1), ylim = c(.2, 1.1))+ facet_wrap(~ dag) 6.4 Categorical Variables 6.4.1 Indicator vs. Index variable (binary categories) Taking gender into account for the height model (but not caring about weight). data(Howell1) data_height &lt;- as_tibble(Howell1) %&gt;% mutate(sex = if_else(male == 1, 2, 1)) Modeling as dummy/indicator variable \\[ \\begin{array}{cccr} h_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{m} m_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(178, 20) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{m} &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\beta_N$ prior]}\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] Modeling as index variable \\[ \\begin{array}{ccccr} h_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; &amp;\\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha_{\\textrm{sex}[i]} &amp; &amp;\\textrm{[linear model]}\\\\ \\alpha_j &amp; \\sim &amp; Normal(178, 20) &amp; \\textrm{for}~j = 1..2 &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; &amp;\\textrm{[$\\sigma$ prior]} \\end{array} \\] Demonstrating that in the indicator variable approach, the uncertainty of estimates is higher for the male type (coded as 1), since this one is influenced by the uncertainty of two priors: indicator_prior &lt;- tibble(mu_female = rnorm(1e4, 178, 20), mu_male = rnorm(1e4, 178, 20) + rnorm(1e4, 0, 10)) indicator_prior %&gt;% precis() %&gt;% as.matrix() %&gt;% knitr::kable() mean sd 5.5% 94.5% histogram mu_female 177.7964 19.99779 145.6110 209.8597 ▁▁▁▁▂▃▇▇▇▅▃▁▁▁▁ mu_male 177.5124 22.49842 141.7337 213.0894 ▁▁▁▃▇▇▂▁▁▁ indicator_long &lt;- indicator_prior %&gt;% pivot_longer(cols = everything(), names_to = &quot;sex&quot;, values_to = &quot;height&quot;, names_transform = list(sex = function(str){str_remove(string = str, &quot;mu_&quot;)})) ggplot(indicator_long) + geom_density(data = indicator_long %&gt;% dplyr::select(-sex), aes(x = height, y = ..count..), color = clr0d, fill = fll0) + geom_density(aes(x = height, y = ..count.., color = sex, fill = after_scale(clr_alpha(color)))) + facet_wrap(sex ~ . ) + scale_color_manual(values = c(clr1, clr2), guide = &quot;none&quot;) Implementing the index variable approach: model_hight &lt;- quap( flist = alist( height ~ dnorm(mu, sigma), mu &lt;- alpha[sex], alpha[sex] ~ dnorm(178, 20), sigma ~ dunif(0,50) ), data = data_height ) precis(model_hight, depth = 2) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha[1] 134.91 1.61 132.34 137.48 alpha[2] 142.58 1.70 139.86 145.29 sigma 27.31 0.83 25.99 28.63 hight_posterior_samples &lt;- extract.samples(model_hight) %&gt;% as_tibble() %&gt;% mutate(diff_sex = alpha[ ,1] - alpha[ ,2] ) The expected difference between the considered types is called a contrast: hight_posterior_samples %&gt;% precis() %&gt;% as.matrix() %&gt;% knitr::kable() mean sd 5.5% 94.5% histogram sigma 27.307086 0.822027 25.99673 28.620850 ▁▁▁▁▁▁▃▅▇▇▃▂▁▁▁ alpha.1 134.933243 1.599303 132.39721 137.457066 ▁▁▁▂▅▇▇▅▂▁▁▁▁ alpha.2 142.592035 1.708900 139.84870 145.308164 ▁▁▁▁▁▂▃▇▇▇▃▂▁▁▁ diff_sex -7.658793 2.341238 -11.38310 -3.867645 ▁▁▁▂▇▇▃▁▁▁ p_contrast1 &lt;- hight_posterior_samples %&gt;% ggplot() + geom_density(aes(x = alpha[,1], color = &quot;female&quot;, fill = after_scale(clr_alpha(color))))+ geom_density(aes(x = alpha[,2], color = &quot;male&quot;, fill = after_scale(clr_alpha(color)))) + geom_errorbarh(data = tibble(start = median(hight_posterior_samples$alpha[,1]), end = median(hight_posterior_samples$alpha[,2])), aes(y = 0, xmin = start, xmax = end), height = .01) + scale_color_manual(values = c(clr1, clr2), guide = &quot;none&quot;) + lims(y = c(-.01,.25))+ labs(x = &quot;height&quot;) + theme(axis.title.y = element_blank(), axis.text.y = element_blank()) p_contrast2 &lt;- hight_posterior_samples %&gt;% ggplot() + geom_density(aes(x = alpha[,2] - alpha[,1]), color = clr0d, fill = fll0) + labs(x = &quot;contrast height(male-female)&quot;) + lims(y = c(-.01,.25))+ theme(axis.title.y = element_blank(), axis.text.y = element_blank()) p_contrast1 + p_contrast2 + plot_layout(widths = c(1,.66)) 6.4.2 Multiple categories Taking the broad taxonomic unit into account for the milk model (but not caring about neocortex od weight). houses &lt;- c(&quot;Gryffindor&quot;, &quot;Hufflepuff&quot;, &quot;Ravenclaw&quot;, &quot;Slytherin&quot;) set.seed(63) data_milk_clade &lt;- milk %&gt;% as_tibble() %&gt;% mutate(kcal_std = standardize(`kcal.per.g`), clade_id = as.integer(clade), house_id = sample(rep(1:4, each = 8), size = length(clade)), house = houses[house_id]) \\[ \\begin{array}{ccccr} K_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; &amp;\\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha_{\\textrm{CLADE}[i]} &amp; &amp;\\textrm{[linear model]}\\\\ \\alpha_j &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{for}~j = 1..4 &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; &amp;\\textrm{[$\\sigma$ prior]} \\end{array} \\] model_milk_clade &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha[clade_id], alpha[clade_id] ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = data_milk_clade ) precis(model_milk_clade, depth = 2, pars = &quot;alpha&quot;) %&gt;% as_tibble_rn() %&gt;% mutate(clade_id = str_remove_all(param, pattern = &quot;[a-z\\\\[\\\\]]*&quot;) %&gt;% as.integer(), clade = fct_reorder(levels(data_milk$clade)[clade_id], clade_id)) %&gt;% ggplot(aes(y = clade)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(aes(xmin = `5.5%`, xmax =`94.5%`), color = clr0d, fill = clr0) + geom_point(aes(x = mean), shape = 21, size = 3, color = clr0d, fill = clr0) + scale_y_discrete(&quot;&quot;, limits = rev(levels(data_milk$clade))) + labs(x = &quot;expected kcal_std&quot;) adding another categorical variable: \\[ \\begin{array}{ccccr} K_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; &amp;\\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha_{\\textrm{CLADE}[i]} + \\alpha_{\\textrm{HOUSE}[i]} &amp; &amp;\\textrm{[linear model]}\\\\ \\alpha_{\\textrm{CLADE},j} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{for}~j = 1..4 &amp; \\textrm{[$\\alpha_{\\textrm{CLADE}}$ prior]}\\\\ \\alpha_{\\textrm{HOUSE},j} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{for}~j = 1..4 &amp; \\textrm{[$\\alpha_{\\textrm{CLADE}}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; &amp;\\textrm{[$\\sigma$ prior]} \\end{array} \\] model_milk_house &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha_clade[clade_id] + alpha_house[house_id], alpha_clade[clade_id] ~ dnorm(0, 0.5), alpha_house[house_id] ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = data_milk_clade ) precis(model_milk_house, depth = 2, pars = &quot;alpha&quot;) %&gt;% as_tibble_rn() %&gt;% mutate(type = str_remove(param, pattern = &quot;alpha_&quot;) %&gt;% str_remove(&quot;\\\\[[0-9]\\\\]&quot;), idx = str_extract(param, &quot;[0-9]&quot;) %&gt;% as.integer(), name = if_else(type == &quot;clade&quot;, levels(data_milk$clade)[idx], houses[idx])) %&gt;% ggplot(aes(y = name, color = type)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(aes(xmin = `5.5%`, xmax =`94.5%`)) + geom_point(aes(x = mean, fill = after_scale(clr_lighten(color))), shape = 21, size = 3 ) + scale_color_manual(values = c(clade = clr0d, house = clr3), guide = &quot;none&quot;) + facet_grid(type ~ . , scales = &quot;free_y&quot;, switch = &quot;y&quot;) + labs(x = &quot;expected kcal_std&quot;) + theme(axis.title.y = element_blank(), strip.placement = &quot;outside&quot;) library(rlang) chapter5_models &lt;- env( data_waffle = data_waffle, model_age = model_age, model_marriage = model_marriage, model_waffle = model_waffle, model_multiple = model_multiple, data_divorce_sim = data_divorce_sim, model_multiple_sim = model_multiple_sim, model_age_sim = model_age_sim, model_marriage_sim = model_marriage_sim, model_multiple_sim_codep = model_multiple_sim_codep, model_age_sim_codep = model_age_sim_codep, model_marriage_sim_codep = model_marriage_sim_codep, pred_res_marriage = pred_res_marriage, residuals_marriage = residuals_marriage, pred_res_marriage_mu = pred_res_marriage_mu, pred_res_age = pred_res_age, residuals_age = residuals_age, pred_res_age_mu = pred_res_age_mu, data_spurious = data_spurious, model_spurious = model_spurious, model_counterfactual = model_counterfactual, data_milk = data_milk, model_milk_draft = model_milk_draft, model_milk_cortex = model_milk_cortex, model_milk_weight = model_milk_weight, model_milk_multi = model_milk_multi, data_milk_sim1 = data_milk_sim1, model_milk_cortex_sim = model_milk_cortex_sim, model_milk_weight_sim = model_milk_weight_sim, model_milk_multi_sim = model_milk_multi_sim, data_height = data_height, model_hight = model_hight, data_milk_clade = data_milk_clade, model_milk_clade = model_milk_clade, model_milk_house = model_milk_house ) write_rds(chapter5_models, &quot;envs/chapter5_models.rds&quot;) 6.5 Homework E1 \\[ \\begin{array}{ccclr} 1) &amp; \\mu_i &amp; = &amp; \\alpha + \\beta x_{i} &amp;\\textrm{[simple linear regression]}\\\\ 2) &amp; \\mu_i &amp; = &amp; \\beta_{x} x_{i} + \\beta_{z} z_{i} &amp;\\textrm{[multiple linear regression]}\\\\ 3) &amp; \\mu_i &amp; = &amp; \\alpha + \\beta (x_{i} - z_{i}) &amp;\\textrm{[simple linear regression]}\\\\ 4) &amp; \\mu_i &amp; = &amp; \\alpha + \\beta_{x} x_{i} + \\beta_{z} z_{i} &amp;\\textrm{[multiple linear regression]}\\\\ \\end{array} \\] E2 \\[ \\begin{array}{cclr} d_i &amp; = &amp; \\alpha + \\beta_{y} y_i + \\beta_{p} p_{i}&amp; \\textrm{[linear model]}\\\\ \\end{array} \\] E3 \\[ \\begin{array}{ccclr} 1) &amp; t_i &amp; = &amp; \\alpha_{f} + \\beta_{ff} f_i &amp; \\textrm{[linear model]}\\\\ 2) &amp; t_i &amp; = &amp; \\alpha_{s} + \\beta_{ss} s_{i} &amp; \\textrm{[linear model]}\\\\ 3) &amp; t_i &amp; = &amp; \\alpha + \\beta_{f} f_i + \\beta_{s} s_{i} &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] \\(\\beta_{f} \\ge 0\\) \\(\\beta_{ss} \\ge 0\\) \\(t \\sim f\\) (\\(\\beta_{f} \\gt \\beta_{ff}\\)) \\(t \\sim s\\) (\\(\\beta_{s} \\gt \\beta_{ss}\\)) \\(f \\sim -s\\) E4 1), 3), 4) and 5) (models should contain \\(k - 1\\) indicator variables) M1 n &lt;- 100 data_spurious2 &lt;- tibble(u = rnorm(n), x = rnorm(n, mean = u), y = rnorm(n, mean = -u), z = rnorm(n, mean = u) ) data_spurious2 %&gt;% ggpairs() model_spurious2a &lt;- quap( flist = alist( z ~ dnorm(mu, sigma), mu &lt;- alpha + beta_x * x, alpha ~ dnorm(0, .2), beta_x ~ dnorm(0, .75), sigma ~ dexp(1)), data = data_spurious2 ) model_spurious2b &lt;- quap( flist = alist( z ~ dnorm(mu, sigma), mu &lt;- alpha + beta_y * y, alpha ~ dnorm(0, .2), beta_y ~ dnorm(0, .75), sigma ~ dexp(1)), data = data_spurious2 ) model_spurious2c &lt;- quap( flist = alist( z ~ dnorm(mu, sigma), mu &lt;- alpha + beta_x * x + beta_y * y, alpha ~ dnorm(0, .2), beta_x ~ dnorm(0, .75), beta_y ~ dnorm(0, .75), sigma ~ dexp(1)), data = data_spurious2 ) ct_spur &lt;- coeftab(model_spurious2a, model_spurious2b, model_spurious2c, se = TRUE) plot_coeftab(ct_spur) M2 data_masked &lt;- tibble(u = rnorm(n), x = rnorm(n, mean = u), y = rnorm(n, mean = u), z = rnorm(n, mean = x-y) ) data_masked %&gt;% ggpairs() model_masked_a &lt;- quap( flist = alist( z ~ dnorm(mu, sigma), mu &lt;- alpha + beta_x * x, alpha ~ dnorm(0, .2), beta_x ~ dnorm(0, .75), sigma ~ dexp(1)), data = data_masked ) model_masked_b &lt;- quap( flist = alist( z ~ dnorm(mu, sigma), mu &lt;- alpha + beta_y * y, alpha ~ dnorm(0, .2), beta_y ~ dnorm(0, .75), sigma ~ dexp(1)), data = data_masked ) model_masked_c &lt;- quap( flist = alist( z ~ dnorm(mu, sigma), mu &lt;- alpha + beta_x * x + beta_y * y, alpha ~ dnorm(0, .2), beta_x ~ dnorm(0, .75), beta_y ~ dnorm(0, .75), sigma ~ dexp(1)), data = data_masked ) ct_masked &lt;- coeftab(model_masked_a, model_masked_b, model_masked_c, se = TRUE) plot_coeftab(ct_masked) M3 dag &lt;- dagify( D ~ A, M ~ A, exposure = &quot;A&quot;, outcome = &quot;M&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,.5,1), y = c(1, .4, 1))) %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag, clr_in = clr3) + scale_y_continuous(limits = c(.35, 1.05)) + coord_equal() M4 data_waffle_lds &lt;- data_waffle %&gt;% left_join(read_tsv(&quot;data/lds_by_state_2019.tsv&quot;)) %&gt;% mutate(lds_std = standardize(lds_perc), lds_perc_log10 = log10(lds_perc), lds_log10_std = standardize(lds_perc_log10)) data_waffle_lds %&gt;% dplyr::select(lds_perc, lds_perc_log10) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value)) + geom_histogram(bins = 10, color = clr0d, fill = fll0) + facet_wrap(name ~ ., scales = &quot;free&quot;) model_lds &lt;- quap( flist = alist( divorce_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_age * median_age_std + beta_marriage * marriage_std + beta_lds * lds_log10_std, alpha ~ dnorm(0, .2), beta_age ~ dnorm(0, .5), beta_marriage ~ dnorm(0, .5), beta_lds ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_waffle_lds ) precis(model_lds) #&gt; mean sd 5.5% 94.5% #&gt; alpha 6.262694e-07 0.09382055 -0.1499427 0.14994399 #&gt; beta_age -6.980543e-01 0.15085783 -0.9391543 -0.45695439 #&gt; beta_marriage 7.802884e-02 0.16280138 -0.1821592 0.33821689 #&gt; beta_lds -2.954296e-01 0.14942991 -0.5342475 -0.05661175 #&gt; sigma 7.511933e-01 0.07463517 0.6319118 0.87047467 precis(model_lds, depth = 2, pars = &quot;beta&quot;) %&gt;% as_tibble_rn() %&gt;% mutate(type = str_remove(param, pattern = &quot;beta_&quot;)) %&gt;% ggplot(aes(y = type)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(aes(xmin = `5.5%`, xmax =`94.5%`), color = clr0d) + geom_point(aes(x = mean), color = clr0d, fill = clr0, shape = 21, size = 3 ) + theme(axis.title.y = element_blank()) posterior_prediction &lt;- link(model_lds) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(data_waffle$divorce_std)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;divorce_predicted&quot;) %&gt;% group_by(row_idx) %&gt;% summarise(divorce_predicted_mean = mean(divorce_predicted), lower_pi = PI(divorce_predicted)[1], upper_pi = PI(divorce_predicted)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx)) %&gt;% left_join(data_waffle %&gt;% mutate(row_idx = row_number()), . ) posterior_simmulation &lt;- sim(model_lds) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(data_waffle$divorce_std)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;divorce_predicted&quot;) %&gt;% group_by(row_idx) %&gt;% summarise(lower_pi = PI(divorce_predicted)[1], upper_pi = PI(divorce_predicted)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx)) %&gt;% left_join(data_waffle %&gt;% mutate(row_idx = row_number()), . ) ggplot(mapping = aes(x = divorce_std)) + geom_abline(slope = 1, size = .7, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(data = posterior_prediction, aes(ymin = lower_pi, ymax = upper_pi, color = Loc %in% c(&quot;ID&quot;, &quot;UT&quot;)))+ geom_point(data = posterior_prediction, aes(y = divorce_predicted_mean, color = Loc %in% c(&quot;ID&quot;, &quot;UT&quot;), fill = after_scale(clr_lighten(color ,.5))), shape = 21, size = 1.5)+ geom_text(data = posterior_prediction %&gt;% filter(Loc %in% c(&quot;ID&quot;, &quot;ME&quot;, &quot;RI&quot;, &quot;UT&quot;)), aes(x = divorce_std - .15, y = divorce_predicted_mean, label = Loc)) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) M5 dag1 &lt;- dagify( O ~ W + E + P, W ~ P, E ~ P, exposure = &quot;P&quot;, outcome = &quot;O&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,.5,1, .5), y = c(1,1, 1,.4))) %&gt;% mutate(stage = if_else(name == &quot;O&quot;, &quot;response&quot;, if_else(name %in% c(&quot;W&quot;, &quot;E&quot;, &quot;P&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag1, clr_in = clr3) + # plot_dag(dag2, clr_in = clr3) &amp; # scale_y_continuous(limits = c(.35, 1.05)) &amp; coord_equal() with \\(o\\) as obesity rate \\(p\\) as gasoline price \\(e\\) as money spend on eating out \\(w\\) as average distance walked \\[ \\begin{array}{cclr} o_i &amp; \\sim &amp; Normal(\\mu_i, \\sigma) &amp; \\textrm{[likelyhood]}\\\\ \\mu_i &amp; = &amp; \\alpha_{p} + \\beta_{p} p_i &amp; \\textrm{[linear model (price only)]}\\\\ \\mu_i &amp; = &amp; \\alpha_{w} + \\beta_{w} w_i &amp; \\textrm{[linear model (walking)]}\\\\ \\mu_i &amp; = &amp; \\alpha_{e} + \\beta_{e} e_i &amp; \\textrm{[linear model (eating out)]}\\\\ \\mu_i &amp; = &amp; \\alpha_{m} + \\beta_{pp} + \\beta_{ww} w_i + p_i + \\beta_{ee} e_i &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] H1 dagitty(&#39;dag{ M -&gt; A -&gt; D }&#39;) %&gt;% impliedConditionalIndependencies() #&gt; D _||_ M | A This reads as conditional on \\(A\\), \\(D\\) is independent from \\(M\\). given the results from model_multiple, this seems plausible as the multiple model greatly reduces the effect of beat_M: precis(model_multiple) %&gt;% round(digits = 2) %&gt;% as.matrix() %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.10 -0.16 0.16 beta_A -0.61 0.15 -0.85 -0.37 beta_M -0.07 0.15 -0.31 0.18 sigma 0.79 0.08 0.66 0.91 plot_coeftab(ct) + scale_color_manual(values = rep(clr0d, 3), guide = &quot;none&quot;) Actually this one is a markov equivalent of the dag investigated in the main text (and all members of that set are consistent with the model): dag_h1 &lt;- dagitty(&#39;dag{ M -&gt; A -&gt; D }&#39;) coordinates(dag_h1) &lt;- list( x = c( M = 0, A = 1, D = .5), y = c( M = 1, A = 1, D = .3)) dag_h1 %&gt;% node_equivalent_dags() %&gt;% mutate(stage = &quot;predictor&quot;) %&gt;% plot_dag() + coord_equal(xlim = c(-.1, 1.1), ylim = c(.2, 1.1))+ facet_wrap(~ dag) H2 model_counterfactual_marriage &lt;- quap( flist = alist( # A -&gt; D divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ), # M -&gt; A median_age_std ~ dnorm( mu_A, sigma_A ), mu_A &lt;- alpha_A + beta_MA * marriage_std, alpha_A ~ dnorm( 0, 0.2 ), beta_MA ~ dnorm( 0, 0.5 ), sigma_A ~ dexp(1) ), data = data_waffle ) precis(model_counterfactual_marriage) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.10 -0.16 0.16 beta_A -0.57 0.11 -0.74 -0.39 sigma 0.79 0.08 0.66 0.91 alpha_A 0.00 0.09 -0.14 0.14 beta_MA -0.69 0.10 -0.85 -0.54 sigma_A 0.68 0.07 0.57 0.79 M_seq &lt;- seq(-2, 2, length.out = 30) data_sim &lt;- sim(fit = model_counterfactual_marriage, data = tibble(marriage_std = M_seq), vars = c(&quot;median_age_std&quot;, &quot;divorce_std&quot;)) %&gt;% unpack_sim() data_sim_pi &lt;- data_sim %&gt;% group_by(row_idx, parameter) %&gt;% summarise(mean = mean(value), PI_lower = PI(value)[1], PI_upper = PI(value)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx), marriage_std = M_seq[row_idx]) %&gt;% arrange(parameter, marriage_std) data_sim_pi %&gt;% ggplot() + geom_smooth(aes(x = marriage_std, y = mean, ymin = PI_lower, ymax = PI_upper, color = parameter, fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(clr0d, clr3), guide = &quot;none&quot;) + # labs(y = &quot;counterfactual value&quot;, title = &quot;Counterfactual effects of age at marriage on&quot;) + facet_wrap(parameter ~ .) M_seq2 &lt;- c(data_waffle$median_age_std, data_waffle$median_age_std/2) m_rate_california &lt;- which(data_waffle$Location == &quot;Idaho&quot;) M_seq2 &lt;- c(data_waffle$median_age_std[m_rate_california], data_waffle$median_age_std[m_rate_california]/2) data_sim2 &lt;- sim(fit = model_counterfactual_marriage, data = tibble(marriage_std = M_seq2), vars = c(&quot;median_age_std&quot;, &quot;divorce_std&quot;)) %&gt;% data.frame() %&gt;% pivot_longer(cols = everything()) %&gt;% separate(name, into = c(&quot;param&quot;, &quot;rn&quot;), sep = &#39;\\\\.&#39;, convert = TRUE) %&gt;% mutate(group = c(&quot;org&quot;, &quot;half&quot;)[1 + (rn &gt; (length(M_seq2)/2))]) %&gt;% filter(param == &quot;divorce_std&quot;) %&gt;% dplyr::select(-rn) %&gt;% # mutate(value = value * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) %&gt;% pivot_wider(names_from = group, values_from = value) %&gt;% unnest() %&gt;% mutate(diff = half - org) data_sim2 %&gt;% ggplot(aes(x = diff)) + geom_density(fill = fll0, color = clr0d) data_sim2 %&gt;% ggplot() + geom_density(aes(x = org, color = &quot;orgiginal&quot;, fill = after_scale(clr_alpha(color)))) + geom_density(aes(x = half, color = &quot;half&quot;, fill = after_scale(clr_alpha(color)))) + scale_color_manual(values = c(original = clr0d, half = clr3)) + labs(x = &quot;divorce_std&quot;) + theme(legend.position = &quot;bottom&quot;) mean(data_sim2$diff) #&gt; [1] 0.4360082 Halfing a states marriage rate would on average increase the divorce rate by ~ 0 standard deviations. H3 dag1 &lt;- dagify( K ~ M + N, N ~ M, exposure = &quot;M&quot;, outcome = &quot;K&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,1,.5), y = c(1,1,.4))) %&gt;% mutate(stage = if_else(name == &quot;K&quot;, &quot;response&quot;, if_else(name %in% c(&quot;M&quot;, &quot;N&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag1, clr_in = clr3) + coord_equal() model_counterfactual_milk &lt;- quap( flist = alist( # M -&gt; K &lt;- N kcal_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_MK * mass_std + beta_NK * neocortex_std, alpha ~ dnorm( 0, 0.2 ), beta_MK ~ dnorm( 0, 0.5 ), beta_NK ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ), # M -&gt; N neocortex_std ~ dnorm( mu_N, sigma_N ), mu_N &lt;- alpha_N + beta_MN * mass_std, alpha_N ~ dnorm( 0, 0.2 ), beta_MN ~ dnorm( 0, 0.5 ), sigma_N ~ dexp(1) ), data = data_milk ) precis(model_counterfactual_milk) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.13 -0.20 0.20 beta_MK -0.75 0.23 -1.12 -0.37 beta_NK 0.64 0.23 0.27 1.01 sigma 0.69 0.12 0.49 0.88 alpha_N 0.00 0.12 -0.19 0.19 beta_MN 0.68 0.15 0.44 0.93 sigma_N 0.63 0.11 0.46 0.80 W_seq &lt;- seq(-2, 2, length.out = 30) data_sim &lt;- sim(fit = model_counterfactual_milk, data = tibble(mass_std = W_seq), vars = c(&quot;neocortex_std&quot;, &quot;kcal_std&quot;)) %&gt;% unpack_sim() data_sim_pi &lt;- data_sim %&gt;% group_by(row_idx, parameter) %&gt;% summarise(mean = mean(value), PI_lower = PI(value)[1], PI_upper = PI(value)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx), mass_std = M_seq[row_idx]) %&gt;% arrange(parameter, mass_std) data_sim_pi %&gt;% ggplot() + geom_smooth(aes(x = mass_std, y = mean, ymin = PI_lower, ymax = PI_upper, color = parameter, fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(clr0d, clr3), guide = &quot;none&quot;) + # labs(y = &quot;counterfactual value&quot;, title = &quot;Counterfactual effects of age at marriage on&quot;) + facet_wrap(parameter ~ .) M_seq2 &lt;- (log(c(15, 30)) - mean(log(milk$mass))) / sd(log(milk$mass)) data_sim2 &lt;- sim(fit = model_counterfactual_milk, data = tibble(mass_std = M_seq2), vars = c(&quot;neocortex_std&quot;, &quot;kcal_std&quot;)) %&gt;% data.frame() %&gt;% pivot_longer(cols = everything()) %&gt;% separate(name, into = c(&quot;param&quot;, &quot;rn&quot;), sep = &#39;\\\\.&#39;, convert = TRUE) %&gt;% mutate(group = c(&quot;org&quot;, &quot;double&quot;)[1 + (rn &gt; (length(M_seq2)/2))]) %&gt;% filter(param == &quot;kcal_std&quot;) %&gt;% dplyr::select(-rn) %&gt;% # mutate(value = value * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) %&gt;% pivot_wider(names_from = group, values_from = value) %&gt;% unnest() %&gt;% mutate(diff = double - org) data_sim2 %&gt;% ggplot(aes(x = diff)) + geom_density(fill = fll0, color = clr0d) data_sim2 %&gt;% ggplot() + geom_density(aes(x = org, color = &quot;orgiginal&quot;, fill = after_scale(clr_alpha(color)))) + geom_density(aes(x = double, color = &quot;double&quot;, fill = after_scale(clr_alpha(color)))) + scale_color_manual(values = c(original = clr0d, double = clr3)) + labs(x = &quot;kcal_std&quot;) + theme(legend.position = &quot;bottom&quot;) quantile(data_sim2$diff, probs = c(.05, .5, .95)) #&gt; 5% 50% 95% #&gt; -1.9762716 -0.1036398 1.7891183 mean(data_sim2$diff) #&gt; [1] -0.0910389 Following the paths of the dag to get the causal effect. To then get to the magnitude of the contrast, scale by max - min. prec_out &lt;- precis(model_counterfactual_milk) # ((M -&gt; N) * (M -&gt; K) ) + (M -&gt; K) * delta_input (prec_out[&quot;beta_MN&quot;, &quot;mean&quot;] * prec_out[&quot;beta_NK&quot;, &quot;mean&quot;] + prec_out[&quot;beta_MK&quot;, &quot;mean&quot;] ) * diff(M_seq2) #&gt; [1] -0.126499 H4 data_south &lt;- data_waffle %&gt;% dplyr::select(Location, South, ends_with(&quot;_std&quot;)) dag &lt;- dagify( D ~ M + A + S, M ~ A, A ~ S, exposure = &quot;A&quot;, outcome = &quot;M&quot;) %&gt;% tidy_dagitty(.dagitty = ., layout = tibble(x = c(0,.5, .5, 1), y = c(1, .6, 1.4, 1))) %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M&quot;, &quot;S&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag, clr_in = clr3) + scale_y_continuous(limits = c(.5, 1.5)) + coord_equal() dagitty(&#39;dag{ D &lt;- A -&gt; M; D &lt;- S -&gt; A; M -&gt; D }&#39;) %&gt;% impliedConditionalIndependencies() #&gt; M _||_ S | A model_south_multi &lt;- quap( flist = alist( marriage_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_SD * South + beta_AD * median_age_std, alpha ~ dnorm(0, .2), beta_SD ~ dnorm(0,.5), beta_AD ~ dnorm(0,.5), sigma ~ dexp(1) ), data = data_south ) precis(model_south_multi) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.04 0.10 -0.12 0.19 beta_SD -0.17 0.19 -0.48 0.14 beta_AD -0.71 0.10 -0.87 -0.56 sigma 0.68 0.07 0.57 0.78 M could be independent of S (large spread around zero) precis(model_south_multi)[&quot;beta_SD&quot;, ] %&gt;% round(digits = 2) #&gt; mean sd 5.5% 94.5% #&gt; beta_SD -0.17 0.19 -0.48 0.14 Additional scenario (from Jake Thompson) dag_coords &lt;- tibble(name = c(&quot;S&quot;, &quot;A&quot;, &quot;M&quot;, &quot;D&quot;), x = c(1, 1, 2, 3), y = c(3, 1, 2, 1)/2) dagify(D ~ A + M, M ~ A + S, A ~ S, coords = dag_coords) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M&quot;, &quot;S&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(.3, 1.7)) + coord_equal() div_dag &lt;- dagitty(&quot;dag{S -&gt; M -&gt; D; S -&gt; A -&gt; D; A -&gt; M}&quot;) impliedConditionalIndependencies(div_dag) #&gt; D _||_ S | A, M model_south_multi2 &lt;- quap( flist = alist( divorce_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_S * South + beta_A * median_age_std + beta_M * marriage_std, alpha ~ dnorm(0, .2), beta_S ~ dnorm(0,.5), beta_A ~ dnorm(0,.5), beta_M ~ dnorm(0,.5), sigma ~ dexp(1) ), data = data_south ) precis(model_south_multi2) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha -0.08 0.11 -0.25 0.09 beta_S 0.35 0.22 0.01 0.69 beta_A -0.56 0.15 -0.80 -0.32 beta_M -0.04 0.15 -0.28 0.19 sigma 0.76 0.08 0.64 0.88 precis(model_south_multi2)[&quot;beta_S&quot;, ] %&gt;% round(digits = 2) #&gt; mean sd 5.5% 94.5% #&gt; beta_S 0.35 0.22 0.01 0.69 6.6 {brms} section 6.6.1 Age at marriage Model Note the sample_prior = TRUE to also sample from the prior (as well as from the posterior). Prior samples are extracted with prior_draws(). brms_c5_model_age &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + median_age_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, sample_prior = TRUE, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_age&quot;) brms_age_prior &lt;- prior_draws(brms_c5_model_age) %&gt;% as_tibble() brms_age_prior %&gt;% slice_sample(n = 50) %&gt;% rownames_to_column(&quot;draw&quot;) %&gt;% expand(nesting(draw, Intercept, b), a = c(-2, 2)) %&gt;% mutate(d = Intercept + b * a) %&gt;% ggplot(aes(a,d, group = draw)) + geom_line(color = clr0d %&gt;% clr_alpha()) + labs(x = &quot;median_age_std&quot;, y = &quot;divorce_rate_std&quot;) Getting to the posterior predictions with fitted(): nd &lt;- tibble(median_age_std = seq(from = -3, to = 3.2, length.out = 30)) # now use `fitted()` to get the model-implied trajectories fitted(object = brms_c5_model_age, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = median_age_std)) + geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, color = clr0d, fill = fll0) + geom_point(data = data_waffle, aes(y = divorce_std), color = clr_dark )+ labs(x = &quot;median_age_std&quot;, y = &quot;divorce_rate_std&quot;) \\(\\rightarrow\\) The posterior for median_age_std (\\(\\beta_{age}\\)) is reliably negative (look at Estimate and 95% quantiles )… print(brms_c5_model_age) #&gt; Family: gaussian #&gt; Links: mu = identity; sigma = identity #&gt; Formula: divorce_std ~ 1 + median_age_std #&gt; Data: data_waffle (Number of observations: 50) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; Intercept 0.00 0.10 -0.20 0.20 1.00 3936 2758 #&gt; median_age_std -0.57 0.11 -0.79 -0.34 1.00 3906 3129 #&gt; #&gt; Family Specific Parameters: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; sigma 0.82 0.08 0.68 1.00 1.00 4302 3021 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). 6.6.2 Marriage rate Model brms_c5_model_marriage &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + marriage_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;brms/brms_c5_model_marriage&quot;) … smaller magnitude for the marriage rate model: print(brms_c5_model_marriage) #&gt; Family: gaussian #&gt; Links: mu = identity; sigma = identity #&gt; Formula: divorce_std ~ 1 + marriage_std #&gt; Data: data_waffle (Number of observations: 50) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; Intercept 0.00 0.11 -0.22 0.22 1.00 4602 2813 #&gt; marriage_std 0.35 0.13 0.09 0.61 1.00 4325 3000 #&gt; #&gt; Family Specific Parameters: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; sigma 0.95 0.10 0.78 1.16 1.00 4404 3059 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). nd &lt;- tibble(marriage_std = seq(from = -2.5, to = 3.5, length.out = 30)) # now use `fitted()` to get the model-implied trajectories fitted(object = brms_c5_model_marriage, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = marriage_std)) + geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, color = clr0d, fill = fll0) + geom_point(data = data_waffle, aes(y = divorce_std), color = clr_dark )+ labs(x = &quot;marriage_rate_std&quot;, y = &quot;divorce_rate_std&quot;) 6.6.3 Multiple regression brms_c5_model_multiple &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + marriage_std + median_age_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_multiple&quot;) print(brms_c5_model_multiple) #&gt; Family: gaussian #&gt; Links: mu = identity; sigma = identity #&gt; Formula: divorce_std ~ 1 + marriage_std + median_age_std #&gt; Data: data_waffle (Number of observations: 50) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; Intercept 0.00 0.10 -0.19 0.19 1.00 3829 2943 #&gt; marriage_std -0.06 0.16 -0.37 0.25 1.00 3291 2718 #&gt; median_age_std -0.60 0.16 -0.92 -0.29 1.00 2859 2440 #&gt; #&gt; Family Specific Parameters: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; sigma 0.83 0.09 0.68 1.02 1.00 3553 2380 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). mixedup::summarise_model(brms_c5_model_multiple) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 0.68 0.83 0.68 1.02 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; Intercept 0.00 0.10 -0.19 0.19 #&gt; marriage_std -0.06 0.16 -0.37 0.25 #&gt; median_age_std -0.60 0.16 -0.92 -0.29 bind_cols( as_draws_df(brms_c5_model_age) %&gt;% transmute(`brms_age-beta_age` = b_median_age_std), as_draws_df(brms_c5_model_marriage) %&gt;% transmute(`brms_marriage-beta_marriage` = b_marriage_std), as_draws_df(brms_c5_model_multiple) %&gt;% transmute(`brms_multi-beta_marriage` = b_marriage_std, `brms_multi-beta_age` = b_median_age_std) ) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% separate(col = name, into = c(&quot;fit&quot;, &quot;parameter&quot;), sep = &quot;-&quot;) %&gt;% ggplot(aes(x = mean, xmin = ll, xmax = ul, y = fit)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_pointrange(color = clr0d, fill = clr0, shape = 21) + facet_wrap(~ parameter, ncol = 1, labeller = label_parsed) + theme(axis.title = element_blank()) Simulating divorce data n &lt;- 50 sim_d &lt;- tibble(age = rnorm(n, mean = 0, sd = 1), mar = rnorm(n, mean = -age, sd = 1), div = rnorm(n, mean = age, sd = 1)) brms_c5_model_age_sim &lt;- update(brms_c5_model_age, newdata = sim_d, formula = div ~ 1 + age, seed = 42, file = &quot;brms/brms_c5_model_age_sim&quot;) brms_c5_model_marriage_sim &lt;- update(brms_c5_model_marriage, newdata = sim_d, formula = div ~ 1 + mar, seed = 42, file = &quot;brms/brms_c5_model_marriage_sim&quot;) brms_c5_model_multiple_sim &lt;- update(brms_c5_model_multiple, newdata = sim_d, formula = div ~ 1 + mar + age, seed = 42, file = &quot;brms/brms_c5_model_multiple_sim&quot;) bind_cols( as_draws_df(brms_c5_model_age_sim) %&gt;% transmute(`brms_age-beta_age` = b_age), as_draws_df(brms_c5_model_marriage_sim) %&gt;% transmute(`brms_marriage-beta_marriage` = b_mar), as_draws_df(brms_c5_model_multiple_sim) %&gt;% transmute(`brms_multi-beta_marriage` = b_mar, `brms_multi-beta_age` = b_age) ) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% separate(col = name, into = c(&quot;fit&quot;, &quot;parameter&quot;), sep = &quot;-&quot;) %&gt;% ggplot(aes(x = mean, xmin = ll, xmax = ul, y = fit)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_pointrange(color = clr0d, fill = clr0, shape = 21) + facet_wrap(~ parameter, ncol = 1, labeller = label_parsed) + theme(axis.title = element_blank()) 6.6.4 Multivariate Posteriors brms_c5_model_residuals_marriage &lt;- brm( data = data_waffle, family = gaussian, marriage_std ~ 1 + median_age_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_residuals_marriage&quot;) fitted(brms_c5_model_residuals_marriage) %&gt;% data.frame() %&gt;% bind_cols(data_waffle) %&gt;% as_tibble() %&gt;% ggplot(aes(x = median_age_std, y = marriage_std)) + geom_point(color = clr_dark) + geom_segment(aes(xend = median_age_std, yend = Estimate), size = .5, linetype = 3) + geom_line(aes(y = Estimate), color = clr0d) + ggrepel::geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;WY&quot;, &quot;ND&quot;, &quot;ME&quot;, &quot;HI&quot;, &quot;DC&quot;)), aes(label = Loc), size = 3, seed = 14, family = fnt_sel) + labs(x = &quot;median_age_std&quot;, y = &quot;marriage_std&quot;) residual_data &lt;- residuals(brms_c5_model_residuals_marriage) %&gt;% as_tibble() %&gt;% bind_cols(data_waffle) brms_c5_model_residuals_data &lt;- brm( data = residual_data, family = gaussian, divorce_std ~ 1 + Estimate, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_residuals_data&quot;) nd &lt;- tibble(Estimate = seq(from = -2, to = 2, length.out = 30)) residuals_intervals &lt;- fitted(object = brms_c5_model_residuals_data, newdata = nd) %&gt;% as_tibble() %&gt;% rename(mean = &quot;Estimate&quot;) %&gt;% bind_cols(nd) residual_data %&gt;% ggplot(aes(x = Estimate, y = divorce_std)) + geom_smooth(data = residuals_intervals, aes(y = mean, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, color = clr0d, fill = fll0) + geom_vline(xintercept = 0, linetype = 3, color = clr_dark) + geom_point(color = clr_dark) + ggrepel::geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;WY&quot;, &quot;ND&quot;, &quot;ME&quot;, &quot;HI&quot;, &quot;DC&quot;)), aes(label = Loc), size = 3, seed = 5, family = fnt_sel) Don’t use residuals as input data for another model - this ignores a ton of uncertainty: residual_data %&gt;% ggplot(aes(x = Estimate, y = divorce_std)) + geom_vline(xintercept = 0, linetype = 3, color = clr_dark) + geom_pointrange(aes(xmin = `Q2.5`, xmax = `Q97.5`), color = clr0d, fill = clr0, shape = 21) + ggrepel::geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;RI&quot;, &quot;ME&quot;, &quot;UT&quot;, &quot;ID&quot;)), aes(label = Loc), size = 3, seed = 5, family = fnt_sel) Posterior prediction plot: fitted(brms_c5_model_multiple) %&gt;% as_tibble() %&gt;% bind_cols(data_waffle) %&gt;% ggplot(aes(x = divorce_std, y = Estimate)) + geom_abline(slope = 1, linetype = 3, color = clr_dark) + geom_pointrange(aes(ymin = `Q2.5`, ymax = `Q97.5`), color = clr0d, fill = clr0, shape = 21) + ggrepel::geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;RI&quot;, &quot;ME&quot;, &quot;UT&quot;, &quot;ID&quot;)), aes(label = Loc), size = 3, seed = 5, family = fnt_sel) brms_c5_model_spurious &lt;- brm( data = data_spurious, family = gaussian, y ~ 1 + x_real + x_spur, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_spurious&quot;) mixedup::extract_fixef(brms_c5_model_spurious) #&gt; # A tibble: 3 × 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Intercept 0.05 0.096 -0.135 0.242 #&gt; 2 x_real 0.902 0.146 0.619 1.18 #&gt; 3 x_spur 0.094 0.108 -0.113 0.309 6.6.5 Counterfactual plots At this point, it’s important to recognize we have two regression models. As a first step, we might specify each model separately in a bf() function and save them as objects (Estimating multivariate models with brms). divorce_model &lt;- bf(divorce.std ~ 1 + median.age.std + marriage.std) marriage_model &lt;- bf(marriage.std ~ 1 + median.age.std) divorce_model &lt;- bf(divorcestd ~ 1 + medianagestd + marriagestd) marriage_model &lt;- bf(marriagestd ~ 1 + medianagestd) Next we will combine our bf() objects with the + operator within the brm() function. For a model like this, we also specify set_rescor(FALSE) to prevent brms from adding a residual correlation between d and m. Also, notice how each prior statement includes a resp argument. This clarifies which sub-model the prior refers to. # can&#39;t use _ or . in column names in this context data_waffle_short &lt;- data_waffle %&gt;% set_names(nm = names(data_waffle) %&gt;% str_remove_all(&quot;_&quot;)) brms_c5_model_counterfactual &lt;- brm( data = data_waffle_short, family = gaussian, divorce_model + marriage_model + set_rescor(FALSE), prior = c(prior(normal(0, 0.2), class = Intercept, resp = divorcestd), prior(normal(0, 0.5), class = b, resp = divorcestd), prior(exponential(1), class = sigma, resp = divorcestd), prior(normal(0, 0.2), class = Intercept, resp = marriagestd), prior(normal(0, 0.5), class = b, resp = marriagestd), prior(exponential(1), class = sigma, resp = marriagestd)), chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_counterfactual&quot;) print(brms_c5_model_counterfactual) #&gt; Family: MV(gaussian, gaussian) #&gt; Links: mu = identity; sigma = identity #&gt; mu = identity; sigma = identity #&gt; Formula: divorcestd ~ 1 + medianagestd + marriagestd #&gt; marriagestd ~ 1 + medianagestd #&gt; Data: data_waffle_short (Number of observations: 50) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS #&gt; divorcestd_Intercept -0.00 0.10 -0.20 0.19 1.00 5659 #&gt; marriagestd_Intercept -0.00 0.09 -0.18 0.17 1.00 5705 #&gt; divorcestd_medianagestd -0.61 0.16 -0.90 -0.29 1.00 3490 #&gt; divorcestd_marriagestd -0.06 0.15 -0.36 0.25 1.00 3354 #&gt; marriagestd_medianagestd -0.69 0.10 -0.89 -0.49 1.00 4910 #&gt; Tail_ESS #&gt; divorcestd_Intercept 2695 #&gt; marriagestd_Intercept 3063 #&gt; divorcestd_medianagestd 2968 #&gt; divorcestd_marriagestd 2948 #&gt; marriagestd_medianagestd 3278 #&gt; #&gt; Family Specific Parameters: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; sigma_divorcestd 0.83 0.09 0.68 1.02 1.00 5112 3245 #&gt; sigma_marriagestd 0.71 0.08 0.58 0.89 1.00 4852 2896 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). nd &lt;- tibble(medianagestd = seq(from = -2, to = 2, length.out = 30), marriagestd = 0) predict(brms_c5_model_counterfactual, resp = &quot;divorcestd&quot;, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = medianagestd, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_smooth(stat = &quot;identity&quot;, fill = fll0, color = clr0d, size = .2) + labs(subtitle = &quot;Total counterfactual effect of A on D&quot;, x = &quot;manipulated median_age_std&quot;, y = &quot;counterfactual divorce_std&quot;) nd &lt;- tibble(marriagestd = seq(from = -2, to = 2, length.out = 30), medianagestd = 0) predict(brms_c5_model_counterfactual, resp = &quot;divorcestd&quot;, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = marriagestd, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_smooth(stat = &quot;identity&quot;, fill = fll0, color = clr0d, size = .2) + labs(subtitle = &quot;Total counterfactual effect of M on D&quot;, x = &quot;manipulated marriage_std&quot;, y = &quot;counterfactual divorce_std&quot;) 6.6.6 Masked Relationships brms_c5_model_milk_draft &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 1 + neocortex_std, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, sample_prior = TRUE, file = &quot;brms/brms_c5_model_milk_draft&quot;) set.seed(42) prior_draws(brms_c5_model_milk_draft) %&gt;% slice_sample(n = 50) %&gt;% rownames_to_column() %&gt;% expand(nesting(rowname, Intercept, b), neocortex_std = c(-2, 2)) %&gt;% mutate(kcal_std = Intercept + b * neocortex_std) %&gt;% ggplot(aes(x = neocortex_std, y = kcal_std)) + geom_line(aes(group = rowname), color = clr0d %&gt;% clr_alpha()) + coord_cartesian(ylim = c(-2, 2)) + labs(x = &quot;neocortex_std&quot;, y = &quot;kcal_std&quot;, subtitle = &quot;Intercept ~ dnorm(0, 1); b ~ dnorm(0, 1)&quot;) brms_c5_model_milk_cortex &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 1 + neocortex_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, sample_prior = TRUE, file = &quot;brms/brms_c5_model_milk_cortex&quot;) set.seed(42) prior_draws(brms_c5_model_milk_cortex) %&gt;% slice_sample(n = 50) %&gt;% rownames_to_column() %&gt;% expand(nesting(rowname, Intercept, b), neocortex_std = c(-2, 2)) %&gt;% mutate(kcal_std = Intercept + b * neocortex_std) %&gt;% ggplot(aes(x = neocortex_std, y = kcal_std)) + geom_line(aes(group = rowname), color = clr0d %&gt;% clr_alpha()) + coord_cartesian(ylim = c(-2, 2)) + labs(x = &quot;neocortex_std&quot;, y = &quot;kcal_std&quot;, subtitle = &quot;Intercept ~ dnorm(0, 0.2); b ~ dnorm(0, 0.5)&quot;) bind_rows( as_draws_df(brms_c5_model_milk_draft) %&gt;% select(b_Intercept:sigma), as_draws_df(brms_c5_model_milk_cortex) %&gt;% select(b_Intercept:sigma) ) %&gt;% mutate(fit = rep(c(&quot;milk_draft&quot;, &quot;milk_cortex&quot;), each = n() / 2)) %&gt;% pivot_longer(-fit, names_to = &quot;parameter&quot;) %&gt;% group_by(parameter, fit) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% mutate(fit = factor(fit, levels = c(&quot;milk_draft&quot;, &quot;milk_cortex&quot;))) %&gt;% ggplot(aes(x = mean, xmin = ll, xmax = ul, y = fit)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_pointrange(color = clr0d, fill = clr0, shape = 21) + facet_wrap(~ parameter, ncol = 1) + theme(axis.title = element_blank()) nd &lt;- tibble(neocortex_std = seq(from = -2.5, to = 2, length.out = 30)) fitted(brms_c5_model_milk_cortex, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = neocortex_std, y = Estimate, ymin = Q25, ymax = Q75)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = fll0) + geom_smooth(stat = &quot;identity&quot;, fill = fll0, color = clr0d, size = .2) + geom_point(data = data_milk, aes(x = neocortex_std, y = kcal_std), inherit.aes = FALSE, color = clr_dark) + labs(y = &#39;kcal_std&#39;) brms_c5_model_milk_weight &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 1 + mass_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, sample_prior = TRUE, file = &quot;brms/brms_c5_model_milk_weight&quot;) nd &lt;- tibble(mass_std = seq(from = -2.5, to = 2.5, length.out = 30)) fitted(brms_c5_model_milk_weight, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = mass_std, y = Estimate, ymin = Q25, ymax = Q75)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = fll0) + geom_smooth(stat = &quot;identity&quot;, fill = fll0, color = clr0d, size = .2) + geom_point(data = data_milk, aes(x = mass_std, y = kcal_std), inherit.aes = FALSE, color = clr_dark) + labs(y = &#39;kcal_std&#39;) brms_c5_model_milk_multi &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 1 + neocortex_std + mass_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_milk_multi&quot;) bind_cols( as_draws_df(brms_c5_model_milk_cortex) %&gt;% transmute(`cortex-beta_N` = b_neocortex_std), as_draws_df(brms_c5_model_milk_weight) %&gt;% transmute(`weight-beta_M` = b_mass_std), as_draws_df(brms_c5_model_milk_multi) %&gt;% transmute(`multi-beta_N` = b_neocortex_std, `multi-beta_M` = b_mass_std) ) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% separate(name, into = c(&quot;fit&quot;, &quot;parameter&quot;), sep = &quot;-&quot;) %&gt;% ggplot(aes(x = mean, y = fit, xmin = ll, xmax = ul)) + geom_pointrange(color = clr0d, fill = clr0, shape = 21) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + ylab(NULL) + facet_wrap(~ parameter, ncol = 1) nd &lt;- tibble(neocortex_std = seq(from = -2.5, to = 2, length.out = 30), mass_std = 0) fitted(brms_c5_model_milk_multi, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = neocortex_std, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = fll0) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = fll0, color = clr0d, size = .2) + labs(subtitle = &quot;Counterfactual holding M = 0&quot;, x = &quot;neocortex_std&quot;, y = &quot;kcal_std&quot;) nd &lt;- tibble(mass_std = seq(from = -2.5, to = 2.5, length.out = 30), neocortex_std = 0) fitted(brms_c5_model_milk_multi, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = mass_std, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = fll0) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = fll0, color = clr0d, size = .2) + labs(subtitle = &quot;Counterfactual holding M = 0&quot;, x = &quot;mass_std&quot;, y = &quot;kcal_std&quot;) brms_c5_model_milk_multi_sim &lt;- update( brms_c5_model_milk_multi, newdata = data_milk_sim1, formula = kcal_std ~ 1 + neocortex_std + mass_std, seed = 42, file = &quot;brms/brms_c5_model_milk_multi_sim&quot;) brms_c5_model_milk_cortex_sim &lt;- update( brms_c5_model_milk_cortex, formula = kcal_std ~ 1 + neocortex_std, seed = 42, file = &quot;brms/brms_c5_model_milk_cortex_sim&quot;) brms_c5_model_milk_weight_sim &lt;- update( brms_c5_model_milk_weight, formula = kcal_std ~ 1 + mass_std, seed = 42, file = &quot;brms/brms_c5_model_milk_weight_sim&quot;) mixedup::extract_fixef(brms_c5_model_milk_cortex_sim) #&gt; # A tibble: 2 × 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Intercept 0.003 0.162 -0.324 0.321 #&gt; 2 neocortex_std 0.123 0.231 -0.325 0.584 mixedup::extract_fixef(brms_c5_model_milk_weight_sim) #&gt; # A tibble: 2 × 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Intercept 0.005 0.152 -0.293 0.307 #&gt; 2 mass_std -0.283 0.221 -0.708 0.158 mixedup::extract_fixef(brms_c5_model_milk_multi_sim) #&gt; # A tibble: 3 × 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Intercept -0.047 0.081 -0.208 0.112 #&gt; 2 neocortex_std 0.982 0.096 0.794 1.17 #&gt; 3 mass_std -1.04 0.118 -1.26 -0.808 6.6.7 Categorical Variables 6.6.7.1 Binary Categories For an indicator variable, we need this to be a factor(): data_height &lt;- data_height %&gt;% mutate(sex = factor(sex)) brms_c5_model_height &lt;- brm( data = data_height, family = gaussian, height ~ 0 + sex, prior = c(prior(normal(178, 20), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_height&quot;) contrasts with {brms} library(tidybayes) as_draws_df(brms_c5_model_height) %&gt;% mutate(diff_fm = b_sex1 - b_sex2) %&gt;% gather(key, value, -`lp__`) %&gt;% group_by(key) %&gt;% mean_qi(value, .width = .89) %&gt;% filter(!grepl(key, pattern = &quot;^\\\\.&quot;)) %&gt;% knitr::kable() key value .lower .upper .width .point .interval b_sex1 134.901752 132.38783 137.448902 0.89 mean qi b_sex2 142.593033 139.91077 145.293809 0.89 mean qi diff_fm -7.691281 -11.49839 -3.924036 0.89 mean qi sigma 26.767597 25.51963 28.079138 0.89 mean qi 6.6.7.2 Many Categories brms_c5_model_milk_clade &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 0 + clade, prior = c(prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_milk_clade&quot;) library(bayesplot) (mcmc_intervals_data(brms_c5_model_milk_clade, prob = .5) %&gt;% filter(grepl(parameter, pattern = &quot;^b&quot;)) %&gt;% ggplot(aes(y = parameter)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_linerange(aes(xmin = ll, xmax = hh), lwd = .2, color = clr2) + geom_pointrange(aes(xmin = l, x = m, xmax = h), lwd = .7, shape = 21, color = clr2, fill = clr_lighten(clr2,.2))) + theme(axis.title = element_blank()) as_draws_df(brms_c5_model_milk_clade) %&gt;% select(starts_with(&quot;b&quot;)) %&gt;% as_tibble() %&gt;% set_names(x = . , nm = names(.) %&gt;% str_remove(&quot;b_clade&quot;)) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = reorder(name, value))) + # note how we used `reorder()` to arrange the coefficients geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_pointinterval(point_interval = mode_hdi, .width = .89, size = 2, shape = 21, color = clr2, fill = clr_lighten(clr2,.2)) + labs(title = &quot;My tidybayes-based coefficient plot&quot;, x = &quot;expected kcal (std)&quot;, y = NULL) naïve {brms} model fit: brms_c5_model_milk_house &lt;- brm( data = data_milk_clade, family = gaussian, kcal_std ~ 0 + clade + house, prior = c(prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_milk_house&quot;) \\(\\rightarrow\\) there are only three house levels 🤨. mixedup::extract_fixef(brms_c5_model_milk_house) #&gt; # A tibble: 7 × 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 cladeApe -0.431 0.261 -0.932 0.082 #&gt; 2 cladeNewWorldMonkey 0.326 0.253 -0.173 0.824 #&gt; 3 cladeOldWorldMonkey 0.497 0.286 -0.075 1.04 #&gt; 4 cladeStrepsirrhine -0.504 0.294 -1.04 0.088 #&gt; 5 houseHufflepuff -0.175 0.285 -0.742 0.378 #&gt; 6 houseRavenclaw -0.129 0.278 -0.667 0.413 #&gt; 7 houseSlytherin 0.489 0.293 -0.109 1.04 precis(model_milk_house, depth = 2) %&gt;% as.matrix() %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha_clade[1] -0.4205362 0.2603510 -0.8366273 -0.0044451 alpha_clade[2] 0.3836736 0.2596808 -0.0313464 0.7986937 alpha_clade[3] 0.5664463 0.2890333 0.1045153 1.0283773 alpha_clade[4] -0.5055652 0.2966455 -0.9796621 -0.0314684 alpha_house[1] -0.1025635 0.2617090 -0.5208251 0.3156981 alpha_house[2] -0.1996998 0.2754408 -0.6399074 0.2405079 alpha_house[3] -0.1603306 0.2690551 -0.5903326 0.2696713 alpha_house[4] 0.4866255 0.2875133 0.0271236 0.9461274 sigma 0.6631322 0.0881257 0.5222904 0.8039741 But there is no overall intercept, α, that stands for the expected value when all the predictors are set to 0. When we use the typical formula syntax with brms, we can suppress the overall intercept when for a single index variable with the &lt;criterion&gt; ~ 0 + &lt;index variable&gt; syntax. That’s exactly what we did with our b5.9 model. The catch is this approach only works with one index variable within brms. Even though we suppressed the default intercept with our formula, kcal_std ~ 0 + clade + house, we ended up loosing the first category of the second variable, house. […] The solution is the use the non-linear syntax. brms_c5_model_milk_house_correct_index &lt;- brm(data = data_milk_clade, family = gaussian, bf(kcal_std ~ 0 + a + h, a ~ 0 + clade, h ~ 0 + house, nl = TRUE), prior = c(prior(normal(0, 0.5), nlpar = a), prior(normal(0, 0.5), nlpar = h), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_milk_house_correct_index&quot;) mixedup::extract_fixef(brms_c5_model_milk_house_correct_index) #&gt; # A tibble: 8 × 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 a_cladeApe -0.395 0.28 -0.936 0.146 #&gt; 2 a_cladeNewWorldMonkey 0.363 0.28 -0.183 0.902 #&gt; 3 a_cladeOldWorldMonkey 0.527 0.307 -0.112 1.11 #&gt; 4 a_cladeStrepsirrhine -0.455 0.321 -1.10 0.167 #&gt; 5 h_houseGryffindor -0.097 0.284 -0.658 0.445 #&gt; 6 h_houseHufflepuff -0.196 0.298 -0.771 0.396 #&gt; 7 h_houseRavenclaw -0.159 0.285 -0.715 0.39 #&gt; 8 h_houseSlytherin 0.468 0.31 -0.138 1.07 as_draws_df(brms_c5_model_milk_house_correct_index) %&gt;% pivot_longer(starts_with(&quot;b_&quot;)) %&gt;% mutate(name = str_remove(name, &quot;b_&quot;) %&gt;% str_remove(., &quot;clade&quot;) %&gt;% str_remove(., &quot;house&quot;) %&gt;% str_replace(., &quot;World&quot;, &quot; World &quot;)) %&gt;% separate(name, into = c(&quot;predictor&quot;, &quot;level&quot;), sep = &quot;_&quot;) %&gt;% mutate(predictor = if_else(predictor == &quot;a&quot;, &quot;clade&quot;, &quot;house&quot;)) %&gt;% ggplot(aes(x = value, y = reorder(level, value))) + # note how we used `reorder()` to arrange the coefficients geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_pointinterval(point_interval = mode_hdi, .width = .89, size = 2, color = clr0d, fill = clr0, shape = 21 ) + labs(x = &quot;expected_kcal_std&quot;, y = NULL) + facet_wrap(~ predictor, scales = &quot;free_y&quot;) 6.6.8 Alternative ways to model multiple categories 6.6.8.1 Contrast Coding data_contrast &lt;- data_height %&gt;% mutate(sex_c = if_else(sex == &quot;1&quot;, -0.5, 0.5)) brms_c5_model_height_contrast &lt;- brm( data = data_contrast, family = gaussian, height ~ 1 + sex_c, prior = c(prior(normal(178, 20), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_height_contrast&quot;) Our posterior for \\(\\alpha\\), above, is designed to capture the average_of_the_group_means_in_height, not mean_height. In cases where the sample sizes in the two groups were equal, these two would be same. Since we have different numbers of males and females in our data, the two values differ a bit as_draws_df(brms_c5_model_height_contrast) %&gt;% mutate(male = b_Intercept - b_sex_c * 0.5, female = b_Intercept + b_sex_c * 0.5, `female - male` = b_sex_c) %&gt;% pivot_longer(male:`female - male`) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(.width = .95, shape = 21, fill = fll0, color = clr0d, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;height&quot;) + facet_wrap(~ name, scales = &quot;free&quot;) 6.6.9 Multilevel ANOVA (This might make sense after reading Chapter 13…) \\[ \\begin{array}{ccccr} K_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; &amp;\\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + u_{j[i]} &amp; &amp;\\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.5) &amp; &amp;\\textrm{[$\\alpha$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; &amp;\\textrm{[$\\sigma$ prior]} \\\\ u_{j[i]} &amp; \\sim &amp; Normal(0, \\sigma_{CLADE}) &amp; \\textrm{for}~j = 1..4 &amp;\\textrm{[u prior]}\\\\ \\sigma_{CLADE} &amp; \\sim &amp; Exponential(1) &amp; &amp;\\textrm{[$\\sigma_{CLADE}$ prior]} \\\\ \\end{array} \\] the four clade-specific deviations from that mean are captured by the four levels of \\(u_j\\), which are themselves modeled as normally distributed with a mean of zero (because they are deviations, after all) and a standard deviation \\(\\sigma_{CLADE}\\) brms_c5_model_milk_anova &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 1 + (1 | clade), prior = c(prior(normal(0, 0.5), class = Intercept), prior(exponential(1), class = sigma), prior(exponential(1), class = sd)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;brms/brms_c5_model_milk_anova&quot;) as_draws_df(brms_c5_model_milk_anova) %&gt;% mutate(Ape = b_Intercept + `r_clade[Ape,Intercept]`, `New World Monkey` = b_Intercept + `r_clade[New.World.Monkey,Intercept]`, `Old World Monkey` = b_Intercept + `r_clade[Old.World.Monkey,Intercept]`, Strepsirrhine = b_Intercept + `r_clade[Strepsirrhine,Intercept]`) %&gt;% pivot_longer(Ape:Strepsirrhine) %&gt;% ggplot(aes(x = value, y = reorder(name, value))) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_pointinterval(point_interval = mode_hdi, .width = .89, size = 2, color = clr0d, fill = clr0, shape = 21 ) + labs(x = &quot;expected_kcal_std&quot;, y = NULL) 6.7 pymc3 section × "],["rethinking-chapter-6.html", "7 Rethinking: Chapter 6 7.1 Multicolliniarity 7.2 Post-treatment bias 7.3 Collider Bias 7.4 Homework 7.5 {brms} section 7.6 pymc3 section", " 7 Rethinking: Chapter 6 The Haunted DAG &amp; Causal Terror by Richard McElreath, building on the Summaries by Solomon Kurz and Jake Thompson. Simulating section-distortion (Berkson’s paradox) n &lt;- 200 p &lt;- .1 set.seed(42) data_sim &lt;- tibble(newsworthy = rnorm(n), trustworthy = rnorm(n), score = newsworthy + trustworthy, treshold = quantile(score, 1 - p), selected = score &gt;= treshold) data_sim %&gt;% ggplot(aes(x = newsworthy, y = trustworthy, color = selected)) + geom_smooth(data = data_sim %&gt;% filter(selected), method = &quot;lm&quot;, se = FALSE, fullrange = TRUE, size = .5) + geom_point(aes(fill = after_scale(clr_alpha(color))), shape = 21, size = 2.5) + scale_color_manual(values = c(`TRUE` = clr1, `FALSE` = clr0d))+ coord_cartesian(xlim = range(data_sim$newsworthy) * 1.05, ylim = range(data_sim$trustworthy) * 1.05, expand = 0) + coord_equal(ylim = range(data_sim$trustworthy) * 1.05) + theme(legend.position = &quot;bottom&quot;) 7.1 Multicolliniarity Simulating multicollinear legs library(rethinking) n &lt;- 100 set.seed(909) data_legs &lt;- tibble( height = rnorm(n = n, mean = 10, sd = 2), leg_proportion = runif(n, min = 0.4, max = 0.5), left_leg = leg_proportion * height + rnorm(n, 0, .02), right_leg = leg_proportion * height + rnorm(n, 0, .02), ) model_legs_multicollinear &lt;- quap( flist = alist( height ~ dnorm(mu, sigma), mu &lt;- alpha + beta_left * left_leg + beta_right * right_leg, alpha ~ dnorm(10, 100), beta_left ~ dnorm(2, 10), beta_right ~ dnorm(2, 10), sigma ~ dexp(1) ), data = data_legs ) precis(model_legs_multicollinear) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.98 0.28 0.53 1.44 beta_left 0.21 2.53 -3.83 4.25 beta_right 1.78 2.53 -2.26 5.83 sigma 0.62 0.04 0.55 0.69 precis(model_legs_multicollinear, depth = 2) %&gt;% as_tibble_rn() %&gt;% ggplot(aes(y = param)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(aes(xmin = `5.5%`, xmax =`94.5%`), color = clr0d) + geom_point(aes(x = mean), shape = 21, size = 3 , color = clr0d, fill = clr0) + scale_y_discrete(limits = c(&quot;sigma&quot;, &quot;beta_right&quot;, &quot;beta_left&quot;, &quot;alpha&quot;)) + theme(axis.title.y = element_blank()) leg_posterior_samples &lt;- extract.samples(model_legs_multicollinear) %&gt;% as_tibble() p_cor &lt;- leg_posterior_samples %&gt;% ggplot(aes(x = beta_right, y = beta_left)) + geom_point(color = clr0d, fill = clr0, shape = 21, alpha = .5) p_sum &lt;- leg_posterior_samples %&gt;% ggplot(aes(x = beta_right + beta_left)) + geom_vline(xintercept = 1/mean(data_legs$leg_proportion), color = clr_dark, linetype = 3) + geom_density(color = clr0d, fill = clr0, alpha = .5, adjust = .4) p_cor + p_sum Milk example data(milk) data_milk &lt;- milk %&gt;% as_tibble() %&gt;% drop_na(kcal.per.g:perc.lactose) %&gt;% mutate(across(where(is.double), standardize, .names = &quot;{str_remove(str_remove(.col,&#39;perc.&#39;),&#39;.per.g&#39;)}_std&quot;)) Model fat only model_milk_fat &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_fat * fat_std, alpha ~ dnorm(0,.2), beta_fat ~ dnorm(0,.5), sigma ~ dexp(1) ), data = data_milk ) precis(model_milk_fat) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.08 -0.12 0.12 beta_fat 0.86 0.08 0.73 1.00 sigma 0.45 0.06 0.36 0.54 Model lactose only model_milk_lactose &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_lactose * lactose_std, alpha ~ dnorm(0,.2), beta_lactose ~ dnorm(0,.5), sigma ~ dexp(1) ), data = data_milk ) precis(model_milk_lactose) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.07 -0.11 0.11 beta_lactose -0.90 0.07 -1.02 -0.79 sigma 0.38 0.05 0.30 0.46 Multicollinear model model_milk_multicollinear &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_fat * fat_std + beta_lactose * lactose_std, alpha ~ dnorm(0,.2), beta_fat ~ dnorm(0,.5), beta_lactose ~ dnorm(0,.5), sigma ~ dexp(1) ), data = data_milk ) precis(model_milk_multicollinear) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.07 -0.11 0.11 beta_fat 0.24 0.18 -0.05 0.54 beta_lactose -0.68 0.18 -0.97 -0.38 sigma 0.38 0.05 0.30 0.46 data_milk %&gt;% dplyr::select(kcal_std, fat_std, lactose_std) %&gt;% ggpairs( lower = list(continuous = wrap(ggally_points, colour = clr0d, size = 1.5, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll0, color = clr0d, adjust = .9)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;)) ) dagify(K ~ L + F, L ~ D, F ~ D, coords = tibble(name = c(&quot;K&quot;, &quot;L&quot;, &quot;F&quot;, &quot;D&quot;), x = c(.5, 0, 1, .5), y = c(.6, 1, 1, 1))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;K&quot;, &quot;response&quot;, if_else(name %in% c(&quot;L&quot;, &quot;F&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(.55, 1.05)) + scale_x_continuous(limits = c(-.05, 1.05)) + coord_equal() Simulating Multicollinearity simluate_collinearity &lt;- function(seed = 42, r = .9, data = data_milk ){ data &lt;- data %&gt;% mutate(x = rnorm(n = nrow(cur_data()), mean = `perc.fat` * r, sd = sqrt((1 - r ^ 2) * var(`perc.fat`)))) mod &lt;- lm(kcal.per.g ~ perc.fat + x, data = data) sqrt( diag( vcov(mod) ))[2] } # reapeat_simulation &lt;- function(r = .9, n = 100){ # stddev &lt;- replicate( n, simluate_collinearity(r)) # tibble(r = r, stddev_mean = mean(stddev), stddev_sd = sd(stddev)) # } n_seed &lt;- 100 n_rho &lt;- 60 simulation_means &lt;- crossing(seed = 1:n_seed, rho = seq(from = 0, to = .99, length.out = n_rho)) %&gt;% mutate(parameter_sd = purrr::map2_dbl(seed, rho, simluate_collinearity)) %&gt;% group_by(rho) %&gt;% summarise(mean = mean(parameter_sd), ll = quantile(parameter_sd, prob = .025), ul = quantile(parameter_sd, prob = .975)) simulation_means %&gt;% ggplot(aes(x = rho, y = mean, ymin = ll, ymax = ul)) + geom_smooth(stat = &#39;identity&#39;, size = .6, color = clr0d, fill = fll0) 7.2 Post-treatment bias Simulating fungus data n &lt;- 100 set.seed(71) data_fungus &lt;- tibble( h_0 = rnorm(n, 10, 2), treatment = rep(0:1, each = n/2), fungus = rbinom(n = n, size = 1, prob = .5 - treatment * .4 ), h_1 = h_0 + rnorm(n, 5 - 3 * fungus) ) precis(data_fungus) %&gt;% knit_precis() param mean sd 5.5% 94.5% histogram h_0 9.96 2.10 6.57 13.08 ▁▂▂▂▇▃▂▃▁▁▁▁ treatment 0.50 0.50 0.00 1.00 ▇▁▁▁▁▁▁▁▁▇ fungus 0.23 0.42 0.00 1.00 ▇▁▁▁▁▁▁▁▁▂ h_1 14.40 2.69 10.62 17.93 ▁▁▃▇▇▇▁▁ selecting a prior precis(tibble(sim_p = rlnorm(1e4, 0, .25))) %&gt;% knit_precis() param mean sd 5.5% 94.5% histogram sim_p 1.04 0.26 0.67 1.5 ▁▁▃▇▇▃▁▁▁▁▁▁ \\[ \\begin{array}{rclr} h_{1,i} &amp; \\sim &amp; Normal( \\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; h_{0,i} \\times p &amp; \\textrm{[linear model]}\\\\ p &amp; \\sim &amp; Log-Normal(0, 0.25) &amp; \\textrm{[$p$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] \\(\\rightarrow\\) the main mass of the prior is between 40% shrinkage and 50% growth. Model without treatment model_fungus_no_treatment &lt;- quap( flist = alist( h_1 ~ dnorm( mu, sigma ), mu &lt;- h_0 * p, p ~ dlnorm( 0, .25 ), sigma ~ dexp( 1 ) ), data = data_fungus ) precis(model_fungus_no_treatment) %&gt;% knit_precis() param mean sd 5.5% 94.5% p 1.43 0.02 1.40 1.45 sigma 1.79 0.13 1.59 1.99 Model with treatment and fungus (post-treatment variable) \\[ \\begin{array}{rclr} h_{1,i} &amp; \\sim &amp; Normal( \\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; h_{0,i} \\times p &amp; \\textrm{[linear model]}\\\\ p &amp; = &amp; \\alpha + \\beta_{T} T_{i} + \\beta_{F} F_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Log-Normal(0, 0.25) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{T} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_{T}$ prior]}\\\\ \\beta_{F} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_{F}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_fungus_post_treatment &lt;- quap( flist = alist( h_1 ~ dnorm( mu, sigma ), mu &lt;- h_0 * p, p &lt;- alpha + beta_treatment * treatment + beta_fungus * fungus, alpha ~ dlnorm( 0, .2 ), beta_treatment ~ dnorm(0,.5), beta_fungus ~ dnorm(0,.5), sigma ~ dexp( 1 ) ), data = data_fungus ) precis(model_fungus_post_treatment) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 1.48 0.02 1.44 1.52 beta_treatment 0.00 0.03 -0.05 0.05 beta_fungus -0.27 0.04 -0.33 -0.21 sigma 1.41 0.10 1.25 1.57 Model with treatment but without fungus \\[ \\begin{array}{rclr} h_{1,i} &amp; \\sim &amp; Normal( \\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; h_{0,i} \\times p &amp; \\textrm{[linear model]}\\\\ p &amp; = &amp; \\alpha + \\beta_{T} T_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Log-Normal(0, 0.25) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{T} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_{T}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_fungus_only_treatment &lt;- quap( flist = alist( h_1 ~ dnorm( mu, sigma ), mu &lt;- h_0 * p, p &lt;- a + beta_treatment * treatment, a ~ dlnorm( 0, .25 ), beta_treatment ~ dnorm(0,.5), sigma ~ dexp( 1 ) ), data = data_fungus ) precis(model_fungus_only_treatment) %&gt;% knit_precis() param mean sd 5.5% 94.5% a 1.38 0.03 1.34 1.42 beta_treatment 0.08 0.03 0.03 0.14 sigma 1.75 0.12 1.55 1.94 d-separation dagify(&quot;H_1&quot; ~ H_0 + F, F ~ T, coords = tibble(name = c(&quot;H_0&quot;, &quot;H_1&quot;, &quot;F&quot;, &quot;T&quot;), x = c(0, .5, .75, 1), y = c(0, 0, 0, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;H_1&quot;, &quot;response&quot;, if_else(name %in% c(&quot;H_0&quot;, &quot;F&quot;, &quot;T&quot;), &quot;predictor&quot;, &quot;confounds&quot;)), name = str_replace(name, &quot;([A-Z])_([0-9])&quot;, &quot;\\\\1\\\\[\\\\2\\\\]&quot;)) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(-.05, .05)) + scale_x_continuous(limits = c(-.05, 1.05)) + coord_equal() Directional Separation of H_1 and T occurs after conditioning on F: library(dagitty) impliedConditionalIndependencies(&quot;dag{ H_0 -&gt; H_1 &lt;- F &lt;- T}&quot;) #&gt; F _||_ H_0 #&gt; H_0 _||_ T #&gt; H_1 _||_ T | F dagify(H_1 ~ H_0 + M, F ~ T, F ~ M, coords = tibble(name = c(&quot;H_0&quot;, &quot;H_1&quot;, &quot;M&quot; , &quot;F&quot;, &quot;T&quot;), x = c(0, .5, .75, 1, 1.5), y = c(1, 1, .7, 1, 1))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;H_1&quot;, &quot;response&quot;, if_else(name %in% c(&quot;H_0&quot;, &quot;F&quot;, &quot;T&quot;), &quot;predictor&quot;, &quot;confounds&quot;)), name = str_replace(name, &quot;([A-Z])_([0-9])&quot;, &quot;\\\\1\\\\[\\\\2\\\\]&quot;)) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(.65, 1.05)) + scale_x_continuous(limits = c(-.05, 1.55)) + coord_equal() n &lt;- 1e4 set.seed(71) data_moisture &lt;- tibble( h_0 = rnorm(n, 10, 2), treatment = rep(0:1, each = n/2), moisture = rbern(n), fungus = rbinom(n = n, size = 1, prob = .5 - treatment * .4 + moisture * .4), h_1 = h_0 + rnorm(n, 5 + 3 * moisture) ) precis(data_moisture) %&gt;% knit_precis() param mean sd 5.5% 94.5% histogram h_0 10.04 2.01 6.81 13.22 ▁▁▁▂▇▇▂▁▁ treatment 0.50 0.50 0.00 1.00 ▇▁▁▁▁▁▁▁▁▇ moisture 0.50 0.50 0.00 1.00 ▇▁▁▁▁▁▁▁▁▇ fungus 0.50 0.50 0.00 1.00 ▇▁▁▁▁▁▁▁▁▇ h_1 16.55 2.69 12.22 20.84 ▁▁▁▃▇▇▅▂▁▁ Moisture-Model with treatment and fungus (post-treatment variable) model_moisture_post_treatment &lt;- quap( flist = alist( h_1 ~ dnorm( mu, sigma ), mu &lt;- h_0 * p, p &lt;- alpha + beta_treatment * treatment + beta_fungus * fungus, alpha ~ dlnorm( 0, .2 ), beta_treatment ~ dnorm(0,.5), beta_fungus ~ dnorm(0,.5), sigma ~ dexp( 1 ) ), data = data_moisture ) precis(model_moisture_post_treatment) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 1.53 0.00 1.52 1.54 beta_treatment 0.05 0.00 0.05 0.06 beta_fungus 0.13 0.00 0.12 0.14 sigma 2.13 0.02 2.11 2.16 Moisture-Model with treatment but without fungus model_moisture_only_treatment &lt;- quap( flist = alist( h_1 ~ dnorm( mu, sigma ), mu &lt;- h_0 * p, p &lt;- a + beta_treatment * treatment, a ~ dlnorm( 0, .25 ), beta_treatment ~ dnorm(0,.5), sigma ~ dexp( 1 ) ), data = data_moisture ) precis(model_moisture_only_treatment) %&gt;% knit_precis() param mean sd 5.5% 94.5% a 1.62 0.00 1.62 1.63 beta_treatment 0.00 0.00 0.00 0.01 sigma 2.22 0.02 2.19 2.25 7.3 Collider Bias # data_happy &lt;- sim_happiness(seed = 1977, N_years = 66) %&gt;% # as_tibble() progress_year &lt;- function(data, year, max_age = 65, n_births = 20, aom = 18){ new_cohort &lt;- tibble( age = 1, married = as.integer(0), happiness = seq(from = -2, to = 2, length.out = n_births), year_of_birth = year) data %&gt;% mutate(age = age + 1) %&gt;% bind_rows(., new_cohort) %&gt;% mutate(married = if_else(age &gt;= aom &amp; married == 0, rbern(n(), inv_logit(happiness - 4)), married )) %&gt;% filter(age &lt;= max_age) } sim_tidy &lt;- function(seed = 1977, n_years = 1000, max_age = 65, n_births = 20, aom = 18){ set.seed(seed) empty_tibble &lt;- tibble(age = double(), married = integer(), happiness = double()) 1:n_years %&gt;% reduce(.f = progress_year, .init = empty_tibble, max_age = max_age, n_births = n_births, aom = aom) } data_married &lt;- sim_tidy(seed = 2021, n_years = 65, n_births = 21) data_married %&gt;% mutate(married = factor(married, labels = c(&quot;unmarried&quot;, &quot;married&quot;))) %&gt;% ggplot(aes(x = age, y = happiness, color = married)) + geom_point(size = 1.75, shape = 21, aes(fill = after_scale(clr_alpha(color)))) + scale_color_manual(NULL, values = c(married = clr2, unmarried = clr0d)) + scale_x_continuous(expand = c(.015, .015)) + theme(panel.grid = element_blank(), legend.position = &quot;bottom&quot;) \\[ \\begin{array}{rclr} happyness_{i} &amp; \\sim &amp; Normal( \\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha_{married[i]} + \\beta_{age} age_{i} &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] data_married_adults &lt;- data_married %&gt;% filter(age &gt;= 18) %&gt;% mutate(age_trans = (age - 18)/ diff(c(18, 65)), married_idx = married + 1L) model_happy_married &lt;- quap( flist = alist( happiness ~ dnorm(mu, sigma), mu &lt;- alpha[married_idx] + beta_age * age_trans, alpha[married_idx] ~ dnorm( 0, 1 ), beta_age ~ dnorm( 0, 2 ), sigma ~ dexp( 1 ) ), data = data_married_adults ) precis(model_happy_married, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha[1] -0.21 0.06 -0.31 -0.11 alpha[2] 1.33 0.08 1.20 1.47 beta_age -0.81 0.11 -0.99 -0.64 sigma 0.97 0.02 0.94 1.01 model_happy &lt;- quap( flist = alist( happiness ~ dnorm(mu, sigma), mu &lt;- alpha + beta_age * age_trans, alpha ~ dnorm( 0, 1 ), beta_age ~ dnorm( 0, 2 ), sigma ~ dexp( 1 ) ), data = data_married_adults ) precis(model_happy, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.00 0.07 -0.12 0.12 beta_age 0.00 0.13 -0.21 0.21 sigma 1.21 0.03 1.17 1.25 7.3.1 The haunted DAG Education example (including Grandparents, Parents, Children and the unobserved Neighborhood) p_dag1 &lt;- dagify(C ~ P + G, P ~ G, coords = tibble(name = c(&quot;G&quot;, &quot;P&quot;, &quot;C&quot;), x = c(0, 1.5, 1.5), y = c(1, 1, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;C&quot;, &quot;response&quot;, if_else(name %in% c(&quot;P&quot;, &quot;G&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(-.05, 1.05)) + scale_x_continuous(limits = c(-.05, 1.55)) + coord_equal() p_dag2 &lt;- dagify(C ~ P + G + U, P ~ G + U, coords = tibble(name = c(&quot;G&quot;, &quot;P&quot;, &quot;C&quot;, &quot;U&quot;), x = c(0, 1.5, 1.5, 2), y = c(1, 1, 0, .5))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;C&quot;, &quot;response&quot;, if_else(name %in% c(&quot;P&quot;, &quot;G&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(-.05, 1.05)) + scale_x_continuous(limits = c(-.05, 2.05)) + coord_equal() p_dag1 + p_dag2 + plot_annotation(tag_levels = &quot;a&quot;) &amp; theme(plot.tag = element_text(family = fnt_sel)) n &lt;- 200 beta_gp &lt;- 1 # direct effect of G -&gt; P beta_gc &lt;- 0 # direct effect of G -&gt; C beta_pc &lt;- 1 # direct effect of P -&gt; C beta_U &lt;- 2 # direct effect of U on both C and P set.seed(1) data_education &lt;- tibble( unobserved = 2 * rbern(n, .5) - 1, grandparents = rnorm( n ), parents = rnorm( n, beta_gp * grandparents + beta_U * unobserved), children = rnorm( n, beta_gc * grandparents + beta_pc * parents + beta_U * unobserved) ) model_education &lt;- quap( flist = alist( children ~ dnorm(mu, sigma), mu &lt;- alpha + beta_pc * parents + beta_gc * grandparents, alpha ~ dnorm( 0, 1 ), c( beta_pc, beta_gc) ~ dnorm( 0, 1 ), sigma ~ dexp(1) ), data = data_education ) precis(model_education) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha -0.12 0.10 -0.28 0.04 beta_pc 1.79 0.04 1.72 1.86 beta_gc -0.84 0.11 -1.01 -0.67 sigma 1.41 0.07 1.30 1.52 data_education_plot &lt;- data_education %&gt;% mutate(across(grandparents:children, standardize, .names = &quot;{.col}_std&quot;)) %&gt;% mutate(parents_inner = between(parents_std, left = quantile(parents_std, probs = .45), right = quantile(parents_std, probs = .60))) data_education_plot %&gt;% ggplot(aes(x = grandparents_std, y = children_std)) + geom_smooth(data = data_education_plot %&gt;% filter(parents_inner), method = &quot;lm&quot;, se = FALSE, size = .5, color = clr_dark, fullrange = TRUE) + geom_point(aes(color = factor(unobserved), fill = after_scale(clr_alpha(color,.8)), shape = parents_inner), size = 2.5) + scale_color_manual(values = c(clr0d, clr2), guide = &quot;none&quot;) + scale_shape_manual(values = c(`FALSE` = 1, `TRUE` = 21), guide = &quot;none&quot;) + coord_equal() model_education_resolved &lt;- quap( flist = alist( children ~ dnorm(mu, sigma), mu &lt;- alpha + beta_pc * parents + beta_gc * grandparents + beta_u * unobserved, alpha ~ dnorm( 0, 1 ), c( beta_pc, beta_gc, beta_u ) ~ dnorm( 0, 1 ), sigma ~ dexp(1) ), data = data_education ) precis(model_education_resolved) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha -0.12 0.07 -0.24 -0.01 beta_pc 1.01 0.07 0.91 1.12 beta_gc -0.04 0.10 -0.20 0.11 beta_u 2.00 0.15 1.76 2.23 sigma 1.02 0.05 0.94 1.10 7.3.2 Shutting the Backdoor The four elements that construct DAGs: p_dag1 &lt;- dagify( X ~ Z, Y ~ Z, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), x = c(0, 1, .5), y = c(1, 1, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + ggtitle(&quot;Fork&quot;) p_dag2 &lt;- dagify( Z ~ X, Y ~ Z, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), x = c(0, 1, .5), y = c(1, 0, .5))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + ggtitle(&quot;Pipe&quot;) p_dag3 &lt;- dagify( Z ~ X + Y, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), x = c(0, 1, .5), y = c(0, 0 , 1))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + ggtitle(&quot;Collider&quot;) p_dag4 &lt;- dagify( Z ~ X + Y, D ~ Z, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, &quot;D&quot;), x = c(0, 1, .5, .5), y = c(0, 0 , 1, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, &quot;D&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + ggtitle(&quot;Collider&quot;) p_dag1 + p_dag2 + p_dag3 + p_dag4 + plot_annotation(tag_levels = &quot;a&quot;) + plot_layout(nrow = 1) &amp; coord_fixed(ratio = .6) &amp; scale_y_continuous(limits = c(-.1, 1.1)) &amp; scale_x_continuous(limits = c(-.1, 1.1)) &amp; theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) a. Fork: \\(X \\perp \\!\\!\\! \\perp Y | Z\\) b. Pipe: \\(X \\perp \\!\\!\\! \\perp Y | Z\\) c. Collider: \\(X \\perp \\!\\!\\! \\perp Y\\), but \\(X \\not\\!\\perp\\!\\!\\!\\perp Y | Z\\) d. Descendant: \\(X \\perp \\!\\!\\! \\perp Y\\), but \\(X \\not\\!\\perp\\!\\!\\!\\perp Y | Z\\) (to a lesser extent) 7.3.3 Two Roads dag_roads &lt;- dagify( U ~ A, X ~ U, Y ~ X + C, C ~ A, B ~ U + C, exposure = &quot;X&quot;, outcome = &quot;Y&quot;, coords = tibble(name = c(&quot;U&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;X&quot;, &quot;Y&quot;), x = c(0, .5, .5, 1, 0, 1), y = c(.7, 1, .4 , .7, 0, 0))) dag_roads %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;Y&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + coord_fixed(ratio = .6) &amp; scale_y_continuous(limits = c(-.1, 1.1)) &amp; scale_x_continuous(limits = c(-.1, 1.1)) &amp; theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) adjustmentSets(dag_roads,exposure = &quot;X&quot;, outcome = &quot;Y&quot;) #&gt; { C } #&gt; { A } #&gt; { U } dag_roads &lt;- dagitty( &quot;dag { U [unobserved] X -&gt; Y X &lt;- U &lt;- A -&gt; C -&gt; Y U -&gt; B &lt;- C }&quot; ) adjustmentSets(dag_roads,exposure = &quot;X&quot;, outcome = &quot;Y&quot;) #&gt; { C } #&gt; { A } 7.3.4 Backdoor Waffles dag_waffles &lt;- dagify( D ~ A + M + W, A ~ S, M ~ A + S, W ~ S, exposure = &quot;W&quot;, outcome = &quot;D&quot;, coords = tibble(name = c(&quot;S&quot;, &quot;A&quot;, &quot;M&quot;, &quot;W&quot;, &quot;D&quot;), x = c(0, 0, .5, 1, 1), y = c(1, 0, .5 , 1, 0))) dag_waffles %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;W&quot;, &quot;A&quot;, &quot;M&quot;, &quot;S&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + coord_fixed(ratio = .6) &amp; scale_y_continuous(limits = c(-.1, 1.1)) &amp; scale_x_continuous(limits = c(-.1, 1.1)) &amp; theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) adjustmentSets(dag_waffles) #&gt; { A, M } #&gt; { S } Test the implications of the DAG by investigating the conditional independencies implied by the DAG in the real data (by conditioning on the respective variables): impliedConditionalIndependencies(dag_waffles) #&gt; A _||_ W | S #&gt; D _||_ S | A, M, W #&gt; M _||_ W | S Exporting models and data for re-use library(rlang) chapter6_models &lt;- env( data_legs = data_legs, model_legs_multicollinear = model_legs_multicollinear, data_milk = data_milk, model_milk_fat = model_milk_fat, model_milk_lactose = model_milk_lactose, model_milk_multicollinear = model_milk_multicollinear, data_fungus = data_fungus, model_fungus_no_treatment = model_fungus_no_treatment, model_fungus_post_treatment = model_fungus_post_treatment, model_fungus_only_treatment = model_fungus_only_treatment, data_moisture = data_moisture, model_moisture_post_treatment = model_moisture_post_treatment, model_moisture_only_treatment = model_moisture_only_treatment, data_married_adults = data_married_adults, model_happy_married = model_happy_married, model_happy = model_happy, data_education = data_education, model_education = model_education, model_education_resolved = model_education_resolved ) write_rds(chapter6_models, &quot;envs/chapter6_models.rds&quot;) 7.4 Homework E1 multicollinearity (including two highly correlated predictors, so that a ridge of explanatory combinations prevents the precise estimate of both) post-treatment bias (masking an association by assuming all efects are caused by an intermediate descendant) collider bias (introducing an association as a selection effect) E2 Fruit mass as a function of FF and crown area FF \\(\\rightarrow\\) Fruit mass \\(\\leftarrow\\) crown area E3 In all cases, we wonder about the influence of \\(X\\) on \\(Y\\). The elemental confounds influence how conditioning on \\(Z\\) (or \\(D\\)) effects our inference. The Fork (\\(X \\leftarrow Z \\rightarrow Y\\)): \\(X \\perp \\!\\!\\! \\perp Y | Z\\), but \\(X \\not\\!\\perp\\!\\!\\!\\perp Y\\) The Pipe (\\(X \\rightarrow Z \\rightarrow Y\\)): \\(X \\perp \\!\\!\\! \\perp Y | Z\\), but \\(X \\not\\!\\perp\\!\\!\\!\\perp Y\\) The Collider (\\(X \\rightarrow Z \\leftarrow Y\\)): \\(X \\perp \\!\\!\\! \\perp Y\\), but \\(X \\not\\!\\perp\\!\\!\\!\\perp Y | Z\\) The Descendant (\\(X \\rightarrow Z \\leftarrow Y; Z \\rightarrow D\\), \\(X \\rightarrow Z \\rightarrow Y; Z \\rightarrow D\\)): conditioning on \\(D\\) has the same (slightly weaker) effect like conditioning on \\(Z\\) E4 As bias sample acts like selection for a specific value of a trait (eg. an article was selected for publication). This is implicitly conditioning on a third variable (also like eg. the 45-60 percentile of parents). M1 dag_roads_v &lt;- dagify( U ~ A, X ~ U, Y ~ X + C + V, C ~ A + V, B ~ U + C, exposure = &quot;X&quot;, outcome = &quot;Y&quot;, coords = tibble(name = c(&quot;U&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;X&quot;, &quot;Y&quot;, &quot;V&quot;), x = c(0, .5, .5, 1, 0, 1, 1.3), y = c(.7, 1, .4 , .7, 0, 0, .35))) dag_roads_v %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;Y&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + coord_fixed(ratio = .6) &amp; scale_y_continuous(limits = c(-.1, 1.4)) &amp; scale_x_continuous(limits = c(-.1, 1.4)) &amp; theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) adjustmentSets(dag_roads_v, exposure = &quot;X&quot;, outcome = &quot;Y&quot;) #&gt; { C, V } #&gt; { A } #&gt; { U } dag_roads_v &lt;- dagitty( &quot;dag { U [unobserved] V [unobserved] X -&gt; Y X &lt;- U &lt;- A -&gt; C -&gt; Y U -&gt; B &lt;- C Y &lt;- V -&gt; C }&quot; ) adjustmentSets(dag_roads_v, exposure = &quot;X&quot;, outcome = &quot;Y&quot;) #&gt; { A } … there are now \\(x + 1\\) paths. Condition on A, since conditioning on C would open the collider between A and V allowing for the flow of information through the backdoor V. M2 dagify( Z ~ X, Y ~ Z, exposure = &quot;X&quot;, outcome = &quot;Y&quot;, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), x = c(0, 1, .5), y = c(0, 0, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;Y&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;Z&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + coord_fixed(ratio = .6) &amp; scale_y_continuous(limits = c(-.1, .1)) &amp; scale_x_continuous(limits = c(-.1, 1.1)) &amp; theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) n &lt;- 1e4 data_sim_multicol &lt;- tibble(x = rnorm(n), z = rnorm(n, mean = x, sd = .05), y = rnorm(n, mean = z)) data_sim_multicol %&gt;% ggpairs( lower = list(continuous = wrap(ggally_points, colour = clr0d, size = 1.5, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll0, color = clr0d, adjust = .9)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;)) ) model_sim_multicol &lt;- quap( flist = alist( y ~ dnorm(mu, sigma), mu &lt;- alpha + beta_x * x + beta_z * z, alpha ~ dnorm( 0, .2 ), beta_x ~ dnorm( 0, .5), beta_z ~ dnorm( 0, .5), sigma ~ dexp(1) ), data = data_sim_multicol ) precis(model_sim_multicol) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha -0.01 0.01 -0.03 0.00 beta_x -0.10 0.17 -0.38 0.17 beta_z 1.12 0.17 0.84 1.39 sigma 1.00 0.01 0.98 1.01 precis(model_sim_multicol, depth = 2) %&gt;% as_tibble_rn() %&gt;% ggplot(aes(y = param)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(aes(xmin = `5.5%`, xmax =`94.5%`), color = clr0d) + geom_point(aes(x = mean), shape = 21, size = 3 , color = clr0d, fill = clr0) + scale_y_discrete(limits = c(&quot;sigma&quot;, &quot;beta_z&quot;, &quot;beta_x&quot;, &quot;alpha&quot;)) + theme(axis.title.y = element_blank()) Here the effect of \\(Z\\) on \\(Y\\) is not obscured by including \\(X\\). The difference is that here we are breaking a pipe by conditioning on \\(Z\\). M3 dag_h1 &lt;- dagify( X ~ Z, Y ~ X + Z + A, Z ~ A, exposure = &quot;X&quot;, outcome = &quot;Y&quot;, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, &quot;A&quot;), x = c(0, 1, .5, 1), y = c(0, 0, .7, .7))) dag_h2 &lt;- dagify( Z ~ X, Y ~ X + Z + A, Z ~ A, exposure = &quot;X&quot;, outcome = &quot;Y&quot;, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, &quot;A&quot;), x = c(0, 1, .5, 1), y = c(0, 0, .7, .7))) dag_h3 &lt;- dagify( X ~ A, Y ~ X, Z ~ X + Y + A, exposure = &quot;X&quot;, outcome = &quot;Y&quot;, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, &quot;A&quot;), x = c(0, 1, .5, 0), y = c(0, 0, .7, .7))) dag_h4 &lt;- dagify( X ~ A, Y ~ X + Z, Z ~ X + A, exposure = &quot;X&quot;, outcome = &quot;Y&quot;, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, &quot;A&quot;), x = c(0, 1, .5, 0), y = c(0, 0, .7, .7))) list(dag_h1, dag_h2, dag_h3, dag_h4) %&gt;% purrr::map(.f = function(dag){ dag %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;Y&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;Z&quot;, &quot;A&quot;), &quot;predictor&quot;, &quot;confounds&quot;)))}) %&gt;% purrr::map(plot_dag, clr_in = clr3) %&gt;% wrap_plots() + plot_annotation(tag_levels = &quot;a&quot;) &amp; coord_fixed(ratio = .6) &amp; scale_y_continuous(limits = c(-.1, .8)) &amp; scale_x_continuous(limits = c(-.1, 1.1)) &amp; theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) a. condition on \\(Z\\) to close both backdoor paths into \\(Y\\) b. none (the information flow through \\(Z\\) is causal and thus desired, and \\(A\\) is blocked by the collider in \\(Z\\)) c. none - there are no open backdoors into \\(Y\\) d. condition on \\(A\\) to enable the information from desired (causal) information from \\(Z\\) while removing the undesired information from \\(A\\) list(dag_h1, dag_h2, dag_h3, dag_h4) %&gt;% purrr::map(adjustmentSets) #&gt; [[1]] #&gt; { Z } #&gt; #&gt; [[2]] #&gt; {} #&gt; #&gt; [[3]] #&gt; {} #&gt; #&gt; [[4]] #&gt; { A } H1 data(WaffleDivorce) data_waffle &lt;- WaffleDivorce %&gt;% as_tibble() %&gt;% drop_na(everything()) %&gt;% mutate(across(where(is.double), standardize, .names = &quot;{str_to_lower(.col)}_std&quot;), waffle_std = standardize(WaffleHouses)) %&gt;% dplyr::select(Location,MedianAgeMarriage, Marriage, Divorce, WaffleHouses, South, medianagemarriage_std, marriage_std, divorce_std, waffle_std) dag_waffles &lt;- dagify(D ~ A + M + W, M ~ A + S, A ~ S, W ~ S, exposure = &quot;W&quot;, outcome = &quot;D&quot;, coords = tibble(name = c(&quot;S&quot;, &quot;A&quot;, &quot;M&quot;, &quot;D&quot;, &quot;W&quot;), x = c(0, 0, .5, 1, 1), y = c(1, 0, .5, 0, 1))) dag_waffles %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M&quot;, &quot;S&quot;, &quot;W&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(-.1, 1.1)) + coord_fixed(ratio = .6) adjustmentSets(dag_waffles) #&gt; { A, M } #&gt; { S } \\(\\rightarrow\\) there are four paths from \\(W\\) to \\(D\\) - the only causal one being \\(W \\rightarrow D\\). To close the other three, we can condition on \\(S\\) which will close two different forks, efectiffley removing all non-causaul backdoor paths. model_waffle &lt;- quap( flist = alist( divorce_std ~ dnorm(mu, sigma), mu &lt;- alpha[South] + beta_w * waffle_std, alpha[South] ~ dnorm(0, .5), beta_w ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_waffle ) precis(model_waffle, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha[1] 0.00 0.13 -0.21 0.21 alpha[2] 0.00 0.50 -0.80 0.80 beta_w 0.24 0.13 0.03 0.45 sigma 0.95 0.09 0.80 1.10 precis(model_waffle, depth = 2) %&gt;% as_tibble_rn() %&gt;% ggplot(aes(y = param)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(aes(xmin = `5.5%`, xmax =`94.5%`), color = clr0d) + geom_point(aes(x = mean), shape = 21, size = 3 , color = clr0d, fill = clr0) + scale_y_discrete(limits = c(&quot;sigma&quot;, &quot;beta_w&quot;, &quot;alpha[2]&quot;, &quot;alpha[1]&quot;)) + theme(axis.title.y = element_blank()) \\(\\rightarrow\\) The total causal influence of \\(W\\) on \\(D\\) should be in the order of 0.24 standard deviations. H2 impliedConditionalIndependencies(dag_waffles) #&gt; A _||_ W | S #&gt; D _||_ S | A, M, W #&gt; M _||_ W | S model_waffel_test1 &lt;- quap( flist = alist( medianagemarriage_std ~ dnorm(mu, sigma), mu &lt;- alpha[South] + beta_w * waffle_std, alpha[South] ~ dnorm(0, .2), beta_w ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_waffle ) precis(model_waffel_test1, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha[1] 0.00 0.11 -0.18 0.18 alpha[2] 0.00 0.20 -0.32 0.32 beta_w -0.11 0.13 -0.32 0.10 sigma 0.97 0.10 0.82 1.13 \\(\\rightarrow\\) the influence of \\(W\\) on \\(A\\) is moderate (-0.11), with a wide posterior distribution both on either side of zero (-0.32, 0.1). Based on this we can not find a definitive influence from \\(W\\) on \\(A\\), model_waffel_test2 &lt;- quap( flist = alist( divorce_std ~ dnorm(mu, sigma), mu &lt;- alpha[South] + beta_a * medianagemarriage_std + beta_m * marriage_std + beta_w * waffle_std, alpha[South] ~ dnorm(0, .2), c(beta_a, beta_m, beta_w) ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_waffle ) precis(model_waffel_test2, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha[1] 0.00 0.10 -0.15 0.15 alpha[2] 0.00 0.20 -0.32 0.32 beta_a -0.58 0.15 -0.82 -0.35 beta_m -0.05 0.15 -0.29 0.19 beta_w 0.18 0.11 0.01 0.35 sigma 0.77 0.08 0.64 0.89 waffel_test2_samples &lt;- extract.samples(model_waffel_test2) %&gt;% as_tibble() %&gt;% mutate(contrast = alpha[,1] - alpha[,2]) waffel_test2_samples_quantiles &lt;- tibble(x = quantile(waffel_test2_samples$contrast, prob = c(.055, .5, .945)), percentile = c(&quot;lower&quot;, &quot;median&quot;, &quot;upper&quot;)) %&gt;% pivot_wider(names_from = &quot;percentile&quot;, values_from = &quot;x&quot;) waffel_test2_samples %&gt;% ggplot(aes(x = contrast)) + geom_density(color = clr0d, fill = fll0) + geom_pointrange(data = waffel_test2_samples_quantiles, aes(xmin = lower, x = median, xmax = upper, y = 0), color = clr0d, fill = clr0, shape = 21) \\(\\rightarrow\\) the influence of \\(S\\) on \\(D\\) is moderate after conditioning on \\(A\\), \\(M\\) and \\(W\\) (median of the contrast effect: 0.0026) model_waffel_test3 &lt;- quap( flist = alist( marriage_std ~ dnorm(mu, sigma), mu &lt;- alpha[South] + beta_w * waffle_std, alpha[South] ~ dnorm(0, .2), beta_w ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_waffle ) precis(model_waffel_test3, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha[1] 0.00 0.11 -0.18 0.18 alpha[2] 0.00 0.20 -0.32 0.32 beta_w 0.03 0.13 -0.19 0.24 sigma 0.98 0.10 0.83 1.13 \\(\\rightarrow\\) the influence of \\(W\\) on \\(M\\) is moderate (0.03), with a wide posterior distribution both on either side of zero (-0.19, 0.24). Based on this we can not find a definitive influence from \\(W\\) on \\(M\\), H3 From Wikipedia: Weights range from 2.2–14 kg data(foxes) data_fox &lt;- foxes %&gt;% as_tibble() %&gt;% drop_na(everything()) %&gt;% mutate(across(-group, standardize, .names = &quot;{str_to_lower(.col)}_std&quot;)) fox_weight_range &lt;- tibble(weight = c(2.2, 14), weight_std = (weight - mean(data_fox$weight))/ sd(data_fox$weight)) dag_fox &lt;- dagify( W ~ F + G, G ~ F, F ~ A, exposure = &quot;A&quot;, outcome = &quot;W&quot;, coords = tibble(name = c(&quot;W&quot;, &quot;F&quot;, &quot;G&quot;, &quot;A&quot;), x = c(.5, 0, 1, .5), y = c(0, .5, .5, 1))) dag_fox %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;W&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;F&quot;, &quot;G&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr2) + scale_y_continuous(limits = c(-.1, 1.1)) + coord_fixed(ratio = .6) adjustmentSets(dag_fox) #&gt; {} model_fox_area &lt;- quap( flist = alist( weight_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_area * area_std, alpha ~ dnorm(0,.2), beta_area ~ dnorm(0, .25), sigma ~ dexp(1) ), data = data_fox ) extract.prior(model_fox_area) %&gt;% as_tibble() %&gt;% filter(row_number() &lt; 150) %&gt;% ggplot() + geom_abline(aes(slope = beta_area, intercept = alpha), color = clr_alpha(clr0d)) + geom_hline(data = fox_weight_range, aes(yintercept = weight_std), color = clr_dark, linetype = 3) + scale_x_continuous(limits = c(-3,3)) + scale_y_continuous(limits = c(-3,3)) + labs(x = &quot;area_std&quot;, y = &quot;weight_std&quot;) precis(model_fox_area) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.00 0.08 -0.13 0.13 beta_area 0.02 0.09 -0.12 0.16 sigma 0.99 0.06 0.89 1.09 The slope of area very close to zero with the posterior distribution heavy on either side 🤷 H4 adjustmentSets(dag_fox, exposure = &quot;F&quot;, outcome = &quot;W&quot;) #&gt; {} model_fox_food &lt;- quap( flist = alist( weight_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_food * avgfood_std, alpha ~ dnorm(0,.2), beta_food ~ dnorm(0, .25), sigma ~ dexp(1) ), data = data_fox ) precis(model_fox_food, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.00 0.08 -0.13 0.13 beta_food -0.02 0.09 -0.16 0.12 sigma 0.99 0.06 0.89 1.09 H5 adjustmentSets(dag_fox, exposure = &quot;G&quot;, outcome = &quot;W&quot;) #&gt; { F } … we need to condition on \\(F\\). model_fox_group &lt;- quap( flist = alist( weight_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_food * avgfood_std + beta_group * groupsize_std, alpha ~ dnorm(0,.2), c(beta_food, beta_group) ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_fox ) precis(model_fox_group, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.00 0.08 -0.13 0.13 beta_food 0.48 0.18 0.19 0.76 beta_group -0.57 0.18 -0.86 -0.29 sigma 0.94 0.06 0.84 1.04 precis(model_fox_group, depth = 2) %&gt;% as_tibble_rn() %&gt;% ggplot(aes(y = param)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(aes(xmin = `5.5%`, xmax =`94.5%`), color = clr0d) + geom_point(aes(x = mean), shape = 21, size = 3 , color = clr0d, fill = clr0) + scale_y_discrete(limits = c(&quot;sigma&quot;, &quot;beta_group&quot;, &quot;beta_food&quot;, &quot;alpha&quot;)) + theme(axis.title.y = element_blank()) H6 dag &lt;- dagify( OD ~ FD + GS + DD, FD ~ DD + FF, exposure = &quot;FD&quot;, outcome = &quot;OD&quot;, coords = tibble(name = c(&quot;FF&quot;, &quot;FD&quot;, &quot;DD&quot;, &quot;OD&quot;, &quot;GS&quot;), x = c(0, .5, 0, 1, 1.5), y = c(1, .5, 0, .5, .5))) dag %&gt;% tidy_dagitty() %&gt;% mutate(stage = if_else(name == &quot;OD&quot;, &quot;response&quot;, &quot;predictor&quot;)) %&gt;% plot_dag(clr_in = clr3) + coord_fixed(ratio = .6) adjustmentSets(dag) #&gt; { DD } impliedConditionalIndependencies(dag) #&gt; DD _||_ FF #&gt; DD _||_ GS #&gt; FD _||_ GS #&gt; FF _||_ GS #&gt; FF _||_ OD | DD, FD H7 n &lt;- 1e4 data_fruit &lt;- tibble( fruitfall = rnorm(n), dipteryx_density = rnorm(n), fruit_density = rnorm(n, mean = fruitfall + dipteryx_density), group_size = rnorm(n), od_area = rlnorm(n = n, meanlog = fruit_density + dipteryx_density - group_size)) %&gt;% mutate(across(everything(), standardize, .names = &quot;{.col}_std&quot;)) model_fruit &lt;- quap( flist = alist( od_area_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_fd * fruit_density_std + beta_dd * dipteryx_density_std, alpha ~ dnorm(0, .2), c(beta_fd, beta_dd) ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_fruit ) precis(model_fruit) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.00 0.01 -0.02 0.02 beta_fd 0.12 0.01 0.10 0.14 beta_dd 0.05 0.01 0.04 0.07 sigma 0.99 0.01 0.98 1.00 7.5 {brms} section 7.5.1 Multicolliniarity Legs Model brms_c6_model_legs_multicollinear &lt;- brm( data = data_legs, family = gaussian, height ~ 1 + left_leg + right_leg, prior = c(prior(normal(10, 100), class = Intercept), prior(normal(2, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, sample_prior = TRUE, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_legs_multicollinear&quot;) library(tidybayes) library(ggdist) as_draws_df(brms_c6_model_legs_multicollinear) %&gt;% dplyr::select(starts_with(&quot;b&quot;), sigma) %&gt;% as_tibble() %&gt;% set_names(x = . , nm = names(.) %&gt;% str_remove(&quot;b_&quot;)) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = reorder(name, value))) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_pointinterval(color = clr0dd, fill = clr0, shape = 21) + scale_y_discrete(limits = c(&quot;sigma&quot;, &quot;left_leg&quot;, &quot;Intercept&quot;)) + theme_minimal(base_family = fnt_sel) + theme(axis.title = element_blank()) Check various seeds to illustrate the difficulty to fit the legs jointly: simulate_leg_data &lt;- function(seed = 42){ n &lt;- 100 set.seed(seed) data_legs &lt;- tibble( height = rnorm(n = n, mean = 10, sd = 2), leg_proportion = runif(n, min = 0.4, max = 0.5), left_leg = leg_proportion * height + rnorm(n, 0, .02), right_leg = leg_proportion * height + rnorm(n, 0, .02), ) tibble(seed = seed, fit = list(update(brms_c6_model_legs_multicollinear, newdata = data_legs, seed = 42, refresh = 0))) } 1:4 %&gt;% map_dfr(simulate_leg_data) %&gt;% mutate(posterior = purrr::map(fit, as_draws_df )) %&gt;% unnest(cols = posterior) %&gt;% dplyr::select(seed, starts_with(&quot;b&quot;), sigma) %&gt;% set_names(x = . , nm = names(.) %&gt;% str_remove(&quot;b_&quot;)) %&gt;% pivot_longer(-seed) %&gt;% ggplot(aes(x = value, y = reorder(name, value))) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_halfeye(aes(fill_ramp = stat(cut_cdf_qi(cdf, .width = c(0.66, 0.95,1)))), fill = fll0dd, adjust = .7, normalize = &quot;groups&quot;, height = .8) + scale_fill_ramp_discrete(range = c(.85, 0.15), guide = &quot;none&quot;) + scale_y_discrete(limits = c(&quot;sigma&quot;, &quot;left_leg&quot;, &quot;right_leg&quot;, &quot;Intercept&quot;)) + coord_cartesian(ylim = c(0.9, 5), expand = 0) + facet_wrap(seed ~ ., ncol = 1, labeller = label_both) + theme_minimal(base_family = fnt_sel) + theme(axis.title = element_blank()) p_ridge &lt;- as_draws_df(brms_c6_model_legs_multicollinear) %&gt;% as_tibble() %&gt;% dplyr::select(starts_with(&quot;b&quot;), sigma) %&gt;% set_names(x = . , nm = names(.) %&gt;% str_remove(&quot;b_&quot;)) %&gt;% ggplot(aes(x = left_leg, y = right_leg)) + geom_point(color = clr0d, fill = fll0, shape = 21, size = .6) p_sum &lt;- as_draws_df(brms_c6_model_legs_multicollinear) %&gt;% as_tibble() %&gt;% dplyr::select(starts_with(&quot;b&quot;), sigma) %&gt;% set_names(x = . , nm = names(.) %&gt;% str_remove(&quot;b_&quot;)) %&gt;% ggplot(aes(x = left_leg + right_leg)) + stat_halfeye(aes(fill_ramp = stat(cut_cdf_qi(cdf, .width = c(0.66, 0.95,1)))), fill = fll0dd, adjust = .7, normalize = &quot;groups&quot;, height = .8) + scale_fill_ramp_discrete(range = c(.85, 0.15), guide = &quot;none&quot;) p_ridge + p_sum Model single leg brms_c6_model_model_leg &lt;- brm( data = data_legs, family = gaussian, height ~ 1 + left_leg, prior = c(prior(normal(10, 100), class = Intercept), prior(normal(2, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, sample_prior = TRUE, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_leg&quot;) as_draws_df(brms_c6_model_model_leg) %&gt;% dplyr::select(starts_with(&quot;b&quot;), sigma) %&gt;% as_tibble() %&gt;% set_names(x = . , nm = names(.) %&gt;% str_remove(&quot;b_&quot;)) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = reorder(name, value))) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_pointinterval(color = clr0dd, fill = clr0, shape = 21) + scale_y_discrete(limits = c(&quot;sigma&quot;, &quot;left_leg&quot;, &quot;Intercept&quot;)) + theme_minimal(base_family = fnt_sel) + theme(axis.title = element_blank()) Multicollinear milk model brms_c6_model_milk_multicollinear_1 &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 1 + lactose_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_milk_multicollinear1&quot; ) brms_c6_model_milk_multicollinear_2 &lt;- update( brms_c6_model_milk_multicollinear_1, newdata = data_milk, formula = kcal_std ~ 1 + fat_std, seed = 42, file = &quot;brms/brms_c6_model_milk_multicollinear2&quot; ) brms_c6_model_milk_multicollinear_3 &lt;- update( brms_c6_model_milk_multicollinear_1, newdata = data_milk, formula = kcal_std ~ 1 + lactose_std + fat_std, seed = 42, file = &quot;brms/brms_c6_model_milk_multicollinear3&quot; ) posterior_summary(brms_c6_model_milk_multicollinear_3) %&gt;% round(digits = 2) %&gt;% knitr::kable() Estimate Est.Error Q2.5 Q97.5 b_Intercept 0.00 0.07 -0.15 0.14 b_lactose_std -0.67 0.20 -1.06 -0.27 b_fat_std 0.25 0.20 -0.15 0.65 sigma 0.41 0.06 0.32 0.55 lp__ -17.30 1.51 -21.09 -15.42 7.5.2 Post-treatment bias brms_c6_model_fungus_no_treatment &lt;- brm( data = data_fungus, family = gaussian, h_1 ~ 0 + h_0, prior = c(prior(lognormal(0, 0.25), class = b, lb = 0), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_fungus_no_treatment&quot; ) mixedup::summarise_model(brms_c6_model_fungus_no_treatment) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 3.33 1.82 1.58 2.10 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; h_0 1.43 0.02 1.39 1.46 To fit the original model also in {brms}, we are translating \\[ \\begin{array}{rclr} \\mu_i &amp; = &amp; h_{0,i} \\times p &amp; \\textrm{[linear model]}\\\\ p &amp; = &amp; \\alpha + \\beta_{T} T_{i} + \\beta_{F} F_{i} &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] into \\[ \\begin{array}{rclr} \\mu_i &amp; = &amp; h_{0,i} \\times ( \\alpha + \\beta_{T} T_{i} + \\beta_{F} F_{i}) &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] \\(\\rightarrow\\) beware of the non-linear {brms} syntax used here! brms_c6_model_fungus_post_treatment &lt;- brm( data = data_fungus, family = gaussian, bf(h_1 ~ h_0 * (a + t * treatment + f * fungus), a + t + f ~ 1, nl = TRUE), prior = c(prior(lognormal(0, 0.2), nlpar = a, lb = 0), prior(normal(0, 0.5), nlpar = t), prior(normal(0, 0.5), nlpar = f), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_fungus_post_treatment&quot; ) mixedup::summarise_model(brms_c6_model_fungus_post_treatment) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 2.09 1.45 1.26 1.67 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; a_Intercept 1.48 0.03 1.43 1.53 #&gt; t_Intercept 0.00 0.03 -0.05 0.06 #&gt; f_Intercept -0.27 0.04 -0.34 -0.19 omitting fungus from the model: brms_c6_model_fungus_only_treatment &lt;- brm( data = data_fungus, family = gaussian, bf(h_1 ~ h_0 * (a + t * treatment), a + t ~ 1, nl = TRUE), prior = c(prior(lognormal(0, 0.2), nlpar = a, lb = 0), prior(normal(0, 0.5), nlpar = t), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_fungus_only_treatment&quot; ) mixedup::summarise_model(brms_c6_model_fungus_only_treatment) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 3.18 1.78 1.56 2.04 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; a_Intercept 1.38 0.03 1.33 1.43 #&gt; t_Intercept 0.09 0.04 0.02 0.16 including moisture as a fork: brms_c6_model_moisture_post_treatment &lt;- update( brms_c6_model_fungus_post_treatment, newdata = data_moisture, seed = 42, file = &quot;brms/brms_c6_model_moisture_post_treatment&quot; ) mixedup::summarise_model(brms_c6_model_moisture_post_treatment) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 4.54 2.13 2.10 2.16 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; a_Intercept 1.53 0.00 1.52 1.54 #&gt; t_Intercept 0.05 0.00 0.05 0.06 #&gt; f_Intercept 0.13 0.00 0.12 0.14 brms_c6_model_moisture_only_treatment &lt;- update( brms_c6_model_fungus_only_treatment, newdata = data_moisture, seed = 42, file = &quot;brms/brms_c6_model_moisture_only_treatment&quot; ) mixedup::summarise_model(brms_c6_model_moisture_only_treatment) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 4.93 2.22 2.19 2.25 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; a_Intercept 1.62 0.00 1.61 1.63 #&gt; t_Intercept 0.00 0.00 -0.01 0.01 7.5.3 Collider Bias data_married_factor &lt;- data_married_adults %&gt;% mutate(married_f = c(&quot;single&quot;, &quot;married&quot;)[married + 1] %&gt;% factor()) brms_c6_model_happy_married &lt;-brm( data = data_married_factor, family = gaussian, happiness ~ 0 + married_f + age_trans, prior = c(prior(normal(0, 1), class = b, coef = married_fmarried), prior(normal(0, 1), class = b, coef = married_fsingle), prior(normal(0, 2), class = b, coef = age_trans), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_happy_married&quot;) mixedup::summarise_model(brms_c6_model_happy_married) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 0.96 0.98 0.94 1.02 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; married_fmarried 1.34 0.08 1.18 1.50 #&gt; married_fsingle -0.21 0.06 -0.33 -0.09 #&gt; age_trans -0.81 0.11 -1.03 -0.61 brms_c6_model_happy &lt;-brm( data = data_married_factor, family = gaussian, happiness ~ 0 + Intercept + age_trans, prior = c(prior(normal(0, 1), class = b, coef = Intercept), prior(normal(0, 2), class = b, coef = age_trans), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_happy&quot;) mixedup::summarise_model(brms_c6_model_happy) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 1.47 1.21 1.16 1.27 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; age_trans 0.00 0.07 -0.13 0.13 The haunted DAG brms_c6_model_education &lt;-brm( data = data_education, family = gaussian, children ~ 0 + Intercept + parents + grandparents, prior = c(prior(normal(0, 1), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_education&quot;) mixedup::summarise_model(brms_c6_model_education) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 2.04 1.43 1.30 1.58 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; Intercept -0.12 0.10 -0.32 0.08 #&gt; parents 1.79 0.04 1.70 1.88 #&gt; grandparents -0.83 0.11 -1.04 -0.63 brms_c6_model_education_resolved &lt;- update( brms_c6_model_education, newdata = data_education, formula = children ~ 0 + Intercept + parents + grandparents + unobserved, seed = 42, file = &quot;brms/brms_c6_model_education_resolved&quot; ) mixedup::summarise_model(brms_c6_model_education_resolved) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 1.07 1.04 0.94 1.15 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; Intercept -0.12 0.07 -0.26 0.02 #&gt; parents 1.01 0.07 0.88 1.15 #&gt; grandparents -0.04 0.10 -0.24 0.15 #&gt; unobserved 2.00 0.15 1.70 2.27 7.5.4 Summary data_waffle &lt;- data_waffle %&gt;% mutate(south_f = factor(South)) brms_c6_model_waffle_1 &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + waffle_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_waffle_1&quot;) brms_c6_model_waffle_2 &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + waffle_std + south_f, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_waffle_2&quot;) brms_c6_model_waffle_3 &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + waffle_std + medianagemarriage_std + marriage_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_waffle_3&quot;) brms_c6_model_waffle_4 &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + waffle_std + medianagemarriage_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_waffle_4&quot;) brms_c6_model_waffle_5 &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + waffle_std + marriage_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_waffle_5&quot;) formula &lt;- c(glue(&quot;d {mth(&#39;\\U007E&#39;)} 1 + w&quot;), glue(&quot;d {mth(&#39;\\U007E&#39;)} 1 + w + s&quot;), glue(&quot;d {mth(&#39;\\U007E&#39;)} 1 + w + a + m&quot;), glue(&quot;d {mth(&#39;\\U007E&#39;)} 1 + w + a&quot;), glue(&quot;d {mth(&#39;\\U007E&#39;)} 1 + w + m&quot;)) tibble(model = 1:5, fit = str_c(&quot;brms_c6_model_waffle_&quot;, model)) %&gt;% mutate(y = str_c(model, &quot;: &quot;, formula), post = purrr::map(fit, ~get(.) %&gt;% as_draws_df() %&gt;% dplyr::select(waffle_std = b_waffle_std))) %&gt;% unnest(post) %&gt;% ggplot(aes(x = waffle_std, y = y, color = fit %in% c(&quot;brms_c6_model_waffle_2&quot;, &quot;brms_c6_model_waffle_3&quot;))) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_pointinterval(aes(fill = after_scale(clr_lighten(color))), shape = 21) + scale_color_manual(values = c(clr0dd, clr2)) + labs(x = str_c(&quot;*&quot;,mth(&quot;\\U03B2&quot;),&quot;&lt;sub&gt;w&lt;/sub&gt;*&quot;), y = NULL) + coord_cartesian(xlim = c(-0.4, 0.6)) + theme(axis.text.y = element_markdown(hjust = 0), axis.title.x = element_markdown(), legend.position = &quot;none&quot;) 7.6 pymc3 section × "],["rethinking-chapter-7.html", "8 Rethinking: Chapter 7 8.1 The Problem with Parameters 8.2 Entropy and Accuracy 8.3 Golem taming: regularization 8.4 Model comparison 8.5 Homework 8.6 {brms} section 8.7 pymc3 section", " 8 Rethinking: Chapter 7 Ulysses’ Compass by Richard McElreath, building on the Summaries by Solomon Kurz and Jake Thompson. Figure 8.1: Between Scylla and Charybdis by Adolf Hirémy-Hirschl (1910). 8.1 The Problem with Parameters library(rethinking) data_brainsize &lt;- tibble( species = c(&quot;afarensis&quot;, &quot;africanus&quot;, &quot;habilis&quot;, &quot;boisei&quot;, &quot;rudolfensis&quot;, &quot;ergaster&quot;, &quot;sapiens&quot;), brain_size = c(438, 452, 612, 521, 752, 871, 1350), mass = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5)) %&gt;% mutate(brain_size_scl = brain_size/ max(brain_size), mass_std = standardize(mass)) data_brainsize %&gt;% ggplot(aes(x = mass, y = brain_size)) + geom_point(shape = 21, color = clr2, fill = fll2, size = 3) + ggrepel::geom_text_repel(aes(label = species), force = 30, min.segment.length = unit(.1, &quot;npc&quot;), family = fnt_sel, fontface = &quot;italic&quot;) + coord_fixed(ratio = .03) 8.1.1 The burial of R2 \\[ R^{2} = \\frac{var(outcome) - var(residuals)}{var(outcome)} = 1 - \\frac{var(residuals)}{var(outcome)} \\] 8.1.1.1 Linear Model \\[ \\begin{array}{rclr} b_{i} &amp; \\sim &amp; Normal( \\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{m} m_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0.5, 1) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{m} &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\beta_{T}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Log-Normal(0, 1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_brain_size &lt;- quap( flist = alist( brain_size_scl ~ dnorm(mu, exp(log_sigma)), mu &lt;- alpha + beta_m * mass_std, alpha ~ dnorm(0.5, 1), beta_m ~ dnorm(0, 10), log_sigma ~ dnorm( 0, 1 ) ), data = data_brainsize ) precis(model_brain_size) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.53 0.07 0.42 0.64 beta_m 0.17 0.07 0.05 0.29 log_sigma -1.71 0.29 -2.18 -1.24 extract_r2 &lt;- function(quap_fit, decimals = 5){ data &lt;- sim(quap_fit) %&gt;% as_tibble() %&gt;% set_names(nm = data_brainsize$species) %&gt;% summarise(across(everything(), mean, .names = &quot;{.col}&quot;)) %&gt;% pivot_longer(everything(), names_to = &quot;species&quot;, values_to = &quot;mean_brainsize&quot;) %&gt;% mutate(brain_size_scl = data_brainsize$brain_size_scl, diff = mean_brainsize - brain_size_scl) round(1 - var2(data$diff) / var2(data$brain_size_scl), digits = decimals) } set.seed(12) extract_r2(model_brain_size) #&gt; [1] 0.47746 8.1.2 Higher order polynomials \\[ \\begin{array}{rclcr} b_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; &amp;\\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{1} m_{i} + \\beta_{2} m_{i}^2 &amp; &amp;\\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0.5, 1) &amp; &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{j} &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{for}~j = 1..2 &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Log-Normal(0, 1) &amp; &amp;\\textrm{[$\\sigma$ prior]} \\end{array} \\] model_brain_size2 &lt;- quap( flist = alist( brain_size_scl ~ dnorm(mu, exp(log_sigma)), mu &lt;- alpha + beta_m[1] * mass_std + beta_m[2] * mass_std ^ 2, alpha ~ dnorm(0.5, 1), beta_m ~ dnorm(0, 10), log_sigma ~ dnorm( 0, 1 ) ), data = data_brainsize, start = list(beta_m = rep(0, 2)) ) model_brain_size3 &lt;- quap( flist = alist( brain_size_scl ~ dnorm(mu, exp(log_sigma)), mu &lt;- alpha + beta_m[1] * mass_std + beta_m[2] * mass_std ^ 2 + beta_m[3] * mass_std ^ 3, alpha ~ dnorm(0.5, 1), beta_m ~ dnorm(0, 10), log_sigma ~ dnorm( 0, 1 ) ), data = data_brainsize, start = list(beta_m = rep(0, 3)) ) model_brain_size4 &lt;- quap( flist = alist( brain_size_scl ~ dnorm(mu, exp(log_sigma)), mu &lt;- alpha + beta_m[1] * mass_std + beta_m[2] * mass_std ^ 2 + beta_m[3] * mass_std ^ 3 + beta_m[4] * mass_std ^ 4, alpha ~ dnorm(0.5, 1), beta_m ~ dnorm(0, 10), log_sigma ~ dnorm( 0, 1 ) ), data = data_brainsize, start = list(beta_m = rep(0, 4)) ) model_brain_size5 &lt;- quap( flist = alist( brain_size_scl ~ dnorm(mu, exp(log_sigma)), mu &lt;- alpha + beta_m[1] * mass_std + beta_m[2] * mass_std ^ 2 + beta_m[3] * mass_std ^ 3 + beta_m[4] * mass_std ^ 4 + beta_m[5] * mass_std ^ 5, alpha ~ dnorm(0.5, 1), beta_m ~ dnorm(0, 10), log_sigma ~ dnorm( 0, 1 ) ), data = data_brainsize, start = list(beta_m = rep(0, 5)) ) model_brain_size6 &lt;- quap( flist = alist( brain_size_scl ~ dnorm(mu, exp(log_sigma)), mu &lt;- alpha + beta_m[1] * mass_std + beta_m[2] * mass_std ^ 2 + beta_m[3] * mass_std ^ 3 + beta_m[4] * mass_std ^ 4 + beta_m[5] * mass_std ^ 5 + beta_m[6] * mass_std ^ 6, alpha ~ dnorm(0.5, 1), beta_m ~ dnorm(0, 10), log_sigma ~ dnorm( 0, 1 ) ), data = data_brainsize, start = list(beta_m = rep(0, 6)) ) mass_seq &lt;- seq( from = min(data_brainsize$mass_std) - .15, to = max(data_brainsize$mass_std) + .15, length.out = 101) plot_poly &lt;- function(mod, ylim){ model_posterior_samples &lt;- extract.samples(mod) %&gt;% as.data.frame() %&gt;% as_tibble() model_posterior_prediction_samples &lt;- link(mod, data = tibble(mass_std = mass_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = mass_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;mass_std&quot;, values_to = &quot;brain_size_scl&quot;) %&gt;% mutate(mass_std = as.numeric(mass_std)) model_posterior_prediction_pi &lt;- model_posterior_prediction_samples %&gt;% group_by(mass_std) %&gt;% summarise(mean = mean(brain_size_scl), PI_lower = PI(brain_size_scl)[1], PI_upper = PI(brain_size_scl)[2]) %&gt;% ungroup() p &lt;- ggplot(mapping = aes(x = mass_std * sd(data_brainsize$mass) + mean(data_brainsize$mass))) + geom_smooth(data = model_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean * max(data_brainsize$brain_size), ymin = PI_lower * max(data_brainsize$brain_size), ymax = PI_upper * max(data_brainsize$brain_size)), color = clr2, fill = fll2, size = .2) + geom_point(data = data_brainsize, aes(y = brain_size_scl * max(data_brainsize$brain_size)), color = rgb(0,0,0,.5), size = 1) + labs(x = &quot;mass&quot;, y = &quot;brain_size&quot;, title = glue(&quot;*R&lt;sup&gt;2&lt;/sup&gt;:* {extract_r2(mod, decimals = 2)}&quot;)) + coord_cartesian(ylim = ylim) + theme(plot.title = element_markdown()) if(identical(mod, model_brain_size6)) { p &lt;- p + geom_hline(yintercept = 0, color = clr_dark, linetype = 3 ) } p } list(model_brain_size, model_brain_size2,model_brain_size3, model_brain_size4, model_brain_size5, model_brain_size6) %&gt;% purrr::map2(.y = list(c(420, 1400), c(420, 1400), c(420, 1400), c(300, 1950), c(300, 1950), c(-400, 1500)), plot_poly) %&gt;% wrap_plots() + plot_annotation(tag_levels = &quot;a&quot;) 8.1.3 Underfitting Leave one out (LOO) model_loo &lt;- function(idx = 0, mod_degree = 1){ data &lt;- data_brainsize[-idx, ] if(mod_degree == 1){ current_mod &lt;- quap( flist = alist( brain_size_scl ~ dnorm(mu, exp(log_sigma)), mu &lt;- alpha + beta_m[1] * mass_std, alpha ~ dnorm(0.5, 1), beta_m ~ dnorm(0, 10), log_sigma ~ dnorm( 0, 1 ) ), data = data, start = list(beta_m = rep(0, 1)) ) } else if(mod_degree == 4) { current_mod &lt;- quap( flist = alist( brain_size_scl ~ dnorm(mu, exp(log_sigma)), mu &lt;- alpha + beta_m[1] * mass_std + beta_m[2] * mass_std ^ 2 + beta_m[3] * mass_std ^ 3 + beta_m[4] * mass_std ^ 4, alpha ~ dnorm(0.5, 1), beta_m ~ dnorm(0, 10), log_sigma ~ dnorm( 0, 1 ) ), data = data, start = list(beta_m = rep(0, 4)) ) } else { stop(&quot;`mod_degree` needs to be either 1 or 4&quot;) } model_posterior_prediction_samples &lt;- link(current_mod, data = tibble(mass_std = mass_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = mass_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;mass_std&quot;, values_to = &quot;brain_size_scl&quot;) %&gt;% mutate(mass_std = as.numeric(mass_std)) model_posterior_prediction_pi &lt;- model_posterior_prediction_samples %&gt;% group_by(mass_std) %&gt;% summarise(mean = mean(brain_size_scl)) %&gt;% ungroup() %&gt;% mutate(idx = idx, mod_degree = mod_degree) model_posterior_prediction_pi } cross_df(list(idx = seq_along(data_brainsize$species), mod_degree = c(1, 4))) %&gt;% pmap_dfr(model_loo) %&gt;% ggplot(mapping = aes(x = mass_std * sd(data_brainsize$mass) + mean(data_brainsize$mass))) + geom_line(aes(y = mean * max(data_brainsize$brain_size), group = factor(idx), color = idx)) + geom_point(data = data_brainsize %&gt;% mutate(idx = row_number()), aes(y = brain_size_scl * max(data_brainsize$brain_size), fill = idx, color = after_scale(clr_darken(fill))), size = 2, shape = 21) + labs(x = &quot;mass&quot;, y = &quot;brain_size&quot;) + facet_wrap(mod_degree ~ ., labeller = label_both) + scale_color_gradientn(colors = c(clr0dd, clr0, clr2), guide = &quot;none&quot;) + scale_fill_gradientn(colors = c(clr0dd, clr0, clr2), guide = &quot;none&quot;) + coord_cartesian(ylim = c(0, 2e3)) + theme(plot.title = element_markdown()) 8.2 Entropy and Accuracy 8.2.1 Entropy Definition of Information Entropy \\[ H(p) = - E~\\textrm{log}(p_{i}) = - \\sum_{i = 1}^n p_{i}~\\textrm{log}(p_{i}) \\] or verbally: The uncertainty contained in a probability distribution is the average log-probability of an event. which fulfills the requirements: uncertainty should be continuous uncertainty should increase with the number of possible events uncertainty should be additive Example for \\(p_1 = 0.3\\) and \\(p_2 = 0.7\\): \\[ H(p) = - \\big( p_{1} \\textrm{log}(p_{1}) + p_{2} \\textrm{log}(p_{2}) \\big) \\approx 0.61 \\] p &lt;- c( .3, .7 ) - sum( p * log(p) ) #&gt; [1] 0.6108643 Compared to Abu Dhabi (“it hardly ever rains”) p &lt;- c( .01, .99 ) - sum( p * log(p) ) #&gt; [1] 0.05600153 Entropy increases with th dimensionality of the prediction problem (eg. predicting 🌧/ 🌨 / ☀️) p &lt;- c( .15, .5, .7 ) - sum( p * log(p) ) #&gt; [1] 0.880814 8.2.2 Accuracy Divergence: The additional uncertainty induced by using probabilities from one distribution to describe another distribution. The Kullback-Leibler Divergence (KL): \\[ D_{KL}(p, q) = \\sum_{i} p_{i} \\big( \\textrm{log}(p_{i}) - \\textrm{log}(q_{i})\\big) = \\sum_{i} p_{i} \\textrm{log}\\left(\\frac{p_{i}}{q_{i}}\\right) \\] tibble(p1 = .3, p2 = .7, q1 = seq(from = .01, to = .99, by = .01), q2 = 1 - q1, d_kl = p1 * log(p1 / q1) + p2 * log(p2 / q2)) %&gt;% ggplot(aes(x = q1, y = d_kl)) + geom_line(color = clr2) + geom_vline(xintercept = .3, color = clr_dark, linetype = 3) 8.2.3 Estimating Divergence Log-probability score to compare the predictive accuracy of different models: \\[ S(q) = \\sum_{i} \\textrm(log) (q_{i}) \\] where \\(i\\) indexes each case and \\(q_{i}\\) is the likelihood for each case. A (re-scaled) equivalent is given with the deviance: \\[ D(q) = -2 \\sum_{i} \\textrm(log) (q_{i}) \\] and it’s Bayesian version the Log-pointwise-predictive density: \\[ lppd(y, \\Theta) = \\sum_{i} \\textrm{log} \\frac{1}{S} \\sum_{s} p (y_{i} | \\Theta_{s}) \\] where \\(S\\) is the number of samples and \\(\\Theta_{s}\\) is the s-th set of sampled parameter values in the posterior distribution. # lppd &lt;- function (fit, ...) { # ll &lt;- sim(fit, ll = TRUE, ...) # n &lt;- ncol(ll) # ns &lt;- nrow(ll) # f &lt;- function(i) log_sum_exp(ll[, i]) - log(ns) # lppd &lt;- sapply(1:n, f) # return(lppd) # } set.seed(1) lppd(model_brain_size, n = 1e4) #&gt; [1] 0.6098668 0.6483438 0.5496093 0.6234934 0.4648143 0.4347605 -0.8444632 8.2.4 Scoring the right data tibble(model_degree = 1:6, model = list(model_brain_size, model_brain_size2, model_brain_size3, model_brain_size4, model_brain_size5, model_brain_size6)) %&gt;% mutate(log_prob_score = map_dbl(model, .f = function(mod){sum(lppd(mod))})) #&gt; # A tibble: 6 × 3 #&gt; model_degree model log_prob_score #&gt; &lt;int&gt; &lt;list&gt; &lt;dbl&gt; #&gt; 1 1 &lt;map&gt; 2.42 #&gt; 2 2 &lt;map&gt; 2.65 #&gt; 3 3 &lt;map&gt; 3.69 #&gt; 4 4 &lt;map&gt; 5.32 #&gt; 5 5 &lt;map&gt; 14.1 #&gt; 6 6 &lt;map&gt; 39.6 n_cores &lt;- 8 run_sim &lt;- function(k, n_samples, n_sim = 1e3, b_sigma = 100){ mcreplicate(n_sim, sim_train_test(N = n_samples, k = k, b_sigma = b_sigma), mc.cores = n_cores) %&gt;% t() %&gt;% as_tibble() %&gt;% summarise(mean_p = mean(V1), mean_q = mean(V2), sd_p = sd(V1), sd_q = sd(V2)) %&gt;% mutate(k = k, n_samples = n_samples, b_sigma = b_sigma) } tictoc::tic() data_sim &lt;- cross_df(list(k = 1:5, n_samples = c(20, 100))) %&gt;% pmap_dfr(run_sim) tictoc::toc() write_rds(data_sim, &quot;data/rethinking_c6_data_sim.Rds&quot;) tictoc::tic() data_sim_var_beta &lt;- crossing(k = 1:5, n_samples = c(20, 100), b_sigma = c(1, 0.5, 0.2)) %&gt;% pmap_dfr(run_sim) tictoc::toc() write_rds(data_sim_var_beta, &quot;data/rethinking_c6_data_sim_var_beta.Rds&quot;) data_sim &lt;- read_rds(&quot;data/rethinking_c6_data_sim.Rds&quot;) x_dodge &lt;- .3 data_sim %&gt;% ggplot() + geom_pointrange(aes(x = k - .5 * x_dodge, ymin = mean_p - sd_p, y = mean_p, ymax = mean_p + sd_p, color = &quot;train&quot;, fill = after_scale(clr_lighten(color))), shape = 21) + geom_pointrange(aes(x = k + .5 * x_dodge, ymin = mean_q - sd_q, y = mean_q, ymax = mean_q + sd_q, color = &quot;test&quot;, fill = after_scale(clr_lighten(color))), shape = 21) + scale_color_manual(&quot;&quot;, values = c(train = clr0dd, test = clr2)) + facet_wrap(n_samples ~ ., scales = &quot;free&quot;, label = label_both) + labs(x = &quot;number of parameters&quot;, y = &quot;deviance&quot;) + theme(legend.position = &quot;bottom&quot;) p_curves &lt;- ggplot() + stat_function(fun = function(x){dnorm(x = x, mean = 0, sd = .2)}, color = clr0dd, linetype = 1, xlim = c(-3, 3), n = 301) + stat_function(fun = function(x){dnorm(x = x, mean = 0, sd = .5)}, color = clr0dd, linetype = 2, xlim = c(-3, 3), n = 301) + stat_function(fun = function(x){dnorm(x = x, mean = 0, sd = 1)}, color = clr0dd, linetype = 3, xlim = c(-3, 3), n = 501) + labs(x = &quot;parameter value&quot;, y = &quot;density&quot;) data_sim_var_beta &lt;- read_rds(&quot;data/rethinking_c6_data_sim_var_beta.Rds&quot;) p_lines &lt;- data_sim_var_beta %&gt;% dplyr::select(mean_p, mean_q,k:b_sigma) %&gt;% pivot_longer(cols = mean_p:mean_q, names_to = &quot;set&quot;, values_to = &quot;mean&quot;) %&gt;% ggplot() + geom_line(aes(x = k, y = mean, linetype = factor(b_sigma), color = set)) + geom_point(data = data_sim %&gt;% dplyr::select(mean_p, mean_q,k:b_sigma) %&gt;% pivot_longer(cols = mean_p:mean_q, names_to = &quot;set&quot;, values_to = &quot;mean&quot;), aes(x = k, y = mean, color = set, fill = after_scale(clr_lighten(color))), shape = 21, size = .9) + scale_color_manual(&quot;&quot;, values = c(mean_p = clr0dd, mean_q = clr2), labels = c(mean_p = &quot;test&quot;, mean_q = &quot;train&quot;)) + scale_linetype_manual(&quot;beta prior width&quot;, values = c(`1` = 3, `0.5` = 2, `0.2` = 1)) + facet_wrap(n_samples ~ ., scales = &quot;free&quot;, label = label_both) + labs(x = &quot;number of parameters&quot;, y = &quot;deviance&quot;) + theme(legend.position = &quot;bottom&quot;) p_curves + p_lines + plot_annotation(tag_levels = &quot;a&quot;) + plot_layout(widths = c(.5, 1), guides = &quot;collect&quot;) &amp; theme(legend.position = &quot;bottom&quot;) 8.3 Golem taming: regularization 8.3.1 Cross-validataion leave-one-out cross-validataion (loocv): dropping one data point in each fold (resulting in \\(n\\) test sets, so \\(n\\) refits of the model and \\(n\\) posterior distributions) Pareto-smoothed importance sampling cross-validataion (PSIS): approximates loocv by sampling from the original posterior while taking the importance/weight of each data point into account (provides feedback about it’s ow reliability, no model re-fitting necessary) \\[ s_{PSIS} = \\sqrt{N~\\textrm{var}(\\textrm{psis}_{i})} \\] The importance sampling estimate of out-of-sample lppd: \\[ lppd_{IS} = \\sum_{i=1}^{N} \\textrm{log} \\frac{\\Sigma_{s=1}^{S} r(\\theta_{s}p(y_{i}|\\theta_{s}))}{\\sigma_{s=1}^{S} r(\\theta_{s})} \\] The Pareto distribution \\[ p(r | u, \\sigma, k) = \\sigma^{-1} \\big(1 + k (r - u) \\sigma ^{-1}\\big)^{-\\frac{1}{k}-1} \\] ggplot() + stat_function(fun = function(x){dpareto(x = x, xmin = 1, alpha = 1)},aes(color = &quot;1&quot;), xlim = c(0, 5), n = 201)+ stat_function(fun = function(x){dpareto(x = x, xmin = 1, alpha = .7)},aes(color = &quot;0.7&quot;), xlim = c(0, 5), n = 201)+ stat_function(fun = function(x){dpareto(x = x, xmin = 1, alpha = .3)},aes(color = &quot;0.3&quot;), xlim = c(0, 5), n = 201)+ stat_function(fun = function(x){dpareto(x = x, xmin = 1, alpha = .1)},aes(color = &quot;0.1&quot;), xlim = c(0, 5), n = 201) + scale_color_manual(&quot;alpha&quot;, values = c(`1` = clr0dd, `0.7` = clr1, `0.3` = clr2, `0.1` = clr3)) + coord_cartesian(ylim = c(-.1, 15), expand = 0) + theme(legend.position = &quot;bottom&quot;) 8.3.2 Information Criteria 8.3.2.1 Akaike information criterion Only for legacy reasons \\[ AIC = D_{train} + 2p = -2 lppd + 2p \\] AIC is an approximation that depends on flat priors posterior distribution is \\(\\sim\\) gaussian sample size \\(N\\) is much greater than numbers of parameters \\(k\\) 8.3.2.2 Widely Applicable Information Criterion \\[ WAIC(y, \\Theta) = -2 (lppd - \\underbrace{\\sum_{i} \\textrm{var}_{\\theta}~\\textrm{log}~p(y_{i}|\\theta)}_{\\textrm{penalty term}}) \\] WAIC calculations data(cars) set.seed(94) model_cars &lt;- quap( flist = alist( dist ~ dnorm(mu, sigma), mu &lt;- alpha + beta_speed * speed, alpha ~ dnorm(0, 100), beta_speed ~ dnorm(0, 10), sigma ~ dexp(1) ), data = cars ) set.seed(94) n_samples &lt;- 1e3 n_cases &lt;- nrow(cars) cars_posterior_predictive_samples &lt;- extract.samples(model_cars, n = n_samples) %&gt;% as_tibble() logprob &lt;- sapply(1:n_samples, function(idx){ mu &lt;- cars_posterior_predictive_samples$alpha[idx] + cars_posterior_predictive_samples$beta_speed[idx] * cars$speed dnorm( cars$dist, mean = mu, sd = cars_posterior_predictive_samples$sigma[idx], log = TRUE)}) lppd &lt;- sapply(1:n_cases, function(i){log_sum_exp(logprob[i,]) - log(n_samples)}) pWAIC &lt;- sapply(1:n_cases, function(i){var(logprob[i,])}) -2 * (sum(lppd) - sum(pWAIC)) #&gt; [1] 423.3127 waic_vec &lt;- -2 * (lppd - pWAIC) sqrt(n_cases * var(waic_vec)) #&gt; [1] 17.81271 8.3.3 Comparing CV, PSIS and WAIC make_sim &lt;- function(n, k, b_sigma) { r &lt;- mcreplicate(n_sim, sim_train_test(N = n, k = k, b_sigma = b_sigma, WAIC = TRUE, LOOCV = TRUE, LOOIC = TRUE), mc.cores = n_cores) t &lt;- tibble( deviance_os = mean(unlist(r[2, ])), deviance_w = mean(unlist(r[3, ])), deviance_p = mean(unlist(r[11, ])), deviance_c = mean(unlist(r[19, ])), error_w = mean(unlist(r[7, ])), error_p = mean(unlist(r[15, ])), error_c = mean(unlist(r[20, ])) ) return(t) } n_sim &lt;- 1e3 n_cores &lt;- 8 tictoc::tic() data_sim_scores &lt;- crossing(n = c(20, 100), k = 1:5, b_sigma = c(0.5, 100)) %&gt;% mutate(sim = pmap(list(n, k, b_sigma), make_sim)) %&gt;% unnest(sim) tictoc::toc() # 119984.727 sec elapsed write_rds(data_sim_scores, &quot;data/rethinking_c6_data_sim_scores.Rds&quot;) data_sim_scores &lt;- read_rds(&quot;data/rethinking_c6_data_sim_scores.Rds&quot;) data_sim_scores %&gt;% pivot_longer(deviance_w:deviance_c) %&gt;% mutate(criteria = ifelse(name == &quot;deviance_w&quot;, &quot;WAIC&quot;, ifelse(name == &quot;deviance_p&quot;, &quot;PSIS&quot;, &quot;CV&quot;))) %&gt;% ggplot(aes(x = k)) + geom_line(aes(y = value, color = criteria)) + geom_point(aes(y = deviance_os, shape = factor(b_sigma)), size = 1.5) + scale_shape_manual(&quot;sigma&quot;, values = c(19, 1)) + scale_color_manual(values = c(clr0dd, clr1, clr2)) + labs(x = &quot;number of parameters (k)&quot;, y = &quot;average deviance&quot;) + facet_grid(n ~ b_sigma, scales = &quot;free_y&quot;, labeller = label_both)+ theme(legend.position = &quot;bottom&quot;) 8.4 Model comparison 8.4.1 Model mis-selection chapter6_models &lt;- read_rds(&quot;envs/chapter6_models.rds&quot;) set.seed(11) WAIC(chapter6_models$model_fungus_post_treatment) #&gt; WAIC lppd penalty std_err #&gt; 1 361.4511 -177.1724 3.553198 14.17033 set.seed(77) comp_waic &lt;- compare(chapter6_models$model_fungus_post_treatment, chapter6_models$model_fungus_no_treatment, chapter6_models$model_fungus_only_treatment, func = WAIC) %&gt;% as_tibble_rn() %&gt;% mutate(model = str_remove(param,&quot;.*\\\\$&quot;), mod = model %&gt;% purrr::map(.f = function(m){chapter6_models[[m]]}), deviance = mod %&gt;% purrr::map_dbl(.f = rethinking::deviance), model = str_remove(model,&quot;model_fungus_&quot;)) %&gt;% dplyr::select(-param) comp_waic %&gt;% dplyr::select(-mod) %&gt;% dplyr::select(model, everything()) #&gt; # A tibble: 3 × 8 #&gt; model WAIC SE dWAIC dSE pWAIC weight deviance #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 post_treatment 361. 14.2 0 NA 3.57 1.00e+ 0 354. #&gt; 2 only_treatment 403. 11.3 41.3 10.5 2.65 1.08e- 9 397. #&gt; 3 no_treatment 406. 11.8 44.7 12.2 1.70 1.98e-10 402. WAIC and PSIS result in similar values: compare(chapter6_models$model_fungus_post_treatment, chapter6_models$model_fungus_no_treatment, chapter6_models$model_fungus_only_treatment, func = PSIS) %&gt;% as_tibble_rn() #&gt; # A tibble: 3 × 7 #&gt; PSIS SE dPSIS dSE pPSIS weight param #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 362. 14.3 0 NA 3.93 1.00e+ 0 chapter6_models$model_fungus_post_trea… #&gt; 2 403. 11.3 40.6 10.4 2.69 1.50e- 9 chapter6_models$model_fungus_only_trea… #&gt; 3 406. 11.7 43.9 12.2 1.63 2.99e-10 chapter6_models$model_fungus_no_treatm… tibble(post_treatment = WAIC(chapter6_models$model_fungus_post_treatment, pointwise = TRUE)$WAIC, only_treatment = WAIC(chapter6_models$model_fungus_only_treatment, pointwise = TRUE)$WAIC, no_treatment = WAIC(chapter6_models$model_fungus_no_treatment, pointwise = TRUE)$WAIC) %&gt;% mutate(model_difference_post_only = post_treatment - only_treatment, model_difference_no_only = no_treatment - only_treatment) %&gt;% summarise(`post-only` = sqrt(n()[[1]] * var(model_difference_post_only)), `no-only` = sqrt(n()[[1]] * var(model_difference_no_only))) %&gt;% pivot_longer(everything(), names_to = &quot;comparison&quot;, values_to = &quot;se_of_model_differnce&quot;) #&gt; # A tibble: 2 × 2 #&gt; comparison se_of_model_differnce #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 post-only 10.5 #&gt; 2 no-only 4.90 \\(\\rightarrow\\) compare to WAIC table $dSE[[2]] estimating the model difference for a z-score of \\(\\sim\\) 2.6 (99%)\" comp_waic$dWAIC[[2]] + c(-1, 1) * comp_waic$dSE[[2]] * 2.6 #&gt; [1] 14.12572 68.47753 library(tidybayes) comp_waic %&gt;% ggplot() + geom_vline(xintercept = comp_waic$WAIC[[1]], color = clr_dark, linetype = 3) + geom_pointinterval(aes(y = model, x = WAIC, xmin = WAIC - SE, xmax = WAIC + SE, color = &quot;WAIC&quot;, fill = after_scale(clr_lighten(color))), shape = 21, size = 2.5) + geom_point(aes(y = model, x = deviance, color = &quot;WAIC&quot;)) + geom_pointinterval(data = comp_waic %&gt;% filter(row_number() &gt; 1), aes(y = as.numeric(factor(model)) + .25, x = WAIC, xmin = WAIC - dSE, xmax = WAIC + dSE, color = &quot;dSE&quot;), shape = 17, size = .9) + scale_color_manual(values = c(WAIC = clr2, dSE = clr0dd)) + theme(legend.position = &quot;bottom&quot;) set.seed(93) compare(chapter6_models$model_fungus_post_treatment, chapter6_models$model_fungus_no_treatment, chapter6_models$model_fungus_only_treatment)@dSE %&gt;% round(digits = 2) %&gt;% as_tibble() %&gt;% set_names(., nm = names(.) %&gt;% str_remove(pattern = &quot;.*model_fungus_&quot;)) %&gt;% mutate(` ` = comp_waic$model) %&gt;% dplyr::select(` `, everything()) %&gt;% knitr::kable() post_treatment no_treatment only_treatment post_treatment NA 12.22 10.49 only_treatment 12.22 NA 4.86 no_treatment 10.49 4.86 NA Weight of a model (last column of compare(), the relative support for each model): \\[ w_{i} = \\frac{\\textrm{exp}(-0.5\\Delta_{i})}{\\Sigma_{j}\\textrm{exp}(-0.5\\Delta_{j})} \\] These weights are important for model averaging. 8.4.2 Outliers and other illusions chapter5_models &lt;- read_rds(&quot;envs/chapter5_models.rds&quot;) set.seed(24071847) compare(chapter5_models$model_age, chapter5_models$model_marriage, chapter5_models$model_multiple) %&gt;% as_tibble_rn() #&gt; # A tibble: 3 × 7 #&gt; WAIC SE dWAIC dSE pWAIC weight param #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 127. 14.2 0 NA 4.41 0.679 chapter5_models$model_age #&gt; 2 129. 14.4 1.51 0.883 5.48 0.320 chapter5_models$model_multiple #&gt; 3 141. 11.0 13.5 10.4 3.71 0.000805 chapter5_models$model_marriage psis_k &lt;- tibble(waic_penalty = (function(){set.seed(set.seed(23)); WAIC(chapter5_models$model_multiple, pointwise = TRUE)$penalty})(), psis_k = (function(){set.seed(set.seed(23)); PSIS(chapter5_models$model_multiple, pointwise = TRUE)$k})(), location = chapter5_models$data_waffle$Loc) p_psis_k &lt;- psis_k %&gt;% ggplot(aes(x = psis_k ,y = waic_penalty)) + geom_vline(xintercept = .5, color = clr_dark, linetype = 3) + geom_point(shape = 21, size = 2, color = clr2, fill = fll2) + geom_text(data = psis_k %&gt;% filter(location %in% c(&quot;ME&quot;, &quot;ID&quot;)), aes(x = psis_k - .15, label = location)) p_dens &lt;- ggplot() + stat_function(fun = function(x){dnorm(x = x, sd = .8)}, xlim = c(-4, 4), n = 201, aes(color = &quot;gaussian&quot;), linetype = 3)+ stat_function(fun = function(x){dstudent(x = x, nu = 2, sigma = .55)}, xlim = c(-4, 4), n = 201, aes(color = &quot;student t&quot;)) + labs(y = &quot;density&quot;, x = &quot;value&quot;) p_logdens &lt;- ggplot() + stat_function(fun = function(x){-log(dnorm(x = x, sd = .8))}, xlim = c(-4, 4), n = 201, aes(color = &quot;gaussian&quot;), linetype = 3)+ stat_function(fun = function(x){-log(dstudent(x = x, nu = 2, sigma = .55))}, xlim = c(-4, 4), n = 201, aes(color = &quot;student t&quot;)) + labs(y = &quot;- log density&quot;, x = &quot;value&quot;) p_psis_k + p_dens + p_logdens + plot_layout(guides = &quot;collect&quot;) &amp; scale_color_manual(&quot;distribution&quot;, values = c(gaussian = clr0dd, `student t` = clr2)) &amp; theme(legend.position = &quot;bottom&quot;) chapter5_models$model_multiple #&gt; #&gt; Quadratic approximate posterior distribution #&gt; #&gt; Formula: #&gt; divorce_std ~ dnorm(mu, sigma) #&gt; mu &lt;- alpha + beta_M * marriage_std + beta_A * median_age_std #&gt; alpha ~ dnorm(0, 0.2) #&gt; beta_A ~ dnorm(0, 0.5) #&gt; beta_M ~ dnorm(0, 0.5) #&gt; sigma ~ dexp(1) #&gt; #&gt; Posterior means: #&gt; alpha beta_A beta_M sigma #&gt; -2.484974e-08 -6.135134e-01 -6.538068e-02 7.851183e-01 #&gt; #&gt; Log-likelihood: -59.24 model_multiple_sudent &lt;- quap( flist = alist( divorce_std ~ dstudent(2, mu, sigma), mu &lt;- alpha + beta_M * marriage_std + beta_A * median_age_std, alpha ~ dnorm(0, 0.2), beta_A ~ dnorm(0, 0.5), beta_M ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = chapter5_models$data_waffle ) PSIS(chapter5_models$model_multiple) #&gt; PSIS lppd penalty std_err #&gt; 1 130.2264 -65.11318 6.373124 15.15747 With the Student T distribution as a the likelihood, \\(k\\) is reduced as there is more mass in the tails of the distribution (thus, Idaho is less surprising) PSIS(model_multiple_sudent) #&gt; PSIS lppd penalty std_err #&gt; 1 133.7128 -66.85638 6.890912 11.90639 precis(chapter5_models$model_multiple) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.00 0.10 -0.16 0.16 beta_A -0.61 0.15 -0.85 -0.37 beta_M -0.07 0.15 -0.31 0.18 sigma 0.79 0.08 0.66 0.91 precis(model_multiple_sudent) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.02 0.10 -0.14 0.17 beta_A -0.70 0.13 -0.91 -0.49 beta_M 0.00 0.21 -0.33 0.34 sigma 0.55 0.08 0.42 0.68 Also, as the influence of Idaho is reduced, the estimate of beta_A is decreased in the updated model. library(rlang) chapter7_models &lt;- env( data_brainsize = data_brainsize, model_brain_size = model_brain_size, model_brain_size2 = model_brain_size2, model_brain_size3 = model_brain_size3, model_brain_size4 = model_brain_size4, model_brain_size5 = model_brain_size5, model_brain_size6 = model_brain_size6, cars = cars, model_cars = model_cars ) 8.5 Homework E1 Criteria defining information entropy The measure of uncertainty should be continuous (so there should be sudden shifts in the change of uncertainty for minor changes in the underlying probabilities) The measure of uncertainty should increase with the number of possible events (higher uncertainties are expected as the dimensionality of the underlying possibility increases) The measure of uncertainty should be additive (the sum of the uncertainty of all sub-events should result exactly in total uncertainty) E2 recall Information Entropy \\[ H(p) = - \\sum_{i=1}^{n} p_i \\textrm{log}(p_i) = -\\big( p_1 \\textrm{log}(p_1) + p_2 \\textrm{log}(p_2) \\big) \\] p_coin &lt;- c(tails = .3, heads = .7) -sum(p_coin * log(p_coin)) #&gt; [1] 0.6108643 E3 p_die &lt;- c(`1` = .2, `2` = .25, `3` = .25, `4` = .3) -sum(p_die * log(p_die)) #&gt; [1] 1.376227 E4 using L’Hôpital’s Rule for the limit \\(\\textrm{lim}_{p_{i}\\rightarrow\\infty} p_{i} \\textrm{log}(p_i) =0\\): p_die &lt;- c(`1` = 1/3, `2` = 1/3, `3` = 1/3, `4` = 0) -sum(c(p_die[1:3] * log(p_die[1:3]), 0)) #&gt; [1] 1.098612 M1 \\[ \\begin{array}{rcl} AIC &amp; = &amp; D_{train} + 2p\\\\ &amp; = &amp; -2 lppd + 2p \\\\ &amp;&amp;\\\\ WAIC(y, \\Theta) &amp; = &amp;-2 (lppd - \\underbrace{\\sum_{i} \\textrm{var}_{\\theta}~\\textrm{log}~p(y_{i}|\\theta)}_{\\textrm{penalty term}})\\\\ &amp; = &amp; -2 lppd + 2 \\sum_{i} \\textrm{var}_{\\theta}~\\textrm{log}~p(y_{i}|\\theta)\\\\ \\end{array} \\] \\(AIC = WAIC\\) if: \\[ \\begin{array}{rcl} p &amp; = &amp; \\sum_{i} \\textrm{var}_{\\theta}~\\textrm{log}~p(y_{i}|\\theta)\\\\ \\end{array} \\] We can expect the more general criterion (WAIC) to match the more specific one (AIC), if: the priors are flat and overwhelmed by the likelihood the posterior distribution is approximately gaussian the number of samples (\\(n\\)) is much larger than the number of parameters (\\(k\\)) M2 Both model selection and model comparison list the WAIC of a suite of models. For model selection however, all but the best performing models are discarded, while in model comparison the distribution of the criterion and the relative performance can be used to investigate subtle influences on the individual influences on the models. This provides much more context for the following inferences. M3 The magnitude of information criteria are dependent on the number of observations. models fitted to different data sets are thus not comparable: sim_waic &lt;- tibble(sample_size = rep(c(100, 500, 1e3), each = 100)) %&gt;% mutate(data = purrr::map( sample_size, .f = function(sample_size){ tibble(x = rnorm(n = sample_size), y = rnorm(n = sample_size, .5 + .75 * x)) %&gt;% mutate(across(everything(), standardize),) }), model = purrr::map( data, function(data){ quap( flist = alist( y ~ dnorm(mu, sigma), mu &lt;- alpha + beta * x, alpha ~ dnorm(0, .2), beta ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data, start = list(alpha = 0, beta = 0, sigma = .2)) } ), lppd = map_dbl(model, ~sum(rethinking::lppd(.x))), information_criterion = purrr:::map( model, function(model){ tibble(WAIC = rethinking::WAIC(model)$WAIC, PSIS = suppressMessages(rethinking::PSIS(model)$PSIS)) } )) %&gt;% unnest(information_criterion) sim_waic %&gt;% dplyr::select(sample_size, lppd:PSIS) %&gt;% pivot_longer(cols = lppd:PSIS, names_to = &quot;information_criterion&quot;) %&gt;% ggplot(aes(x = value, y = factor(sample_size), color = information_criterion)) + stat_slab(slab_type = &quot;pdf&quot;, aes(fill = after_scale(clr_alpha(color))), size = .5, trim = FALSE, n = 301) + scale_color_manual(values = c(clr0dd, clr2, clr3), guide = &quot;none&quot;) + labs(y = &quot;sample_size&quot;) + facet_wrap(information_criterion ~ ., scales = &quot;free&quot;) + theme(axis.title.x = element_blank()) M4 Given the WAIC formula A more concentrated prior translates to less variation in the prior (thus a smaller variance). Given the formula for \\(WAIC\\), where the effective number of parameters (\\(p_{WAIC}\\)) is represented by the penalty term \\(\\sum_{i} \\textrm{var}_{\\theta}~\\textrm{log}~p(y_{i}|\\theta)\\): \\[ WAIC(y, \\Theta) = -2 (lppd - \\underbrace{\\sum_{i} \\textrm{var}_{\\theta}~\\textrm{log}~p(y_{i}|\\theta)}_{\\textrm{penalty term}}) \\] A smaller \\(\\textrm{var}_{\\theta}\\) will also decrease the entire \\(p_{WAIC}\\) library(cli) quap_prior_var &lt;- function(df, p_idx) { if(p_idx == prior_widths[[1]]){ mod &lt;- quap( flist = alist( y ~ dnorm(mu, sigma), mu &lt;- alpha + beta_1 * x1 + beta_2 * x2 + beta_3 * x3, alpha ~ dnorm(0, .2), c(beta_1, beta_2, beta_3) ~ dnorm(0, prior_widths[[1]]), sigma ~ dexp(1) ), data = df, start = list(alpha = 0, beta_1 = 0, beta_2 = 0, beta_3 = 0, sigma = .2) ) } else if(p_idx == prior_widths[[2]]){ mod &lt;- quap( flist = alist( y ~ dnorm(mu, sigma), mu &lt;- alpha + beta_1 * x1 + beta_2 * x2 + beta_3 * x3, alpha ~ dnorm(0, .2), c(beta_1, beta_2, beta_3) ~ dnorm(0, prior_widths[[2]]), sigma ~ dexp(1) ), data = df, start = list(alpha = 0, beta_1 = 0, beta_2 = 0, beta_3 = 0, sigma = .2)) } else if(p_idx == prior_widths[[3]]){ mod &lt;- quap( flist = alist( y ~ dnorm(mu, sigma), mu &lt;- alpha + beta_1 * x1 + beta_2 * x2 + beta_3 * x3, alpha ~ dnorm(0, .2), c(beta_1, beta_2, beta_3) ~ dnorm(0, prior_widths[[3]]), sigma ~ dexp(1) ), data = df, start = list(alpha = 0, beta_1 = 0, beta_2 = 0, beta_3 = 0, sigma = .2)) } else if(p_idx == prior_widths[[4]]){ mod &lt;- quap( flist = alist( y ~ dnorm(mu, sigma), mu &lt;- alpha + beta_1 * x1 + beta_2 * x2 + beta_3 * x3, alpha ~ dnorm(0, .2), c(beta_1, beta_2, beta_3) ~ dnorm(0, prior_widths[[4]]), sigma ~ dexp(1) ), data = df, start = list(alpha = 0, beta_1 = 0, beta_2 = 0, beta_3 = 0, sigma = .2) ) } else { stop(glue::glue(&quot;p_idx needs to be one of ({prior_widths[[1]]}, {prior_widths[[2]]}, {prior_widths[[3]]} or {prior_widths[[4]]})&quot;)) } cli_progress_update(.envir = .GlobalEnv) tibble(model = list(mod)) %&gt;% mutate(lppd = sum(rethinking::lppd(mod)), WAIC = rethinking::WAIC(mod)$WAIC, PSIS = suppressMessages(rethinking::PSIS(mod)$PSIS)) } set.seed(42) prior_widths &lt;- c(.1, .32, 1, 3.2) n &lt;- 75 cli_progress_bar(&quot;Simulate | Prior Width&quot;, total = n * length(prior_widths)) prior_sim &lt;- tibble(prior_sd = rep(prior_widths, each = n)) %&gt;% mutate( sample_data = purrr::map(1:n(), function(x, prior_sd) { n &lt;- 20 tibble(x1 = rnorm(n = n), x2 = rnorm(n = n), x3 = rnorm(n = n)) %&gt;% mutate(y = rnorm(n = n, mean = 0.3 + 0.8 * x1 + 0.6 * x2 + 1.2 * x3), across(everything(), standardize)) }), mod = map2(sample_data, prior_sd, quap_prior_var)) %&gt;% unnest(mod) %&gt;% mutate(p_waic = .5 * (WAIC + 2 * lppd)) prior_sim %&gt;% ggplot(aes(x = p_waic, y = factor(prior_sd))) + stat_slab(slab_type = &quot;pdf&quot;, fill = fll0, color = clr0dd, size = .5, adjust = .5, trim = FALSE, n = 501) + labs(y = &quot;prior_sd&quot;) M5 Informative priors prevent overfitting because the prevent the model from simply encoding the data. M6 Overly informative priors result in underfitting because the likelihood has only a marginal influence on the posterior. As a result, the posterior simply resembles the prior. H1 data(Laffer) data_laffer &lt;- Laffer %&gt;% as_tibble() %&gt;% mutate(dplyr::across(everything(), standardize, .names = &quot;{.col}_std&quot;), tax_rate_std_sq = tax_rate_std ^ 2) model_laffer_0 &lt;- quap( flist = alist( tax_revenue_std ~ dnorm(mu, simga), mu &lt;- alpha + 0 * tax_rate_std, alpha ~ dnorm(0, .2), simga ~ dexp(1) ), data = data_laffer ) tax_seq &lt;- seq(from = min(data_laffer$tax_rate_std) - .05 * diff(range(data_laffer$tax_rate_std)), to = max(data_laffer$tax_rate_std) + .05 * diff(range(data_laffer$tax_rate_std)), length.out = 101) tax_model_0_posterior_prediction_samples &lt;- link(model_laffer_0, data = data.frame(tax_rate_std = tax_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = tax_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;tax_rate_std&quot;, values_to = &quot;tax_revenue_std&quot;) %&gt;% mutate(tax_rate_std = as.numeric(tax_rate_std), tax_revenue = tax_revenue_std * sd(data_laffer$tax_revenue_std) + mean(data_laffer$tax_revenue_std), tax_rate = tax_rate_std * sd(data_laffer$tax_rate) + mean(data_laffer$tax_rate)) tax_model_0_posterior_prediction_pi &lt;- tax_model_0_posterior_prediction_samples %&gt;% group_by(tax_rate) %&gt;% summarise(mean = mean(tax_revenue_std), PI_lower = PI(tax_revenue_std)[1], PI_upper = PI(tax_revenue_std)[2]) %&gt;% ungroup() model_laffer_1 &lt;- quap( flist = alist( tax_revenue_std ~ dnorm(mu, simga), mu &lt;- alpha + beta_1 * tax_rate_std, alpha ~ dnorm(0, .2), beta_1 ~ dnorm(0, .5), simga ~ dexp(1) ), data = data_laffer ) tax_model_1_posterior_prediction_samples &lt;- link(model_laffer_1, data = data.frame(tax_rate_std = tax_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = tax_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;tax_rate_std&quot;, values_to = &quot;tax_revenue_std&quot;) %&gt;% mutate(tax_rate_std = as.numeric(tax_rate_std), tax_revenue = tax_revenue_std * sd(data_laffer$tax_revenue_std) + mean(data_laffer$tax_revenue_std), tax_rate = tax_rate_std * sd(data_laffer$tax_rate) + mean(data_laffer$tax_rate)) tax_model_1_posterior_prediction_pi &lt;- tax_model_1_posterior_prediction_samples %&gt;% group_by(tax_rate) %&gt;% summarise(mean = mean(tax_revenue_std), PI_lower = PI(tax_revenue_std)[1], PI_upper = PI(tax_revenue_std)[2]) %&gt;% ungroup() model_laffer_2 &lt;- quap( flist = alist( tax_revenue_std ~ dnorm(mu, simga), mu &lt;- alpha + beta_1 * tax_rate_std + beta_2 * tax_rate_std ^ 2, alpha ~ dnorm(0, .2), c(beta_1, beta_2) ~ dnorm(0, .5), simga ~ dexp(1) ), data = data_laffer ) tax_model_2_posterior_prediction_samples &lt;- link(model_laffer_2, data = data.frame(tax_rate_std = tax_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = tax_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;tax_rate_std&quot;, values_to = &quot;tax_revenue_std&quot;) %&gt;% mutate(tax_rate_std = as.numeric(tax_rate_std), tax_revenue = tax_revenue_std * sd(data_laffer$tax_revenue_std) + mean(data_laffer$tax_revenue_std), tax_rate = tax_rate_std * sd(data_laffer$tax_rate) + mean(data_laffer$tax_rate)) tax_model_2_posterior_prediction_pi &lt;- tax_model_2_posterior_prediction_samples %&gt;% group_by(tax_rate) %&gt;% summarise(mean = mean(tax_revenue_std), PI_lower = PI(tax_revenue_std)[1], PI_upper = PI(tax_revenue_std)[2]) %&gt;% ungroup() n_knots &lt;- 5 knot_list &lt;- quantile(data_laffer$tax_rate_std, probs = seq(0, 1, length.out = n_knots)) library(splines) b_spline_laffer &lt;- bs(data_laffer$tax_rate_std, knots = knot_list[-c(1, n_knots)], degree = 3, intercept = TRUE) model_laffer_3 &lt;- quap( flist = alist( tax_revenue_std ~ dnorm(mu, sigma), mu &lt;- alpha + B %*% w, alpha ~ dnorm(0, .5), w ~ dnorm(0, .5), sigma ~ dexp(1) ), data = list(tax_revenue_std = data_laffer$tax_revenue_std, B = b_spline_laffer), start = list(w = rep(0, ncol(b_spline_laffer))) ) b_spline_tax &lt;- bs(tax_seq, knots = knot_list[-c(1, n_knots)], degree = 3, intercept = TRUE) tax_model_3_posterior_prediction_samples &lt;- link(model_laffer_3, data = list(B = b_spline_tax)) %&gt;% as_tibble() %&gt;% set_names(nm = tax_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;tax_rate_std&quot;, values_to = &quot;tax_revenue_std&quot;) %&gt;% mutate(tax_rate_std = as.numeric(tax_rate_std), tax_revenue = tax_revenue_std * sd(data_laffer$tax_revenue_std) + mean(data_laffer$tax_revenue_std), tax_rate = tax_rate_std * sd(data_laffer$tax_rate) + mean(data_laffer$tax_rate)) tax_model_3_posterior_prediction_pi &lt;- tax_model_3_posterior_prediction_samples %&gt;% group_by(tax_rate) %&gt;% summarise(mean = mean(tax_revenue_std), PI_lower = PI(tax_revenue_std)[1], PI_upper = PI(tax_revenue_std)[2]) %&gt;% ungroup() rethinking::compare(model_laffer_0, model_laffer_1, model_laffer_2, model_laffer_3, func = WAIC) %&gt;% knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight model_laffer_1 89.18 22.74 0.00 NA 6.14 0.33 model_laffer_2 89.46 25.26 0.28 3.03 7.35 0.29 model_laffer_0 90.05 21.21 0.87 3.44 5.19 0.22 model_laffer_3 90.67 23.47 1.49 2.49 7.93 0.16 First of all, all models do share a substantial share of the weight within the model comparison making it hard to opt for one in particular. Although the model with the lowest \\(WAIC\\) is in fact the quadratic one, the data falls almost exclusively inside the rising part of the parabola - the situation for the spline is similar. There might be a saturation effect after an initial increase, but since even the flat model (model_laffer_0) rrecives a substantial share of the weight (\\(\\sim\\) 22 %), the other models might just be over fitting the outliers while actually there tax revenue and tax rate might be independent. H2 model_laffer_0_student &lt;- quap( flist = alist( tax_revenue_std ~ dstudent(2, mu, sigma), mu &lt;- alpha + 0 * tax_rate_std, alpha ~ dnorm(0, 0.2), sigma ~ dexp(1) ), data = data_laffer ) tax_model_0_student_posterior_prediction_samples &lt;- link(model_laffer_0_student, data = data.frame(tax_rate_std = tax_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = tax_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;tax_rate_std&quot;, values_to = &quot;tax_revenue_std&quot;) %&gt;% mutate(tax_rate_std = as.numeric(tax_rate_std), tax_revenue = tax_revenue_std * sd(data_laffer$tax_revenue_std) + mean(data_laffer$tax_revenue_std), tax_rate = tax_rate_std * sd(data_laffer$tax_rate) + mean(data_laffer$tax_rate)) tax_model_0_student_posterior_prediction_pi &lt;- tax_model_0_student_posterior_prediction_samples %&gt;% group_by(tax_rate) %&gt;% summarise(mean = mean(tax_revenue_std), PI_lower = PI(tax_revenue_std)[1], PI_upper = PI(tax_revenue_std)[2]) %&gt;% ungroup() model_laffer_1_student &lt;- quap( flist = alist( tax_revenue_std ~ dstudent(2, mu, sigma), mu &lt;- alpha + beta_1 * tax_rate_std, alpha ~ dnorm(0, 0.2), beta_1 ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_laffer ) tax_model_1_student_posterior_prediction_samples &lt;- link(model_laffer_1_student, data = data.frame(tax_rate_std = tax_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = tax_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;tax_rate_std&quot;, values_to = &quot;tax_revenue_std&quot;) %&gt;% mutate(tax_rate_std = as.numeric(tax_rate_std), tax_revenue = tax_revenue_std * sd(data_laffer$tax_revenue_std) + mean(data_laffer$tax_revenue_std), tax_rate = tax_rate_std * sd(data_laffer$tax_rate) + mean(data_laffer$tax_rate)) tax_model_1_student_posterior_prediction_pi &lt;- tax_model_1_student_posterior_prediction_samples %&gt;% group_by(tax_rate) %&gt;% summarise(mean = mean(tax_revenue_std), PI_lower = PI(tax_revenue_std)[1], PI_upper = PI(tax_revenue_std)[2]) %&gt;% ungroup() model_laffer_2_student &lt;- quap( flist = alist( tax_revenue_std ~ dstudent(2, mu, sigma), mu &lt;- alpha + beta_1 * tax_rate_std + beta_2 * tax_rate_std ^ 2, alpha ~ dnorm(0, 0.2), c(beta_1, beta_2) ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_laffer ) tax_model_2_student_posterior_prediction_samples &lt;- link(model_laffer_2_student, data = data.frame(tax_rate_std = tax_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = tax_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;tax_rate_std&quot;, values_to = &quot;tax_revenue_std&quot;) %&gt;% mutate(tax_rate_std = as.numeric(tax_rate_std), tax_revenue = tax_revenue_std * sd(data_laffer$tax_revenue_std) + mean(data_laffer$tax_revenue_std), tax_rate = tax_rate_std * sd(data_laffer$tax_rate) + mean(data_laffer$tax_rate)) tax_model_2_student_posterior_prediction_pi &lt;- tax_model_2_student_posterior_prediction_samples %&gt;% group_by(tax_rate) %&gt;% summarise(mean = mean(tax_revenue_std), PI_lower = PI(tax_revenue_std)[1], PI_upper = PI(tax_revenue_std)[2]) %&gt;% ungroup() model_laffer_3_student &lt;- quap( flist = alist( tax_revenue_std ~ dstudent(2, mu, sigma), mu &lt;- alpha + B %*% w, alpha ~ dnorm(0, .5), w ~ dnorm(0, .5), sigma ~ dexp(1) ), data = list(tax_revenue_std = data_laffer$tax_revenue_std, B = b_spline_laffer), start = list(w = rep(0, ncol(b_spline_laffer))) ) tax_model_3_student_posterior_prediction_samples &lt;- link(model_laffer_3_student, data = list(B = b_spline_tax)) %&gt;% as_tibble() %&gt;% set_names(nm = tax_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;tax_rate_std&quot;, values_to = &quot;tax_revenue_std&quot;) %&gt;% mutate(tax_rate_std = as.numeric(tax_rate_std), tax_revenue = tax_revenue_std * sd(data_laffer$tax_revenue_std) + mean(data_laffer$tax_revenue_std), tax_rate = tax_rate_std * sd(data_laffer$tax_rate) + mean(data_laffer$tax_rate)) tax_model_3_student_posterior_prediction_pi &lt;- tax_model_3_student_posterior_prediction_samples %&gt;% group_by(tax_rate) %&gt;% summarise(mean = mean(tax_revenue_std), PI_lower = PI(tax_revenue_std)[1], PI_upper = PI(tax_revenue_std)[2]) %&gt;% ungroup() get_psis_and_waic &lt;- function(model, model_type, m_idx){ PSIS(model, pointwise = TRUE) %&gt;% as_tibble() %&gt;% dplyr::select(PSIS = PSIS, lppd_psis = lppd, `psis-k` = k) %&gt;% set_names(nm = names(.) %&gt;% str_c(., &quot;_&quot;,model_type,&quot;_m&quot;,m_idx)) %&gt;% bind_cols(WAIC(model, pointwise = TRUE) %&gt;% as_tibble() %&gt;% dplyr::select(WAIC = WAIC, lppd_waic = lppd, `waic-penalty` = penalty) %&gt;% set_names(nm = names(.) %&gt;% str_c(., &quot;_&quot;,model_type,&quot;_m&quot;,m_idx))) } data_laffer_waic &lt;- tibble(model = list(model_laffer_0, model_laffer_1, model_laffer_2, model_laffer_3, model_laffer_0_student, model_laffer_1_student, model_laffer_2_student, model_laffer_3_student), model_type = rep(c(&quot;norm&quot;, &quot;student&quot;), each = 4), m_idx = c(0:3, 0:3)) %&gt;% pmap(get_psis_and_waic) %&gt;% reduce(bind_cols, .init = data_laffer) %&gt;% mutate(rn = row_number()) p0 &lt;- ggplot(mapping = aes(x = tax_rate)) + geom_smooth(data = tax_model_0_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = &quot;normal&quot;, fill = after_scale(clr_alpha(color))), size = .2) + geom_point(data = data_laffer_waic, aes(y = tax_revenue_std, size = WAIC_norm_m0), color = clr_dark, fill = fll0, shape = 21) + geom_text(data = data_laffer_waic %&gt;% filter(rn %in% c(1, 11, 12)), aes(y = tax_revenue_std, label = rn), family = fnt_sel) + labs(subtitle = &quot;0: flat&quot;) p1 &lt;- ggplot(mapping = aes(x = tax_rate)) + geom_smooth(data = tax_model_1_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = &quot;normal&quot;, fill = after_scale(clr_alpha(color))), size = .2) + geom_point(data = data_laffer_waic, aes(y = tax_revenue_std, size = WAIC_norm_m1), color = clr_dark, fill = fll0, shape = 21) + labs(subtitle = &quot;1: linear&quot;) p2 &lt;- ggplot(mapping = aes(x = tax_rate)) + geom_smooth(data = tax_model_2_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = &quot;normal&quot;, fill = after_scale(clr_alpha(color))), size = .2) + geom_point(data = data_laffer_waic, aes(y = tax_revenue_std, size = WAIC_norm_m2), color = clr_dark, fill = fll0, shape = 21) + labs(subtitle = &quot;2: quadratic&quot;) p3 &lt;- ggplot(mapping = aes(x = tax_rate)) + geom_smooth(data = tax_model_3_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = &quot;normal&quot;, fill = after_scale(clr_alpha(color))), size = .2) + geom_point(data = data_laffer_waic, aes(y = tax_revenue_std, size = WAIC_norm_m3), color = clr_dark, fill = fll0, shape = 21) + labs(subtitle = &quot;3: spline&quot;) p0s &lt;- ggplot(mapping = aes(x = tax_rate)) + geom_smooth(data = tax_model_0_student_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = &quot;student&quot;, fill = after_scale(clr_alpha(color))), size = .2) + geom_point(data = data_laffer_waic, aes(y = tax_revenue_std, size = WAIC_student_m0), color = clr_dark, fill = fll0, shape = 21) + labs(subtitle = &quot;0: flat&quot;) p1s &lt;- ggplot(mapping = aes(x = tax_rate)) + geom_smooth(data = tax_model_1_student_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = &quot;student&quot;, fill = after_scale(clr_alpha(color))), size = .2) + geom_point(data = data_laffer_waic, aes(y = tax_revenue_std, size = WAIC_student_m1), color = clr_dark, fill = fll0, shape = 21) + labs(subtitle = &quot;1: linear&quot;) p2s &lt;- ggplot(mapping = aes(x = tax_rate)) + geom_smooth(data = tax_model_2_student_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = &quot;student&quot;, fill = after_scale(clr_alpha(color))), size = .2) + geom_point(data = data_laffer_waic, aes(y = tax_revenue_std, size = WAIC_student_m2), color = clr_dark, fill = fll0, shape = 21) + labs(subtitle = &quot;2: quadratic&quot;) p3s &lt;- ggplot(mapping = aes(x = tax_rate)) + geom_smooth(data = tax_model_3_student_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = &quot;student&quot;, fill = after_scale(clr_alpha(color))), size = .2) + geom_point(data = data_laffer_waic, aes(y = tax_revenue_std, size = WAIC_student_m3), color = clr_dark, fill = fll0, shape = 21) + labs(subtitle = &quot;3: spline&quot;) waic_range &lt;- data_laffer_waic %&gt;% pivot_longer(cols = starts_with(&quot;WAIC&quot;)) %&gt;% .$value %&gt;% range() p0 + p1 + p2 + p3 + p0s + p1s + p2s + p3s + plot_layout(nrow = 2, guides = &quot;collect&quot;) &amp; theme(plot.subtitle = element_text(hjust = .5), legend.position = &quot;bottom&quot;) &amp; scale_size_continuous(&quot;WAIC&quot;, breaks = c(5,10,15,20), range = c(.5, 7), limits = waic_range) &amp; scale_color_manual(&quot;Prior Type&quot;, values = c(normal = clr0d, student = clr1)) &amp; labs(y = &quot;tax_revenue&quot;) data_laffer_waic %&gt;% dplyr::select(c(starts_with(&quot;waic&quot;,ignore.case = FALSE), starts_with(&quot;psis&quot;,ignore.case = FALSE))) %&gt;% mutate(rn = row_number()) %&gt;% pivot_longer(-rn) %&gt;% separate(name, into = c(&quot;statistic&quot;, &quot;model_type&quot;, &quot;model&quot;), sep = &quot;_&quot;) %&gt;% pivot_wider(names_from = &quot;statistic&quot;, values_from = &quot;value&quot;) %&gt;% ggplot(aes(x = `psis-k`, y = `waic-penalty`)) + geom_vline(xintercept = .5, linetype = 3, color = clr_dark) + geom_point(aes(color = model_type, fill = after_scale(clr_alpha(color))), shape = 21, size = 1.5) + ggrepel::geom_text_repel(data = . %&gt;% filter(rn %in% c(1, 11, 12)), aes(label = rn), family = fnt_sel) + facet_grid(model_type ~ model, scales = &quot;free&quot;)+ scale_color_manual(&quot;Prior Type&quot;, values = c(normal = clr0d, student = clr1)) + theme(legend.position = &quot;bottom&quot;, panel.background = element_rect(fill = &quot;transparent&quot;, color = clr0d)) H3 data_birds &lt;- tibble( island = 1:3, species_a = c(.2, .8, .05), species_b = c(.2, .1, .15), species_c = c(.2, .05, .7), species_d = c(.2, .025, .05), species_e = c(.2, .025, .05) ) data_birds %&gt;% group_by(island) %&gt;% mutate(total = sum(across(starts_with(&quot;species&quot;))), entropy = -sum(across(starts_with(&quot;species&quot;), .fns = ~(function(x){x * log(x)})(.x)))) %&gt;% ungroup() #&gt; # A tibble: 3 × 8 #&gt; island species_a species_b species_c species_d species_e total entropy #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 0.2 0.2 0.2 0.2 0.2 1 1.61 #&gt; 2 2 0.8 0.1 0.05 0.025 0.025 1 0.743 #&gt; 3 3 0.05 0.15 0.7 0.05 0.05 1 0.984 The entropy in island 1 is highest as all bird species are equally common. In contrast island 2 has the lowest entropy which translates to the most irregular bird distribution. data_birds_compact &lt;- tibble(island = 1:3, birds = list(data_birds[1, 2:6] %&gt;% unlist(), data_birds[2, 2:6] %&gt;% unlist(), data_birds[3, 2:6] %&gt;% unlist())) kl_div &lt;- function(p,q){ sum(p * log(p/q)) } kl_birds &lt;- function(i1, i2){ kl_div(p = data_birds_compact[i1,]$birds[[1]], q = data_birds_compact[i2,]$birds[[1]]) } cross_df(list(i1 = 1:3, i2 = 1:3 )) %&gt;% mutate(kl_divergence = map2_dbl(i1, i2,.f = kl_birds)) %&gt;% ggplot(aes(x = i1, y = i2, fill = kl_divergence)) + geom_tile() + geom_text(aes(label = round(kl_divergence, digits = 2)), color = &quot;white&quot;, family = fnt_sel) + scale_fill_gradientn(colours = c(clr0d, clr1)) + coord_equal() Islands 1 and 3 predict each other reasonably well - these are the islands that have the highest entropy to begin with and which are thus not easily surprised. Using Island 1 as predictor (i2 / q) does generally produce the lowest KL divergences. H4 dagify(M ~ A + H, coords = tibble(name = c(&quot;M&quot;, &quot;A&quot;, &quot;H&quot;), x = c(.5, 0, 1), y = c(0, 1, 1))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;M&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;H&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(-.05, 1.05)) + scale_x_continuous(limits = c(-.05, 1.05)) + coord_fixed(ratio = .6) chapter6_models$model_happy #&gt; #&gt; Quadratic approximate posterior distribution #&gt; #&gt; Formula: #&gt; happiness ~ dnorm(mu, sigma) #&gt; mu &lt;- alpha + beta_age * age_trans #&gt; alpha ~ dnorm(0, 1) #&gt; beta_age ~ dnorm(0, 2) #&gt; sigma ~ dexp(1) #&gt; #&gt; Posterior means: #&gt; alpha beta_age sigma #&gt; 1.028282e-07 -1.313332e-07 1.210334e+00 #&gt; #&gt; Log-likelihood: -1623.32 chapter6_models$model_happy #&gt; #&gt; Quadratic approximate posterior distribution #&gt; #&gt; Formula: #&gt; happiness ~ dnorm(mu, sigma) #&gt; mu &lt;- alpha + beta_age * age_trans #&gt; alpha ~ dnorm(0, 1) #&gt; beta_age ~ dnorm(0, 2) #&gt; sigma ~ dexp(1) #&gt; #&gt; Posterior means: #&gt; alpha beta_age sigma #&gt; 1.028282e-07 -1.313332e-07 1.210334e+00 #&gt; #&gt; Log-likelihood: -1623.32 rethinking::compare(chapter6_models$model_happy, chapter6_models$model_happy_married, func = WAIC) %&gt;% knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight chapter6_models\\(model_happy_married | 2817.31| 41.49| 0.00| NA| 3.74| 1| |chapter6_models\\)model_happy 3252.08 28.37 434.77 38.87 2.41 0 The model comparison very strongly favors the model including age as predictor of happiness (model_happy_married). This is probably due to the effect that conditioning on marriage opens this collider and introduces a spurious correlation. This correlation can be used to predict inside the small world despite not having any causal justification. H5 data(foxes) data_fox &lt;- foxes %&gt;% as_tibble() %&gt;% drop_na(everything()) %&gt;% mutate(across(-group, standardize, .names = &quot;{str_to_lower(.col)}_std&quot;)) fox_weight_range &lt;- tibble(weight = c(2.2, 14), weight_std = (weight - mean(data_fox$weight))/ sd(data_fox$weight)) dag_fox &lt;- dagify( W ~ F + G, G ~ F, F ~ A, exposure = &quot;A&quot;, outcome = &quot;W&quot;, coords = tibble(name = c(&quot;W&quot;, &quot;F&quot;, &quot;G&quot;, &quot;A&quot;), x = c(.5, 0, 1, .5), y = c(0, .5, .5, 1))) dag_fox %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;W&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;F&quot;, &quot;G&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr2) + scale_y_continuous(limits = c(-.1, 1.1)) + coord_fixed(ratio = .6) model_fox_1 &lt;- quap( flist = alist( weight_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_food * avgfood_std + beta_groupsize * groupsize_std + beta_area * area_std, alpha ~ dnorm(0,.2), c(beta_food, beta_groupsize, beta_area) ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_fox ) model_fox_2 &lt;- quap( flist = alist( weight_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_food * avgfood_std + beta_groupsize * groupsize_std, alpha ~ dnorm(0,.2), c(beta_food, beta_groupsize) ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_fox ) model_fox_3 &lt;- quap( flist = alist( weight_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_groupsize * groupsize_std + beta_area * area_std, alpha ~ dnorm(0,.2), c(beta_groupsize, beta_area) ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_fox ) model_fox_4 &lt;- quap( flist = alist( weight_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_food * avgfood_std, alpha ~ dnorm(0,.2), beta_food ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_fox ) model_fox_5 &lt;- quap( flist = alist( weight_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_area * area_std, alpha ~ dnorm(0,.2), beta_area ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_fox ) compare(model_fox_1, model_fox_2, model_fox_3, model_fox_4, model_fox_5, func = WAIC) %&gt;% knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight model_fox_1 323.46 16.32 0.00 NA 4.96 0.39 model_fox_2 323.77 16.09 0.31 3.62 3.67 0.34 model_fox_3 324.26 15.79 0.80 2.96 3.89 0.26 model_fox_4 333.54 13.85 10.08 7.22 2.46 0.00 model_fox_5 333.89 13.80 10.43 7.27 2.72 0.00 There are two groups of models (A: model_fox_1-3 ans B: model_fox_4-5). The group A closes all the backdoor paths into \\(W\\). Also in group B, conditioning on A or F is basically equivalent because F is a descendant of A and an intermediate between A and the rest of the DAG. 8.6 {brms} section 8.6.1 The problem with parameters 8.6.1.1 More parameters (almost) always improve fit. brms_c7_model_brain_size &lt;- brm( data = data_brainsize, family = gaussian, brain_size_scl ~ 1 + mass_std, prior = c(prior(normal(0.5, 1), class = Intercept), prior(normal(0, 10), class = b), prior(lognormal(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c7_model_brain_size&quot; ) mixedup::summarise_model(brms_c7_model_brain_size) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 0.07 0.27 0.13 0.60 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; Intercept 0.53 0.11 0.32 0.76 #&gt; mass_std 0.17 0.12 -0.07 0.41 brms_R2_is_bad &lt;- function(brm_fit, seed = 7, ...) { set.seed(seed) p &lt;- brms:::predict.brmsfit(brm_fit, summary = F, ...) r &lt;- apply(p, 2, mean) - data_brainsize$brain_size_scl 1 - rethinking::var2(r) / rethinking::var2(data_brainsize$brain_size_scl) } brms_R2_is_bad(brms_c7_model_brain_size) #&gt; [1] 0.4873914 brms_c7_model_brain_size2 &lt;- update( brms_c7_model_brain_size, newdata = data_brainsize, formula = brain_size_scl ~ 1 + mass_std + I(mass_std^2), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c7_model_brain_size2&quot;) brms_c7_model_brain_size3 &lt;- update( brms_c7_model_brain_size, newdata = data_brainsize, formula = brain_size_scl ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, control = list(adapt_delta = .995), file = &quot;brms/brms_c7_model_brain_size3&quot;) brms_c7_model_brain_size4 &lt;- update( brms_c7_model_brain_size, newdata = data_brainsize, formula = brain_size_scl ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^4), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, control = list(adapt_delta = .9995, max_treedepth = 15), file = &quot;brms/brms_c7_model_brain_size4&quot;) brms_c7_model_brain_size5 &lt;- update( brms_c7_model_brain_size, newdata = data_brainsize, formula = brain_size_scl ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^5), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, control = list(adapt_delta = .99995, max_treedepth = 15), file = &quot;brms/brms_c7_model_brain_size5&quot;) defining a custom response distribution (setting \\(\\sigma\\) to a constant value to be in line with the model of the sixth order polynomial). custom_normal &lt;- custom_family( &quot;custom_normal&quot;, dpars = &quot;mu&quot;, links = &quot;identity&quot;, type = &quot;real&quot; ) stan_funs &lt;- &quot;real custom_normal_lpdf(real y, real mu) { return normal_lpdf(y | mu, 0.001); } real custom_normal_rng(real mu) { return normal_rng(mu, 0.001); } &quot; stanvars &lt;- stanvar(scode = stan_funs, block = &quot;functions&quot;) brms_c7_model_brain_size6 &lt;- brm( data = data_brainsize, family = custom_normal, brain_size_scl ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^4) + I(mass_std^5) + I(mass_std^6), prior = c(prior(normal(0.5, 1), class = Intercept), prior(normal(0, 10), class = b)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, stanvars = stanvars, control = list(max_treedepth = 15), file = &quot;brms/brms_c7_model_brain_size6&quot;) Defining custom functions to work with the special case for the last model (which includes family = custom_normal) expose_functions(brms_c7_model_brain_size6, vectorize = TRUE) posterior_epred_custom_normal &lt;- function(prep) { mu &lt;- prep$dpars$mu mu } posterior_predict_custom_normal &lt;- function(i, prep, ...) { mu &lt;- prep$dpars$mu mu custom_normal_rng(mu) } log_lik_custom_normal &lt;- function(i, prep) { mu &lt;- prep$dpars$mu y &lt;- prep$data$Y[i] custom_normal_lpdf(y, mu) } make_r2_figure &lt;- function(brms_fit, ylim = range(data_brainsize$brain_size_scl)) { # compute the R2 r2 &lt;- brms_R2_is_bad(brms_fit) # define the new data nd &lt;- tibble(mass_std = seq(from = -2, to = 2, length.out = 200)) # simulate and wrangle fitted(brms_fit, newdata = nd, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% # plot! ggplot(aes(x = mass_std)) + geom_lineribbon(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5), color = clr0dd, size = 1/2, fill = fll0) + geom_point(data = data_brainsize, aes(y = brain_size_scl), color = clr_dark) + labs(subtitle = bquote(italic(R)^2==.(round(r2, digits = 2))), x = &quot;body mass (std)&quot;, y = &quot;brain volume (std)&quot;) + coord_cartesian(xlim = c(-1.2, 1.5), ylim = ylim) } tibble(brms_fit = list(brms_c7_model_brain_size, brms_c7_model_brain_size2, brms_c7_model_brain_size3, brms_c7_model_brain_size4, brms_c7_model_brain_size5, brms_c7_model_brain_size6), ylim = list(range(data_brainsize$brain_size_scl),range(data_brainsize$brain_size_scl), range(data_brainsize$brain_size_scl), c(.25, 1.1), c(.1, 1.4), c(-0.25, 1.5))) %&gt;% pmap(make_r2_figure) %&gt;% wrap_plots(nrow = 2) 8.6.1.2 Too few parameters hurts, too brain_loo_lines &lt;- function(brms_fit, row, ...) { nd &lt;- tibble(mass_std = seq(from = -2, to = 2, length.out = 200)) # refit the model new_fit &lt;- update(brms_fit, newdata = filter(data_brainsize, row_number() != row), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, refresh = 0, ...) # pull the lines values fitted(new_fit, newdata = nd) %&gt;% data.frame() %&gt;% select(Estimate) %&gt;% bind_cols(nd) } poly1_fits &lt;- tibble(row = 1:7) %&gt;% mutate(post = purrr::map(row, ~brain_loo_lines(brms_fit = brms_c7_model_brain_size, row = .))) %&gt;% unnest(post) poly4_fits &lt;- tibble(row = 1:7) %&gt;% mutate(post = purrr::map(row, ~brain_loo_lines(brms_fit = brms_c7_model_brain_size4, row = ., control = list(adapt_delta = .9995)))) %&gt;% unnest(post) p1 &lt;- poly1_fits %&gt;% ggplot(aes(x = mass_std, y = Estimate)) + coord_cartesian(xlim = range(data_brainsize$mass_std), ylim = range(data_brainsize$brain_size_scl)) + labs(subtitle = &quot;brms_c7_model_brain_size&quot;) p2 &lt;- poly4_fits %&gt;% ggplot(aes(x = mass_std, y = Estimate)) + coord_cartesian(xlim = range(data_brainsize$mass_std), ylim = c(-0.1, 1.4)) + labs(subtitle = &quot;brms_c7_model_brain_size4&quot;) p1 + p2 &amp; geom_line(aes(group = row), color = clr0d, size = 1/2, alpha = 1/2) &amp; geom_point(data = data_brainsize, aes(y = brain_size_scl), color = clr_dark) 8.6.2 Entropy and accuracy 8.6.2.1 Firing the weatherperson Current weatherperson weatherperson &lt;- tibble(prediction = rep(c(1,.6), c(3,7)), observed = rep(c(emo::ji(&quot;cloud_with_rain&quot;), emo::ji(&quot;sunny&quot;)), c(3,7))) weatherperson %&gt;% t() %&gt;% knitr::kable(col.names = 1:10) 1 2 3 4 5 6 7 8 9 10 prediction 1.0 1.0 1.0 0.6 0.6 0.6 0.6 0.6 0.6 0.6 observed 🌧 🌧 🌧 ☀️ ☀️ ☀️ ☀️ ☀️ ☀️ ☀️ newcomer newcomer &lt;- tibble(prediction = rep(0, 10), observed = rep(c(emo::ji(&quot;cloud_with_rain&quot;), emo::ji(&quot;sunny&quot;)), c(3,7))) newcomer %&gt;% t() %&gt;% knitr::kable(col.names = 1:10) 1 2 3 4 5 6 7 8 9 10 prediction 0 0 0 0 0 0 0 0 0 0 observed 🌧 🌧 🌧 ☀️ ☀️ ☀️ ☀️ ☀️ ☀️ ☀️ weather_happy &lt;- weatherperson %&gt;% rename(weatherperson = &quot;prediction&quot;) %&gt;% mutate(newcomer = newcomer$prediction, observed_code = if_else(observed == emo::ji(&quot;cloud_with_rain&quot;), 1, 0), day = row_number()) %&gt;% pivot_longer(c(weatherperson, newcomer), names_to = &quot;person&quot;, values_to = &quot;prediction&quot;) %&gt;% mutate(hit = ifelse(prediction == observed_code, 1, 1 - prediction - observed_code), happy = if_else(prediction == observed_code, if_else(observed_code == 1, -1, 0), if_else(observed_code == 1, -5 * (observed_code - prediction), -1 * (prediction - observed_code)))) weather_happy %&gt;% pivot_wider(id_cols = observed:day, names_from = &quot;person&quot;, values_from = prediction:happy) %&gt;% dplyr::select(-day) %&gt;% t() %&gt;% knitr::kable(col.names = 1:10) 1 2 3 4 5 6 7 8 9 10 observed 🌧 🌧 🌧 ☀️ ☀️ ☀️ ☀️ ☀️ ☀️ ☀️ observed_code 1 1 1 0 0 0 0 0 0 0 prediction_weatherperson 1.0 1.0 1.0 0.6 0.6 0.6 0.6 0.6 0.6 0.6 prediction_newcomer 0 0 0 0 0 0 0 0 0 0 hit_weatherperson 1.0 1.0 1.0 0.4 0.4 0.4 0.4 0.4 0.4 0.4 hit_newcomer 0 0 0 1 1 1 1 1 1 1 happy_weatherperson -1.0 -1.0 -1.0 -0.6 -0.6 -0.6 -0.6 -0.6 -0.6 -0.6 happy_newcomer -5 -5 -5 0 0 0 0 0 0 0 weather_happy %&gt;% group_by(person) %&gt;% summarise(total_hit = sum(hit), total_happy = sum(happy)) #&gt; # A tibble: 2 × 3 #&gt; person total_hit total_happy #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 newcomer 7 -15 #&gt; 2 weatherperson 5.8 -7.2 weather_happy %&gt;% count(person, hit) %&gt;% mutate(power = hit ^ n) %&gt;% group_by(person) %&gt;% summarise(joint_likelihood = prod(power)) #&gt; # A tibble: 2 × 2 #&gt; person joint_likelihood #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 newcomer 0 #&gt; 2 weatherperson 0.00164 8.6.2.2 Divergence depends upon direction tibble(direction = c(&quot;Earth to Mars&quot;, &quot;Mars to Earth&quot;), p_1 = c(.01, .7), q_1 = c(.7, .01)) %&gt;% mutate(p_2 = 1 - p_1, q_2 = 1 - q_1) %&gt;% mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2))) #&gt; # A tibble: 2 × 6 #&gt; direction p_1 q_1 p_2 q_2 d_kl #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Earth to Mars 0.01 0.7 0.99 0.3 1.14 #&gt; 2 Mars to Earth 0.7 0.01 0.3 0.99 2.62 log-pointwise-predictive-density (lppd) with {brms} (not included, we have to do it manually) (lppd_by_sepc &lt;- brms_c7_model_brain_size %&gt;% log_lik() %&gt;% as_tibble() %&gt;% set_names(pull(data_brainsize, species)) %&gt;% pivot_longer(everything(), names_to = &quot;species&quot;, values_to = &quot;log_prob&quot;) %&gt;% mutate(prob = exp(log_prob)) %&gt;% group_by(species) %&gt;% summarise(log_probability_score = mean(prob) %&gt;% log()) %&gt;% ungroup()) #&gt; # A tibble: 7 × 2 #&gt; species log_probability_score #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 afarensis 0.379 #&gt; 2 africanus 0.406 #&gt; 3 boisei 0.392 #&gt; 4 ergaster 0.225 #&gt; 5 habilis 0.325 #&gt; 6 rudolfensis 0.264 #&gt; 7 sapiens -0.593 lppd_by_sepc %&gt;% summarise(total_log_probability_score = sum(log_probability_score)) #&gt; # A tibble: 1 × 1 #&gt; total_log_probability_score #&gt; &lt;dbl&gt; #&gt; 1 1.40 8.6.2.3 Computing the lppd brms_log_prob &lt;- brms_c7_model_brain_size %&gt;% log_lik() %&gt;% as_tibble() brms_prob &lt;- brms_log_prob %&gt;% set_names(pull(data_brainsize, species)) %&gt;% # add an s iteration index, for convenience mutate(s = 1:n()) %&gt;% # make it long pivot_longer(-s, names_to = &quot;species&quot;, values_to = &quot;log_prob&quot;) %&gt;% # compute the probability scores mutate(prob = exp(log_prob)) brms_prob_score &lt;- brms_prob %&gt;% group_by(species) %&gt;% summarise(log_probability_score = mean(prob) %&gt;% log()) %&gt;% ungroup() brms_prob_score %&gt;% summarise(total_log_probability_score = sum(log_probability_score)) #&gt; # A tibble: 1 × 1 #&gt; total_log_probability_score #&gt; &lt;dbl&gt; #&gt; 1 1.40 8.6.2.4 Scoring the right data brms_lppd &lt;- function(brms_fit) { log_lik(brms_fit) %&gt;% data.frame() %&gt;% pivot_longer(everything(), values_to = &quot;log_prob&quot;) %&gt;% mutate(prob = exp(log_prob)) %&gt;% group_by(name) %&gt;% summarise(log_probability_score = mean(prob) %&gt;% log()) %&gt;% summarise(total_log_probability_score = sum(log_probability_score)) } tibble(name = str_c(&quot;brms_c7_model_brain_size&quot;, c(&quot;&quot;,2:6))) %&gt;% mutate(brms_fit = purrr::map(name, get)) %&gt;% mutate(lppd = purrr::map(brms_fit, ~brms_lppd(.))) %&gt;% unnest(lppd) #&gt; # A tibble: 6 × 3 #&gt; name brms_fit total_log_probability_score #&gt; &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; #&gt; 1 brms_c7_model_brain_size &lt;brmsfit&gt; 1.40 #&gt; 2 brms_c7_model_brain_size2 &lt;brmsfit&gt; 0.557 #&gt; 3 brms_c7_model_brain_size3 &lt;brmsfit&gt; 0.608 #&gt; 4 brms_c7_model_brain_size4 &lt;brmsfit&gt; -0.213 #&gt; 5 brms_c7_model_brain_size5 &lt;brmsfit&gt; 5.87 #&gt; 6 brms_c7_model_brain_size6 &lt;brmsfit&gt; 26.0 8.6.3 Information criteria 8.6.3.1 WAIC calculation brms_c7_model_cars &lt;- brm(data = cars, family = gaussian, dist ~ 1 + speed, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c7_model_cars&quot;) n_cases &lt;- nrow(cars) log_likelihood_i &lt;- log_lik(brms_c7_model_cars) %&gt;% as_tibble() %&gt;% set_names(str_pad(1:n_cases,width = 2, pad = 0)) dim(log_likelihood_i) #&gt; [1] 4000 50 log_mu_l &lt;- log_likelihood_i %&gt;% pivot_longer(everything(), names_to = &quot;i&quot;, values_to = &quot;log_likelihood&quot;) %&gt;% mutate(likelihood = exp(log_likelihood)) %&gt;% group_by(i) %&gt;% summarise(log_mean_likelihood = mean(likelihood) %&gt;% log()) ( cars_lppd &lt;- log_mu_l %&gt;% summarise(lppd = sum(log_mean_likelihood)) %&gt;% pull(lppd) ) #&gt; [1] -206.62 computing \\(p_{WAIC}\\) and \\(V(y_{i})\\) (varinace in the log-likelihood) v_i &lt;- log_likelihood_i %&gt;% pivot_longer(everything(), names_to = &quot;i&quot;, values_to = &quot;log_likelihood&quot;) %&gt;% group_by(i) %&gt;% summarise(var_loglikelihood = var(log_likelihood)) pwaic &lt;- v_i %&gt;% summarise(pwaic = sum(var_loglikelihood)) %&gt;% pull() pwaic #&gt; [1] 4.080994 \\(WAIC = -2 (lppd - p_{WAIC})\\) -2 * (cars_lppd - pwaic) #&gt; [1] 421.402 {brms} function: waic(brms_c7_model_cars) #&gt; #&gt; Computed from 4000 by 50 log-likelihood matrix #&gt; #&gt; Estimate SE #&gt; elpd_waic -210.7 8.2 #&gt; p_waic 4.1 1.5 #&gt; waic 421.4 16.4 #&gt; #&gt; 2 (4.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. Pointwise values: waic(brms_c7_model_cars)$pointwise %&gt;% as_tibble() #&gt; # A tibble: 50 × 3 #&gt; elpd_waic p_waic waic #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 -3.65 0.0217 7.30 #&gt; 2 -4.02 0.0949 8.05 #&gt; 3 -3.68 0.0209 7.37 #&gt; 4 -4.00 0.0597 7.99 #&gt; 5 -3.59 0.0102 7.18 #&gt; 6 -3.74 0.0210 7.48 #&gt; 7 -3.60 0.0103 7.21 #&gt; 8 -3.62 0.0110 7.23 #&gt; 9 -3.98 0.0351 7.96 #&gt; 10 -3.77 0.0171 7.54 #&gt; # … with 40 more rows 8.6.4 Model comparison 8.6.4.1 Model mis-selection brms_c6_model_fungus_post_treatment &lt;- read_rds(&quot;brms/brms_c6_model_fungus_post_treatment.rds&quot;) brms_c6_model_fungus_no_treatment &lt;- read_rds(&quot;brms/brms_c6_model_fungus_no_treatment.rds&quot;) brms_c6_model_fungus_only_treatment &lt;- read_rds(&quot;brms/brms_c6_model_fungus_only_treatment.rds&quot;) brms_c6_model_fungus_post_treatment &lt;- add_criterion(brms_c6_model_fungus_post_treatment, criterion = &quot;waic&quot;) brms_c6_model_fungus_no_treatment &lt;- add_criterion(brms_c6_model_fungus_no_treatment, criterion = &quot;waic&quot;) brms_c6_model_fungus_only_treatment &lt;- add_criterion(brms_c6_model_fungus_only_treatment, criterion = &quot;waic&quot;) brms_fungus_compare &lt;- loo_compare(brms_c6_model_fungus_post_treatment, brms_c6_model_fungus_no_treatment, brms_c6_model_fungus_only_treatment, criterion = &quot;waic&quot;) print(brms_fungus_compare, simplify = FALSE) #&gt; elpd_diff se_diff elpd_waic se_elpd_waic #&gt; brms_c6_model_fungus_post_treatment 0.0 0.0 -180.7 6.7 #&gt; brms_c6_model_fungus_only_treatment -20.6 4.9 -201.3 5.4 #&gt; brms_c6_model_fungus_no_treatment -22.3 5.8 -203.0 5.7 #&gt; p_waic se_p_waic waic se_waic #&gt; brms_c6_model_fungus_post_treatment 3.4 0.5 361.4 13.5 #&gt; brms_c6_model_fungus_only_treatment 2.5 0.3 402.7 10.8 #&gt; brms_c6_model_fungus_no_treatment 1.6 0.2 406.0 11.4 With respect to the output, notice the elpd_diff column and the adjacent se_diff column. Those are our WAIC differences in the elpd metric. The models have been rank ordered from the highest (i.e., brms_c6_model_fungus_post_treatment) to the lowest (i.e., brms_c6_model_fungus_no_treatment). The scores listed are the differences of brms_c6_model_fungus_post_treatment minus the comparison model. Since brms_c6_model_fungus_post_treatment is the comparison model in the top row, the values are naturally 0 (i.e., \\(x−x=0\\)). But now here’s another critical thing to understand: Since the {brms} version 2.8.0 update, WAIC and LOO differences are no longer reported in the \\(−2\\times x\\) metric. Remember how multiplying (lppd - pwaic) by -2 is a historic artifact associated with the frequentist \\(\\chi^2\\) test? We’ll, the makers of the {loo} package aren’t fans and they no longer support the conversion. So here’s the deal. The substantive interpretations of the differences presented in an elpd_diff metric will be the same as if presented in a WAIC metric. But if we want to compare our elpd_diff results to those in the text, we will have to multiply them by -2. And also, if we want the associated standard error in the same metric, we’ll need to multiply the se_diff column by 2 brms_c6_model_fungus_post_treatment &lt;- add_criterion(brms_c6_model_fungus_post_treatment, criterion = &quot;loo&quot;) brms_c6_model_fungus_no_treatment &lt;- add_criterion(brms_c6_model_fungus_no_treatment, criterion = &quot;loo&quot;) brms_c6_model_fungus_only_treatment &lt;- add_criterion(brms_c6_model_fungus_only_treatment, criterion = &quot;loo&quot;) loo_compare(brms_c6_model_fungus_post_treatment, brms_c6_model_fungus_no_treatment, brms_c6_model_fungus_only_treatment, criterion = &quot;loo&quot;) %&gt;% print(simplify = FALSE) #&gt; elpd_diff se_diff elpd_loo se_elpd_loo #&gt; brms_c6_model_fungus_post_treatment 0.0 0.0 -180.7 6.7 #&gt; brms_c6_model_fungus_only_treatment -20.6 4.9 -201.3 5.4 #&gt; brms_c6_model_fungus_no_treatment -22.3 5.8 -203.0 5.7 #&gt; p_loo se_p_loo looic se_looic #&gt; brms_c6_model_fungus_post_treatment 3.4 0.5 361.5 13.5 #&gt; brms_c6_model_fungus_only_treatment 2.6 0.3 402.7 10.8 #&gt; brms_c6_model_fungus_no_treatment 1.6 0.2 406.0 11.4 computing the standard error of the WAIC difference for brms_c6_model_fungus_post_treatment and brms_c6_model_fungus_only_treatment n &lt;- length(brms_c6_model_fungus_no_treatment$criteria$waic$pointwise[, &quot;waic&quot;]) tibble(waic_no_treatment = brms_c6_model_fungus_no_treatment$criteria$waic$pointwise[, &quot;waic&quot;], waic_post_treatment = brms_c6_model_fungus_post_treatment$criteria$waic$pointwise[, &quot;waic&quot;]) %&gt;% mutate(diff = waic_no_treatment - waic_post_treatment) %&gt;% summarise(diff_se = sqrt(n * var(diff))) #&gt; # A tibble: 1 × 1 #&gt; diff_se #&gt; &lt;dbl&gt; #&gt; 1 11.6 brms_fungus_compare[2, 2] * 2 #&gt; [1] 9.840665 (brms_fungus_compare[2, 1] * -2) + c(-1, 1) * (brms_fungus_compare[2, 2] * 2) * 2.6 #&gt; [1] 15.65820 66.82966 brms_fungus_compare[, 7:8] %&gt;% data.frame() %&gt;% rownames_to_column(&quot;model_name&quot;) %&gt;% as_tibble() %&gt;% mutate(model_name = fct_reorder(model_name, waic, .desc = TRUE)) %&gt;% ggplot(aes(x = waic, y = model_name, xmin = waic - se_waic, xmax = waic + se_waic)) + geom_pointrange(color = clr0dd, fill = clr0, shape = 21) + labs(title = &quot;custom WAIC plot&quot;, x = NULL, y = NULL) + theme(axis.ticks.y = element_blank()) in {brms}, weights need to be computed explicitly for the models model_weights(brms_c6_model_fungus_post_treatment, brms_c6_model_fungus_no_treatment, brms_c6_model_fungus_only_treatment, weights = &quot;waic&quot;) %&gt;% round(digits = 2) #&gt; brms_c6_model_fungus_post_treatment brms_c6_model_fungus_no_treatment #&gt; 1 0 #&gt; brms_c6_model_fungus_only_treatment #&gt; 0 8.6.5 Outliers and other illusions brms_c5_model_age &lt;- read_rds(&quot;brms/brms_c5_model_age.rds&quot;) %&gt;% add_criterion(criterion = &quot;loo&quot;) brms_c5_model_marriage &lt;- read_rds(&quot;brms/brms_c5_model_marriage.rds&quot;) %&gt;% add_criterion(criterion = &quot;loo&quot;) brms_c5_model_multiple &lt;- read_rds(&quot;brms/brms_c5_model_multiple.rds&quot;) %&gt;% add_criterion(criterion = &quot;loo&quot;) loo_compare(brms_c5_model_age, brms_c5_model_marriage, brms_c5_model_multiple, criterion = &quot;loo&quot;) %&gt;% print(simplify = FALSE) #&gt; elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo #&gt; brms_c5_model_age 0.0 0.0 -63.0 6.5 3.7 1.8 #&gt; brms_c5_model_multiple -0.8 0.4 -63.8 6.4 4.7 1.9 #&gt; brms_c5_model_marriage -6.8 4.7 -69.7 5.0 3.0 0.9 #&gt; looic se_looic #&gt; brms_c5_model_age 125.9 12.9 #&gt; brms_c5_model_multiple 127.5 12.8 #&gt; brms_c5_model_marriage 139.4 10.0 loo(brms_c5_model_multiple) #&gt; #&gt; Computed from 4000 by 50 log-likelihood matrix #&gt; #&gt; Estimate SE #&gt; elpd_loo -63.8 6.4 #&gt; p_loo 4.7 1.9 #&gt; looic 127.5 12.8 #&gt; ------ #&gt; Monte Carlo SE of elpd_loo is 0.1. #&gt; #&gt; All Pareto k estimates are good (k &lt; 0.5). #&gt; See help(&#39;pareto-k-diagnostic&#39;) for details. library(loo) loo(brms_c5_model_multiple) %&gt;% pareto_k_ids(threshold = 0.4) #&gt; [1] 13 20 chapter5_models$data_waffle %&gt;% slice(13) %&gt;% select(Location:Loc) #&gt; # A tibble: 1 × 2 #&gt; Location Loc #&gt; &lt;fct&gt; &lt;fct&gt; #&gt; 1 Idaho ID pareto_k_values(loo(brms_c5_model_multiple))[13] #&gt; [1] 0.4485035 brms_c5_model_multiple &lt;- add_criterion(brms_c5_model_multiple, &quot;waic&quot;, file = &quot;brms/brms_c5_model_multiple&quot;) p1 &lt;- tibble(pareto_k = brms_c5_model_multiple$criteria$loo$diagnostics$pareto_k, p_waic = brms_c5_model_multiple$criteria$waic$pointwise[, &quot;p_waic&quot;], Loc = pull(chapter5_models$data_waffle, Loc)) %&gt;% ggplot(aes(x = pareto_k, y = p_waic, color = Loc == &quot;ID&quot;)) + geom_vline(xintercept = .5, linetype = 3, color = clr_dark) + geom_point(aes(shape = Loc == &quot;ID&quot;)) + geom_text(data = . %&gt;% filter(p_waic &gt; 0.5), aes(x = pareto_k - 0.03, label = Loc), hjust = 1) + scale_color_manual(values = c(clr0dd, clr2)) + scale_shape_manual(values = c(1, 19)) + labs(subtitle = &quot;Gaussian model (brms_c5_model_multiple)&quot;) + theme(legend.position = &quot;none&quot;) To use the Student-t distribution in {brms} set family = student (note the inclusion of \\(\\nu\\) in bf()) brms_c7_model_multiple &lt;- brm( data = chapter5_models$data_waffle, family = student, bf(divorce_std ~ 1 + marriage_std + median_age_std, nu = 2), prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c7_model_multiple&quot;) brms_c7_model_multiple &lt;- add_criterion(brms_c7_model_multiple, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) p2 &lt;- tibble(pareto_k = brms_c7_model_multiple$criteria$loo$diagnostics$pareto_k, p_waic = brms_c7_model_multiple$criteria$waic$pointwise[, &quot;p_waic&quot;], Loc = pull(chapter5_models$data_waffle, Loc)) %&gt;% ggplot(aes(x = pareto_k, y = p_waic, color = Loc == &quot;ID&quot;)) + geom_vline(xintercept = .5, linetype = 3, color = clr_dark) + geom_point(aes(shape = Loc == &quot;ID&quot;)) + geom_text(data = . %&gt;% filter(p_waic &gt; 0.5), aes(x = pareto_k - 0.03, label = Loc), hjust = 1) + scale_color_manual(values = c(clr0dd, clr2)) + scale_shape_manual(values = c(1, 19)) + labs(subtitle = &quot;Student-t model (brms_c7_model_multiple)&quot;) + theme(legend.position = &quot;none&quot;) p1 + p2 loo_compare(brms_c5_model_multiple, brms_c7_model_multiple, criterion = &quot;waic&quot;) %&gt;% print(simplify = FALSE) #&gt; elpd_diff se_diff elpd_waic se_elpd_waic p_waic #&gt; brms_c5_model_multiple 0.0 0.0 -63.7 6.4 4.6 #&gt; brms_c7_model_multiple -2.7 3.0 -66.4 5.8 6.1 #&gt; se_p_waic waic se_waic #&gt; brms_c5_model_multiple 1.8 127.3 12.7 #&gt; brms_c7_model_multiple 1.0 132.8 11.6 bind_rows(as_draws_df(brms_c5_model_multiple), as_draws_df(brms_c7_model_multiple)) %&gt;% as_tibble() %&gt;% mutate(fit = rep(c(&quot;Gaussian (brms_c5_model_multiple)&quot;, &quot;Student-t (brms_c7_model_multiple)&quot;), each = n() / 2)) %&gt;% pivot_longer(b_Intercept:sigma) %&gt;% mutate(name = factor(name, levels = c(&quot;b_Intercept&quot;, &quot;b_median_age_std&quot;, &quot;b_marriage_std&quot;, &quot;sigma&quot;), labels = c(&quot;alpha&quot;, &quot;beta[a]&quot;, &quot;beta[m]&quot;, &quot;sigma&quot;))) %&gt;% ggplot(aes(x = value, y = fit, color = fit)) + stat_pointinterval(.width = .95, aes(fill = after_scale(clr_lighten(color))), size = 3, shape = 21) + facet_wrap(~ name, ncol = 1)+ scale_color_manual(values = c(clr0dd, clr2), guide = &quot;none&quot;) + labs(x = &quot;posterior&quot;, y = NULL) 8.6.6 {brms} \\(r^2\\) bayes_R2(brms_c5_model_multiple) %&gt;% round(digits = 3) %&gt;% as_tibble() %&gt;% knitr::kable() Estimate Est.Error Q2.5 Q97.5 0.329 0.087 0.149 0.479 rbind(bayes_R2(brms_c5_model_multiple), bayes_R2(brms_c5_model_age), bayes_R2(brms_c5_model_marriage)) %&gt;% as_tibble() %&gt;% mutate(model = c(&quot;brms_c5_model_multiple&quot;, &quot;brms_c5_model_age&quot;, &quot;brms_c5_model_marriage&quot;), r_square_posterior_mean = round(Estimate, digits = 3)) %&gt;% select(model, r_square_posterior_mean) #&gt; # A tibble: 3 × 2 #&gt; model r_square_posterior_mean #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 brms_c5_model_multiple 0.329 #&gt; 2 brms_c5_model_age 0.326 #&gt; 3 brms_c5_model_marriage 0.13 brms_r2 &lt;- cbind(bayes_R2(brms_c5_model_multiple, summary = FALSE), bayes_R2(brms_c5_model_age, summary = FALSE), bayes_R2(brms_c5_model_marriage, summary = FALSE)) %&gt;% as_tibble() %&gt;% set_names(c(&quot;multiple&quot;, &quot;age&quot;, &quot;marriage&quot;)) p1 &lt;- brms_r2 %&gt;% pivot_longer(everything(), names_to = &quot;model&quot;, values_to = &quot;r2&quot;) %&gt;% ggplot() + geom_density(aes(x = r2, color = model, fill = after_scale(clr_alpha(color))), size = .5, adjust = .5) + scale_x_continuous(NULL, limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL) + scale_color_manual(values = c(age = clr0, marriage = clr0dd, multiple = clr2)) + labs(subtitle = expression(italic(R)^2~distributions)) + facet_wrap(model ~ ., ncol = 1) + theme(legend.position = &quot;none&quot;) p2 &lt;- brms_r2 %&gt;% mutate(diff = multiple - marriage) %&gt;% ggplot(aes(x = diff, y = 0)) + stat_halfeye(point_interval = median_qi, .width = .95, shape = 21, fill = fll0, color = clr0dd) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(italic(marriage)~has~lower~italic(R)^2~than~italic(multiple)), x = expression(Delta*italic(R)^2)) p1 + p2 8.7 pymc3 section × "],["rethinking-chapter-8.html", "9 Rethinking: Chapter 8 9.1 Building an Interaction 9.2 Symetry of Interactions 9.3 Continuous Interaction 9.4 Homework 9.5 {brms} section 9.6 pymc3 section", " 9 Rethinking: Chapter 8 Conditional Manatees by Richard McElreath, building on the Summaries by Solomon Kurz. 9.1 Building an Interaction Investigating how ruggedness influences countries GDP, conditional on whether the country is African or not. library(rethinking) p_dag1 &lt;- dagify(G ~ R + C + U, R ~ U, coords = tibble(name = c(&quot;R&quot;, &quot;G&quot;, &quot;C&quot;, &quot;U&quot;), x = c(0, .5, 1, .5), y = c(1, 1, 1, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;G&quot;, &quot;response&quot;, if_else(name %in% c(&quot;R&quot;, &quot;C&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) p_dag2 &lt;- dagify(G ~ R + H + U, R ~ U, H ~ R + C, coords = tibble(name = c(&quot;R&quot;, &quot;G&quot;, &quot;C&quot;, &quot;H&quot;, &quot;U&quot;), x = c(.33, 0, 1, .66, 0), y = c(.5, 1, .5, .5, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;G&quot;, &quot;response&quot;, if_else(name %in% c(&quot;R&quot;, &quot;C&quot;, &quot;H&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) p_dag1 + p_dag2 + plot_annotation(tag_levels = &quot;a&quot;) &amp; scale_y_continuous(limits = c(-.1, 1.1)) &amp; scale_x_continuous(limits = c(-.05, 1.05)) &amp; coord_fixed(ratio = .55) &amp; theme(plot.tag = element_text(family = fnt_sel)) \\(\\rightarrow\\) the DAGs are agnostic to interactions: all they show is that in a, \\(G\\) is a function of both \\(R\\) and \\(C\\) (\\(G = f(R, C)\\)) regardless of the existence of interaction between the two influences. Importing the ruggedness data: data(rugged) data_rugged &lt;- rugged %&gt;% as_tibble() %&gt;% mutate(log_gdp = log(rgdppc_2000)) %&gt;% filter(complete.cases(rgdppc_2000)) %&gt;% mutate(log_gdp_std = log_gdp / mean(log_gdp), rugged_std = rugged / max(rugged), cont_idx = as.integer(2 - cont_africa)) First sketch of the model (without interaction) \\[ \\begin{array}{rclr} log(y_{i}) &amp; \\sim &amp; Normal( \\mu_{i}, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_{i} &amp; = &amp; \\alpha + \\beta (r_{i} - \\overline{r}) &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(1, 1) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta &amp; \\sim &amp; Normal(0, 1) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] (rugged_mean &lt;- mean(data_rugged$rugged_std)) #&gt; [1] 0.2149601 model_rugged_draft &lt;- quap( flist = alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta * ( rugged_std - 0.215 ), alpha ~ dnorm(1, 1), beta ~ dnorm(0, 1), sigma ~ dexp(1) ), data = data_rugged ) Prior predictions set.seed(13) rugged_priors &lt;- extract.prior(model_rugged_draft) %&gt;% as_tibble() prior_prediction_range &lt;- c(-.2, 1.2) rugged_prior_predictions &lt;- link(model_rugged_draft, post = rugged_priors, data = list(rugged_std = prior_prediction_range)) %&gt;% as_tibble() %&gt;% set_names(nm = as.character(prior_prediction_range)) %&gt;% mutate(.draw = row_number()) p_prior_draft &lt;- rugged_prior_predictions %&gt;% filter(.draw &lt; 51) %&gt;% ggplot() + geom_segment(aes(x = -.2, xend = 1.2, y = `-0.2`, yend = `1.2`, group = .draw, color = .draw ==26), size = .4, alpha = .6) + labs(title = glue(&quot;{mth(&#39;\\U03B1 \\U007E&#39;)} Normal(1, 1)&lt;br&gt;{mth(&#39;\\U03B2 \\U007E&#39;)} Normal(0, 1)&quot;), x = &quot;ruggedness_std&quot;, y = &quot;log GDP (prop of mean)&quot;) + scale_color_manual(values = c(`FALSE` = clr0d, `TRUE` = clr1), guide = &quot;none&quot;) Proportion of extreme slopes (\\(\\gt 0.6\\)) within the prior: sum(abs(rugged_priors$beta) &gt; .6) / length(rugged_priors$beta) #&gt; [1] 0.534 Restricting the priors for \\(\\alpha\\) and \\(\\beta\\) to more reasonable ranges: \\[ \\begin{array}{rclr} \\alpha &amp; \\sim &amp; Normal(1, 0.1) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta &amp; \\sim &amp; Normal(0, 0.3) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\end{array} \\] model_rugged_restricted &lt;- quap( flist = alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta * ( rugged_std - 0.215 ), alpha ~ dnorm(1, 0.1), beta ~ dnorm(0, 0.3), sigma ~ dexp(1) ), data = data_rugged ) rugged_restricted_priors &lt;- extract.prior(model_rugged_restricted) %&gt;% as_tibble() rugged_restricted_prior_predictions &lt;- link(model_rugged_restricted, post = rugged_restricted_priors, data = list(rugged_std = prior_prediction_range)) %&gt;% as_tibble() %&gt;% set_names(nm = as.character(prior_prediction_range)) %&gt;% mutate(.draw = row_number()) p_prior_restricted &lt;- rugged_restricted_prior_predictions %&gt;% filter(.draw &lt; 51) %&gt;% ggplot() + geom_segment(aes(x = -.2, xend = 1.2, y = `-0.2`, yend = `1.2`), color = clr0d, size = .4, alpha = .6) + labs(title = glue(&quot;{mth(&#39;\\U03B1 \\U007E&#39;)} Normal(1, 0.1)&lt;br&gt;{mth(&#39;\\U03B2 \\U007E&#39;)} Normal(0, 0.3)&quot;), x = &quot;ruggedness_std&quot;, y = &quot;log GDP (prop of mean)&quot;) p_prior_draft + p_prior_restricted + plot_annotation(tag_levels = &quot;a&quot;) &amp; geom_hline(yintercept = range(data_rugged$log_gdp_std), linetype = 3, color = clr_dark) &amp; coord_cartesian(ylim = c(.5, 1.5), xlim = c(-.2, 1.2)) &amp; theme(plot.title = element_markdown(), plot.tag = element_text(family = fnt_sel)) precis(model_rugged_restricted) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 1.00 0.01 0.98 1.02 beta 0.00 0.05 -0.09 0.09 sigma 0.14 0.01 0.12 0.15 9.1.1 Adding an index variable is not enough Updating the model to include a index variable: \\[ \\begin{array}{rclr} \\mu_{i} &amp; = &amp; \\alpha_{CID[i]} + \\beta (r_{i} - \\overline{r}) &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] model_rugged_index &lt;- quap( flist = alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;- alpha[cont_idx] + beta * ( rugged_std - 0.215 ), alpha[cont_idx] ~ dnorm(1, 0.1), beta ~ dnorm(0, 0.3), sigma ~ dexp(1) ), data = data_rugged ) compare(model_rugged_restricted, model_rugged_index) #&gt; WAIC SE dWAIC dSE pWAIC #&gt; model_rugged_index -252.2759 15.30586 0.00000 NA 4.248819 #&gt; model_rugged_restricted -188.8157 13.20582 63.46027 15.12848 2.665296 #&gt; weight #&gt; model_rugged_index 1.00000e+00 #&gt; model_rugged_restricted 1.65874e-14 precis(model_rugged_index, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha[1] 0.88 0.02 0.85 0.91 alpha[2] 1.05 0.01 1.03 1.07 beta -0.05 0.05 -0.12 0.03 sigma 0.11 0.01 0.10 0.12 rugged_index_posterior &lt;- extract.samples(model_rugged_index) %&gt;% as_tibble() %&gt;% mutate(diff = alpha[,1 ] - alpha[,2]) PI(rugged_index_posterior$diff) #&gt; 5% 94% #&gt; -0.1996424 -0.1382548 rugged_range &lt;- seq(from = -.01, to = 1.01, length.out = 51) draw_posterior_samples &lt;- function(idx, model){ link(model, data = tibble(cont_idx = idx, rugged_std = rugged_range)) %&gt;% as_tibble() %&gt;% set_names(nm = as.character(rugged_range)) %&gt;% pivot_longer(everything(), values_to = &quot;log_gdp_std&quot;, names_to = &quot;rugged_std&quot;) %&gt;% mutate(cont_idx = idx, rugged_std = as.numeric(rugged_std)) } rugged_index_posterior &lt;- bind_rows(draw_posterior_samples(1, model = model_rugged_index), draw_posterior_samples(2, model = model_rugged_index)) rugged_index_posterior_pi &lt;- rugged_index_posterior %&gt;% group_by(rugged_std, cont_idx) %&gt;% summarise(mean = mean(log_gdp_std), PI_lower = PI(log_gdp_std, prob = .97)[1], PI_upper = PI(log_gdp_std, prob = .97)[2]) %&gt;% ungroup() ggplot(mapping = aes(x = rugged_std)) + geom_smooth(data = rugged_index_posterior_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = factor(cont_idx), fill = after_scale(clr_alpha(clr_lighten(color)))), size = .2) + geom_point(data = data_rugged, aes(y = log_gdp_std, color = factor(cont_idx), fill = after_scale(clr_alpha(color))), size = 1.5, shape = 21) + labs(y = &quot;log GDP (prop of mean)&quot;) + scale_color_manual(&quot;Continent&quot;, values = c(`1` = clr1, `2` = clr0dd), labels = c(`1` = &quot;Africa&quot;, `2` = &quot;Other&quot;)) 9.1.2 Adding an interaction does work Updating the model to also include a random slope: \\[ \\begin{array}{rclr} \\mu_{i} &amp; = &amp; \\alpha_{CID[i]} + \\beta_{CID[i]} (r_{i} - \\overline{r}) &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] model_rugged_slope &lt;- quap( flist = alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;- alpha[cont_idx] + beta[cont_idx] * ( rugged_std - 0.215 ), alpha[cont_idx] ~ dnorm(1, 0.1), beta[cont_idx] ~ dnorm(0, 0.3), sigma ~ dexp(1) ), data = data_rugged ) precis(model_rugged_slope, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha[1] 0.89 0.02 0.86 0.91 alpha[2] 1.05 0.01 1.03 1.07 beta[1] 0.13 0.07 0.01 0.25 beta[2] -0.14 0.05 -0.23 -0.06 sigma 0.11 0.01 0.10 0.12 compare(model_rugged_restricted, model_rugged_index, model_rugged_slope, func = PSIS) %&gt;% knit_precis(param_name = &quot;model&quot;) model PSIS SE dPSIS dSE pPSIS weight model_rugged_slope -258.72 15.27 0.00 NA 5.38 0.96 model_rugged_index -252.31 15.31 6.41 6.91 4.21 0.04 model_rugged_restricted -188.75 13.39 69.97 15.51 2.71 0.00 library(ggdist) set.seed(42) (data_rugged_psis &lt;- PSIS(model_rugged_slope, pointwise = TRUE) %&gt;% as_tibble() %&gt;% bind_cols(data_rugged)) %&gt;% ggplot(aes(x = factor(cont_idx), y = k)) + geom_boxplot(#adjust = 1, aes(color = factor(cont_idx), fill = after_scale(clr_alpha(color))))+ geom_text(data = . %&gt;% filter(k &gt; .25), aes(x = cont_idx + .1, label = country), hjust = 0, family = fnt_sel) + scale_color_manual(&quot;Continent&quot;, values = c(`1` = clr1, `2` = clr0dd), labels = c(`1` = &quot;Africa&quot;, `2` = &quot;Other&quot;)) + labs(x = &quot;continent&quot;) rugged_slope_posterior &lt;- bind_rows(draw_posterior_samples(1, model = model_rugged_slope), draw_posterior_samples(2, model = model_rugged_slope)) rugged_slope_posterior_pi &lt;- rugged_slope_posterior %&gt;% group_by(rugged_std, cont_idx) %&gt;% summarise(mean = mean(log_gdp_std), PI_lower = PI(log_gdp_std, prob = .97)[1], PI_upper = PI(log_gdp_std, prob = .97)[2]) %&gt;% ungroup() ggplot(mapping = aes(x = rugged_std)) + geom_smooth(data = rugged_slope_posterior_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = factor(cont_idx), fill = after_scale(clr_alpha(clr_lighten(color)))), size = .2) + geom_point(data = data_rugged, aes(y = log_gdp_std, color = factor(cont_idx), fill = after_scale(clr_alpha(color))), size = 1.5, shape = 21) + ggrepel::geom_text_repel(data = data_rugged_psis %&gt;% filter(k &gt; .25), aes(y = log_gdp_std, x = rugged_std + .1, label = country), force = 20, hjust = 0, family = fnt_sel) + labs(y = &quot;log GDP (prop of mean)&quot;) + facet_wrap(cont_idx ~ .) + scale_color_manual(&quot;Continent&quot;, values = c(`1` = clr1, `2` = clr0dd), labels = c(`1` = &quot;Africa&quot;, `2` = &quot;Other&quot;)) 9.2 Symetry of Interactions Rewriting the linear model to highlight the symmetry between conditional slope and conditional intercept: \\[ \\mu_{i} = \\underbrace{(2 - CID_{i}) \\big( \\alpha_{1} + \\beta_{1} (r_{i} - \\overline{r}) \\big)}_{CID[i] = 1} + \\underbrace{(CID_{i} - 1) \\big( \\alpha_{2} + \\beta_{2} (r_{i} - \\overline{r}) \\big)}_{CID[i] = 2} \\] Plotting a counterfactual effect of comparing the association of log GPD with being in Africa while holding ruggedness constant: rugged_slope_posterior_delta &lt;- rugged_slope_posterior %&gt;% group_by(cont_idx, rugged_std) %&gt;% mutate(.draw = row_number()) %&gt;% ungroup() %&gt;% pivot_wider(names_from = cont_idx, values_from = log_gdp_std, names_prefix = &quot;log_gdp_std_&quot;) %&gt;% mutate(delta_log_gdp_std = log_gdp_std_1 - log_gdp_std_2) rugged_slope_posterior_delta_pi &lt;- rugged_slope_posterior_delta %&gt;% group_by(rugged_std) %&gt;% summarise(mean = mean(delta_log_gdp_std), PI_lower = PI(delta_log_gdp_std, prob = .97)[1], PI_upper = PI(delta_log_gdp_std, prob = .97)[2]) %&gt;% ungroup() ggplot(mapping = aes(x = rugged_std)) + geom_hline(yintercept = 0, color = clr_dark, linetype = 3) + geom_smooth(data = rugged_slope_posterior_delta_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr0d, fill = fll0, size = .2) + geom_text(data = tibble(y = .03 * c(-1,1), lab = c(&quot;Africa lower GDP&quot;, &quot;Africa higher GDP&quot;)), aes(x = .01, y = y, label = lab), family = fnt_sel, hjust = 0) + labs(y = &quot;expected difference log GDP&quot;) … it it simultaneously consistent with the data and the model, that the influence of ruggedness depends on the continent and that the influence of the continent depends on ruggedness. 9.3 Continuous Interaction data(tulips) precis(tulips) %&gt;% knit_precis() param mean sd 5.5% 94.5% histogram bed NaN NA NA NA water 2.00 0.83 1.00 3.00 ▇▁▁▁▇▁▁▁▁▇ shade 2.00 0.83 1.00 3.00 ▇▁▁▁▇▁▁▁▁▇ blooms 128.99 92.68 4.31 280.79 ▅▇▇▂▃▁▁▁ data_tulips &lt;- tulips %&gt;% as_tibble() %&gt;% mutate(blooms_std = blooms / max(blooms), water_cent = water - mean(water), shade_cent = shade - mean(shade)) dagify(B ~ S + W, coords = tibble(name = c(&quot;B&quot;, &quot;W&quot;, &quot;S&quot;), x = c(.5, 0, 1), y = c(1, 1, 1))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;B&quot;, &quot;response&quot;, if_else(name %in% c(&quot;W&quot;, &quot;S&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + coord_fixed(ratio = .6, xlim = c(-.05, 1.05), ylim = c(.9, 1.1)) We are going to build two models, one without any interaction and one with. 9.3.1 Model without Interaction \\[ \\begin{array}{rclr} B_{i} &amp; \\sim &amp; Normal( \\mu_{i}, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_{i} &amp; = &amp; \\alpha + \\beta_{W} (W_{i} - \\overline{W}) + \\beta_{S} (S_{i} - \\overline{S}) &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0.5, 0.25) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{W} &amp; \\sim &amp; Normal(0, 0.25) &amp; \\textrm{[$\\beta_{W}$ prior]}\\\\ \\beta_{S} &amp; \\sim &amp; Normal(0, 0.25) &amp; \\textrm{[$\\beta_{S}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] Considerations for the priors: Ranges assigned if we used \\(alpha = Normal(0.5, 1)\\) instead: alpha &lt;- rnorm(1e4, 0.5, 1); sum(alpha &lt; 0 | alpha &gt; 1) / length(alpha) #&gt; [1] 0.6251 updating to a more restrictive prior (used in the model): alpha &lt;- rnorm(1e4, 0.5, .25); sum(alpha &lt; 0 | alpha &gt; 1) / length(alpha) #&gt; [1] 0.0498 model_tulips_simple &lt;- quap( flist = alist( blooms_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_w * water_cent + beta_s * shade_cent, alpha ~ dnorm(.5, .25), beta_w ~ dnorm(0,.25), beta_s ~ dnorm(0,.25), sigma ~ dexp(1) ), data = data_tulips ) precis(model_tulips_simple) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.36 0.03 0.31 0.41 beta_w 0.21 0.04 0.15 0.26 beta_s -0.11 0.04 -0.17 -0.05 sigma 0.16 0.02 0.12 0.19 9.3.2 Model with Interaction Implementing the continuous interaction \\[ \\begin{array}{rcl} \\mu_{i} &amp; = &amp; \\alpha + \\gamma_{W,i} \\beta_{W} W_{i} + \\beta_{S} S_{i} \\\\ \\gamma_{W,i} &amp; = &amp; \\beta_{W} + \\beta_{WS} S_{i}\\\\ \\end{array} \\] which allows the substitution \\[ \\begin{array}{rcl} \\mu_{i} &amp; = &amp; \\alpha + \\underbrace{(\\beta_{W} + \\beta_{WS} S_{i})}_{\\gamma_{W,i} } \\beta_{W} W_{i} + \\beta_{S} S_{i} \\\\ &amp; = &amp; \\alpha + \\beta_{W} W_{i} + \\beta_{S} S_{i} + \\beta_{WS} W_{i} S_{i}\\\\ \\end{array} \\] Using this for the complete model: \\[ \\begin{array}{rclr} B_{i} &amp; \\sim &amp; Normal( \\mu_{i}, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_{i} &amp; = &amp; \\alpha + \\beta_{W} W_{i} + \\beta_{S} S_{i} + \\beta_{WS} W_{i} S_{i} &amp; \\textrm{[linear model]} \\\\ \\alpha &amp; \\sim &amp; Normal(0.5, 0.25) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{W} &amp; \\sim &amp; Normal(0, 0.25) &amp; \\textrm{[$\\beta_{W}$ prior]}\\\\ \\beta_{S} &amp; \\sim &amp; Normal(0, 0.25) &amp; \\textrm{[$\\beta_{S}$ prior]}\\\\ \\beta_{WS} &amp; \\sim &amp; Normal(0, 0.25) &amp; \\textrm{[$\\beta_{S}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_tulips_interaction &lt;- quap( flist = alist( blooms_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_w * water_cent + beta_s * shade_cent + beta_ws * water_cent * shade_cent, alpha ~ dnorm(.5, .25), beta_w ~ dnorm(0,.25), beta_s ~ dnorm(0,.25), beta_ws ~ dnorm(0,.25), sigma ~ dexp(1) ), data = data_tulips ) precis(model_tulips_interaction) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.36 0.02 0.32 0.40 beta_w 0.21 0.03 0.16 0.25 beta_s -0.11 0.03 -0.16 -0.07 beta_ws -0.14 0.04 -0.20 -0.09 sigma 0.12 0.02 0.10 0.15 9.3.3 Plotting the Posterior Predictions tulip_range &lt;- cross_df(list(water_cent = -1:1, shade_cent = -1:1)) %&gt;% mutate(rn = row_number()) tulips_simple_posterior &lt;- link(model_tulips_simple, data = tulip_range) %&gt;% as_tibble() %&gt;% set_names(nm = 1:length(names(.))) %&gt;% mutate(.draw = row_number()) %&gt;% pivot_longer(-.draw, names_to = &quot;rn&quot;, values_to = &quot;blooms&quot;) %&gt;% mutate(rn = as.numeric(rn), model = &quot;simple&quot;) %&gt;% left_join(tulip_range) tulips_interaction_posterior &lt;- link(model_tulips_interaction, data = tulip_range) %&gt;% as_tibble() %&gt;% set_names(nm = 1:length(names(.))) %&gt;% mutate(.draw = row_number()) %&gt;% pivot_longer(-.draw, names_to = &quot;rn&quot;, values_to = &quot;blooms&quot;) %&gt;% mutate(rn = as.numeric(rn), model = &quot;interaction&quot;) %&gt;% left_join(tulip_range) tulips_posterior &lt;- tulips_simple_posterior %&gt;% bind_rows(tulips_interaction_posterior) %&gt;% mutate(model = factor(model, levels = c(&quot;simple&quot;, &quot;interaction&quot;))) tulips_posterior_pi &lt;- tulips_posterior %&gt;% group_by(model, rn,water_cent, shade_cent ) %&gt;% summarise(mean = mean(blooms), PI_lower = PI(blooms, prob = .97)[1], PI_upper = PI(blooms, prob = .97)[2]) %&gt;% ungroup() tulips_posterior %&gt;% ggplot(aes(x = water_cent)) + geom_line(data = . %&gt;% filter(.draw &lt; 21), aes(y = blooms , group = .draw, color = .draw == 20)) + geom_point(data = data_tulips, aes(y = blooms_std), color = clr_dark) + facet_grid(model ~ shade_cent, labeller = label_both,switch = &quot;y&quot;) + scale_color_manual(values = c(`FALSE` = fll0, `TRUE` = fll1)) + theme(legend.position = &quot;bottom&quot;, strip.placement = &quot;outside&quot;) tulips_posterior_pi %&gt;% ggplot(aes(x = water_cent)) + geom_point(data = data_tulips, aes(y = blooms_std), color = clr_dark) + geom_smooth(stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr0dd, fill = clr_alpha(clr0dd), size = .5) + facet_grid(model ~ shade_cent, labeller = label_both,switch = &quot;y&quot;) + theme(legend.position = &quot;bottom&quot;, strip.placement = &quot;outside&quot;) 9.3.4 Plotting the Prior Predictions set.seed(5) tulips_simple_prior &lt;- extract.prior(model_tulips_simple, data = tulip_range) %&gt;% as_tibble() %&gt;% mutate(.draw = row_number(), grid = rep(list(cross_df(list(water_cent = -1:1, shade_cent = -1:1))), n()), model = &quot;simple&quot;) %&gt;% unnest(grid) %&gt;% mutate(blooms = alpha + beta_w * water_cent + beta_s * shade_cent) tulips_interaction_prior &lt;- extract.prior(model_tulips_interaction, data = tulip_range) %&gt;% as_tibble() %&gt;% mutate(.draw = row_number(), grid = rep(list(cross_df(list(water_cent = -1:1, shade_cent = -1:1))), n()), model = &quot;interaction&quot;) %&gt;% unnest(grid) %&gt;% mutate(blooms = alpha + beta_w * water_cent + beta_s * shade_cent + beta_ws * water_cent * shade_cent) tulips_prior &lt;- tulips_simple_prior %&gt;% bind_rows(tulips_interaction_prior) %&gt;% mutate(model = factor(model, levels = c(&quot;simple&quot;, &quot;interaction&quot;))) tulips_prior_pi &lt;- tulips_prior %&gt;% group_by(model, water_cent, shade_cent ) %&gt;% summarise(mean = mean(blooms), PI_lower = PI(blooms, prob = .97)[1], PI_upper = PI(blooms, prob = .97)[2]) %&gt;% ungroup() tulips_prior %&gt;% ggplot(aes(x = water_cent)) + geom_line(data = . %&gt;% filter(.draw &lt; 21), aes(y = blooms , group = .draw, color = .draw == 20)) + geom_hline(yintercept = c(0, 1), linetype = 3, color = clr_dark) + facet_grid(model ~ shade_cent, labeller = label_both, switch = &quot;y&quot;) + scale_color_manual(values = c(`FALSE` = clr_alpha(clr0d), `TRUE` = fll1)) + theme(legend.position = &quot;bottom&quot;, strip.placement = &quot;outside&quot;) + coord_cartesian(ylim = c(-.35,1.35)) tulips_prior_pi %&gt;% ggplot(aes(x = water_cent)) + geom_smooth(stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr0dd, fill = clr_alpha(clr0dd), size = .5) + geom_hline(yintercept = c(0, 1), linetype = 3, color = clr_dark) + facet_grid(model ~ shade_cent, labeller = label_both,switch = &quot;y&quot;) + theme(legend.position = &quot;bottom&quot;, strip.placement = &quot;outside&quot;) library(rlang) chapter8_models &lt;- env( data_rugged = data_rugged, model_rugged_draft = model_rugged_draft, model_rugged_restricted = model_rugged_restricted, model_rugged_index = model_rugged_index, model_rugged_slope = model_rugged_slope, data_tulips = data_tulips ) write_rds(chapter8_models, &quot;envs/chapter8_models.rds&quot;) 9.4 Homework E1 Bread dough rises because of yeast * temperature Education * country leads to higher income Gasoline * key makes car go E2 Which ones of the following explanations invoke an interaction Caramelizing onions requires cooking over low heat and making sure the onions do not dry out. A car will go faster when it has more cylinders and when it has a better fuel injector. Most people acquire their political beliefs from their parents, unless they get them instead from their friends. Intelligent animals tend to be either highly social or have manipulative appendages (hands, tentacles, etc.). E3 \\[ \\begin{array}{rrcl} 1. &amp; caramel &amp; = &amp; \\alpha + \\beta_{temp} \\times temp + \\beta_{moist} \\times moist + \\beta_{drying} \\times temp \\times moist\\\\ 2. &amp; max\\_speed &amp; = &amp; \\alpha_{[cyl]} + \\beta_{efficieny[cyl]} \\times inject \\\\ 3. &amp; polit &amp; = &amp; \\alpha_{[influence\\_group]} \\\\ 4. &amp; intelligence &amp; = &amp; \\alpha_{[appendages]} + beta_{sociality} \\times sociality\\\\ \\end{array} \\] M1 This could be due to a three-way interaction, where the effect of water on blooms and of light on blooms, as well as the interaction effect of water and light on blooms also depend on temperature, with high temperature intervening negatively on all other effects. M2 \\[ \\begin{array}{rcl} \\mu_{i} &amp; = &amp; \\alpha_{[temp]} + \\beta_{W[temp]} W_{i} + \\beta_{S[temp]} S_{i} + \\beta_{WS[temp]} W_{i} S_{i}\\\\ \\end{array} \\] with \\(\\alpha_{[hot]} = 0\\) \\(\\beta_{W[hot]} = 0\\) \\(\\beta_{S[hot]} = 0\\) \\(\\beta_{WS[hot]} = 0\\) M3 n &lt;- 5e2 raven_data &lt;- tibble( wolves_std = rep(round(c(1, 3, 10)/10, digits = 2), each = n), #rbinom(n, size = 10, .45), wolve_area = rnorm(n = n * 3), raven = rnorm(n = n * 3, mean = wolves_std + wolve_area / wolves_std, sd = 1.5)) raven_data %&gt;% ggplot(aes(x = wolve_area, y = raven)) + geom_point(color = clr_alpha(clr0d)) + facet_wrap(wolves_std~ ., labeller = label_both) There should be non-linear effects once the wolves deplete all their food sources. M4 model_tulips_interaction_restricted &lt;- quap( flist = alist( blooms_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_w * water_cent + beta_s * shade_cent + beta_ws * water_cent * shade_cent, alpha ~ dnorm(.5, .25), beta_w ~ dunif(0,.5), beta_s ~ dunif(-.5,0), beta_ws ~ dnorm(0,.5), sigma ~ dexp(1) ), data = data_tulips, start = list(beta_w = .25,beta_s = -.25, beta_ws = .2) ) precis(model_tulips_interaction_restricted) %&gt;% knit_precis() param mean sd 5.5% 94.5% beta_w 0.21 0.03 0.16 0.26 beta_s -0.12 0.03 -0.16 -0.07 beta_ws -0.15 0.04 -0.20 -0.09 alpha 0.36 0.02 0.32 0.40 sigma 0.12 0.02 0.10 0.15 # set.seed(5) tulips_simple_prior_restricted &lt;- extract.prior(model_tulips_interaction_restricted, data = tulip_range) %&gt;% as_tibble() %&gt;% mutate(.draw = row_number(), grid = rep(list(cross_df(list(water_cent = -1:1, shade_cent = -1:1))), n()), model = &quot;simple&quot;) %&gt;% unnest(grid) %&gt;% mutate(blooms = alpha + beta_w * water_cent + beta_s * shade_cent) tulips_interaction_prior_restricted &lt;- extract.prior(model_tulips_interaction_restricted, data = tulip_range) %&gt;% as_tibble() %&gt;% mutate(.draw = row_number(), grid = rep(list(cross_df(list(water_cent = -1:1, shade_cent = -1:1))), n()), model = &quot;interaction&quot;) %&gt;% unnest(grid) %&gt;% mutate(blooms = alpha + beta_w * water_cent + beta_s * shade_cent + beta_ws * water_cent * shade_cent) tulips_prior_restricted_pi &lt;- tulips_interaction_prior_restricted %&gt;% group_by( water_cent, shade_cent ) %&gt;% summarise(mean = mean(blooms), PI_lower = PI(blooms, prob = .97)[1], PI_upper = PI(blooms, prob = .97)[2]) %&gt;% ungroup() tulips_interaction_prior_restricted %&gt;% ggplot(aes(x = water_cent)) + geom_line(data = . %&gt;% filter(.draw &lt; 41), aes(y = blooms , group = .draw, color = .draw == 20)) + geom_hline(yintercept = c(0, 1), linetype = 3, color = clr_dark) + facet_grid(. ~ shade_cent, labeller = label_both, switch = &quot;y&quot;) + scale_color_manual(values = c(`FALSE` = clr_alpha(clr0d), `TRUE` = fll1)) + theme(legend.position = &quot;bottom&quot;, strip.placement = &quot;outside&quot;) + coord_cartesian(ylim = c(-.35,1.35)) H1 model_tulips_bed_plain &lt;- quap( flist = alist( blooms_std ~ dnorm(mu, sigma), mu &lt;- alpha[bed] + beta_w * water_cent + beta_s * shade_cent + beta_ws * water_cent * shade_cent, alpha[bed] ~ dnorm(.5, .25), beta_w ~ dnorm(0,.25), beta_s ~ dnorm(0,.25), beta_ws ~ dnorm(0,.25), sigma ~ dexp(1) ), data = data_tulips ) precis(model_tulips_bed_plain, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha[1] 0.27 0.04 0.22 0.33 alpha[2] 0.40 0.04 0.34 0.45 alpha[3] 0.41 0.04 0.35 0.47 beta_w 0.21 0.03 0.17 0.25 beta_s -0.11 0.03 -0.15 -0.07 beta_ws -0.14 0.03 -0.19 -0.09 sigma 0.11 0.01 0.08 0.13 H2 compare(model_tulips_interaction, model_tulips_bed_plain) %&gt;% knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight model_tulips_bed_plain -22.98 9.96 0.00 NA 9.94 0.62 model_tulips_interaction -22.03 10.49 0.95 7.8 6.63 0.38 There seems to be some preference for the model including bed, but the weight is only at ~66% and the SEs are quite large compared to the \\(\\Delta WAIC\\). posterior distributions of the model parameters library(tidybayes) library(tidybayes.rethinking) model_tulips_bed_plain %&gt;% recover_types(data_tulips) %&gt;% spread_draws(alpha[bed], beta_w, beta_s, beta_ws) %&gt;% dplyr::select(-.chain, -.iteration) %&gt;% pivot_longer(cols = -c(bed, .draw), names_to = &quot;parameter&quot;) %&gt;% ggplot(aes(y = bed, x = value)) + # geom_vline(xintercept = 0, linetype = 3, color = clr_dark) + stat_halfeye(color = clr_dark,aes( fill = parameter)) + facet_wrap(parameter ~ ., scales = &quot;free_x&quot;, nrow = 1) + scale_fill_manual(values = c(fll0,fll1,fll2,fll3), guide = &quot;none&quot;) The improvement of the bed model seems to stem from the fact that tulips in the ‘a’ treatment tend to have a lower blooming score. posterior predictive model_tulips_bed_plain %&gt;% recover_types(data_tulips) %&gt;% spread_draws(alpha[bed], beta_w, beta_s, beta_ws) %&gt;% dplyr::select(-.chain, -.iteration) %&gt;% mutate(new_data = list(crossing(water_cent = c(-1,1), shade_cent = c(-1:1)))) %&gt;% unnest(new_data) %&gt;% mutate(blooms = alpha + beta_w * water_cent + beta_s * shade_cent + beta_ws * water_cent * shade_cent) %&gt;% ggplot(aes(x = water_cent)) + geom_line(data = . %&gt;% group_by(bed) %&gt;% filter(.draw &lt; 51) %&gt;% ungroup(), aes(y = blooms , group = str_c(shade_cent, .draw), color = .draw == 50)) + geom_point(data = data_tulips, aes(y = blooms_std), color = clr_dark) + facet_grid(shade_cent ~ bed, labeller = label_both,switch = &quot;y&quot;) + scale_color_manual(values = c(`FALSE` = fll0, `TRUE` = fll1)) + theme(legend.position = &quot;bottom&quot;, strip.placement = &quot;outside&quot;) H3 psis_k &lt;- tibble(waic_penalty = (function(){ set.seed(set.seed(23)) WAIC(model_rugged_slope, pointwise = TRUE)$penalty})(), psis_k = (function(){ set.seed(set.seed(23)) PSIS(model_rugged_slope, pointwise = TRUE)$k})(), country = data_rugged$country, cont_africa = data_rugged$cont_africa ) psis_k %&gt;% ggplot(aes(x = psis_k ,y = waic_penalty)) + geom_vline(xintercept = .5, color = clr_dark, linetype = 3) + geom_point(shape = 21, size = 2, color = clr0dd, fill = fll0) + ggrepel::geom_text_repel(data = psis_k, aes(x = psis_k, label = country), family = &quot;Josefin Sans&quot;, direction = &quot;both&quot;, max.overlaps = 1) + facet_wrap(cont_africa ~ . ) + labs(subtitle = &quot;gaussian&quot;) The Seychelles, Tajikistan and Switzerland seem to be the most influential points. All of those have a very rugged topography and extreme GDP values (both positive and negative) model_rugged_robust &lt;- quap( flist = alist( log_gdp_std ~ dstudent(nu = 2, mu, sigma), mu &lt;- alpha[cont_idx] + beta[cont_idx] * ( rugged_std - 0.215 ), alpha[cont_idx] ~ dnorm(1, 0.1), beta[cont_idx] ~ dnorm(0, 0.3), sigma ~ dexp(1) ), data = data_rugged ) psis_k_robust &lt;- tibble(waic_penalty = (function(){ set.seed(set.seed(23)) WAIC(model_rugged_robust, pointwise = TRUE)$penalty})(), psis_k = (function(){ set.seed(set.seed(23)) PSIS(model_rugged_robust, pointwise = TRUE)$k})(), country = data_rugged$country, cont_africa = data_rugged$cont_africa ) psis_k_robust %&gt;% ggplot(aes(x = psis_k ,y = waic_penalty)) + geom_vline(xintercept = .5, color = clr_dark, linetype = 3) + geom_point(shape = 21, size = 2, color = clr1, fill = fll1) + ggrepel::geom_text_repel(data = psis_k_robust, aes(x = psis_k, label = country), family = &quot;Josefin Sans&quot;, direction = &quot;both&quot;, max.overlaps = 1) + facet_wrap(cont_africa ~ . ) + labs(subtitle = &quot;student-t&quot;) rugged_slope_posterior_robust &lt;- bind_rows( draw_posterior_samples(1, model = model_rugged_robust), draw_posterior_samples(2, model = model_rugged_robust)) rugged_slope_posterior_robust_pi &lt;- rugged_slope_posterior_robust %&gt;% group_by(rugged_std, cont_idx) %&gt;% summarise(mean = mean(log_gdp_std), PI_lower = PI(log_gdp_std, prob = .97)[1], PI_upper = PI(log_gdp_std, prob = .97)[2]) %&gt;% ungroup() ggplot(mapping = aes(x = rugged_std)) + geom_smooth(data = rugged_slope_posterior_robust_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = factor(cont_idx), fill = after_scale(clr_alpha(clr_lighten(color)))), size = .2) + geom_point(data = data_rugged, aes(y = log_gdp_std, color = factor(cont_idx), fill = after_scale(clr_alpha(color))), size = 1.5, shape = 21) + ggrepel::geom_text_repel(data = data_rugged %&gt;% filter(country %in% c(&quot;Seychelles&quot;, &quot;Switzerland&quot;, &quot;Tajikistan&quot;, &quot;Lesotho&quot;, &quot;Nepal&quot;)), aes(y = log_gdp_std, x = rugged_std + .1, label = country), force = 20, hjust = 0, family = fnt_sel) + labs(y = &quot;log GDP (prop of mean)&quot;) + facet_wrap(cont_idx ~ .) + scale_color_manual(&quot;Continent&quot;, values = c(`1` = clr1, `2` = clr0dd), labels = c(`1` = &quot;Africa&quot;, `2` = &quot;Other&quot;)) H4 data(nettle) data_nettle &lt;- nettle %&gt;% as_tibble() %&gt;% mutate(lang.per.capita = num.lang / k.pop, lang_per_cap_log = log(lang.per.capita), lang_per_cap_log_std = standardize(lang_per_cap_log), area_log_std = standardize(log(area)), season_duration_scl = mean.growing.season / max(mean.growing.season), season_sd_scl = sd.growing.season / max(sd.growing.season)) data_nettle %&gt;% dplyr::select(lang_per_cap_log:season_sd_scl) %&gt;% precis() #&gt; mean sd 5.5% 94.5% #&gt; lang_per_cap_log -5.456606e+00 1.5207459 -7.8066714131 -3.4116252 #&gt; lang_per_cap_log_std -1.934656e-16 1.0000000 -1.5453372117 1.3447223 #&gt; area_log_std 2.619731e-17 1.0000000 -1.4475375475 1.3737581 #&gt; season_duration_scl 5.867905e-01 0.2619871 0.0692416667 0.9926458 #&gt; season_sd_scl 2.894700e-01 0.1816151 0.0008943782 0.5961755 #&gt; histogram #&gt; lang_per_cap_log ▁▁▂▅▇▂▂▁▁▁ #&gt; lang_per_cap_log_std ▁▁▁▂▃▇▇▂▃▁▁▁▁ #&gt; area_log_std ▁▁▁▅▃▇▅▇▃▁▁ #&gt; season_duration_scl ▃▁▃▂▃▇▇▇▃▃ #&gt; season_sd_scl ▃▇▇▇▂▂▁▁▁▁ \\[ \\begin{array}{rclr} L_{i} &amp; \\sim &amp; Normal( \\mu_{i}, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_{i} &amp; = &amp; \\alpha + \\beta_{D} D_{i} + \\beta_{A} A_{i}&amp; \\textrm{[linear model]} \\\\ \\alpha &amp; \\sim &amp; Log-Normal(0, 0.15) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{D} &amp; \\sim &amp; Normal(0, 0.25) &amp; \\textrm{[$\\beta_{W}$ prior]}\\\\ \\beta_{A} &amp; \\sim &amp; Normal(0, 0.25) &amp; \\textrm{[$\\beta_{S}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_lang_dur_area &lt;- quap( flist = alist( lang_per_cap_log_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_d * season_duration_scl + beta_a * area_log_std , alpha ~ dnorm(0, 0.15), c(beta_d, beta_a) ~ dnorm(0, 0.25), sigma ~ dexp(1) ), data = data_nettle ) precis(model_lang_dur_area) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha -0.07 0.11 -0.25 0.11 beta_d 0.19 0.18 -0.10 0.48 beta_a -0.22 0.10 -0.38 -0.06 sigma 0.94 0.08 0.81 1.06 area_classes &lt;- quantile(data_nettle$area_log_std, seq(0,1,length.out = 4)) area_means &lt;- (area_classes[1:3]+area_classes[2:4])/2 model_lang_dur_area %&gt;% spread_draws(alpha, beta_d, beta_a) %&gt;% dplyr::select(-.chain, -.iteration) %&gt;% mutate(new_data = list(crossing(season_duration_scl = range(data_nettle$season_duration_scl), area_log_std = area_means))) %&gt;% unnest(new_data) %&gt;% mutate(lang_per_cap_log_std = alpha + beta_d * season_duration_scl + beta_a * area_log_std, area_class = cut(area_log_std,area_classes + c(-.1, rep(0, 2), .1)), area_mean = area_means[as.numeric(area_class)] %&gt;% round(digits = 2)) %&gt;% ggplot(aes(x = season_duration_scl)) + geom_line(data = . %&gt;% filter(.draw &lt; 51) %&gt;% ungroup(), aes(y = lang_per_cap_log_std , group = str_c(.draw, area_log_std), color = area_log_std)) + geom_hline(yintercept = range(data_nettle$lang_per_cap_log_std), linetype = 3, color = clr_dark) + geom_point(data = data_nettle %&gt;% mutate(area_class = cut(area_log_std,area_classes + c(-.1, rep(0, 2), .1)), area_mean = area_means[as.numeric(area_class)] %&gt;% round(digits = 2)), aes(y = lang_per_cap_log_std, fill = area_log_std, color = after_scale(clr_darken(fill))), shape = 21) + scale_color_gradientn(colours = c(fll0, fll1), guide = &quot;none&quot;) + scale_fill_gradientn(colours = c(clr0, clr1), guide = &quot;none&quot;) + facet_wrap(. ~ area_mean, labeller = label_both) + theme(legend.position = &quot;bottom&quot;, strip.placement = &quot;outside&quot;) \\[ \\begin{array}{rclr} L_{i} &amp; \\sim &amp; Normal( \\mu_{i}, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_{i} &amp; = &amp; \\alpha + \\beta_{S} S_{i} + \\beta_{A} A_{i}&amp; \\textrm{[linear model]} \\\\ \\alpha &amp; \\sim &amp; Log-Normal(0, 0.15) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{S} &amp; \\sim &amp; Normal(0, 0.25) &amp; \\textrm{[$\\beta_{W}$ prior]}\\\\ \\beta_{A} &amp; \\sim &amp; Normal(0, 0.25) &amp; \\textrm{[$\\beta_{S}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_lang_sd_area &lt;- quap( flist = alist( lang_per_cap_log_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_s * season_sd_scl + beta_a * area_log_std , alpha ~ dnorm(0, 0.15), c(beta_s, beta_a) ~ dnorm(0, 0.25), sigma ~ dexp(1) ), data = data_nettle ) precis(model_lang_sd_area) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.02 0.10 -0.14 0.17 beta_s -0.09 0.22 -0.45 0.26 beta_a -0.23 0.10 -0.39 -0.06 sigma 0.95 0.08 0.82 1.07 model_lang_sd_area %&gt;% spread_draws(alpha, beta_s, beta_a) %&gt;% dplyr::select(-.chain, -.iteration) %&gt;% mutate(new_data = list(crossing(season_sd_scl = range(data_nettle$season_sd_scl), area_log_std = area_means))) %&gt;% unnest(new_data) %&gt;% mutate(lang_per_cap_log_std = alpha + beta_s * season_sd_scl + beta_a * area_log_std, area_class = cut(area_log_std,area_classes + c(-.1, rep(0, 2), .1)), area_mean = area_means[as.numeric(area_class)] %&gt;% round(digits = 2)) %&gt;% ggplot(aes(x = season_sd_scl)) + geom_line(data = . %&gt;% filter(.draw &lt; 51) %&gt;% ungroup(), aes(y = lang_per_cap_log_std , group = str_c(.draw, area_log_std), color = area_log_std)) + geom_hline(yintercept = range(data_nettle$lang_per_cap_log_std), linetype = 3, color = clr_dark) + geom_point(data = data_nettle %&gt;% mutate(area_class = cut(area_log_std,area_classes + c(-.1, rep(0, 2), .1)), area_mean = area_means[as.numeric(area_class)] %&gt;% round(digits = 2)), aes(y = lang_per_cap_log_std, fill = area_log_std, color = after_scale(clr_darken(fill))), shape = 21) + scale_color_gradientn(colours = c(fll0, fll1), guide = &quot;none&quot;) + scale_fill_gradientn(colours = c(clr0, clr1), guide = &quot;none&quot;) + facet_wrap(. ~ area_mean, labeller = label_both) + theme(legend.position = &quot;bottom&quot;, strip.placement = &quot;outside&quot;) model_lang_inter &lt;- quap( flist = alist( lang_per_cap_log_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_d * season_duration_scl + beta_s * season_sd_scl + beta_ds * season_duration_scl * season_sd_scl, alpha ~ dnorm(0, 0.15), c(beta_d, beta_s) ~ dnorm(0, 0.25), beta_ds ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = data_nettle ) precis(model_lang_inter) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha -0.04 0.12 -0.23 0.14 beta_d 0.32 0.19 0.02 0.63 beta_s -0.20 0.23 -0.57 0.17 beta_ds -0.38 0.43 -1.07 0.31 sigma 0.94 0.08 0.82 1.07 sd_classes &lt;- quantile(data_nettle$season_sd_scl, seq(0,1,length.out = 4)) sd_means &lt;- (sd_classes[1:3]+sd_classes[2:4])/2 model_lang_inter %&gt;% spread_draws(alpha, beta_d, beta_s, beta_ds) %&gt;% dplyr::select(-.chain, -.iteration) %&gt;% mutate(new_data = list(crossing(season_duration_scl = range(data_nettle$season_duration_scl), season_sd_scl = sd_means))) %&gt;% unnest(new_data) %&gt;% mutate(lang_per_cap_log_std = alpha + beta_d * season_duration_scl + beta_s * season_sd_scl + beta_ds * season_duration_scl * season_sd_scl, sd_class = cut(season_sd_scl ,sd_classes + c(-.1, rep(0, 2), .1)), sd_mean = sd_means[as.numeric(sd_class)] %&gt;% round(digits = 2)) %&gt;% ggplot(aes(x = season_duration_scl)) + geom_line(data = . %&gt;% filter(.draw &lt; 51) %&gt;% ungroup(), aes(y = lang_per_cap_log_std , group = str_c(.draw, season_sd_scl), color = season_sd_scl)) + geom_hline(yintercept = range(data_nettle$lang_per_cap_log_std), linetype = 3, color = clr_dark) + geom_point(data = data_nettle %&gt;% mutate(sd_class = cut(season_sd_scl ,sd_classes + c(-.1, rep(0, 2), .1)), sd_mean = sd_means[as.numeric(sd_class)] %&gt;% round(digits = 2)), aes(y = lang_per_cap_log_std, fill = season_sd_scl, color = after_scale(clr_darken(fill))), shape = 21) + scale_color_gradientn(colours = c(fll0, fll1), guide = &quot;none&quot;) + scale_fill_gradientn(colours = c(clr0, clr1), guide = &quot;none&quot;) + facet_wrap(. ~ sd_mean, labeller = label_both) + theme(legend.position = &quot;bottom&quot;, strip.placement = &quot;outside&quot;) compare(model_lang_dur_area, model_lang_sd_area, model_lang_inter) %&gt;% knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight model_lang_dur_area 208.93 16.88 0.00 NA 4.13 0.40 model_lang_inter 208.96 17.59 0.03 4.49 3.60 0.39 model_lang_sd_area 210.23 16.91 1.30 1.20 4.09 0.21 H5 data(&quot;Wines2012&quot;) data_wine &lt;- Wines2012 %&gt;% as_tibble() %&gt;% mutate(score_std = standardize(score), wine_nat_idx = wine.amer + 1, judge_nat_idx = judge.amer + 1, wine_idx = as.numeric(wine), judge_idx = as.numeric(judge)) unique(data_wine$wine_idx) %&gt;% sort() #&gt; [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 unique(data_wine$judge_idx) %&gt;% sort() #&gt; [1] 1 2 3 4 5 6 7 8 9 \\[ \\begin{array}{rclr} S_{i} &amp; \\sim &amp; Normal( \\mu_{i}, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_{i} &amp; = &amp; \\alpha_{J} + \\alpha_{W} &amp; \\textrm{[linear model]} \\\\ \\alpha_{J} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\alpha_{J}$ prior]}\\\\ \\alpha_{W} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\alpha_{W}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_wine_static &lt;- quap( flist = alist( score_std ~ dnorm(mu, sigma), mu &lt;- alpha_j[judge_idx] + alpha_w[wine_idx], alpha_j[judge_idx]~ dnorm(0, 0.5), alpha_w[wine_idx] ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = data_wine ) precis(model_wine_static, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha_j[1] -0.28 0.19 -0.58 0.01 alpha_j[2] 0.22 0.19 -0.08 0.51 alpha_j[3] 0.21 0.19 -0.09 0.51 alpha_j[4] -0.55 0.19 -0.85 -0.25 alpha_j[5] 0.81 0.19 0.51 1.11 alpha_j[6] 0.48 0.19 0.19 0.78 alpha_j[7] 0.13 0.19 -0.16 0.43 alpha_j[8] -0.67 0.19 -0.97 -0.37 alpha_j[9] -0.35 0.19 -0.65 -0.05 alpha_w[1] 0.12 0.25 -0.27 0.51 alpha_w[2] 0.09 0.25 -0.30 0.48 alpha_w[3] 0.24 0.25 -0.16 0.63 alpha_w[4] 0.48 0.25 0.09 0.87 alpha_w[5] -0.11 0.25 -0.50 0.28 alpha_w[6] -0.32 0.25 -0.71 0.07 alpha_w[7] 0.25 0.25 -0.14 0.64 alpha_w[8] 0.24 0.25 -0.16 0.63 alpha_w[9] 0.07 0.25 -0.32 0.46 alpha_w[10] 0.10 0.25 -0.29 0.50 alpha_w[11] -0.01 0.25 -0.40 0.38 alpha_w[12] -0.03 0.25 -0.42 0.37 alpha_w[13] -0.09 0.25 -0.48 0.30 alpha_w[14] 0.01 0.25 -0.39 0.40 alpha_w[15] -0.19 0.25 -0.58 0.20 alpha_w[16] -0.17 0.25 -0.57 0.22 alpha_w[17] -0.12 0.25 -0.52 0.27 alpha_w[18] -0.75 0.25 -1.14 -0.35 alpha_w[19] -0.14 0.25 -0.53 0.25 alpha_w[20] 0.33 0.25 -0.06 0.73 sigma 0.79 0.04 0.72 0.85 wine_params &lt;- data_wine %&gt;% mutate(j = str_c(judge,&quot;_&quot;,judge_idx), w = str_c(wine,&quot;_&quot;, wine_idx)) %&gt;% dplyr::select(j, w) %&gt;% pivot_longer(everything(),names_to = &quot;type&quot;) %&gt;% filter(!duplicated(value)) %&gt;% arrange(type, value) %&gt;% separate(value, into = c(&quot;name&quot;, &quot;idx&quot;), sep = &quot;_&quot;, convert = TRUE) precis(model_wine_static, depth = 2, pars = &quot;alpha&quot;) %&gt;% as_tibble_rn() %&gt;% mutate(type = str_remove(param, pattern = &quot;alpha_&quot;) %&gt;% str_remove(&quot;\\\\[[0-9]*\\\\]&quot;), idx = str_extract(param, &quot;[0-9]{1,2}&quot;) %&gt;% as.integer()) %&gt;% left_join(wine_params, by = c(&quot;type&quot;, &quot;idx&quot;)) %&gt;% group_by(type) %&gt;% mutate(name = fct_reorder(name, mean), name = str_c(str_pad(30 - as.numeric(name), width = 2, pad = 0),&quot;: &quot;,name)) %&gt;% ungroup() %&gt;% ggplot(aes(y = name, color = type)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_pointinterval(aes(xmin = `5.5%`,x = mean, xmax =`94.5%`)) + scale_color_manual(values = c(j = clr0d, w = clr2), guide = &quot;none&quot;) + facet_wrap(type ~ . , scales = &quot;free_y&quot;, nrow = 1) + labs(x = &quot;score_std&quot;) + theme(axis.title.y = element_blank(), strip.placement = &quot;outside&quot;) H6 \\[ \\begin{array}{rclr} S_{i} &amp; \\sim &amp; Normal( \\mu_{i}, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_{i} &amp; = &amp; \\alpha_{J} + \\alpha_{W} + \\alpha_{F} &amp; \\textrm{[linear model]} \\\\ \\alpha_{W} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\alpha_{W}$ prior]}\\\\ \\alpha_{J} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\alpha_{J}$ prior]}\\\\ \\alpha_{F} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\alpha_{F}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_wine_nat &lt;- quap( flist = alist( score_std ~ dnorm(mu, sigma), mu &lt;- alpha_j[judge_nat_idx] + alpha_w[wine_nat_idx] + alpha_f[flight], alpha_j[judge_nat_idx]~ dnorm(0, 0.25), alpha_w[wine_nat_idx] ~ dnorm(0, 0.25), alpha_f[flight] ~ dnorm(0, 0.25), sigma ~ dexp(1) ), data = data_wine ) precis(model_wine_nat, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha_j[1] -0.10 0.16 -0.36 0.15 alpha_j[2] 0.11 0.16 -0.15 0.36 alpha_w[1] 0.08 0.16 -0.18 0.34 alpha_w[2] -0.08 0.16 -0.34 0.18 alpha_f[1] 0.00 0.16 -0.26 0.26 alpha_f[2] 0.00 0.16 -0.25 0.26 sigma 0.98 0.05 0.90 1.07 wine_params &lt;- tibble(type = rep(c(&quot;w&quot;, &quot;j&quot;, &quot;f&quot;), each = 2), name = c(rep(c(&quot;america&quot;, &quot;france&quot;), 2), levels(data_wine$flight)), idx = rep(1:2, 3)) precis(model_wine_nat, depth = 2, pars = &quot;alpha&quot;) %&gt;% as_tibble_rn() %&gt;% mutate(type = str_remove(param, pattern = &quot;alpha_&quot;) %&gt;% str_remove(&quot;\\\\[[0-9]*\\\\]&quot;), idx = str_extract(param, &quot;[0-9]{1,2}&quot;) %&gt;% as.integer()) %&gt;% left_join(wine_params, by = c(&quot;type&quot;, &quot;idx&quot;)) %&gt;% group_by(type) %&gt;% mutate(name = fct_reorder(name, mean), name = str_c(str_pad(30 - as.numeric(name), width = 2, pad = 0),&quot;: &quot;,name)) %&gt;% ungroup() %&gt;% ggplot(aes(y = name, color = type)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_pointinterval(aes(xmin = `5.5%`,x = mean, xmax =`94.5%`)) + scale_color_manual(values = c(j = clr0d, w = clr2, f = clr_dark), guide = &quot;none&quot;) + facet_wrap(type ~ . , scales = &quot;free_y&quot;, ncol = 1) + labs(x = &quot;score_std&quot;) + theme(axis.title.y = element_blank(), strip.placement = &quot;outside&quot;) H7 \\[ \\begin{array}{rclr} S_{i} &amp; \\sim &amp; Normal( \\mu_{i}, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_{i} &amp; = &amp; \\alpha_{J} + \\alpha_{W} + \\alpha_{F} + \\beta_{JF} \\times J + \\beta_{WF} \\times W + \\beta_{JW} \\times J &amp; \\textrm{[linear model]} \\\\ \\alpha_{W} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\alpha_{W}$ prior]}\\\\ \\alpha_{J} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\alpha_{J}$ prior]}\\\\ \\alpha_{F} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\alpha_{F}$ prior]}\\\\ \\beta_{WF} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\alpha_{WF}$ prior]}\\\\ \\beta_{JF} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\alpha_{JF}$ prior]}\\\\ \\beta_{JW} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\alpha_{JW}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_wine_inter &lt;- quap( flist = alist( score_std ~ dnorm(mu, sigma), mu &lt;- alpha_j[judge_nat_idx] + alpha_w[wine_nat_idx] + alpha_f[flight] + beta_jf[flight] * judge.amer + beta_wf[flight] * wine.amer + beta_jw[wine_nat_idx] * judge.amer, alpha_j[judge_nat_idx]~ dnorm(0, 0.5), alpha_w[wine_nat_idx] ~ dnorm(0, 0.5), alpha_f[flight] ~ dnorm(0, 0.5), beta_jf[flight] ~ dnorm(0, 0.5), beta_wf[flight] ~ dnorm(0, 0.5), beta_jw[wine_nat_idx] ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = data_wine ) precis(model_wine_inter, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha_j[1] -0.09 0.33 -0.62 0.43 alpha_j[2] 0.08 0.39 -0.55 0.70 alpha_w[1] 0.04 0.33 -0.48 0.57 alpha_w[2] -0.06 0.36 -0.64 0.53 alpha_f[1] 0.12 0.32 -0.40 0.64 alpha_f[2] -0.13 0.32 -0.65 0.39 beta_jf[1] 0.06 0.34 -0.48 0.61 beta_jf[2] 0.02 0.34 -0.53 0.56 beta_wf[1] -0.27 0.34 -0.81 0.27 beta_wf[2] 0.21 0.34 -0.33 0.75 beta_jw[1] 0.09 0.35 -0.46 0.65 beta_jw[2] -0.01 0.34 -0.56 0.54 sigma 0.97 0.05 0.89 1.05 p1 &lt;- precis(model_wine_inter, depth = 2, pars = c(&quot;alpha&quot;, &quot;beta&quot;)) %&gt;% as_tibble_rn() %&gt;% mutate(type = str_remove(param, pattern = &quot;alpha_&quot;) %&gt;% str_remove(&quot;\\\\[[0-9]*\\\\]&quot;), idx = str_extract(param, &quot;[0-9]{1,2}&quot;) %&gt;% as.integer(), across(mean:`94.5%`,.fns = (function(x){ x * sd(data_wine$score) + mean(data_wine$score)}))) %&gt;% # left_join(wine_params, by = c(&quot;type&quot;, &quot;idx&quot;)) %&gt;% # group_by(type) %&gt;% # mutate(name = fct_reorder(name, mean), # name = str_c(str_pad(30 - as.numeric(name), width = 2, pad = 0),&quot;: &quot;,name)) %&gt;% # ungroup() %&gt;% ggplot(aes(y = factor(idx), color = type)) + # geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_pointinterval(aes(xmin = `5.5%`,x = mean, xmax =`94.5%`)) + scale_color_manual(values = c(j = clr0d, w = clr0d, f = clr0d, beta_wf = clr1, beta_jw = clr1, beta_jf = clr1), guide = &quot;none&quot;) + facet_grid(type ~ . , scales = &quot;free_y&quot;, switch = &quot;y&quot;) + labs(x = &quot;score&quot;) + scale_x_continuous(limits = c(7.5, 20)) + theme(axis.title.y = element_blank(), strip.placement = &quot;outside&quot;) p2 &lt;- data_wine %&gt;% ggplot(aes(x = score)) + geom_density(adjust = .6, color = clr0dd, fill = fll0) + scale_x_continuous(limits = c(7.5, 20)) p1 / p2 + plot_layout(heights = c(1,.2)) 9.5 {brms} section 9.5.1 Building an Interaction data_rugged_centered &lt;- data_rugged %&gt;% mutate(rugged_std_centered = rugged_std - mean(rugged_std), cont_idx = factor(cont_idx)) brms_c8_model_rugged_draft &lt;- brm( data = data_rugged_centered, family = gaussian, log_gdp_std ~ 1 + rugged_std_centered, prior = c(prior(normal(1, 1), class = Intercept), prior(normal(0, 1), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, sample_prior = TRUE, file = &quot;brms/brms_c8_model_rugged_draft&quot;) prior_rugged_draft &lt;- prior_draws(brms_c8_model_rugged_draft) %&gt;% as_tibble() set.seed(8) p1 &lt;- prior_rugged_draft %&gt;% slice_sample(n = 50) %&gt;% rownames_to_column() %&gt;% expand(nesting(rowname, Intercept, b), rugged_std_centered = c(-2, 2)) %&gt;% mutate(log_gdp_std = Intercept + b * rugged_std_centered, rugged_std = rugged_std_centered + mean(data_rugged_centered$rugged_std)) %&gt;% ggplot(aes(x = rugged_std, y = log_gdp_std, group = rowname)) + geom_hline(yintercept = range(data_rugged_centered$log_gdp_std), linetype = 3, color = clr_dark) + geom_line(color = clr_alpha(clr0d)) + geom_abline(intercept = 1.3, slope = -0.6, color = clr0dd) + labs(subtitle = glue(&quot;{mth(&#39;\\U03B1 \\U007E&#39;)} dnorm(1, 1)&lt;br&gt;{mth(&#39;\\U03B2 \\U007E&#39;)} dnorm(0, 1)&quot;), x = &quot;ruggedness&quot;, y = &quot;log GDP (prop of mean)&quot;) + coord_cartesian(xlim = c(0, 1), ylim = c(0.5, 1.5)) + theme(plot.subtitle = element_markdown()) prior_rugged_draft %&gt;% summarise(a = sum(abs(b) &gt; abs(-0.6)) / nrow(prior_rugged_draft)) #&gt; # A tibble: 1 × 1 #&gt; a #&gt; &lt;dbl&gt; #&gt; 1 0.552 brms_c8_model_rugged_restricted &lt;- brm( data = data_rugged_centered, family = gaussian, log_gdp_std ~ 1 + rugged_std_centered, prior = c(prior(normal(1, 0.1), class = Intercept), prior(normal(0, .3), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, sample_prior = TRUE, file = &quot;brms/brms_c8_model_rugged_restricted&quot;) prior_rugged_restricted &lt;- prior_draws(brms_c8_model_rugged_restricted) %&gt;% as_tibble() set.seed(8) p2 &lt;- prior_rugged_restricted %&gt;% slice_sample(n = 50) %&gt;% rownames_to_column() %&gt;% expand(nesting(rowname, Intercept, b), rugged_std_centered = c(-2, 2)) %&gt;% mutate(log_gdp_std = Intercept + b * rugged_std_centered, rugged_std = rugged_std_centered + mean(data_rugged_centered$rugged_std)) %&gt;% ggplot(aes(x = rugged_std, y = log_gdp_std, group = rowname)) + geom_hline(yintercept = range(data_rugged_centered$log_gdp_std), linetype = 3, color = clr_dark) + geom_line(color = clr_alpha(clr0d)) + labs(subtitle = glue(&quot;{mth(&#39;\\U03B1 \\U007E&#39;)} dnorm(1, 0.1)&lt;br&gt;{mth(&#39;\\U03B2 \\U007E&#39;)} dnorm(0, 0.3)&quot;), x = &quot;ruggedness&quot;, y = &quot;log GDP (prop of mean)&quot;) + coord_cartesian(xlim = c(0, 1), ylim = c(0.5, 1.5)) + theme(plot.subtitle = element_markdown()) prior_rugged_restricted %&gt;% summarise(a = sum(abs(b) &gt; abs(-0.6)) / nrow(prior_rugged_draft)) #&gt; # A tibble: 1 × 1 #&gt; a #&gt; &lt;dbl&gt; #&gt; 1 0.0382 p1 + p2 mixedup::summarize_model(brms_c8_model_rugged_restricted) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 0.02 0.14 0.12 0.15 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; Intercept 1.00 0.01 0.98 1.02 #&gt; rugged_std_centered 0.00 0.06 -0.11 0.12 9.5.1.1 Adding an indicator variable is not enough brms_c8_model_rugged_index &lt;- brm( data = data_rugged_centered, family = gaussian, log_gdp_std ~ 0 + cont_idx + rugged_std_centered, prior = c(prior(normal(1, 0.1), class = b, coef = cont_idx1), prior(normal(1, 0.1), class = b, coef = cont_idx2), prior(normal(0, 0.3), class = b, coef = rugged_std_centered), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c8_model_rugged_index&quot;) brms_c8_model_rugged_restricted &lt;- add_criterion(brms_c8_model_rugged_restricted, &quot;waic&quot;) brms_c8_model_rugged_index &lt;- add_criterion(brms_c8_model_rugged_index, &quot;waic&quot;) loo_compare(brms_c8_model_rugged_restricted, brms_c8_model_rugged_index, criterion = &quot;waic&quot;) %&gt;% print(simplify = FALSE) %&gt;% as.data.frame() %&gt;% knit_precis(param_name = &quot;model&quot;) #&gt; elpd_diff se_diff elpd_waic se_elpd_waic p_waic #&gt; brms_c8_model_rugged_index 0.0 0.0 126.2 7.4 4.1 #&gt; brms_c8_model_rugged_restricted -31.8 7.3 94.4 6.5 2.6 #&gt; se_p_waic waic se_waic #&gt; brms_c8_model_rugged_index 0.8 -252.4 14.8 #&gt; brms_c8_model_rugged_restricted 0.3 -188.8 13.0 model elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic brms_c8_model_rugged_index 0.00 0.00 126.21 7.41 4.08 0.80 -252.42 14.81 brms_c8_model_rugged_restricted -31.81 7.32 94.40 6.48 2.61 0.29 -188.80 12.95 model_weights(brms_c8_model_rugged_restricted, brms_c8_model_rugged_index, weights = &quot;waic&quot;) %&gt;% round(digits = 3) #&gt; brms_c8_model_rugged_restricted brms_c8_model_rugged_index #&gt; 0 1 mixedup::summarize_model(brms_c8_model_rugged_index) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 0.01 0.11 0.10 0.13 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; cont_idx1 0.88 0.02 0.85 0.91 #&gt; cont_idx2 1.05 0.01 1.03 1.07 #&gt; rugged_std_centered -0.05 0.05 -0.14 0.04 posterior_rugged_index &lt;- as_draws_df(brms_c8_model_rugged_index) %&gt;% as_tibble() %&gt;% mutate(diff = b_cont_idx1 - b_cont_idx2) library(tidybayes) qi(posterior_rugged_index$diff, .width = .89) %&gt;% as_tibble() #&gt; # A tibble: 1 × 2 #&gt; V1 V2 #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 -0.200 -0.138 new_data &lt;- crossing(cont_idx = 1:2, rugged_std = seq(from = -0.2, to = 1.2, length.out = 30)) %&gt;% mutate(rugged_std_centered = rugged_std - mean(data_rugged_centered$rugged_std)) fitted_rugged_index &lt;- fitted( brms_c8_model_rugged_index, newdata = new_data, probs = c(.015, .985)) %&gt;% as_tibble() %&gt;% bind_cols(new_data) %&gt;% mutate(cont_africa = c(&quot;Africa&quot;, &quot;not Africa&quot;)[cont_idx]) p1 &lt;- data_rugged_centered %&gt;% mutate(cont_africa = c(&quot;Africa&quot;, &quot;not Africa&quot;)[cont_idx]) %&gt;% ggplot(aes(x = rugged_std, fill = cont_africa, color = cont_africa)) + geom_smooth(data = fitted_rugged_index, aes(y = Estimate, ymin = Q1.5, ymax = Q98.5), stat = &quot;identity&quot;, alpha = .2, size = .3) + geom_point(aes(y = log_gdp_std), size = 1) + scale_fill_manual(values = c(Africa = clr0dd, `not Africa` = clr0d)) + scale_colour_manual(values = c(Africa = clr0dd, `not Africa` = clr0d)) + labs(subtitle = &quot;brms_c8_model_rugged_index&quot;, x = &quot;ruggedness (standardized)&quot;, y = &quot;log GDP (as proportion of mean)&quot;) + coord_cartesian(xlim = c(0, 1)) + theme(legend.background = element_blank(), legend.position = &quot;bottom&quot;, legend.title = element_blank()) Using stat_lineribbon() since ‘boundaries are meaningless’: p2 &lt;- fitted(brms_c8_model_rugged_index, newdata = new_data, summary = FALSE) %&gt;% as_tibble() %&gt;% pivot_longer(everything()) %&gt;% bind_cols( expand(new_data, iter = 1:4000, nesting(cont_idx, rugged_std)) ) %&gt;% mutate(cont_africa = c(&quot;Africa&quot;, &quot;not Africa&quot;)[cont_idx]) %&gt;% ggplot(aes(x = rugged_std, y = value, fill = cont_africa, color = cont_africa)) + stat_lineribbon(.width = seq(from = .03, to = .99, by = .03), alpha = .1, size = 0) + geom_point(data = data_rugged_centered %&gt;% mutate(cont_africa = c(&quot;Africa&quot;, &quot;not Africa&quot;)[cont_idx]), aes(y = log_gdp_std), size = 1) + scale_fill_manual(values = c(Africa = clr0dd, `not Africa` = clr0d), guide = &quot;none&quot;) + scale_colour_manual(values = c(Africa = clr0dd, `not Africa` = clr0d), guide = &quot;none&quot;) + labs(subtitle = &quot;brms_c8_model_rugged_index&quot;, x = &quot;ruggedness (standardized)&quot;, y = &quot;log GDP (as proportion of mean)&quot;) + coord_cartesian(xlim = c(0, 1)) + theme(legend.background = element_blank(), legend.position = &quot;bottom&quot;, legend.title = element_blank()) p1 + p2 + plot_layout(guides = &quot;collect&quot;) &amp; theme(legend.position = &quot;bottom&quot;) 9.5.1.2 Adding an interaction does work Remeber the non-linear syntax for {brms} to use with index variables. brms_c8_model_rugged_slope &lt;- brm( data = data_rugged_centered, family = gaussian, bf(log_gdp_std ~ 0 + a + b * rugged_std_centered, a ~ 0 + cont_idx, b ~ 0 + cont_idx, nl = TRUE), prior = c(prior(normal(1, 0.1), class = b, coef = cont_idx1, nlpar = a), prior(normal(1, 0.1), class = b, coef = cont_idx2, nlpar = a), prior(normal(0, 0.3), class = b, coef = cont_idx1, nlpar = b), prior(normal(0, 0.3), class = b, coef = cont_idx2, nlpar = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c8_model_rugged_slope&quot;) mixedup::summarise_model(brms_c8_model_rugged_slope) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 0.01 0.11 0.10 0.12 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; a_cont_idx1 0.89 0.02 0.86 0.92 #&gt; a_cont_idx2 1.05 0.01 1.03 1.07 #&gt; b_cont_idx1 0.13 0.07 -0.01 0.28 #&gt; b_cont_idx2 -0.14 0.06 -0.25 -0.03 brms_c8_model_rugged_restricted &lt;- add_criterion(brms_c8_model_rugged_restricted, &quot;loo&quot;) brms_c8_model_rugged_index &lt;- add_criterion(brms_c8_model_rugged_index, &quot;loo&quot;) brms_c8_model_rugged_slope &lt;- add_criterion(brms_c8_model_rugged_slope, c(&quot;loo&quot;, &quot;waic&quot;)) loo_compare(brms_c8_model_rugged_restricted, brms_c8_model_rugged_index, brms_c8_model_rugged_slope, criterion = &quot;loo&quot;) %&gt;% print(simplify = FALSE) #&gt; elpd_diff se_diff elpd_loo se_elpd_loo p_loo #&gt; brms_c8_model_rugged_slope 0.0 0.0 129.7 7.3 4.9 #&gt; brms_c8_model_rugged_index -3.5 3.2 126.2 7.4 4.1 #&gt; brms_c8_model_rugged_restricted -35.3 7.5 94.4 6.5 2.6 #&gt; se_p_loo looic se_looic #&gt; brms_c8_model_rugged_slope 0.9 -259.3 14.7 #&gt; brms_c8_model_rugged_index 0.8 -252.4 14.8 #&gt; brms_c8_model_rugged_restricted 0.3 -188.8 13.0 model_weights(brms_c8_model_rugged_restricted, brms_c8_model_rugged_index, brms_c8_model_rugged_slope, weights = &quot;loo&quot;) %&gt;% round(digits = 2) #&gt; brms_c8_model_rugged_restricted brms_c8_model_rugged_index #&gt; 0.00 0.03 #&gt; brms_c8_model_rugged_slope #&gt; 0.97 tibble(k = brms_c8_model_rugged_slope$criteria$loo$diagnostics$pareto_k, row = 1:170) %&gt;% arrange(desc(k)) #&gt; # A tibble: 170 × 2 #&gt; k row #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 0.512 145 #&gt; 2 0.494 93 #&gt; 3 0.262 35 #&gt; 4 0.240 133 #&gt; 5 0.240 36 #&gt; 6 0.195 27 #&gt; 7 0.182 144 #&gt; 8 0.149 67 #&gt; 9 0.143 99 #&gt; 10 0.139 107 #&gt; # … with 160 more rows Robust model variant using a student-t distribution as prior brms_c8_model_rugged_student &lt;- brm( data = data_rugged_centered, family = student, bf(log_gdp_std ~ 0 + a + b * rugged_std_centered, a ~ 0 + cont_idx, b ~ 0 + cont_idx, nl = TRUE), prior = c(prior(normal(1, 0.1), class = b, coef = cont_idx1, nlpar = a), prior(normal(1, 0.1), class = b, coef = cont_idx2, nlpar = a), prior(normal(0, 0.3), class = b, coef = cont_idx1, nlpar = b), prior(normal(0, 0.3), class = b, coef = cont_idx2, nlpar = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c8_model_rugged_student&quot;) brms_c8_model_rugged_student &lt;- add_criterion(brms_c8_model_rugged_student, c(&quot;loo&quot;, &quot;waic&quot;)) loo_compare(brms_c8_model_rugged_slope, brms_c8_model_rugged_student, criterion = &quot;loo&quot;) %&gt;% print(simplify = FALSE) #&gt; elpd_diff se_diff elpd_loo se_elpd_loo p_loo #&gt; brms_c8_model_rugged_slope 0.0 0.0 129.7 7.3 4.9 #&gt; brms_c8_model_rugged_student -1.3 0.3 128.4 7.5 5.0 #&gt; se_p_loo looic se_looic #&gt; brms_c8_model_rugged_slope 0.9 -259.3 14.7 #&gt; brms_c8_model_rugged_student 0.8 -256.7 14.9 tibble(Normal = brms_c8_model_rugged_slope$criteria$loo$diagnostics$pareto_k, `Student-t` = brms_c8_model_rugged_student$criteria$loo$diagnostics$pareto_k) %&gt;% pivot_longer(everything(), values_to = &quot;pareto_k&quot;) %&gt;% ggplot(aes(x = pareto_k, y = name)) + geom_vline(xintercept = .5, linetype = 3, color = clr_dark) + stat_dots(slab_fill = clr0, slab_color = clr0d) + annotate(geom = &quot;text&quot;, x = .485, y = 1.5, label = &quot;threshold&quot;, angle = 90, family = fnt_sel, color = clr_dark) + ylab(NULL) + coord_cartesian(ylim = c(1.5, 2.4)) fixef(brms_c8_model_rugged_slope) %&gt;% round(digits = 2) %&gt;% data.frame() %&gt;% knit_precis() param Estimate Est.Error Q2.5 Q97.5 a_cont_idx1 0.89 0.02 0.86 0.92 a_cont_idx2 1.05 0.01 1.03 1.07 b_cont_idx1 0.13 0.07 -0.01 0.28 b_cont_idx2 -0.14 0.06 -0.25 -0.03 fixef(brms_c8_model_rugged_student) %&gt;% round(digits = 2) %&gt;% data.frame() %&gt;% knit_precis() param Estimate Est.Error Q2.5 Q97.5 a_cont_idx1 0.88 0.02 0.85 0.92 a_cont_idx2 1.05 0.01 1.03 1.07 b_cont_idx1 0.13 0.08 -0.02 0.29 b_cont_idx2 -0.15 0.06 -0.26 -0.03 9.5.2 Plotting the Interaction countries &lt;- c(&quot;Equatorial Guinea&quot;, &quot;South Africa&quot;, &quot;Seychelles&quot;, &quot;Swaziland&quot;, &quot;Lesotho&quot;, &quot;Rwanda&quot;, &quot;Burundi&quot;, &quot;Luxembourg&quot;, &quot;Greece&quot;, &quot;Switzerland&quot;, &quot;Lebanon&quot;, &quot;Yemen&quot;, &quot;Tajikistan&quot;, &quot;Nepal&quot;) fitted_rugged_slope &lt;- fitted(brms_c8_model_rugged_slope, newdata = new_data, probs = c(.015, .985)) %&gt;% data.frame() %&gt;% bind_cols(new_data) %&gt;% mutate(cont_africa = c(&quot;Africa&quot;, &quot;not Africa&quot;)[cont_idx]) data_rugged_centered %&gt;% mutate(cont_africa = c(&quot;Africa&quot;, &quot;not Africa&quot;)[cont_idx]) %&gt;% ggplot(aes(x = rugged_std, y = log_gdp_std, fill = cont_africa, color = cont_africa)) + geom_smooth(data = fitted_rugged_slope, aes(y = Estimate, ymin = Q1.5, ymax = Q98.5), stat = &quot;identity&quot;, alpha = .2, size = .4) + ggrepel::geom_text_repel(data = . %&gt;% filter(country %in% countries), aes(label = country), size = 3, seed = 8, segment.color = clr_dark, min.segment.length = 0) + geom_point(aes(y = log_gdp_std), size = 1) + scale_fill_manual(values = c(Africa = clr0dd, `not Africa` = clr0d)) + scale_colour_manual(values = c(Africa = clr0dd, `not Africa` = clr0d)) + labs(x = &quot;ruggedness (standardized)&quot;, y = &quot;log GDP (as proportion of mean)&quot;) + facet_wrap(. ~ cont_africa) + coord_cartesian(xlim = c(0, 1)) + theme(legend.position = &quot;none&quot;) 9.5.3 Symetry of Interactions fitted(brms_c8_model_rugged_slope, newdata = new_data, summary = FALSE) %&gt;% data.frame() %&gt;% pivot_longer(everything()) %&gt;% bind_cols(expand(new_data, iter = 1:4000, nesting(cont_idx, rugged_std))) %&gt;% select(-name) %&gt;% pivot_wider(names_from = cont_idx, values_from = value) %&gt;% mutate(delta = `1` - `2`) %&gt;% ggplot(aes(x = rugged_std, y = delta)) + stat_lineribbon(.width = .95, fill = fll0, color = clr0d, size = .3) + geom_hline(yintercept = 0, linetype = 3, color = clr_dark) + annotate(geom = &quot;text&quot;, x = .2, y = 0, label = &quot;Africa higher GDP\\nAfrica lower GDP&quot;, family = fnt_sel) + labs(x = &quot;ruggedness (standardized)&quot;, y = &quot;expected difference log GDP&quot;) + coord_cartesian(xlim = c(0, 1), ylim = c(-0.3, 0.2)) 9.5.4 Continuous Interactions Tulip model without interaction brms_c8_model_tulips_simple &lt;- brm( data = data_tulips, family = gaussian, blooms_std ~ 1 + water_cent + shade_cent, prior = c(prior(normal(0.5, 0.25), class = Intercept), prior(normal(0, 0.25), class = b, coef = water_cent), prior(normal(0, 0.25), class = b, coef = shade_cent), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c8_model_tulips_simple&quot;) mixedup::summarize_model(brms_c8_model_tulips_simple) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 0.03 0.18 0.13 0.24 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; Intercept 0.36 0.03 0.29 0.43 #&gt; water_cent 0.20 0.04 0.12 0.28 #&gt; shade_cent -0.11 0.04 -0.19 -0.03 Tulip model with interaction \\[\\begin{align*} \\mu_{i} &amp; = \\alpha + \\color{#B35136}{\\gamma_{W, i}} \\text{W}_{i} + \\beta_{S} \\text{S}_i \\\\ \\color{#B35136}{\\gamma_{W, i}} &amp; = \\color{#B35136}{\\beta_{W} + \\beta_{WS} \\text{S}_{i}}, \\end{align*}\\] \\[\\begin{align*} \\mu_{i} &amp; = \\alpha + \\color{#B35136}{\\underbrace{(\\beta_{W} + \\beta_{WS} \\text{S}_i)}_{\\gamma_{W, i}}} \\text{W}_i + \\beta_{S} \\text{S}_{i} \\\\ &amp; = \\alpha + \\color{#B35136}{\\beta_{W}} \\text{W}_{i} + (\\color{#B35136}{\\beta_{WS} \\text{S}_i} \\cdot \\text{W}_{i}) + \\beta_{S} \\text{S}_{i} \\\\ &amp; = \\alpha + \\color{#B35136}{\\beta_{W}} \\text{W}_{i} + \\beta_{S} \\text{S}_{i} + \\color{#B35136}{\\beta_{WS}} (\\color{#B35136}{\\text{S}_{i}} \\cdot \\text{W}_{i}), \\end{align*}\\] brms_c8_model_tulips_interaction &lt;- brm( data = data_tulips, family = gaussian, blooms_std ~ 1 + water_cent + shade_cent + water_cent:shade_cent, prior = c(prior(normal(0.5, 0.25), class = Intercept), prior(normal(0, 0.25), class = b, coef = water_cent), prior(normal(0, 0.25), class = b, coef = shade_cent), prior(normal(0, 0.25), class = b, coef = &quot;water_cent:shade_cent&quot;), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c8_model_tulips_interaction&quot;) mixedup::summarize_model(brms_c8_model_tulips_interaction) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 0.02 0.14 0.11 0.20 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; Intercept 0.36 0.03 0.31 0.42 #&gt; water_cent 0.21 0.03 0.14 0.27 #&gt; shade_cent -0.11 0.03 -0.18 -0.04 #&gt; water_cent:shade_cent -0.14 0.04 -0.22 -0.06 9.5.5 Plotting the Posterior Predictions set.seed(42) new_data &lt;- crossing(shade_cent = -1:1, water_cent = c(-1, 1)) rbind(fitted(brms_c8_model_tulips_simple, newdata = new_data, summary = FALSE, ndraws = 20), fitted(brms_c8_model_tulips_interaction, newdata = new_data, summary = FALSE, ndraws = 20)) %&gt;% as_tibble() %&gt;% set_names(mutate(new_data, name = str_c(shade_cent, water_cent, sep = &quot;_&quot;)) %&gt;% pull()) %&gt;% mutate(row = 1:n(), model = rep(c(&quot;simple&quot;, &quot;interaction&quot;), each = n() / 2) %&gt;% factor(., levels = c(&quot;simple&quot;, &quot;interaction&quot;))) %&gt;% pivot_longer(-c(row:model), values_to = &quot;blooms_std&quot;) %&gt;% separate(name, into = c(&quot;shade_cent&quot;, &quot;water_cent&quot;), sep = &quot;_&quot;, convert = TRUE) %&gt;% ggplot(aes(x = water_cent, y = blooms_std)) + geom_point(data = data_tulips, color = clr_dark) + geom_line(aes(group = row, color = row %in% c(20,40)), size = .3) + facet_grid(model ~ shade_cent, labeller = label_both) + scale_color_manual(values = c(`FALSE` = clr_alpha(clr0d), `TRUE` = clr_alpha(clr_dark)), guide = &quot;none&quot;) + coord_cartesian(xlim = c(-1, 1), ylim = c(0, 1)) 9.5.6 Plotting Prior Predictions brms_c8_model_tulips_simple_prior &lt;- update( brms_c8_model_tulips_simple, sample_prior = &quot;only&quot;, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c8_model_tulips_simple_prior&quot;) brms_c8_model_tulips_interaction_prior &lt;- update( brms_c8_model_tulips_interaction, sample_prior = &quot;only&quot;, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c8_model_tulips_interaction_prior&quot;) set.seed(42) rbind(fitted(brms_c8_model_tulips_simple_prior, newdata = new_data, summary = FALSE, ndraws = 20), fitted(brms_c8_model_tulips_interaction_prior, newdata = new_data, summary = FALSE, ndraws = 20)) %&gt;% as_tibble() %&gt;% set_names(mutate(new_data, name = str_c(shade_cent, water_cent, sep = &quot;_&quot;)) %&gt;% pull()) %&gt;% mutate(row = rep(1:20, times = 2), model = rep(c(&quot;simple&quot;, &quot;interaction&quot;), each = n() / 2) %&gt;% factor(., levels = c(&quot;simple&quot;, &quot;interaction&quot;))) %&gt;% pivot_longer(-c(row:model), values_to = &quot;blooms_std&quot;) %&gt;% separate(name, into = c(&quot;shade_cent&quot;, &quot;water_cent&quot;), sep = &quot;_&quot;,convert = TRUE) %&gt;% ggplot(aes(x = water_cent, y = blooms_std, group = row)) + geom_hline(yintercept = 0:1, linetype = 3, color = clr_dark) + geom_line(aes(group = row, color = row %in% c(20,40)), size = .3) + facet_grid(model ~ shade_cent, labeller = label_both) + scale_color_manual(values = c(`FALSE` = clr_alpha(clr0d), `TRUE` = clr_alpha(clr_dark)), guide = &quot;none&quot;) + coord_cartesian(xlim = c(-1, 1), ylim = c(-0.5, 1.5), expand = 0) 9.5.7 {brms} conditional_effects() specifically for simple two-way interactions Simple univariate model: brms_c8_model_rugged_restricted$formula #&gt; log_gdp_std ~ 1 + rugged_std_centered c_eff &lt;- conditional_effects(brms_c8_model_rugged_restricted, plot = FALSE) p1 &lt;- plot(c_eff, line_args = list(color = clr0dd, size = .4), plot = FALSE)[[1]] p2 &lt;- plot(c_eff, line_args = list(color = clr0dd, size = .4), point_args = c(color = clr_dark), points = TRUE, plot = FALSE)[[1]] c_eff &lt;- conditional_effects(brms_c8_model_rugged_restricted, spaghetti = TRUE, ndraws = 200, plot = FALSE) p3 &lt;- plot(c_eff, points = TRUE, point_args = c(color = clr_dark), line_args = c(colour = &quot;black&quot;), spaghetti_args = c(colour = clr_alpha(clr0dd, .1)), plot = FALSE)[[1]] p1 + p2 + p3 Simple bi-variate model, no interaction: brms_c8_model_rugged_index$formula #&gt; log_gdp_std ~ 0 + cont_idx + rugged_std_centered c_eff &lt;- conditional_effects(brms_c8_model_rugged_index, plot = FALSE) p &lt;- plot(c_eff, points = TRUE, point_args = c(color = clr_dark), line_args = c(colour = &quot;black&quot;), plot = FALSE) p[[1]] + p[[2]] Non-linear interaction model: brms_c8_model_rugged_slope$formula #&gt; log_gdp_std ~ 0 + a + b * rugged_std_centered #&gt; a ~ 0 + cont_idx #&gt; b ~ 0 + cont_idx c_eff &lt;- conditional_effects(brms_c8_model_rugged_slope, plot = FALSE) p &lt;- plot(c_eff, points = TRUE, point_args = c(color = clr_dark), line_args = c(colour = &quot;black&quot;), plot = FALSE) p[[1]] + p[[2]] c_eff &lt;- conditional_effects(brms_c8_model_rugged_slope, effects = &quot;rugged_std_centered:cont_idx&quot;, plot = FALSE) p &lt;- plot(c_eff, plot = FALSE) p[[1]] + scale_color_manual(values = c(`1` = clr1, `2` = clr0dd)) + scale_fill_manual(values = c(`1` = fll1, `2` = clr_alpha(clr0dd))) + theme(legend.position = &quot;bottom&quot;) c_eff &lt;- conditional_effects(brms_c8_model_rugged_slope, effects = &quot;cont_idx:rugged_std_centered&quot;, plot = FALSE) plot(c_eff, plot = FALSE)[[1]] + scale_color_manual(values = c(clr1, clr0d, clr_dark)) + scale_fill_manual(values = c(clr1, clr0d, clr_dark) %&gt;% clr_lighten()) + theme(legend.position = &quot;bottom&quot;) brms_c8_model_rugged_slope$data %&gt;% summarize(mean = mean(rugged_std_centered), `mean + 1 sd` = mean(rugged_std_centered) + sd(rugged_std_centered), `mean - 1 sd` = mean(rugged_std_centered) - sd(rugged_std_centered)) %&gt;% mutate_all(round, digits = 2) #&gt; mean mean + 1 sd mean - 1 sd #&gt; 1 0 0.19 -0.19 Interaction models with two variables brms_c8_model_tulips_interaction$formula #&gt; blooms_std ~ 1 + water_cent + shade_cent + water_cent:shade_cent c_eff &lt;- conditional_effects(brms_c8_model_tulips_interaction, effects = &quot;water_cent:shade_cent&quot;, int_conditions = list(shade_cent = -1:1), plot = FALSE) plot(c_eff, points = TRUE, plot = FALSE)[[1]] + scale_fill_manual(values = c(clr3, clr0d, clr_dark), guide = &quot;none&quot;) + scale_colour_manual(values = c(clr3, clr0d, clr_dark), guide = &quot;none&quot;) + scale_x_continuous(breaks = -1:1) + facet_wrap(~ shade_cent, labeller = label_both) 9.6 pymc3 section × "],["rethinking-chapter-9.html", "10 Rethinking: Chapter 9 10.1 The island Kingdom 10.2 Metropolis algotithms 10.3 Hamiltonian Monte Carlo 10.4 Easy HMC: rethinking::ulam() 10.5 Care and feeding of your Markov Chain 10.6 Homework 10.7 {brms} section 10.8 pymc3 section", " 10 Rethinking: Chapter 9 Markov Chain Monte Carlo by Richard McElreath, building on the Summaries by Solomon Kurz and Jake Thompson. 10.1 The island Kingdom set.seed(42) n_weeks &lt;- 1e5 positions &lt;- rep(0, n_weeks) current &lt;- 10 for( i in seq_along(positions)){ # record the current position positions[i] &lt;- current # flip the coin to get the proposal proposal &lt;- current + sample(c(-1, 1), size = 1) # connecting the edges to make the archipelago circular if(proposal == 0){proposal &lt;- 10} if(proposal == 11){proposal &lt;- 1} # decide whether to move prob_move &lt;- proposal/current current &lt;- ifelse(runif(1) &lt; prob_move, proposal, current) } data_markov &lt;- tibble( week = seq_along(positions), position = positions) p1 &lt;- data_markov %&gt;% filter(week &lt; 101) %&gt;% ggplot(aes(x = week, y = position)) + geom_point(shape = 21, color = clr0dd, fill = fll0) p2 &lt;- data_markov %&gt;% ggplot(aes(x = position)) + geom_histogram(breaks = 0:10, color = clr0dd, fill = fll0) + scale_x_continuous(breaks = .5 + 0:9, labels = 1:10) + theme(panel.grid.major.x = element_blank()) p1 + p2 10.2 Metropolis algotithms bivariate distribution “with a strong negative correlation of -0.9” \\[\\begin{align*} \\begin{bmatrix} \\text a_1 \\\\ \\text a_2 \\end{bmatrix} &amp; \\sim \\operatorname{MVNormal} \\left (\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf \\Sigma \\right) \\\\ \\mathbf \\Sigma &amp; = \\mathbf{SRS} \\\\ \\mathbf S &amp; = \\begin{bmatrix} 0.22 &amp; 0 \\\\ 0 &amp; 0.22 \\end{bmatrix} \\\\ \\mathbf R &amp; = \\begin{bmatrix} 1 &amp; -.9 \\\\ -.9 &amp; 1 \\end{bmatrix}, \\end{align*}\\] # mean vector mu &lt;- c(0, 0) # variance/covariance matrix sd_a1 &lt;- 0.22 sd_a2 &lt;- 0.22 rho &lt;- -.9 Sigma &lt;- matrix(data = c(sd_a1^2, rho * sd_a1 * sd_a2, rho * sd_a1 * sd_a2, sd_a2^2), nrow = 2) # sample from the distribution with the `mvtnorm::rmvnorm()` function set.seed(9) my_samples &lt;- mvtnorm::rmvnorm(n = 1e3, mean = mu, sigma = Sigma) # just for demo - not actually used data.frame(a = my_samples) %&gt;% as_tibble() %&gt;% set_names(str_c(&quot;a&quot;, 1:2)) %&gt;% summarise(rho = cor(a1, a2)) #&gt; # A tibble: 1 × 1 #&gt; rho #&gt; &lt;dbl&gt; #&gt; 1 -0.911 # define the function x_y_grid &lt;- function(x_start = -1.6, x_stop = 1.6, x_length = 101, y_start = -1.6, y_stop = 1.6, y_length = 101) { x_domain &lt;- seq(from = x_start, to = x_stop, length.out = x_length) y_domain &lt;- seq(from = y_start, to = y_stop, length.out = y_length) x_y_grid_tibble &lt;- tidyr::expand_grid(a1 = x_domain, a2 = y_domain) return(x_y_grid_tibble) } # simulate contour_plot_dat &lt;- x_y_grid() %&gt;% mutate(d = mvtnorm::dmvnorm(as.matrix(.), mean = mu, sigma = Sigma)) p0 &lt;- contour_plot_dat %&gt;% ggplot() + geom_contour(aes(x = a1, y = a2, z = d), size = 1/8, color = clr_dark, breaks = 9^(-(10 * 1:25))) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + scale_shape_manual(values = c(21, 19)) + scale_color_manual(values = c(`FALSE` = clr0, `TRUE` = clr2)) + theme(panel.border = element_rect(color = clr_dark, fill = &quot;transparent&quot;)) metropolis &lt;- function(num_proposals = 50, step_size = 0.1, starting_point = c(-1, 1), seed = 42) { set.seed(seed) # Initialize vectors where we will keep track of relevant candidate_x_history &lt;- rep(-Inf, num_proposals) candidate_y_history &lt;- rep(-Inf, num_proposals) did_move_history &lt;- rep(FALSE, num_proposals) # Prepare to begin the algorithm... current_point &lt;- starting_point for(i in 1:num_proposals) { # &quot;Proposals are generated by adding random Gaussian noise # to each parameter&quot; noise &lt;- rnorm(n = 2, mean = 0, sd = step_size) candidate_point &lt;- current_point + noise # store coordinates of the proposal point candidate_x_history[i] &lt;- candidate_point[1] candidate_y_history[i] &lt;- candidate_point[2] # evaluate the density of our posterior at the proposal point candidate_prob &lt;- mvtnorm::dmvnorm(candidate_point, mean = mu, sigma = Sigma) # evaluate the density of our posterior at the current point current_prob &lt;- mvtnorm::dmvnorm(current_point, mean = mu, sigma = Sigma) # Decide whether or not we should move to the candidate point acceptance_ratio &lt;- candidate_prob / current_prob should_move &lt;- ifelse(runif(n = 1) &lt; acceptance_ratio, TRUE, FALSE) # Keep track of the decision did_move_history[i] &lt;- should_move # Move if necessary if(should_move) { current_point &lt;- candidate_point } } # once the loop is complete, store the relevant results in a tibble results &lt;- tibble::tibble( candidate_x = candidate_x_history, candidate_y = candidate_y_history, accept = did_move_history ) # compute the &quot;acceptance rate&quot; by dividing the total number of &quot;moves&quot; # by the total number of proposals number_of_moves &lt;- results %&gt;% dplyr::pull(accept) %&gt;% sum(.) acceptance_rate &lt;- number_of_moves/num_proposals return(list(results = results, acceptance_rate = acceptance_rate)) } round_1 &lt;- metropolis(num_proposals = 50, step_size = 0.1, starting_point = c(-1,1)) round_2 &lt;- metropolis(num_proposals = 50, step_size = 0.25, starting_point = c(-1,1)) p1 &lt;- p0 + geom_point(data = round_1$results, mapping = aes(x = candidate_x, y = candidate_y, color = accept, fill = after_scale(clr_alpha(color,.2))), shape = 21) + labs(subtitle = str_c(&quot;step size 0.1,\\naccept rate &quot;, round_1$acceptance_rate), x = &quot;a1&quot;) p2 &lt;- p0 + geom_point(data = round_2$results, mapping = aes(x = candidate_x, y = candidate_y, color = accept, fill = after_scale(clr_alpha(color,.2))), shape = 21) + scale_y_continuous(NULL, breaks = NULL, expand = c(0, 0)) + labs(subtitle = str_c(&quot;step size 0.25,\\naccept rate &quot;, round_2$acceptance_rate), x = &quot;a1&quot;) p1 + p2 + plot_layout(guides = &quot;collect&quot;) concentration_sim &lt;- function(dimensions = 1, t = 1e3, seed = 42) { set.seed(seed) y &lt;- rethinking::rmvnorm(t, rep(0, dimensions), diag(dimensions)) rad_dist &lt;- function(y) sqrt(sum(y^2)) rd &lt;- sapply(1:t, function(i) rad_dist( y[i, ])) } data_concentration &lt;- tibble(dimensions = c(1, 10, 100, 1000)) %&gt;% mutate(con = map(dimensions, concentration_sim)) %&gt;% unnest(con) %&gt;% mutate(`# dimensions` = factor(dimensions)) data_concentration %&gt;% ggplot(aes(x = con, fill = `# dimensions`)) + geom_density(size = .4, adjust = .6, aes(color = after_scale(clr_alpha(fill, 1)))) + scale_fill_manual(values = scales::colour_ramp(colors = c(clr0d, clr2) %&gt;% clr_alpha())((0:3)/3)) + xlab(&quot;Radial distance from mode&quot;) + theme(legend.position = &quot;bottom&quot;) 10.3 Hamiltonian Monte Carlo U &lt;- function( q, a = 0, b = 1, k = 0, d = 1){ muy &lt;- q[1] mux &lt;- q[2] U &lt;- sum( dnorm(y , muy, 1, log = TRUE)) + sum( dnorm(x , mux, 1, log = TRUE)) + dnorm(muy, a, b, log = TRUE) + dnorm(mux, k, d, log = TRUE) return(-U) } U_gradient &lt;- function( q , a = 0, b = 1, k = 0, d = 1){ muy &lt;- q[1] mux &lt;- q[2] G1 &lt;- sum( y - muy ) + (a - muy) / b ^ 2 # dU/dmuy G2 &lt;- sum( x - mux ) + (a - mux) / b ^ 2 # dU/dmux return(c(-G1, -G2)) # negative because energy is neg-log-prob } U1d &lt;- function( q, a = 0, b = 1, k = 0, d = 1){ muy &lt;- q[1] U &lt;- sum( dnorm(y , muy, 1, log = TRUE)) + dnorm(muy, a, b, log = TRUE) return(-U) } U1d_gradient &lt;- function( q , a = 0, b = 1, k = 0, d = 1){ muy &lt;- q[1] G1 &lt;- sum( y - muy ) + (a - muy) / b ^ 2 # dU/dmuy return(-G1) # negative because energy is neg-log-prob } library(shape) library(rethinking) library(ggforce) library(ggnewscale) import_trajs &lt;- function(Q, idx,names = c(&quot;x&quot;, &quot;y&quot;)){ traj &lt;- Q$traj %&gt;% as_tibble() %&gt;% set_names(nm = names) %&gt;% mutate(leapfrog_step = row_number()) ptraj &lt;- Q$ptraj %&gt;% as_tibble() %&gt;% set_names(nm = str_c(names, &quot;_p&quot;)) tibble(sample = idx, traj = list(bind_cols(traj, ptraj))) } contour_plot_dat_gaus &lt;- x_y_grid(x_start = -.35,x_stop = .35, y_start = -.35, y_stop = .35) %&gt;% mutate(d = mvtnorm::dmvnorm(as.matrix(.), mean = c(0,0), sigma = matrix(c(1,0,0,1),nrow = 2))) y &lt;- rnorm(50) y &lt;- as.numeric(scale(y)) set.seed(42) S &lt;- list() Q &lt;- list() Q$q &lt;- .15 pr &lt;- .3 step &lt;- .03 n_samples &lt;- 20 L &lt;- 11 for(i in 1:n_samples){ Q &lt;- HMC2(U = U1d, grad_U = U1d_gradient, epsilon = step, L = L, current_q = Q$q) for(j in 1:L){ K0 &lt;- sum(Q$ptraj[j,] ^ 2)/2 # kinetic energy } S[[i]] &lt;- Q } sample_tib &lt;- map2_dfr(S, seq_along(S), import_trajs, names = c(&quot;y&quot;)) %&gt;% unnest(traj) %&gt;% mutate(k = (y_p ^ 2 ) /2 , t = 1 + cumsum(leapfrog_step != 1)) # determining the positions of the iso-lines dnorminv &lt;- function(y){ sqrt(-2 * log(sqrt(2 * pi) * y)) } brks_y &lt;- ((184:192) * .005) brks_y &lt;- ((46:55) * .02) brks_x0 &lt;- dnorminv(exp(-brks_y)) brks_x &lt;- rep(c(-1,1), each = length(brks_y)) * c(rev(brks_x0), brks_x0) sample_tib %&gt;% ggplot(aes(t, y)) + geom_hline(yintercept = brks_x, color = clr0) + geom_link2(aes(group = sample, size = k), color = clr_dag) + scale_size_continuous(range = c(.15,3.5), guide = &quot;none&quot;)+ new_scale(new_aes = &quot;size&quot;)+ geom_point(data = . %&gt;% filter(leapfrog_step == (L + 1)), size = 2.5, color = clr2, fill = clr_lighten(clr2), shape = 21) + geom_point(data = sample_tib %&gt;% filter(row_number() %in% c(1, nrow(sample_tib))), color = clr2, shape = 1, size = 4.5) + scale_y_continuous(limits = c(-.55,.55),breaks = c(-.46,.46), labels = c(&quot;South&quot;, &quot;North&quot;)) + scale_size_manual(values = c(`TRUE` = 2.5, `FALSE` = .5)) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0dd)) + coord_cartesian(ylim = c(-.5, .5), xlim = c(-1, 3 + n_samples*L), expand = 0) + labs(x = &quot;time&quot;, y = &quot;position&quot;)+ theme(panel.border = element_rect(color = clr_dark, fill = &quot;transparent&quot;), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), axis.text.y = element_text(angle = 90, hjust = .5)) monty_plot &lt;- function(L = 11, n_samples = 4, contour_plot_dat, contour_breaks, seed = 42, start_point = c(-.1,.2)){ set.seed(seed) S &lt;- list() Q &lt;- list() Q$q &lt;- start_point pr &lt;- .3 step &lt;- .03 for(i in 1:n_samples){ Q &lt;- HMC2(U = U, grad_U = U_gradient, epsilon = step, L = L, current_q = Q$q) for(j in 1:L){ K0 &lt;- sum(Q$ptraj[j,] ^ 2)/2 # kinetic energy } S[[i]] &lt;- Q } sample_tib &lt;- map2_dfr(S, seq_along(S), import_trajs) %&gt;% unnest(traj) %&gt;% mutate(k = (x_p ^ 2 + y_p ^ 2 ) /2 ) sample_tib %&gt;% ggplot(aes(x, y)) + geom_contour(data = contour_plot_dat, aes(x = a1, y = a2, z = d), size = .2, breaks = contour_breaks, color = clr_dark) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + geom_link2(aes(group = sample, size = k), color = clr_dag) + scale_size_continuous(range = c(.15,3.5), guide = &quot;none&quot;)+ new_scale(new_aes = &quot;size&quot;)+ geom_point(data = . %&gt;% filter(leapfrog_step &gt; 1), aes(size = leapfrog_step == (L + 1), color = leapfrog_step == (L + 1), fill = after_scale(clr_lighten(color,.3))), shape = 21) + geom_point(data = sample_tib %&gt;% filter(row_number() %in% c(1, nrow(sample_tib))), color = clr2, shape = 1, size = 4.5) + scale_size_manual(values = c(`TRUE` = 2.5, `FALSE` = .5)) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0dd)) + theme(panel.border = element_rect(color = clr_dark, fill = &quot;transparent&quot;)) } x &lt;- rnorm(50) x &lt;- as.numeric(scale(x)) monty_plot(contour_plot_dat = contour_plot_dat_gaus, contour_breaks = .001* seq(146,160, by = 2.5), seed = 23) + monty_plot(L = 28, contour_plot_dat = contour_plot_dat_gaus, contour_breaks = .001* seq(146,160, by = 2.5), seed = 23) + theme(axis.title.y = element_blank(), axis.text.y = element_blank()) + plot_layout(guides = &quot;collect&quot;) &amp; theme(legend.position = &quot;bottom&quot;) 10.4 Easy HMC: rethinking::ulam() Revisiting the ruggedness model: \\[ \\begin{array}{rclr} log(y_{i}) &amp; \\sim &amp; Normal( \\mu_{i}, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_{i} &amp; = &amp; \\alpha_{CID[i]} + \\beta_{CID[i]} (r_{i} - \\overline{r}) &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(1, 0.1) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta &amp; \\sim &amp; Normal(0, 0.3) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] data(rugged) data_rugged &lt;- rugged %&gt;% as_tibble() %&gt;% mutate(log_gdp = log(rgdppc_2000)) %&gt;% filter(complete.cases(rgdppc_2000)) %&gt;% mutate(log_gdp_std = log_gdp / mean(log_gdp), rugged_std = rugged / max(rugged), cont_idx = as.integer(2 - cont_africa)) The old way using quap() model_rugged_slope &lt;- quap( flist = alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;- alpha[cont_idx] + beta[cont_idx] * ( rugged_std - 0.215 ), alpha[cont_idx] ~ dnorm(1, 0.1), beta[cont_idx] ~ dnorm(0, 0.3), sigma ~ dexp(1) ), data = data_rugged ) precis(model_rugged_slope, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha[1] 0.89 0.02 0.86 0.91 alpha[2] 1.05 0.01 1.03 1.07 beta[1] 0.13 0.07 0.01 0.25 beta[2] -0.14 0.05 -0.23 -0.06 sigma 0.11 0.01 0.10 0.12 Using Stan / ulam(): data_rugged_list &lt;- data_rugged %&gt;% dplyr::select(log_gdp_std:cont_idx) %&gt;% as.list() model_rugged_slope_stan &lt;- ulam( flist = alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;- alpha[cont_idx] + beta[cont_idx] * ( rugged_std - 0.215 ), alpha[cont_idx] ~ dnorm(1, 0.1), beta[cont_idx] ~ dnorm(0, 0.3), sigma ~ dexp(1) ), data = data_rugged_list, chains = 4, cores = 4 ) Looking at the translated Stan code stancode(model_rugged_slope_stan) #&gt; data{ #&gt; vector[170] log_gdp_std; #&gt; vector[170] rugged_std; #&gt; int cont_idx[170]; #&gt; } #&gt; parameters{ #&gt; vector[2] alpha; #&gt; vector[2] beta; #&gt; real&lt;lower=0&gt; sigma; #&gt; } #&gt; model{ #&gt; vector[170] mu; #&gt; sigma ~ exponential( 1 ); #&gt; beta ~ normal( 0 , 0.3 ); #&gt; alpha ~ normal( 1 , 0.1 ); #&gt; for ( i in 1:170 ) { #&gt; mu[i] = alpha[cont_idx[i]] + beta[cont_idx[i]] * (rugged_std[i] - 0.215); #&gt; } #&gt; log_gdp_std ~ normal( mu , sigma ); #&gt; } The model summaries show(model_rugged_slope_stan) #&gt; Hamiltonian Monte Carlo approximation #&gt; 2000 samples from 4 chains #&gt; #&gt; Sampling durations (seconds): #&gt; warmup sample total #&gt; chain:1 0.05 0.03 0.07 #&gt; chain:2 0.04 0.03 0.07 #&gt; chain:3 0.04 0.03 0.07 #&gt; chain:4 0.04 0.02 0.06 #&gt; #&gt; Formula: #&gt; log_gdp_std ~ dnorm(mu, sigma) #&gt; mu &lt;- alpha[cont_idx] + beta[cont_idx] * (rugged_std - 0.215) #&gt; alpha[cont_idx] ~ dnorm(1, 0.1) #&gt; beta[cont_idx] ~ dnorm(0, 0.3) #&gt; sigma ~ dexp(1) precis(model_rugged_slope_stan, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha[1] 0.89 0.02 0.86 0.91 2435.26 1 alpha[2] 1.05 0.01 1.03 1.07 3005.13 1 beta[1] 0.13 0.08 0.01 0.25 2783.03 1 beta[2] -0.14 0.05 -0.23 -0.06 2377.51 1 sigma 0.11 0.01 0.10 0.12 3068.93 1 extract.samples(model_rugged_slope_stan) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% ggpairs( lower = list(continuous = wrap(my_lower)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll0, color = clr0d, adjust = .7)), upper = list(continuous = wrap(my_upper , size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;)) ) + theme(panel.border = element_rect(color = clr_dark, fill = &quot;transparent&quot;)) The density for \\(\\sigma\\) is skewed, which is expected. 10.4.1 Checking the Chain Trace plot library(ggmcmc) rugged_pars &lt;- c(&quot;alpha[1]&quot;, &quot;alpha[2]&quot;, &quot;beta[1]&quot;, &quot;beta[2]&quot;) clr_chains &lt;- function(n = 4, alpha = .7, col = clr2){scales::colour_ramp(colors = c(clr0dd, col))(seq(0,1,length.out = n))%&gt;% clr_lighten(.2) %&gt;% clr_alpha(alpha = alpha)} ggs(model_rugged_slope_stan@stanfit, inc_warmup = TRUE) %&gt;% mutate(chain = factor(Chain)) %&gt;% filter(Parameter != &quot;sigma&quot;) %&gt;% ggplot(aes(x = Iteration, y = value)) + annotate(geom = &quot;rect&quot;, xmin = 0, xmax = 500, ymin = -Inf, ymax = Inf, fill = clr0d, alpha = .3, size = 0) + geom_line(aes(color = chain), size = .15) + scale_color_manual(values = clr_chains() ) + facet_wrap(~ Parameter, scales = &quot;free_y&quot;) + labs(title = &quot;custom trace plots with warmups via ggmcmc::ggs()&quot;, x = NULL, y = NULL) + theme(legend.position = &quot;bottom&quot;) Trace Rank plot library(bayesplot) model_rugged_slope_stan@stanfit %&gt;% mcmc_rank_overlay(pars = vars(`alpha[1]`:`beta[2]`), n_bins = 60) + scale_color_manual(values = clr_chains() ) + ggtitle(&quot;custom trank plots&quot;) + # coord_cartesian(ylim = c(0, NA)) + theme(legend.position = &quot;bottom&quot;) 10.5 Care and feeding of your Markov Chain set.seed(11) model_flat &lt;- ulam( flist = alist( y ~ dnorm( mu, sigma ), mu &lt;- alpha, alpha ~ dnorm( 0, 1000 ), sigma ~ dexp( 0.0001 ) ), data = list(y = c(-1, 1)), chains = 3 ) precis(model_flat) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha -30.28 334.06 -622.79 381.89 116.47 1.03 sigma 626.24 1475.25 6.93 2764.99 106.15 1.05 p1 &lt;- ggs(model_flat@stanfit, inc_warmup = TRUE) %&gt;% mutate(chain = factor(Chain)) %&gt;% ggplot(aes(x = Iteration, y = value)) + annotate(geom = &quot;rect&quot;, xmin = 0, xmax = 500, ymin = -Inf, ymax = Inf, fill = clr0d, alpha = .3, size = 0) + geom_line(aes(color = chain), size = .25) + scale_color_manual(values = clr_chains(alpha = 1), guide =&quot;none&quot;) + facet_wrap(~ Parameter, scales = &quot;free_y&quot;) + labs(x = NULL, y = NULL) p2 &lt;- model_flat@stanfit %&gt;% mcmc_rank_overlay(pars = vars(`alpha`:`sigma`), n_bins = 60) + scale_color_manual(values = clr_chains(), guide =&quot;none&quot;) p1 / p2 extract.samples(model_flat) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% ggpairs( lower = list(continuous = wrap(my_lower)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll0, color = clr0d, adjust = .7)), upper = list(continuous = wrap(my_upper , size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;)) ) + theme(panel.border = element_rect(color = clr_dark, fill = &quot;transparent&quot;)) set.seed(11) model_less_flat &lt;- ulam( flist = alist( y ~ dnorm( mu, sigma ), mu &lt;- alpha, alpha ~ dnorm( 1, 10 ), sigma ~ dexp( 1 ) ), data = list(y = c(-1, 1)), chains = 3 ) precis(model_less_flat) %&gt;% knit_precis() p1 &lt;- ggs(model_less_flat@stanfit, inc_warmup = TRUE) %&gt;% mutate(chain = factor(Chain)) %&gt;% ggplot(aes(x = Iteration, y = value)) + annotate(geom = &quot;rect&quot;, xmin = 0, xmax = 500, ymin = -Inf, ymax = Inf, fill = clr0d, alpha = .3, size = 0) + geom_line(aes(color = chain), size = .25) + scale_color_manual(values = clr_chains(alpha = 1), guide =&quot;none&quot;) + facet_wrap(~ Parameter, scales = &quot;free_y&quot;) + labs(x = NULL, y = NULL) p2 &lt;- model_less_flat@stanfit %&gt;% mcmc_rank_overlay(pars = vars(`alpha`:`sigma`), n_bins = 60) + scale_color_manual(values = clr_chains(), guide =&quot;none&quot;) p1 / p2 less_flat_draws &lt;- as_draws_df(model_less_flat@stanfit) %&gt;% as_tibble() %&gt;% dplyr::select(alpha, sigma) p1 &lt;- less_flat_draws %&gt;% ggplot(aes(x = alpha)) + geom_density(aes(color = &quot;posterior&quot;, fill = after_scale(clr_alpha(color)))) + stat_function(fun = function(x){dnorm(x, 1 , 10)}, linetype = 2, aes(color = &quot;prior&quot;)) + scale_x_continuous(limits = c(-15, 15)) p2 &lt;- less_flat_draws %&gt;% ggplot(aes(x = sigma)) + geom_density(aes(color = &quot;posterior&quot;, fill = after_scale(clr_alpha(color)))) + stat_function(fun = function(x){dexp(x, 1)}, linetype = 2, aes(color = &quot;prior&quot;)) + scale_x_continuous(limits = c(0, 10)) p1 + p2 + plot_layout(guides = &quot;collect&quot;) &amp; scale_color_manual(&quot;&quot;, values = c(prior = clr_dark, posterior = clr2)) &amp; labs(y = &quot;density&quot;) &amp; theme(legend.position = &quot;bottom&quot;) 10.5.1 Non-identifiable parameters set.seed(41) y &lt;- rnorm(n = 100, mean = 0, sd = 1) \\[ \\begin{array}{rclr} y_{i} &amp; \\sim &amp; Normal( \\mu_{i}, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_{i} &amp; = &amp; \\alpha_{1} + \\alpha_{2} &amp; \\textrm{[linear model]}\\\\ \\alpha_{1} &amp; \\sim &amp; Normal(1, 1000) &amp; \\textrm{[$\\alpha_{1}$ prior]}\\\\ \\alpha_{2} &amp; \\sim &amp; Normal(1, 1000) &amp; \\textrm{[$\\alpha_{2}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_non_identifiable &lt;- ulam( flist = alist( y ~ dnorm(mu, sigma), mu &lt;- alpha_1 + alpha_2, alpha_1 ~ dnorm(0, 1000), alpha_2 ~ dnorm(0, 1000), sigma ~ dexp(1) ), data = list(y = y ), chains = 3 ) precis(model_non_identifiable) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_1 -28.50 242.64 -395.39 439.07 6.75 1.71 alpha_2 28.69 242.64 -438.94 395.54 6.75 1.71 sigma 1.03 0.06 0.94 1.11 76.61 1.02 Suspicious estimates, and terrible n_eff (should be close to number of iterations) and Rhat (should be 1). p1 &lt;- ggs(model_non_identifiable@stanfit, inc_warmup = TRUE) %&gt;% mutate(chain = factor(Chain)) %&gt;% ggplot(aes(x = Iteration, y = value)) + annotate(geom = &quot;rect&quot;, xmin = 0, xmax = 500, ymin = -Inf, ymax = Inf, fill = clr0d, alpha = .3, size = 0) + geom_line(aes(color = chain), size = .25) + scale_color_manual(values = clr_chains(alpha = 1), guide =&quot;none&quot;) + facet_wrap(~ Parameter, scales = &quot;free_y&quot;) + labs(x = NULL, y = NULL) p2 &lt;- model_non_identifiable@stanfit %&gt;% mcmc_rank_overlay(pars = vars(`alpha_1`:`sigma`), n_bins = 60) + scale_color_manual(values = clr_chains(), guide =&quot;none&quot;) p1 / p2 extract.samples(model_non_identifiable) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% ggpairs( lower = list(continuous = wrap(my_lower)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll0, color = clr0d, adjust = .7)), upper = list(continuous = wrap(my_upper , size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;)) ) + theme(panel.border = element_rect(color = clr_dark, fill = &quot;transparent&quot;)) Adding weakly regularizing priors \\[ \\begin{array}{rclr} \\alpha_{1} &amp; \\sim &amp; Normal(1, 10) &amp; \\textrm{[$\\alpha_{1}$ prior]}\\\\ \\alpha_{2} &amp; \\sim &amp; Normal(1, 10) &amp; \\textrm{[$\\alpha_{2}$ prior]}\\\\ \\end{array} \\] model_non_identifiable_regularized &lt;- ulam( flist = alist( y ~ dnorm(mu, sigma), mu &lt;- alpha_1 + alpha_2, alpha_1 ~ dnorm(0, 10), alpha_2 ~ dnorm(0, 10), sigma ~ dexp(1) ), data = list(y = y ), chains = 3 ) precis(model_non_identifiable_regularized) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_1 0.23 6.99 -11.04 11.13 330.16 1 alpha_2 -0.04 6.99 -10.89 11.25 329.91 1 sigma 1.04 0.07 0.94 1.18 616.54 1 p1 &lt;- ggs(model_non_identifiable_regularized@stanfit, inc_warmup = TRUE) %&gt;% mutate(chain = factor(Chain)) %&gt;% ggplot(aes(x = Iteration, y = value)) + annotate(geom = &quot;rect&quot;, xmin = 0, xmax = 500, ymin = -Inf, ymax = Inf, fill = clr0d, alpha = .3, size = 0) + geom_line(aes(color = chain), size = .25) + scale_color_manual(values = clr_chains(alpha = .6, col = clr3), guide =&quot;none&quot;) + facet_wrap(~ Parameter, scales = &quot;free_y&quot;) + labs(x = NULL, y = NULL) p2 &lt;- model_non_identifiable_regularized@stanfit %&gt;% mcmc_rank_overlay(pars = vars(`alpha_1`:`sigma`), n_bins = 60) + scale_color_manual(values = clr_chains(col = clr3), guide =&quot;none&quot;) p1 / p2 extract.samples(model_non_identifiable_regularized) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% ggpairs( lower = list(continuous = wrap(my_lower, col = clr3)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll0, color = clr0d, adjust = .7)), upper = list(continuous = wrap(my_upper , size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;)) ) + theme(panel.border = element_rect(color = clr_dark, fill = &quot;transparent&quot;)) library(rlang) chapter9_models &lt;- env( model_flat = model_flat, model_less_flat = model_less_flat, model_non_identifiable = model_non_identifiable, model_non_identifiable_regularized = model_non_identifiable_regularized, model_rugged_slope = model_rugged_slope, model_rugged_slope_stan = model_rugged_slope_stan, data_rugged = data_rugged, data_rugged_list = data_rugged_list, data_concentration = data_concentration, data_markov = data_markov ) write_rds(chapter9_models, &quot;envs/chapter9_models.rds&quot;) 10.6 Homework E1 For the Metropolis algorithm, the following requirements need to hold: the parameters must be discrete the likelihood function must be Gaussian the the proposal function must be symmetric E2 Gibbs sampling can be more efficient because it used additional information. It applies adaptive sampling with the use of conjugated priors (combination of prior distributions and and likelihoods). E3 Hamiltonian Monte Carlo can not handle discrete parameters, since the length of a step within a chain needs to be of random length (it needs to glide through the parameter). E4 The effective number of samples (n_eff) represents the length on a Markov Chain with no autocorrelation that would provide the same quality of estimates as the provided one. The n_eff can even be longer than the actual Markov Chain used to sample, provided it displays anti-correlation (following E.T. Jaynes quote). E5 \\(\\hat{R}\\) should approach 1 to indicate proper sampling (it can indicate presence issues, but much less their absence). E6 The trace plot should indicate stationarity, convergence and coverage. This means, the average of the chains should not drift, separate chains should wiggle around the same mean and explore (zig-zag) the a large parameter space (aka. good mixing). So, if the chain is wandering around the parameter space, if different chains seem to disagree or if regions of the parameter space are not covered, this indicates issues with the Markov chain. E7 The trace rank plot should indicate that all chains should receive a uniform and equal distribution of ranks - if one is systematically displaying higher or lower ranks in a region, this indicates problems. M1 model_rugged_slope_unif &lt;- ulam( flist = alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;- alpha[cont_idx] + beta[cont_idx] * ( rugged_std - 0.215 ), alpha[cont_idx] ~ dnorm(1, 0.1), beta[cont_idx] ~ dnorm(0, 0.3), sigma ~ dunif(0, 1) ), data = data_rugged_list, chains = 4, cores = 4 ) precis(model_rugged_slope_unif, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha[1] 0.89 0.02 0.86 0.91 2571.83 1 alpha[2] 1.05 0.01 1.03 1.07 2934.16 1 beta[1] 0.13 0.08 0.01 0.25 2244.02 1 beta[2] -0.14 0.06 -0.23 -0.05 2156.33 1 sigma 0.11 0.01 0.10 0.12 2430.06 1 \\(\\rightarrow\\) There is no detectable influence of changing the \\(\\sigma\\) prior. This is likely because there is enough data to overwrite the prior (and the Uniform prior does not go to infinity and is thus more informative than the previous flat prior examples). M2 model_rugged_slope_beta_off &lt;- ulam( flist = alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;- alpha[cont_idx] + beta[cont_idx] * ( rugged_std - 0.215 ), alpha[cont_idx] ~ dnorm(1, 0.1), beta[cont_idx] ~ dexp(0.3), sigma ~ dexp(1) ), data = data_rugged_list, chains = 4, cores = 4 ) precis(model_rugged_slope_beta_off, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha[1] 0.89 0.02 0.86 0.91 1791.63 1 alpha[2] 1.05 0.01 1.03 1.06 2045.17 1 beta[1] 0.15 0.08 0.03 0.27 750.24 1 beta[2] 0.02 0.02 0.00 0.05 1837.84 1 sigma 0.11 0.01 0.10 0.13 1956.89 1 \\(\\rightarrow\\) \\(\\beta_{[2]}\\) is now forced to be positive, despite the fact that the unconstrained model estimated it to be negative. This also influences \\(\\beta_{[1]}\\), but the estimates for \\(\\alpha\\) and \\(\\sigma\\) stay unchanged (n_eff is always lower though). M3 check_warmpup &lt;- function(w){ model_rugged_slope_stan_w &lt;- ulam( flist = alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;- alpha[cont_idx] + beta[cont_idx] * ( rugged_std - 0.215 ), alpha[cont_idx] ~ dnorm(1, 0.1), beta[cont_idx] ~ dnorm(0, 0.3), sigma ~ dexp(1) ), data = data_rugged_list, chains = 4, cores = 4, warmup = w, iter = 1e3 ) precis(model_rugged_slope_stan_w, depth = 2) %&gt;% as.matrix() %&gt;% as.data.frame() %&gt;% rownames_to_column() %&gt;% mutate(w = w) } check_warmpup_output &lt;- c(10, 30, 75) %&gt;% map_dfr(check_warmpup) %&gt;% mutate(across(mean:Rhat4, ~ round(.x, digits = 2))) check_warmpup_output %&gt;% knitr::kable() rowname mean sd 5.5% 94.5% n_eff Rhat4 w alpha[1] 0.89 0.02 0.86 0.91 585.97 1.01 10 alpha[2] 1.05 0.01 1.04 1.07 576.64 1.01 10 beta[1] 0.13 0.10 0.00 0.27 54.79 1.09 10 beta[2] -0.14 0.06 -0.23 -0.05 314.01 1.00 10 sigma 0.11 0.01 0.10 0.12 310.73 1.01 10 alpha[1] 0.89 0.02 0.86 0.91 3824.28 1.00 30 alpha[2] 1.05 0.01 1.03 1.07 4638.37 1.00 30 beta[1] 0.13 0.08 0.01 0.25 1624.62 1.00 30 beta[2] -0.14 0.06 -0.23 -0.05 2445.98 1.00 30 sigma 0.11 0.01 0.10 0.12 2337.22 1.00 30 alpha[1] 0.89 0.02 0.86 0.91 3803.35 1.00 75 alpha[2] 1.05 0.01 1.03 1.07 4143.25 1.00 75 beta[1] 0.13 0.08 0.00 0.25 1438.09 1.00 75 beta[2] -0.14 0.06 -0.23 -0.05 2507.07 1.00 75 sigma 0.11 0.01 0.10 0.12 2239.18 1.00 75 \\(\\rightarrow\\) 30 iterations for the warm-up already is a massive jump in terms of n_eff compared to 10. H1 ggplot() + stat_function(fun = function(x){dcauchy(x,location = 0 ,scale = 1)}, xlim = c(-9, 9), n = 301, geom = &quot;area&quot;, color = clr0d, fill = fll0) + stat_function(fun = function(x){dnorm(x, mean = 0, sd = 1)}, xlim = c(-9, 9), n = 301, geom = &quot;line&quot;, color = clr_dark, linetype = 3) + labs(subtitle = &quot;location: 0\\nscale: 1&quot;, y = &quot;density&quot;) Figure 10.1: the Cauchy distribution (compared to a normal distribution). set.seed(42) model_cauchy &lt;- ulam( flist = alist( a ~ dnorm(0, 1), b ~ dcauchy(0, 1) ), data = list( y = 1), chains = 1 ) precis(model_cauchy) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 a 0.11 1.07 -1.48 1.80 44.92 1 b 0.49 22.75 -14.75 9.06 164.30 1 ggs(model_cauchy@stanfit, inc_warmup = TRUE) %&gt;% ggplot(aes(x = Iteration, y = value)) + annotate(geom = &quot;rect&quot;, xmin = 0, xmax = 500, ymin = -Inf, ymax = Inf, fill = clr0d, alpha = .3, size = 0) + geom_line(size = .25, color = clr_alpha(clr2,.7)) + scale_color_manual(values = clr_chains(alpha = .6, col = clr3), guide =&quot;none&quot;) + facet_wrap(~ Parameter, scales = &quot;free_y&quot;) + labs(x = NULL, y = NULL) \\(\\rightarrow\\) The Cauchy distribution does have much wider tails than the normal distribution. This makes the prior somewhat more flat and less regularizing, which allows the chain to make more excursions to the more extreme parameter space (\\(&gt; 10\\)). H2 chapter5_models &lt;- read_rds(&quot;envs/chapter5_models.rds&quot;) data_waffle_age_list &lt;- chapter5_models$data_waffle %&gt;% dplyr::select(divorce_std, median_age_std) %&gt;% as.list() model_age_stan &lt;- ulam( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_A * median_age_std , alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle_age_list, chains = 4, cores = 4, log_lik = TRUE ) data_waffle_marriage_list &lt;- chapter5_models$data_waffle %&gt;% dplyr::select(divorce_std, marriage_std) %&gt;% as.list() model_marriage_stan &lt;- ulam( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std , alpha ~ dnorm( 0, 0.2 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle_marriage_list, chains = 4, cores = 4, log_lik = TRUE ) data_waffle_multiple_list &lt;- chapter5_models$data_waffle %&gt;% dplyr::select(divorce_std, median_age_std, marriage_std) %&gt;% as.list() model_multiple_stan &lt;- ulam( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle_multiple_list, chains = 4, cores = 4, log_lik = TRUE ) comparing the fits with ulam() compare(model_age_stan, model_marriage_stan, model_multiple_stan) as reference the old fits with quap() compare(chapter5_models$model_age, chapter5_models$model_marriage, chapter5_models$model_multiple) #&gt; WAIC SE dWAIC dSE pWAIC #&gt; chapter5_models$model_age 127.2709 14.41304 0.000000 NA 4.596132 #&gt; chapter5_models$model_multiple 129.1756 14.48622 1.904782 0.989997 5.820602 #&gt; chapter5_models$model_marriage 140.1808 10.89342 12.909923 10.779851 3.512297 #&gt; weight #&gt; chapter5_models$model_age 0.720777820 #&gt; chapter5_models$model_multiple 0.278088613 #&gt; chapter5_models$model_marriage 0.001133567 \\(\\rightarrow\\) The weight is shared between the age only and the age &amp; marriage status models, which are also close but distinct in terms of WAIC. There seems to be a slight preference for the age only model, but that would be debatable — in any case it seems to be the case that age does the heavy lifting in terms of prediction. H3 n &lt;- 100 set.seed(909) data_legs_list &lt;- tibble( height = rnorm(n = n, mean = 10, sd = 2), leg_proportion = runif(n, min = 0.4, max = 0.5), left_leg = leg_proportion * height + rnorm(n, 0, .02), right_leg = leg_proportion * height + rnorm(n, 0, .02)) %&gt;% dplyr::select(height, left_leg, right_leg) %&gt;% as.list() model_legs_multicollinear_stan &lt;- ulam( flist = alist( height ~ dnorm(mu, sigma), mu &lt;- alpha + beta_left * left_leg + beta_right * right_leg, alpha ~ dnorm(10, 100), beta_left ~ dnorm(2, 10), beta_right ~ dnorm(2, 10), sigma ~ dexp(1) ), data = data_legs_list, chains = 4, cores = 4, log_lik = TRUE, start = list(alpha = 10, beta_left = 0, beta_right = 0, sigma = 1) ) model_legs_multicollinear_stan_constrained &lt;- ulam( flist = alist( height ~ dnorm(mu, sigma), mu &lt;- alpha + beta_left * left_leg + beta_right * right_leg, alpha ~ dnorm(10, 100), beta_left ~ dnorm(2, 10), beta_right ~ dnorm(2, 10), sigma ~ dexp(1) ), data = data_legs_list, chains = 4, cores = 4, log_lik = TRUE, constraints = list(beta_right = &quot;lower=0&quot;), start = list(alpha = 10, beta_left = 0, beta_right = 0.1, sigma = 1) ) wrap_elements(ggmatrix_gtable(posterior_pairs(model_legs_multicollinear_stan, col = clr2, title = &quot;unconstrained&quot;))) + wrap_elements(ggmatrix_gtable(posterior_pairs(model_legs_multicollinear_stan_constrained, col = clr2, title = &quot;constrained&quot;))) \\(\\rightarrow\\) containing \\(\\beta_{right}\\) to be positive forces \\(\\beta_{left}\\) to be (mostly) negative. H4 compare(model_legs_multicollinear_stan, model_legs_multicollinear_stan_constrained) #&gt; WAIC SE dWAIC #&gt; model_legs_multicollinear_stan_constrained 194.3730 11.24082 0.0000000 #&gt; model_legs_multicollinear_stan 195.2634 11.24420 0.8903366 #&gt; dSE pWAIC weight #&gt; model_legs_multicollinear_stan_constrained NA 2.834342 0.6094898 #&gt; model_legs_multicollinear_stan 0.6242321 3.243090 0.3905102 precis(model_legs_multicollinear_stan) #&gt; mean sd 5.5% 94.5% n_eff Rhat4 #&gt; alpha 0.9789570 0.27922534 0.5176528 1.4083767 1047.2334 1.0022109 #&gt; beta_left 0.2314571 2.68796128 -4.0451582 4.5062469 843.6087 1.0006904 #&gt; beta_right 1.7642327 2.69026759 -2.5084165 6.0744526 844.8523 1.0007602 #&gt; sigma 0.6343543 0.04710884 0.5641138 0.7106078 1049.2715 0.9995547 precis(model_legs_multicollinear_stan_constrained) #&gt; mean sd 5.5% 94.5% n_eff Rhat4 #&gt; alpha 0.9826006 0.29685701 0.5183266 1.4524584 899.3052 1.002881 #&gt; beta_left -0.7046059 1.79741498 -4.0328126 1.7363541 479.4671 1.001801 #&gt; beta_right 2.7001266 1.80259942 0.2397091 6.0168358 479.8602 1.001750 #&gt; sigma 0.6328119 0.04528771 0.5634368 0.7100542 993.9975 1.000004 \\(\\rightarrow\\) The constrained model has only about half the n_eff compared to the unconstrained one - probably, that is because half of the parameter space is problematic. Particularly in the high density area of 0. H5 set.seed(42) n_weeks &lt;- 1e5 positions &lt;- rep(0, n_weeks) populations &lt;- sample(1:10,size = 10) current &lt;- 10 for( i in seq_along(positions)){ # record the current position positions[i] &lt;- current # flip the coin to get the proposal proposal &lt;- current + sample(c(-1, 1), size = 1) # connecting the edges to make the archipelago circular if(proposal == 0){proposal &lt;- 10} if(proposal == 11){proposal &lt;- 1} # decide whether to move prob_move &lt;- populations[proposal]/populations[current] current &lt;- ifelse(runif(1) &lt; prob_move, proposal, current) } data_markov &lt;- tibble( week = seq_along(positions), position = positions) data_markov_max &lt;- data_markov %&gt;% group_by(position) %&gt;% count() %&gt;% .$n %&gt;% max() p1 &lt;- data_markov %&gt;% filter(week &lt; 101) %&gt;% ggplot(aes(x = week, y = position)) + geom_point(shape = 21, color = clr0dd, fill = fll0) p2 &lt;- data_markov %&gt;% ggplot(aes(x = position)) + geom_histogram(breaks = 0:10, color = clr0dd, fill = fll0) + geom_point(data = tibble(position = 1:10 - .5, population = populations, pop_scaled = population / 10 * data_markov_max), aes(y = pop_scaled), shape = 1, color = clr2) + scale_x_continuous(breaks = .5 + 0:9, labels = 1:10) + labs(y = &quot;pop_scaled, n_visits&quot;)+ theme(panel.grid.major.x = element_blank()) p1 + p2 H6 we kind of did that already 🤔 manual_mcmc &lt;- function(W = 6, L = 3, n_samples = 1e4, p_start = .5, seed = 42){ set.seed(seed) p &lt;- rep( NA, n_samples ) p[1] &lt;- p_start for ( i in 2:n_samples ) { # create a symmetric proposal p_new &lt;- rnorm( n = 1, mean = p[ i - 1], sd = 0.1) if ( p_new &lt; 0 ){ p_new &lt;- abs( p_new ) } if ( p_new &gt; 1 ){ p_new &lt;- 2 - p_new } # compare probabilities of current vs. proposal q0 &lt;- dbinom( W, W + L, p[ i - 1 ] ) q1 &lt;- dbinom( W, W + L, p_new ) # decide whether to move p[i] &lt;- if_else( runif(1) &lt; q1 / q0, p_new, p[i - 1] ) } p } p &lt;- manual_mcmc() p_36 &lt;- manual_mcmc(W = 24, L = 12) p_p &lt;- tibble(x = p) %&gt;% ggplot() + stat_function(fun = function(x){ dbeta( shape1 = 6 + 1, shape2 = 3 + 1, x = x ) / integrate(f = function(x){ dbeta( shape1 = 6 + 1, shape2 = 3 + 1, x = x )}, lower = 0, upper = 1)[[1]] }, xlim = c(0,1), n = 500, geom = &quot;area&quot;, aes(color = &quot;posterior&quot;, fill = after_scale(clr_alpha(color))))+ geom_density(aes(x = x, color = &quot;MCMC&quot;)) + scale_color_manual(&quot;approach&quot;, values = c(posterior = clr0, MCMC = clr2)) + labs(y = &quot;density&quot;, x = &quot;proportion water&quot;, title = &quot;n = 12&quot;) p_p36 &lt;- tibble(x = p_36) %&gt;% ggplot() + stat_function(fun = function(x){ dbeta( shape1 = 24 + 1, shape2 = 12 + 1, x = x ) / integrate(f = function(x){ dbeta( shape1 = 24 + 1, shape2 = 12 + 1, x = x )}, lower = 0, upper = 1)[[1]] }, xlim = c(0,1), n = 500, geom = &quot;area&quot;, aes(color = &quot;posterior&quot;, fill = after_scale(clr_alpha(color))))+ geom_density(aes(x = x, color = &quot;MCMC&quot;)) + scale_color_manual(&quot;approach&quot;, values = c(posterior = clr0, MCMC = clr2)) + labs(y = &quot;density&quot;, x = &quot;proportion water&quot;, title = &quot;n = 36&quot;) p_p + p_p36 + plot_layout(guides = &quot;collect&quot;) &amp; theme(legend.position = &quot;bottom&quot;) H7 Hmm - I’ll pass for now… 🤷 10.7 {brms} section 10.7.1 Easy HMC: brm{} data_rugged_brms &lt;- data_rugged %&gt;% mutate(cont_idx = factor(cont_idx)) %&gt;% dplyr::select(log_gdp_std, rugged_std, cont_idx) brms_c9_model_rugged_slope &lt;- brm( data = data_rugged_brms, family = gaussian, bf(log_gdp_std ~ 0 + a + b * (rugged_std - 0.215), a ~ 0 + cont_idx, b ~ 0 + cont_idx, nl = TRUE), prior = c(prior(normal(1, 0.1), class = b, coef = cont_idx1, nlpar = a), prior(normal(1, 0.1), class = b, coef = cont_idx2, nlpar = a), prior(normal(0, 0.3), class = b, coef = cont_idx1, nlpar = b), prior(normal(0, 0.3), class = b, coef = cont_idx2, nlpar = b), prior(exponential(1), class = sigma)), chains = 1, cores = 1, seed = 42, file = &quot;brms/brms_c9_model_rugged_slope&quot;) library(tidybayes) as_draws_df(brms_c9_model_rugged_slope) %&gt;% pivot_longer(-lp__) %&gt;% group_by(name) %&gt;% mean_hdi(value, .width = .89) #&gt; # A tibble: 8 × 7 #&gt; name value .lower .upper .width .point .interval #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 .chain 1 1 1 0.89 mean hdi #&gt; 2 .draw 500. 1 891 0.89 mean hdi #&gt; 3 .iteration 500. 1 891 0.89 mean hdi #&gt; 4 b_a_cont_idx1 0.887 0.859 0.908 0.89 mean hdi #&gt; 5 b_a_cont_idx2 1.05 1.03 1.07 0.89 mean hdi #&gt; 6 b_b_cont_idx1 0.133 0.0166 0.252 0.89 mean hdi #&gt; 7 b_b_cont_idx2 -0.140 -0.220 -0.0565 0.89 mean hdi #&gt; 8 sigma 0.112 0.102 0.121 0.89 mean hdi {brms} now returns two columns: Bulk_ESS and Tail_ESS. These originate from the same Vehtari, Gelman, et al. (2019) paper that introduced the upcoming change to the \\(\\hat{R}\\). If you plan to report quantile estimates or posterior intervals, we strongly suggest assessing the convergence of the chains for these quantiles. In Section 4.3 we show that convergence of Markov chains is not uniform across the parameter space and propose diagnostics and effective sample sizes specifically for extreme quantiles. This is different from the standard ESS estimate (which we refer to as the “bulk-ESS”), which mainly assesses how well the centre of the distribution is resolved. Instead, these “tail-ESS” measures allow the user to estimate the MCSE for interval estimates. (p. 5, emphasis in the original) parallel::detectCores() #&gt; [1] 8 brms_c9_model_rugged_slope_parallel &lt;- update( brms_c9_model_rugged_slope, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c9_model_rugged_slope_parallel&quot;) vcov(brms_c9_model_rugged_slope_parallel, correlation = TRUE) %&gt;% round(digits = 2) \\[\\begin{bmatrix} 1 &amp;0.01 &amp;0.14 &amp;0.03 \\\\0.01 &amp;1 &amp;0.01 &amp;-0.09 \\\\0.14 &amp;0.01 &amp;1 &amp;0.02 \\\\0.03 &amp;-0.09 &amp;0.02 &amp;1 \\\\ \\end{bmatrix}\\] as_draws_df(brms_c9_model_rugged_slope_parallel) %&gt;% select(b_a_cont_idx1:sigma) %&gt;% general_pairs(col = clr_dark) ggs(brms_c9_model_rugged_slope_parallel, inc_warmup = TRUE) %&gt;% mutate(chain = factor(Chain)) %&gt;% filter(Parameter != &quot;sigma&quot;) %&gt;% ggplot(aes(x = Iteration, y = value)) + annotate(geom = &quot;rect&quot;, xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf, fill = clr0d, alpha = .3, size = 0) + geom_line(aes(color = chain), size = .15) + scale_color_manual(values = clr_chains(col = clr_dark) ) + facet_wrap(~ Parameter, scales = &quot;free_y&quot;) + labs(title = &quot;custom trace plots with warmups via ggmcmc::ggs()&quot;, x = NULL, y = NULL) + theme(legend.position = &quot;bottom&quot;) brms_c9_model_rugged_slope_parallel %&gt;% mcmc_rank_overlay(pars = vars(`b_a_cont_idx1`:`b_b_cont_idx2`), n_bins = 60) + scale_color_manual(values = clr_chains(col = clr_dark) ) + ggtitle(&quot;custom trank plots&quot;) + # coord_cartesian(ylim = c(0, NA)) + theme(legend.position = &quot;bottom&quot;) brms::stancode(brms_c9_model_rugged_slope_parallel) #&gt; // generated with brms 2.16.1 #&gt; functions { #&gt; } #&gt; data { #&gt; int&lt;lower=1&gt; N; // total number of observations #&gt; vector[N] Y; // response variable #&gt; int&lt;lower=1&gt; K_a; // number of population-level effects #&gt; matrix[N, K_a] X_a; // population-level design matrix #&gt; int&lt;lower=1&gt; K_b; // number of population-level effects #&gt; matrix[N, K_b] X_b; // population-level design matrix #&gt; // covariate vectors for non-linear functions #&gt; vector[N] C_1; #&gt; int prior_only; // should the likelihood be ignored? #&gt; } #&gt; transformed data { #&gt; } #&gt; parameters { #&gt; vector[K_a] b_a; // population-level effects #&gt; vector[K_b] b_b; // population-level effects #&gt; real&lt;lower=0&gt; sigma; // dispersion parameter #&gt; } #&gt; transformed parameters { #&gt; } #&gt; model { #&gt; // likelihood including constants #&gt; if (!prior_only) { #&gt; // initialize linear predictor term #&gt; vector[N] nlp_a = X_a * b_a; #&gt; // initialize linear predictor term #&gt; vector[N] nlp_b = X_b * b_b; #&gt; // initialize non-linear predictor term #&gt; vector[N] mu; #&gt; for (n in 1:N) { #&gt; // compute non-linear predictor values #&gt; mu[n] = 0 + nlp_a[n] + nlp_b[n] * (C_1[n] - 0.215); #&gt; } #&gt; target += normal_lpdf(Y | mu, sigma); #&gt; } #&gt; // priors including constants #&gt; target += normal_lpdf(b_a[1] | 1, 0.1); #&gt; target += normal_lpdf(b_a[2] | 1, 0.1); #&gt; target += normal_lpdf(b_b[1] | 0, 0.3); #&gt; target += normal_lpdf(b_b[2] | 0, 0.3); #&gt; target += exponential_lpdf(sigma | 1); #&gt; } #&gt; generated quantities { #&gt; } 10.7.2 Care and feeding of your Markov chain brms_c9_model_flat &lt;- brm( data = list(y = c(-1, 1)), family = gaussian, y ~ 1, prior = c(prior(normal(0, 1000), class = Intercept), prior(exponential(0.0001), class = sigma)), iter = 2000, warmup = 1000, chains = 3, seed = 42, file = &quot;brms/brms_c9_model_flat&quot;) print(brms_c9_model_flat) #&gt; Family: gaussian #&gt; Links: mu = identity; sigma = identity #&gt; Formula: y ~ 1 #&gt; Data: list(y = c(-1, 1)) (Number of observations: 2) #&gt; Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 3000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; Intercept 3.95 317.70 -774.17 805.16 1.02 602 459 #&gt; #&gt; Family Specific Parameters: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; sigma 547.18 1330.76 19.32 3759.47 1.04 98 63 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). Inspection help for divergent transitions: nuts_params(brms_c9_model_flat) %&gt;% distinct(Parameter) #&gt; Parameter #&gt; 1 accept_stat__ #&gt; 2 stepsize__ #&gt; 3 treedepth__ #&gt; 4 n_leapfrog__ #&gt; 5 divergent__ #&gt; 6 energy__ nuts_params(brms_c9_model_flat) %&gt;% filter(Parameter == &quot;divergent__&quot;) %&gt;% count(Value) #&gt; Value n #&gt; 1 0 2682 #&gt; 2 1 318 posterior &lt;- as_draws_df(brms_c9_model_flat, add_chain = TRUE) %&gt;% as_tibble() %&gt;% rename(chain = &quot;.chain&quot;) p1 &lt;- posterior %&gt;% mcmc_trace(pars = vars(b_Intercept:sigma), size = .25) p2 &lt;- posterior %&gt;% mcmc_rank_overlay(pars = vars(b_Intercept:sigma), n_bins = 50) ( (p1 / p2) &amp; scale_color_manual(values = clr_chains(col = clr_dark) ) &amp; theme(legend.position = &quot;none&quot;) ) + plot_annotation(subtitle = &quot;These chains are not healthy&quot;) brms_c9_model_less_flat &lt;- brm( data = list(y = c(-1, 1)), family = gaussian, y ~ 1, prior = c(prior(normal(1, 10), class = Intercept), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 3, seed = 42, file = &quot;brms/brms_c9_model_less_flat&quot;) posterior &lt;- as_draws_df(brms_c9_model_less_flat, add_chain = TRUE) %&gt;% as_tibble() %&gt;% rename(chain = &quot;.chain&quot;) p1 &lt;- posterior %&gt;% mcmc_trace(pars = vars(b_Intercept:sigma), size = .25) p2 &lt;- posterior %&gt;% mcmc_rank_overlay(pars = vars(b_Intercept:sigma), n_bins = 50) ( (p1 / p2) &amp; scale_color_manual(values = clr_chains(col = clr_dark) ) &amp; theme(legend.position = &quot;none&quot;) ) + plot_annotation(subtitle = &quot;Weakly informative priors cleared up the condition right away&quot;) 10.7.3 Non-identifiable parameters set.seed(42) y &lt;- rnorm(100, mean = 0, sd = 1) brms_c9_model_non_identifiable &lt;- brm( data = list(y = y, a1 = 1, a2 = 1), family = gaussian, y ~ 0 + a1 + a2, prior = c(prior(normal(0, 1000), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 3, seed = 42, file = &quot;brms/brms_c9_model_non_identifiable&quot;) posterior &lt;- as_draws_df(brms_c9_model_non_identifiable, add_chain = TRUE) %&gt;% as_tibble() %&gt;% rename(chain = &quot;.chain&quot;) p1 &lt;- posterior %&gt;% mcmc_trace(pars = vars(b_a1:sigma), size = .25) p2 &lt;- posterior %&gt;% mcmc_rank_overlay(pars = vars(b_a1:sigma), n_bins = 50) ( (p1 / p2) &amp; scale_color_manual(values = clr_chains(col = clr_dark) ) &amp; theme(legend.position = &quot;none&quot;) ) + plot_annotation(subtitle = &quot;non identifiable parameter&quot;) brms_c9_model_non_identifiable_regularized &lt;- brm( data = list(y = y, a1 = 1, a2 = 1), family = gaussian, y ~ 0 + a1 + a2, prior = c(prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 3, seed = 9, file = &quot;brms/brms_c9_model_non_identifiable_regularized&quot;) posterior &lt;- as_draws_df(brms_c9_model_non_identifiable_regularized, add_chain = TRUE) %&gt;% as_tibble() %&gt;% rename(chain = &quot;.chain&quot;) p1 &lt;- posterior %&gt;% mcmc_trace(pars = vars(b_a1:sigma), size = .25) p2 &lt;- posterior %&gt;% mcmc_rank_overlay(pars = vars(b_a1:sigma), n_bins = 50) ( (p1 / p2) &amp; scale_color_manual(values = clr_chains(col = clr_dark) ) &amp; theme(legend.position = &quot;none&quot;) ) + plot_annotation(subtitle = &quot;non identifiable parameter (regularized)&quot;) 10.8 pymc3 section × "],["rethinking-chapter-10.html", "11 Rethinking: Chapter 10 11.1 Maximum Entropy 11.2 Generalized linear Models 11.3 Homework 11.4 {brms} section 11.5 pymc3 section", " 11 Rethinking: Chapter 10 Big Entropy and the Generalized Linear Model by Richard McElreath, building on the Summaries by Solomon Kurz. 11.1 Maximum Entropy Recall information entropy (\\(H\\)): \\[ H(p) = - \\sum_{i} p_{i}~log~p_i \\] data_bucket &lt;- tibble( case_id = LETTERS[1:5], p = list( c(0,0,10,0,0), c(0,1,8,1,0), c(0,2,6,2,0), c(1,2,4,2,1), c(2,2,2,2,2) ), ways = c(1, 90,1260,37800, 113400), log_ways = log(ways)/10) %&gt;% mutate(p_norm = map(p, .f = function(q){ q / sum(q)}), entropy = map_dbl(p_norm, .f = function(q){ - sum( if_else(q == 0, 0, q * log(q))) })) p &lt;- data_bucket %&gt;% unnest(p) %&gt;% group_by(case_id) %&gt;% mutate(bucket_idx = row_number()) %&gt;% ungroup() %&gt;% ggplot(aes(x = bucket_idx, y = p)) + geom_bar(stat = &quot;identity&quot;, color = clr0d, fill = fll0) + geom_text(data = data_bucket, aes(x = 5.5, y = 11, label = ways), hjust = 1, family = fnt_sel) + geom_text(data = . %&gt;% filter(p &gt; 0), aes(y = p + 1, label = p), family = fnt_sel, color = clr0dd) + facet_wrap(case_id ~ .) + scale_y_continuous(breaks = seq(0,10, length.out = 5), expand = c(0,0)) + coord_cartesian(ylim = c(0, 12)) + theme(panel.grid.minor = element_blank(), panel.grid.major = element_blank(), axis.ticks.y = element_line(size = .2, color = clr_dark), panel.border = element_rect(color = clr_dark, fill = &quot;transparent&quot;)) pg &lt;- ggplotGrob(p) p2 &lt;- ggplot(data_bucket, aes(log_ways, entropy))+ geom_smooth(method = &quot;lm&quot;, se = FALSE, color = clr_dark, linetype = 3, size = .6) + geom_point(shape = 21, color = clr0d, fill = clr0, size = 2) + geom_text(aes(y = entropy + .2 * c(1,1,-1,-1,-1), label = case_id), family = fnt_sel) + scale_y_continuous(position = &quot;right&quot;) pg$grobs[[which(pg$layout$name == &quot;panel-3-2&quot;)]] &lt;- ggplotGrob(p2) pg$layout$t[[which(pg$layout$name == &quot;panel-3-2&quot;)]] &lt;- 18 cowplot::ggdraw(pg) 11.1.1 The Gaussian Family The generalized normal distribution: \\[ Pr(y | \\mu, \\alpha, \\beta) = \\frac{\\beta}{2\\alpha \\Gamma (1 / \\beta) } e^{-(\\frac{|y-\\mu|}{\\alpha})^\\beta} \\] The normal distribution (\\(\\beta = 2\\)) has largest entropy for a continuous distribution with a known variance. alpha_per_beta &lt;- function(beta, variance = 1) { sqrt((variance * gamma(1 / beta)) / gamma(3 / beta)) } beta_select &lt;- c(1, 1.5, 2, 4) p1 &lt;- crossing(value = seq(from = -5, to = 5, length.out = 501), beta = beta_select) %&gt;% mutate(mu = 0, alpha = alpha_per_beta(beta)) %&gt;% mutate(density = (beta / (2 * alpha * gamma(1 / beta))) * exp(1) ^ (-1 * (abs(value - mu) / alpha) ^ beta),) %&gt;% ggplot(aes(x = value, y = density, group = beta)) + geom_area(aes(color = beta == 2, fill = after_scale(clr_alpha(color, .3))), position = &quot;dodge&quot;) + scale_color_manual(values = c(`TRUE` = clr1, `FALSE` = clr0d), guide = &quot;none&quot;) + coord_cartesian(xlim = c(-4, 4)) p2 &lt;- crossing(value = -8:8, beta = seq(from = .9, to = 4.1, by = .01)) %&gt;% mutate(mu = 0, alpha = alpha_per_beta(beta)) %&gt;% mutate(density = (beta / (2 * alpha * gamma(1 / beta))) * exp(1) ^ (-1 * (abs(value - mu) / alpha) ^ beta)) %&gt;% group_by(beta) %&gt;% summarise(entropy = -sum(density * log(density))) %&gt;% ggplot(aes(x = beta, y = entropy)) + geom_line(color = clr_dark) + geom_segment(data = . %&gt;% filter(beta %in% beta_select), aes(x = beta, xend = beta, y = 0, yend = entropy, color = beta == 2), linetype = 3) + geom_point(data = . %&gt;% filter(beta %in% beta_select), aes(x = beta, y = entropy, color = beta == 2, fill = after_scale(clr_lighten(color))), shape = 21, size = 2) + scale_color_manual(values = c(`TRUE` = clr1, `FALSE` = clr0d), guide = &quot;none&quot;) + xlab(expression(beta(i.e.*&quot;, &quot;*shape))) + coord_cartesian(ylim = c(1.34, 1.42)) p1 + p2 11.1.2 The Binomial Distribution \\[ Pr(y_{1}, y_{2}, ..., y_{n}|n,p) = p^{y}(1 - p)^{n-y} \\] Largest entropy if: two unordered events constant expected value data_binom &lt;- tibble( distribution = LETTERS[1:4], p = list( rep(1/4, 4), c(2/6, 1/6, 1/6, 2/6), c(1/6, 2/6, 2/6, 1/6), c(1/8, 4/8, 2/8, 1/8))) %&gt;% mutate(exp_val = map_dbl(p, function(p){ sum( p * c(0, 1, 1, 2))}), entropy = map_dbl(p, function(p){ - sum( p * log(p) ) })) data_binom #&gt; # A tibble: 4 × 4 #&gt; distribution p exp_val entropy #&gt; &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 A &lt;dbl [4]&gt; 1 1.39 #&gt; 2 B &lt;dbl [4]&gt; 1 1.33 #&gt; 3 C &lt;dbl [4]&gt; 1 1.33 #&gt; 4 D &lt;dbl [4]&gt; 1 1.21 data_binom %&gt;% unnest(p) %&gt;% group_by(distribution) %&gt;% mutate(x = row_number(), key = factor(c(&quot;ww&quot;, &quot;bw&quot;, &quot;wb&quot;, &quot;bb&quot;)[x], levels = c(&quot;ww&quot;, &quot;bw&quot;, &quot;wb&quot;, &quot;bb&quot;))) %&gt;% ungroup() %&gt;% ggplot(aes(x = key, y = p)) + geom_line(color = clr0dd, aes(group = distribution)) + geom_point(color = clr0dd, fill = clr0, shape = 21, size = 2) + facet_wrap(distribution ~ ., nrow = 1) + coord_cartesian(ylim = c(0, 1)) p &lt;- .7 ( A &lt;- c( ( 1 - p ) ^ 2, p * ( 1 - p ), ( 1 - p ) * p, p ^ 2 ) ) #&gt; [1] 0.09 0.21 0.21 0.49 - sum( A * log(A) ) #&gt; [1] 1.221729 library(cli) n &lt;- 1e5 cli_progress_bar(&quot;Simulate |&quot;, total = n) set.seed(42) sim_p &lt;- function( idx, G = 1.4 , n = n){ x123 &lt;- runif(3) x4 &lt;- ( (G) * sum(x123) - x123[2] - x123[3] ) / ( 2 - G ) z &lt;- sum( c( x123, x4 ) ) p &lt;- c( x123, x4) / z cli_progress_update(.envir = .GlobalEnv) tibble(idx = idx, H = -sum(p * log(p)), p = list(p), key = list(factor(c(&quot;ww&quot;, &quot;bw&quot;, &quot;wb&quot;, &quot;bb&quot;), levels = c(&quot;ww&quot;, &quot;bw&quot;, &quot;wb&quot;, &quot;bb&quot;)))) } H &lt;- map_dfr(1:n, sim_p) %&gt;% mutate(rank = rank(H)) %&gt;% arrange(rank) closest_to &lt;- function(x, y){ dif = abs(y - x) dif == min(dif) } d_H &lt;- function(x){demp(x, obs = H$H, density.arg.list = list(adjust = .1))} H_sub &lt;- H %&gt;% mutate(closest_65 = closest_to(H, .65), closest_85 = closest_to(H, .85), closest_100 = closest_to(H, 1), closest_120 = closest_to(H, 1.219), density = d_H(H)) %&gt;% filter(closest_65 | closest_85 | closest_100| closest_120) %&gt;% dplyr::select(-starts_with(&quot;closest&quot;)) %&gt;% mutate(lab = LETTERS[4:1]) p1 &lt;- H %&gt;% ggplot(aes(x = H)) + geom_density(adjust = .1, color = clr0d, fill = fll0) + geom_segment(data = H_sub, aes(xend = H, y = 0, yend = density), color = clr1, linetype = 3)+ geom_point(data = H_sub, aes(y = density), color = clr1, fill = clr_lighten(clr1), shape = 21, size = 2)+ geom_text(data = H_sub, aes(y = density + .5, label = lab), color = clr1, family = fnt_sel) + labs(y = &quot;density&quot;) p2 &lt;- H_sub %&gt;% unnest(c(p, key)) %&gt;% ggplot(aes(x = key, y = p)) + geom_line(color = clr0dd, aes(group = lab)) + geom_point(color = clr0dd, fill = clr0, shape = 21, size = 2) + facet_wrap(lab ~ ., nrow = 2) + coord_cartesian(ylim = c(0, 1)) + labs(x = NULL) p1 + p2 H %&gt;% filter(H == max(H)) %&gt;% .$p %&gt;% .[[1]] %&gt;% round(digits = 5) #&gt; [1] 0.08993 0.20986 0.21028 0.48993 A #&gt; [1] 0.09 0.21 0.21 0.49 11.2 Generalized linear Models tibble(x = seq(from = 0, to = 2, by = .01)) %&gt;% mutate(probability = .45 + x * .4) %&gt;% ggplot(aes(x = x, y = probability)) + geom_rect(xmin = 0, xmax = 2, ymin = 0, ymax = 1, fill = clr0, alpha = .1) + geom_hline(yintercept = 0:1, linetype = 2, color = clr_dark) + geom_line(aes(linetype = probability &gt;= 1), color = clr1, size = 1) + geom_line(data = . %&gt;% filter(probability &gt;= 1), aes(y = 1), size = 2/3, color = clr1) + scale_y_continuous(breaks = c(0, .5, 1)) + coord_cartesian(xlim = c(0, 2), ylim = c(-.02, 1.22), expand = 0) + theme(legend.position = &quot;none&quot;) 11.2.1 Meet the Family p0 &lt;- ggplot()+ stat_function(fun = function(x){dexp(x)}, xlim = c(0,3), geom = &quot;area&quot;, color = clr0d, fill = fll0)+ labs(subtitle = glue(&quot;y {mth(&#39;\\U007E&#39;)} Exponential(*{mth(&#39;\\U03BB&#39;)}*)&quot;), x = &quot;dexp()&quot;) + theme(axis.title.y = element_blank(), plot.subtitle = element_markdown(hjust = .5)) p1 &lt;- ggplot()+ stat_function(fun = function(x){dgamma(x, 4, 2)}, xlim = c(0,5), geom = &quot;area&quot;, color = clr0d, fill = fll0)+ labs(subtitle = glue(&quot;y {mth(&#39;\\U007E&#39;)} Gamma(*{mth(&#39;\\U03BB&#39;)}*, k)&quot;), x = &quot;dgamma()&quot;) + theme(axis.title.y = element_blank(), plot.subtitle = element_markdown(hjust = .5)) p2 &lt;- ggplot(tibble(x = 0:10, y = dpois(x,lambda = 2.5))) + geom_bar(aes(x = as.factor(x), y = y), stat = &quot;identity&quot;,color = clr0d, fill = fll0)+ labs(subtitle = glue(&quot;y {mth(&#39;\\U007E&#39;)} Poisson(*{mth(&#39;\\U03BB&#39;)}*)&quot;), x = &quot;dpois()&quot;) + theme(axis.title.y = element_blank(), plot.subtitle = element_markdown(hjust = .5)) p3 &lt;- ggplot()+ stat_function(fun = function(x){dnorm(x)}, xlim = c(-3,3), geom = &quot;area&quot;, color = clr0d, fill = fll0)+ labs(subtitle = glue(&quot;y {mth(&#39;\\U007E&#39;)} Normal(*{mth(&#39;\\U03BC&#39;)}*, *{mth(&#39;\\U03C3&#39;)}*)&quot;), x = &quot;dnorm()&quot;) + theme(axis.title.y = element_blank(), plot.subtitle = element_markdown(hjust = .5)) p4 &lt;- ggplot(tibble(x = 0:10, y = dbinom(x,size = 10, prob = .85))) + geom_bar(aes(x = as.factor(x), y = y), stat = &quot;identity&quot;,color = clr0d, fill = fll0)+ labs(subtitle = glue(&quot;y {mth(&#39;\\U007E&#39;)} Binomial(n, p)&quot;), x = &quot;dbino()&quot;) + theme(axis.title.y = element_blank(), plot.subtitle = element_markdown(hjust = .5)) library(geomtextpath) n &lt;- 30 lab01 &lt;- tibble(idx = 1:n, tau = seq(0, pi *.5, length.out = n) %&gt;% rev(), x = sin(tau), y = cos(tau), lab = &quot;sum&quot;) %&gt;% bind_rows(tibble(idx = 1:n, tau = seq(-pi * .3, pi *.3, length.out = n), x = (sin(tau) + .85) * 1.5, y = cos(tau)*2 + 1, lab = &quot;large mean&quot;)) a1 &lt;- ggplot(mapping = aes(x = x, y = y, group = lab )) + geom_path(data = lab01 %&gt;% filter(idx &gt;= n-1), color = clr0d, arrow = arrow(type = &quot;closed&quot;,length = unit(4, &quot;pt&quot;))) + geom_textpath(data = lab01 %&gt;% filter(idx &lt; n), aes(label = lab), color = clr0d, family = fnt_sel, size = 3.5, rich = TRUE, linewidth = .4, hjust = .5) + theme_void() lab02 &lt;- tibble(idx = 1:n, tau = seq(0, pi *.5, length.out = n) %&gt;% rev(), x = sin(tau), y = cos(tau), lab = &quot;count events\\nlow rate&quot;) %&gt;% bind_rows(tibble(idx = 1:n, tau = seq(-pi * .3, pi *.3, length.out = n) %&gt;% rev(), x = (sin(tau) + .85) * 1.5, y = cos(tau)*2 + 1, lab = &quot;low probability many trials&quot;))%&gt;% bind_rows(tibble(idx = 1:n, tau = seq(0, pi *.5, length.out = n) %&gt;% rev(), x = -sin(tau) + 2.5, y = cos(tau), lab = &quot;count events&quot;)) a2 &lt;- ggplot(mapping = aes(x = x, y = y, group = lab )) + geom_path(data = lab02 %&gt;% filter(idx &gt;= n-1), color = clr0d, arrow = arrow(type = &quot;closed&quot;,length = unit(4, &quot;pt&quot;))) + geom_textpath(data = lab02 %&gt;% filter(idx &lt; n), aes(label = lab), color = clr0d, family = fnt_sel, size = 3.5, rich = TRUE, linewidth = .4, hjust = .5) + theme_void()+ scale_y_reverse() layout &lt;- &quot;AFD\\nACD\\nBCE\\nBGE&quot; p1 + p2 + p0 + p3 + p4 + a1 + a2 + plot_layout(design = layout) &amp; theme(axis.text = element_blank()) 11.2.2 Linking linear models to distributions For this, we need a link function - two commonly used ones are: The Logit Link. Consider the model: \\[ \\begin{array}{rclr} y_{i} &amp; \\sim &amp; Binomial(n, p_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{logit}(p_{i}) &amp; = &amp; \\alpha + \\beta x_{i} &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] with the logit function being defined as \\[ \\textrm{logit}(p_{i}) = \\textrm{log} \\frac{p_{i}}{1 - p_{i}} \\] So, the statement here claims that \\[ \\textrm{log} \\frac{p_{i}}{1 - p_{i}} = \\alpha + \\beta x_{i} \\] which can be solved for \\(p_{i}\\) to give \\[ p_{i} = \\frac{\\textrm{exp}(\\alpha + \\beta x_{i})}{1 + \\textrm{exp}(\\alpha + \\beta x_{i})} \\] which is known as the logistic or inverse-logit. alpha &lt;- 0 beta &lt;- 4 lines &lt;- tibble(x = seq(from = -1, to = 1, by = .25)) %&gt;% mutate(`log-odds` = alpha + x * beta, probability = exp(alpha + x * beta) / (1 + exp(alpha + x * beta))) beta &lt;- 2 d &lt;- tibble(x = seq(from = -1.5, to = 1.5, length.out = 50)) %&gt;% mutate(`log-odds` = alpha + x * beta, probability = exp(alpha + x * beta) / (1 + exp(alpha + x * beta))) p1 &lt;- d %&gt;% ggplot(aes(x = x, y = `log-odds`)) + geom_hline(data = lines, aes(yintercept = `log-odds`), color = clr0d, linetype = 3) + geom_line(size = .6, color = clr1) + coord_cartesian(xlim = c(-1, 1)) + theme(panel.grid = element_blank()) p2 &lt;- d %&gt;% ggplot(aes(x = x, y = probability)) + geom_hline(data = lines, aes(yintercept = probability), color = clr0d, linetype = 3) + geom_line(size = .6, color = clr1) + coord_cartesian(xlim = c(-1, 1)) + theme(panel.grid = element_blank()) p1 + p2 The Log Link. Consider the model: \\[ \\begin{array}{rclr} y_{i} &amp; \\sim &amp; Normal(\\mu, \\sigma_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{log}(\\sigma_{i}) &amp; = &amp; \\alpha + \\beta x_{i} &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] Solving this for \\(\\sigma_{i}\\) results in \\[ \\sigma_{i} = \\textrm{exp}(\\alpha + \\beta x_{i}) \\] alpha &lt;- 0 beta &lt;- 2 lines &lt;- tibble(`log-measurement` = -3:3, `original measurement` = exp(-3:3)) d &lt;- tibble(x = seq(from = -1.5, to = 1.5, length.out = 50)) %&gt;% mutate(`log-measurement` = alpha + x * beta, `original measurement` = exp(alpha + x * beta)) p1 &lt;- d %&gt;% ggplot(aes(x = x, y = `log-measurement`)) + geom_hline(data = lines, aes(yintercept = `log-measurement`), color = clr0d, linetype = 3) + geom_line(size = .6, color = clr1) + coord_cartesian(xlim = c(-1, 1)) + theme(panel.grid = element_blank()) p2 &lt;- d %&gt;% ggplot(aes(x = x, y = `original measurement`)) + geom_hline(data = lines, aes(yintercept = `original measurement`), color = clr0d, linetype = 3) + geom_line(size = .6, color = clr1) + scale_y_continuous(position = &quot;right&quot;, limits = c(0, 10)) + coord_cartesian(xlim = c(-1, 1)) + theme(panel.grid = element_blank()) p1 + p2 library(rlang) chapter10_models &lt;- env( ) write_rds(chapter10_models, &quot;envs/chapter10_models.rds&quot;) 11.3 Homework There is no homework in chapter 10 😄. 11.4 {brms} section There are no new model in this chapter that would need to be translated to {brms} 🤷. 11.5 pymc3 section × "],["rethinking-chapter-11.html", "12 Rethinking: Chapter 11 12.1 Binomial Regression 12.2 Poisson Regression 12.3 Multinomial and Categorical Models 12.4 Homework 12.5 {brms} section 12.6 pymc3 section", " 12 Rethinking: Chapter 11 God Spiked the Integers by Richard McElreath, building on the Summaries by Solomon Kurz. 12.1 Binomial Regression 12.1.1 Logistic Regression: Prosocial chimpanzees using a logit link function: library(rethinking) data(chimpanzees) translating the prosoc_left and the condition columns into the aggregated column treatment: prosoc_left == 0 and condition == 0 prosoc_left == 1 and condition == 0 prosoc_left == 0 and condition == 1 prosoc_left == 1 and condition == 1 data_chimp &lt;- chimpanzees %&gt;% as_tibble() %&gt;% mutate(treatment = 1 + prosoc_left + 2 * condition, side_idx = prosoc_left + 1, # right 1, left 2 condition_idx = condition + 1) # no partner 1, partner 2 xtabs(~ treatment + prosoc_left + condition, data_chimp) #&gt; , , condition = 0 #&gt; #&gt; prosoc_left #&gt; treatment 0 1 #&gt; 1 126 0 #&gt; 2 0 126 #&gt; 3 0 0 #&gt; 4 0 0 #&gt; #&gt; , , condition = 1 #&gt; #&gt; prosoc_left #&gt; treatment 0 1 #&gt; 1 0 0 #&gt; 2 0 0 #&gt; 3 126 0 #&gt; 4 0 126 \\[ \\begin{array}{rclr} L_{i} &amp; \\sim &amp; Binomial(1, p_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{logit}(p_{i}) &amp; = &amp; \\alpha_{ACTOR[i]} + \\beta_{TREATMENT[i]} &amp; \\textrm{[linear model]}\\\\ \\alpha_{j} &amp; \\sim &amp; ... &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{k} &amp; \\sim &amp; ... &amp; \\textrm{[$\\beta$ prior]} \\end{array} \\] \\[ Binomial(1, p_{i}) = Bernoulli(p_{i}) \\] Finding the right priors \\[ \\begin{array}{rclr} L_{i} &amp; \\sim &amp; Binomial(1, p_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{logit}(p_{i}) &amp; = &amp; \\alpha &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, \\omega) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\end{array} \\] Starting with \\(\\omega = 10\\) to demonstrate the effect of flat priors. model_omega10 &lt;- quap( flist = alist( pulled_left ~ dbinom( 1, p ), logit(p) &lt;- alpha, alpha ~ dnorm( 0, 10 ) ), data = data_chimp ) model_omega15 &lt;- quap( flist = alist( pulled_left ~ dbinom( 1, p ), logit(p) &lt;- alpha, alpha ~ dnorm( 0, 1.5 ) ), data = data_chimp ) set.seed(1999) omega_prior &lt;- extract.prior(model_omega10) %&gt;% as_tibble() %&gt;% mutate(omega = 10) %&gt;% bind_rows(extract.prior(model_omega15) %&gt;% as_tibble() %&gt;% mutate(omega = 1.5)) %&gt;% mutate(p = inv_logit(alpha)) p1 &lt;- omega_prior %&gt;% ggplot(aes(x = p)) + geom_density(adjust = .15, aes(color = factor(omega), fill = after_scale(clr_alpha(color)))) + scale_color_manual(&quot;omega&quot;, values = c(`10` = clr0d, `1.5` = clr2)) + theme(legend.position = &quot;bottom&quot;) model_beta10 &lt;- quap( flist = alist( pulled_left ~ dbinom( 1, p ), logit(p) &lt;- alpha + beta[treatment], alpha ~ dnorm( 0, 1.5 ), beta[treatment] ~ dnorm(0,10) ), data = data_chimp ) model_beta05 &lt;- quap( flist = alist( pulled_left ~ dbinom( 1, p ), logit(p) &lt;- alpha + beta[treatment], alpha ~ dnorm( 0, 1.5 ), beta[treatment] ~ dnorm(0,0.5) ), data = data_chimp ) extract_logit &lt;- function(model, beta_sd, seed = 42){ set.seed(seed) extract.prior(model, n = 1e4) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% mutate(across(beta.1:beta.4, .fns = function(x){inv_logit(x + alpha)}, .names = &quot;p_{.col}&quot;), beta_sd = beta_sd) } data_beta &lt;- list(model_beta10, model_beta05) %&gt;% map2_dfr(.y = c(10, .5), .f = extract_logit) %&gt;% mutate(treatment_diff_1_2 = abs(`p_beta.1` - `p_beta.2`)) p2 &lt;- data_beta %&gt;% ggplot(aes(x = treatment_diff_1_2)) + geom_density(adjust = .15, aes(color = factor(beta_sd), fill = after_scale(clr_alpha(color)))) + scale_color_manual(&quot;sd beta&quot;, values = c(`10` = clr0d, `0.5` = clr2)) + theme(legend.position = &quot;bottom&quot;) p1 + p2 \\[ \\begin{array}{rclr} L_{i} &amp; \\sim &amp; Binomial(1, p_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{logit}(p_{i}) &amp; = &amp; \\alpha_{ACTOR[i]} + \\beta_{TREATMENT[i]} &amp; \\textrm{[linear model]}\\\\ \\alpha_{j} &amp; \\sim &amp; Normal(0, 1.5) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{k} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta$ prior]} \\end{array} \\] data_chimp_list &lt;- data_chimp %&gt;% dplyr::select(pulled_left, actor, treatment) %&gt;% mutate(treatment = as.integer(treatment)) %&gt;% as.list() model_chimp &lt;- ulam( flist = alist( pulled_left ~ dbinom( 1, p ), logit(p) &lt;- alpha[actor] + beta[treatment], alpha[actor] ~ dnorm( 0, 1.5 ), beta[treatment] ~ dnorm(0,0.5) ), data = data_chimp_list, chains = 4, cores = 4, log_lik = TRUE ) precis(model_chimp, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha[1] -0.46 0.33 -1.01 0.07 734.48 1 alpha[2] 3.89 0.76 2.77 5.18 1003.64 1 alpha[3] -0.76 0.34 -1.31 -0.24 759.77 1 alpha[4] -0.75 0.35 -1.32 -0.21 784.36 1 alpha[5] -0.45 0.33 -0.98 0.07 702.23 1 alpha[6] 0.47 0.33 -0.08 1.00 694.50 1 alpha[7] 1.94 0.43 1.25 2.63 703.21 1 beta[1] -0.04 0.29 -0.48 0.44 662.76 1 beta[2] 0.48 0.29 0.03 0.95 612.98 1 beta[3] -0.37 0.29 -0.82 0.09 701.51 1 beta[4] 0.38 0.29 -0.09 0.84 664.27 1 chimp_posterior &lt;- extract.samples(model_chimp) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% mutate(across(starts_with(&quot;alpha&quot;),.fns = inv_logit, .names = &quot;p_{.col}&quot;)) p1 &lt;- chimp_posterior %&gt;% dplyr::select(starts_with(&quot;p&quot;)) %&gt;% pivot_longer(everything(), names_to = &quot;actor&quot;, values_to = &quot;p&quot;) %&gt;% mutate(actor = str_remove(actor,&quot;p_alpha.&quot;) %&gt;% as.integer()) %&gt;% group_by(actor) %&gt;% summarise(p = list(quantile(p, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) %&gt;% ggplot(aes(y = factor(actor, levels = 7:1))) + geom_linerange(aes(xmin = ll, xmax = hh), lwd = .2, color = clr2) + geom_linerange(aes(xmin = l, xmax = h), lwd = .7, color = clr2) + geom_point(aes(x = m), size = 2, shape = 21, color = clr2, fill = clr_lighten(clr2, .2)) + labs(y = &quot;actor&quot;, x = &quot;alpha&quot;) + coord_cartesian(xlim = c(0, 1)) treatment_labels &lt;- c(&quot;R|N&quot;, &quot;L|N&quot;, &quot;R|P&quot;, &quot;L|P&quot;, &quot;R|diff&quot;, &quot;L|diff&quot;) p2 &lt;- chimp_posterior %&gt;% dplyr::select(starts_with(&quot;beta&quot;)) %&gt;% mutate(`beta.5` = `beta.1` - `beta.3` , `beta.6` = `beta.2` - `beta.4` ) %&gt;% pivot_longer(everything(), names_to = &quot;param&quot;, values_to = &quot;p&quot;) %&gt;% mutate(treatment = treatment_labels[str_remove(param, &quot;beta.&quot;) %&gt;% as.integer()]) %&gt;% group_by(treatment) %&gt;% summarise(p = list(quantile(p, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) %&gt;% ggplot(aes(y = factor(treatment, levels = rev(treatment_labels)))) + geom_rect(data = tibble(x = 1),inherit.aes = FALSE, aes(xmin = -Inf, xmax = Inf, ymin = .5, ymax = 2.5), color = clr0d, fill = fll0) + geom_vline(xintercept = 0, linetype = 3, color = clr_dark) + geom_linerange(aes(xmin = ll, xmax = hh), lwd = .2, color = clr2) + geom_linerange(aes(xmin = l, xmax = h), lwd = .7, color = clr2) + geom_point(aes(x = m), size = 2, shape = 21, color = clr2, fill = clr_lighten(clr2, .2)) + scale_y_discrete() + labs(y = &quot;treatment&quot;, x = &quot;beta&quot;) p1 + p2 Posterior prediction plots: chimp_grid &lt;- crossing(actor = 1:7, treatment = 1:4) chimp_posterior_predictions &lt;- link(model_chimp, data = chimp_grid) %&gt;% as.matrix() %&gt;% t() %&gt;% as_tibble() %&gt;% bind_cols(chimp_grid, .) %&gt;% pivot_longer(-c(actor, treatment), values_to = &quot;pulled_left&quot;) %&gt;% dplyr::select(-name) %&gt;% group_by(actor, treatment) %&gt;% summarise(p = list(quantile(pulled_left, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) %&gt;% mutate(type = &quot;post. pred.&quot;, side = c(&quot;L&quot;, &quot;R&quot;)[2 - (treatment == 1 | treatment == 3)], condition = c(&quot;N&quot;, &quot;P&quot;)[1 + (treatment &gt; 2)], lab = treatment_labels[treatment]) data_chimp %&gt;% group_by(actor, treatment) %&gt;% summarise(mean_data = mean(pulled_left), type = &quot;data&quot;) %&gt;% mutate(side = c(&quot;L&quot;, &quot;R&quot;)[2 - (treatment == 1 | treatment == 3)], condition = c(&quot;N&quot;, &quot;P&quot;)[1 + (treatment &gt; 2)], lab = treatment_labels[treatment]) %&gt;% ggplot(aes(x = treatment, y = mean_data)) + geom_hline(yintercept = .5, linetype = 3, color = clr_dark) + geom_line(aes(group = side), color = clr0dd) + geom_point(aes(shape = condition), color = clr0dd, fill = clr0, size = 1.8) + geom_text(data = . %&gt;% filter(actor == 1), aes(y = mean_data - .2 * (1.5 - as.numeric(factor(side))), label = lab), family = fnt_sel) + geom_line(data = chimp_posterior_predictions, aes(y = m, group = side), color = clr0dd) + geom_segment(data = chimp_posterior_predictions, inherit.aes = FALSE, aes(x = treatment, xend = treatment, y = ll, yend = hh), color = clr0dd) + geom_point(data = chimp_posterior_predictions, inherit.aes = FALSE, aes(x = treatment, y = m, shape = condition), color = clr0dd, fill = clr0, size = 1.8) + scale_shape_manual(values = c(&quot;N&quot; = 21, &quot;P&quot; = 19), guide = &quot;none&quot;) + facet_grid(type ~ actor) + scale_x_discrete(expand = c(.2,.2)) + labs(x = NULL, y = &quot;pulled_left&quot;) + lims(y = c(0,1)) + theme(panel.background = element_rect(color = clr0, fill = &quot;transparent&quot;)) data_chimp_list2 &lt;- data_chimp %&gt;% dplyr::select(pulled_left, actor, side_idx, condition_idx) model_chimp_interaction &lt;- ulam( flist = alist( pulled_left ~ dbinom( 1, p ), logit(p) &lt;- alpha[actor] + beta_s[side_idx] + beta_c[condition_idx], alpha[actor] ~ dnorm( 0, 1.5 ), beta_s[side_idx] ~ dnorm(0,0.5), beta_c[condition_idx] ~ dnorm(0,0.5) ), data = data_chimp_list2, chains = 4, cores = 4, log_lik = TRUE ) compare(model_chimp, model_chimp_interaction, func = PSIS) %&gt;% knit_precis(param_name = &quot;model&quot;) model PSIS SE dPSIS dSE pPSIS weight model_chimp_interaction 530.71 19.17 0.00 NA 7.76 0.66 model_chimp 532.02 18.89 1.31 1.35 8.35 0.34 chimp_grid2 &lt;- crossing(actor = 1:7, side_idx = 1:2, condition_idx = 1:2) chimp_interact_posterior_predictions &lt;- link(model_chimp_interaction, data = chimp_grid2) %&gt;% as.matrix() %&gt;% t() %&gt;% as_tibble() %&gt;% bind_cols(chimp_grid2, .) %&gt;% pivot_longer(-c(actor, side_idx, condition_idx), values_to = &quot;pulled_left&quot;) %&gt;% dplyr::select(-name) %&gt;% group_by(actor, side_idx, condition_idx) %&gt;% summarise(p = list(quantile(pulled_left, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) %&gt;% mutate(treatment = factor(side_idx + 2 * (condition_idx - 1))) chimp_interact_posterior_predictions %&gt;% ggplot(aes(x = treatment, y = m)) + geom_hline(yintercept = .5, linetype = 3, color = clr_dark) + geom_line(aes(y = m, group = factor(side_idx)), color = clr0dd) + geom_segment(data = chimp_posterior_predictions, aes(x = treatment, xend = treatment, y = ll, yend = hh), color = clr0dd) + geom_point(aes(x = treatment, y = m, shape = factor(condition_idx)), color = clr0dd, fill = clr0, size = 1.8) + scale_shape_manual(values = c(`1` = 21, `2` = 19), guide = &quot;none&quot;) + facet_grid(. ~ actor) + scale_x_discrete(expand = c(.2,.2)) + labs(x = NULL, y = &quot;pulled_left&quot;) + lims(y = c(0,1)) + theme(axis.text.x = element_blank(), panel.background = element_rect(color = clr0, fill = &quot;transparent&quot;), panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) Adding log-probability calculations to a Stan model stan_posterior &lt;- extract.samples(model_chimp, clean = FALSE) str(stan_posterior) #&gt; List of 5 #&gt; $ alpha : num [1:2000, 1:7] -0.535 -0.495 -0.276 -0.46 -0.967 ... #&gt; $ beta : num [1:2000, 1:4] -0.2448 -0.4169 0.1607 -0.0796 0.4323 ... #&gt; $ log_lik: num [1:2000, 1:504] -0.377 -0.338 -0.637 -0.459 -0.461 ... #&gt; $ p : num [1:2000, 1:504] 0.314 0.287 0.471 0.368 0.369 ... #&gt; $ lp__ : num [1:2000(1d)] -265 -272 -266 -267 -271 ... #&gt; - attr(*, &quot;source&quot;)= chr &quot;ulam posterior: 2000 samples from object&quot; model_chimp_stancode &lt;- stancode(model_chimp) #&gt; data{ #&gt; int pulled_left[504]; #&gt; int treatment[504]; #&gt; int actor[504]; #&gt; } #&gt; parameters{ #&gt; vector[7] alpha; #&gt; vector[4] beta; #&gt; } #&gt; model{ #&gt; vector[504] p; #&gt; beta ~ normal( 0 , 0.5 ); #&gt; alpha ~ normal( 0 , 1.5 ); #&gt; for ( i in 1:504 ) { #&gt; p[i] = alpha[actor[i]] + beta[treatment[i]]; #&gt; p[i] = inv_logit(p[i]); #&gt; } #&gt; pulled_left ~ binomial( 1 , p ); #&gt; } #&gt; generated quantities{ #&gt; vector[504] log_lik; #&gt; vector[504] p; #&gt; for ( i in 1:504 ) { #&gt; p[i] = alpha[actor[i]] + beta[treatment[i]]; #&gt; p[i] = inv_logit(p[i]); #&gt; } #&gt; for ( i in 1:504 ) log_lik[i] = binomial_lpmf( pulled_left[i] | 1 , p[i] ); #&gt; } model_chimp_stan &lt;- stan(model_code = model_chimp_stancode, data = data_chimp_list, chains = 4, cores = 4) compare(model_chimp, model_chimp_stan) %&gt;% knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight model_chimp_stan 531.67 18.92 0.0 NA 8.21 0.54 model_chimp 531.97 18.87 0.3 0.13 8.32 0.46 12.1.2 Relative Shark and Absolute Deer Calculating the proportional odds relative effect size of switching from treatment two to four (adding a partner): (prop_odd &lt;- extract.samples(model_chimp) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% mutate(prop_odds = exp( `beta.4` - `beta.2`)) %&gt;% summarise(average = mean(prop_odds))) #&gt; # A tibble: 1 × 1 #&gt; average #&gt; &lt;dbl&gt; #&gt; 1 0.931 So this relates to a 6.88 % change in odds. 12.1.3 Aggregated binomial: Chimpanzees again, condensed data_chimp_aggregated &lt;- data_chimp %&gt;% group_by(treatment, actor, side_idx, condition_idx) %&gt;% summarise(left_pulls = sum(pulled_left)) %&gt;% ungroup() %&gt;% mutate(across(everything(), .fns = `as.integer`)) model_chimp_aggregated &lt;- ulam( flist = alist( left_pulls ~ dbinom( 18 , p ), logit(p) &lt;- alpha[actor] + beta[treatment], alpha[actor] ~ dnorm(0, 1.5), beta[treatment] ~ dnorm(0, 0.5) ), data = data_chimp_aggregated, chains = 4, cores = 4, log_lik = TRUE ) precis(model_chimp_aggregated, depth = 2) %&gt;% knit_precis(param_name = &quot;model&quot;) model mean sd 5.5% 94.5% n_eff Rhat4 alpha[1] -0.46 0.32 -0.97 0.07 795.24 1.01 alpha[2] 3.87 0.72 2.82 5.10 1070.21 1.00 alpha[3] -0.75 0.33 -1.27 -0.24 736.57 1.01 alpha[4] -0.75 0.33 -1.28 -0.24 741.50 1.01 alpha[5] -0.46 0.32 -0.96 0.05 764.71 1.00 alpha[6] 0.47 0.33 -0.05 0.98 831.35 1.00 alpha[7] 1.96 0.41 1.32 2.62 1041.00 1.01 beta[1] -0.03 0.28 -0.49 0.39 657.78 1.01 beta[2] 0.48 0.27 0.04 0.94 585.93 1.01 beta[3] -0.38 0.28 -0.80 0.06 719.18 1.01 beta[4] 0.38 0.27 -0.07 0.81 683.31 1.00 clr_current &lt;- clr1 chimp_posterior_aggregated &lt;- extract.samples(model_chimp_aggregated) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% mutate(across(starts_with(&quot;alpha&quot;),.fns = inv_logit, .names = &quot;p_{.col}&quot;)) p1 &lt;- chimp_posterior_aggregated %&gt;% dplyr::select(starts_with(&quot;p&quot;)) %&gt;% pivot_longer(everything(), names_to = &quot;actor&quot;, values_to = &quot;p&quot;) %&gt;% mutate(actor = str_remove(actor,&quot;p_alpha.&quot;) %&gt;% as.integer()) %&gt;% group_by(actor) %&gt;% summarise(p = list(quantile(p, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) %&gt;% ggplot(aes(y = factor(actor, levels = 7:1))) + geom_linerange(aes(xmin = ll, xmax = hh), lwd = .2, color = clr_current) + geom_linerange(aes(xmin = l, xmax = h), lwd = .7, color = clr_current) + geom_point(aes(x = m), size = 2, shape = 21, color = clr_current, fill = clr_lighten(clr_current, .2)) + labs(y = &quot;actor&quot;, x = &quot;alpha&quot;) + coord_cartesian(xlim = c(0, 1)) p2 &lt;- chimp_posterior_aggregated %&gt;% dplyr::select(starts_with(&quot;beta&quot;)) %&gt;% mutate(`beta.5` = `beta.1` - `beta.3` , `beta.6` = `beta.2` - `beta.4` ) %&gt;% pivot_longer(everything(), names_to = &quot;param&quot;, values_to = &quot;p&quot;) %&gt;% mutate(treatment = treatment_labels[str_remove(param, &quot;beta.&quot;) %&gt;% as.integer()]) %&gt;% group_by(treatment) %&gt;% summarise(p = list(quantile(p, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) %&gt;% ggplot(aes(y = factor(treatment, levels = rev(treatment_labels)))) + geom_rect(data = tibble(x = 1), inherit.aes = FALSE, aes(xmin = -Inf, xmax = Inf, ymin = .5, ymax = 2.5), color = clr0d, fill = fll0) + geom_vline(xintercept = 0, linetype = 3, color = clr_dark) + geom_linerange(aes(xmin = ll, xmax = hh), lwd = .2, color = clr_current) + geom_linerange(aes(xmin = l, xmax = h), lwd = .7, color = clr_current) + geom_point(aes(x = m), size = 2, shape = 21, color = clr_current, fill = clr_lighten(clr_current, .2)) + scale_y_discrete() + labs(y = &quot;treatment&quot;, x = &quot;beta&quot;) p1 + p2 compare(model_chimp, model_chimp_aggregated, func = PSIS) %&gt;% knit_precis(param_name = &quot;model&quot;) model PSIS SE dPSIS dSE pPSIS weight model_chimp_aggregated 113.92 8.40 0.00 NA 8.25 1 model_chimp 532.02 18.89 418.11 39.99 8.35 0 The huge difference in PSIS are due to the way the data is organized, which makes the multiplicity effect the aggregate model (eg. all they ways 6 successes in 9 trials can be arranged). #&gt; deviance of aggregated 6 in 9 -2 * dbinom(x = 6 , size = 9, prob = .2, log = TRUE) #&gt; [1] 11.79048 #&gt; deviance of dis-aggregated -2 * sum( dbern(x = rep(1:0, c(6, 3)), prob = .2, log = TRUE) ) #&gt; [1] 20.65212 Some Pareto k values are high (&gt;0.5). Set pointwise=TRUE to inspect individual points. . Warning messages: 1: In compare(model_chimp, model_chimp_aggregated, func = PSIS) : Different numbers of observations found for at least two models. Model comparison is valid only for models fit to exactly the same observations. Number of observations for each model: model_chimp 504 model_chimp_aggregated 28 The high Pareto k values result from the fact that the aggregation turns the leave one out procedure into a leave 18 out procedure here. If you want to calculate WAIC or PSIS, you should use a logistic regression data format, not an aggregated format. 12.1.4 Aggregated binomial: Graduate School Admissions data(UCBadmit) data_ucb &lt;- UCBadmit %&gt;% as_tibble() %&gt;% mutate(gid = 3L - as.integer(factor(applicant.gender))) \\[ \\begin{array}{rclr} A_{i} &amp; \\sim &amp; Binomial(N_{i}, p_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{logit}(p_{i}) &amp; = &amp; \\alpha_{GID[i]} &amp; \\textrm{[linear model]}\\\\ \\alpha_{j} &amp; \\sim &amp; Normal(0, 1.5) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\end{array} \\] data_ucb_list &lt;- data_ucb %&gt;% dplyr::select(admit, applications, gid) %&gt;% as.list() model_ucb &lt;- ulam( flist = alist( admit ~ dbinom( applications, p ), logit(p) &lt;- alpha[gid], alpha[gid] ~ dnorm( 0, 1.5 ) ), data = data_ucb_list, chains = 4, cores = 4, log_lik = TRUE ) precis(model_ucb, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha[1] -0.22 0.04 -0.28 -0.16 1509.90 1 alpha[2] -0.83 0.05 -0.91 -0.75 1477.76 1 Calculating a contrast to quantify the difference in the posterior between ♀️ and ♂️ applicants. posterior_ucb &lt;- extract.samples(model_ucb) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% mutate(diff_absolute = `alpha.1` - `alpha.2`, diff_relative = inv_logit(`alpha.1`) - inv_logit(`alpha.2`)) clr_current &lt;- clr2 precis(posterior_ucb) %&gt;% knit_precis() param mean sd 5.5% 94.5% histogram alpha.1 -0.22 0.04 -0.28 -0.16 ▁▁▂▃▇▇▇▅▃▁▁▁▁ alpha.2 -0.83 0.05 -0.91 -0.75 ▁▁▁▅▇▃▁▁▁ diff_absolute 0.61 0.06 0.51 0.71 ▁▁▁▂▇▇▅▁▁▁ diff_relative 0.14 0.01 0.12 0.16 ▁▁▁▁▃▇▇▅▂▁▁▁ Notice the discrepancy, between the model and the data: the model expects ♀️ applicants to do 14% worse than ♂️, however that is only the case in two departments (C and E). ucb_grid &lt;- crossing(dept = factor(LETTERS[1:6]), gid = 1:2) %&gt;% left_join(data_ucb %&gt;% dplyr::select(dept, gid, applications)) ucb_posterior_predictions &lt;- sim(model_ucb, data = ucb_grid) %&gt;% as.matrix() %&gt;% t() %&gt;% as_tibble() %&gt;% bind_cols(ucb_grid, .) %&gt;% pivot_longer(-c(dept, gid, applications), values_to = &quot;admit&quot;) %&gt;% dplyr::select(-name) %&gt;% group_by(dept, gid, applications) %&gt;% summarise(p = quantile(admit/applications, probs = c(.055, .25, .5, .75, .955)), median = median(admit), breaks = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;)) %&gt;% ungroup() %&gt;% pivot_wider(names_from = breaks, values_from = p) %&gt;% mutate(type = &quot;post. pred.&quot;, x = 2 * (as.integer(as.factor(dept)) - 1) + gid) library(geomtextpath) data_ucb %&gt;% group_by(dept, gid) %&gt;% summarise(mean_data = mean(admit/applications), type = &quot;data&quot;) %&gt;% mutate(x = 2 * (as.integer(dept) - 1) + gid) %&gt;% ggplot(aes(x = x, y = mean_data)) + geom_segment(data = ucb_posterior_predictions, aes(x = x, xend = x, y = ll, yend = hh), color = clr_current) + geom_point(data = ucb_posterior_predictions, aes(y = m, shape = factor(gid)), color = clr_current, fill = clr_lighten(clr_current), size = 1.8) + geom_textline(aes(label = dept, group = dept), color = clr0dd, family = fnt_sel) + geom_point(aes(shape = factor(gid)), color = clr0dd, fill = clr0, size = 1.8) + scale_shape_manual(values = c(`2` = 21, `1` = 19), guide = &quot;none&quot;) + scale_x_continuous(breaks = 2 * (1:6) - .5, labels = LETTERS[1:6])+ labs(x = &quot;department&quot;, y = &quot;admit&quot;) + lims(y = c(0,1)) + theme(panel.background = element_rect(color = clr0, fill = &quot;transparent&quot;), panel.grid.major.x = element_blank()) \\[ \\begin{array}{rclr} A_{i} &amp; \\sim &amp; Binomial(N_{i}, p_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{logit}(p_{i}) &amp; = &amp; \\alpha_{GID[i]} + \\delta_{DEPT[i]}&amp; \\textrm{[linear model]}\\\\ \\alpha_{j} &amp; \\sim &amp; Normal(0, 1.5) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\delta_{k} &amp; \\sim &amp; Normal(0, 1.5) &amp; \\textrm{[$\\delta$ prior]}\\\\ \\end{array} \\] data_ucb_list2 &lt;- data_ucb %&gt;% mutate(dept_idx = as.numeric(dept)) %&gt;% dplyr::select(dept_idx, admit, applications, gid) %&gt;% as.list() model_ucb_dept &lt;- ulam( flist = alist( admit ~ dbinom( applications, p ), logit(p) &lt;- alpha[gid] + delta[dept_idx], alpha[gid] ~ dnorm( 0, 1.5 ), delta[dept_idx] ~ dnorm( 0, 1.5 ) ), data = data_ucb_list2, iter = 4000, chains = 4, cores = 4, log_lik = TRUE ) precis(model_ucb_dept, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha[1] -0.53 0.51 -1.36 0.30 735.71 1.00 alpha[2] -0.43 0.52 -1.27 0.39 733.86 1.01 delta[1] 1.11 0.52 0.28 1.94 742.15 1.00 delta[2] 1.06 0.52 0.23 1.90 744.28 1.01 delta[3] -0.15 0.52 -0.99 0.69 736.76 1.01 delta[4] -0.18 0.52 -1.02 0.65 734.53 1.01 delta[5] -0.63 0.52 -1.47 0.21 748.80 1.01 delta[6] -2.18 0.53 -3.04 -1.32 772.62 1.01 posterior_ucb_dept &lt;- extract.samples(model_ucb_dept) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% mutate(diff_absolute = `alpha.1` - `alpha.2`, diff_relative = inv_logit(`alpha.1`) - inv_logit(`alpha.2`)) precis(posterior_ucb_dept %&gt;% dplyr::select(starts_with(&quot;diff&quot;))) %&gt;% knit_precis(param_name = &quot;column&quot;) column mean sd 5.5% 94.5% histogram diff_absolute -0.10 0.08 -0.23 0.03 ▁▁▁▂▅▇▇▅▂▁▁▁▁ diff_relative -0.02 0.02 -0.05 0.01 ▁▁▁▁▂▃▅▇▇▅▂▁▁▁▁ Now, ♂️ applicants seem to have minimal worse chances than ♀️ applicants (~2%). Why did adding departments to the model change inference about the gender so much? data_ucb %&gt;% group_by(dept) %&gt;% mutate(all_n = sum(applications)) %&gt;% ungroup() %&gt;% mutate(fraction = round(applications / all_n, digits = 2)) %&gt;% dplyr::select(dept, applicant.gender, fraction) %&gt;% pivot_wider(names_from = dept, values_from = fraction) #&gt; # A tibble: 2 × 7 #&gt; applicant.gender A B C D E F #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 male 0.88 0.96 0.35 0.53 0.33 0.52 #&gt; 2 female 0.12 0.04 0.65 0.47 0.67 0.48 There is a causal path \\(G \\rightarrow D \\rightarrow A\\) which is closed by conditioning on \\(D\\) (this is an example of a mediation analysis). However issues arise if there is an unobserved confound \\(U\\). dag_ucb &lt;- dagify( A ~ D + G, D ~ G, exposure = &quot;A&quot;, outcome = &quot;G&quot;, coords = tibble(name = c(&quot;G&quot;, &quot;D&quot;, &quot;A&quot;), x = c(0, .5, 1), y = c(0, 1, 0))) dag_ucb_u &lt;- dagify( A ~ D + G + U, D ~ G + U, exposure = &quot;A&quot;, outcome = &quot;G&quot;, coords = tibble(name = c(&quot;G&quot;, &quot;D&quot;, &quot;A&quot;, &quot;U&quot;), x = c(0, .5, 1, 1), y = c(0, 1, 0, 1))) p1 &lt;- dag_ucb %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;A&quot;, &quot;response&quot;, if_else(name %in% c(&quot;G&quot;, &quot;D&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr_current) p2 &lt;- dag_ucb_u %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;A&quot;, &quot;response&quot;, if_else(name %in% c(&quot;G&quot;, &quot;D&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr_current) p1 + p2 &amp; coord_fixed(ratio = .5) &amp; scale_y_continuous(limits = c(-.1, 1.1)) &amp; scale_x_continuous(limits = c(-.1, 1.1)) &amp; theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) as_draws_df(model_ucb_dept@stanfit) %&gt;% as_tibble() %&gt;% dplyr::select(starts_with(&quot;alpha&quot;), starts_with(&quot;delta&quot;)) %&gt;% ggpairs( lower = list(continuous = wrap(my_lower)), diag = list(continuous = wrap(my_diag, fill = fll0, color = clr0d, adjust = .7)), upper = list(continuous = wrap(my_upper , size = 5, color = &quot;black&quot;, family = fnt_sel)) ) + theme(panel.border = element_rect(color = clr_dark, fill = &quot;transparent&quot;)) 12.2 Poisson Regression y &lt;- rbinom( 1e5, 1000, 1/1000) c(mean(y), var(y)) #&gt; [1] 0.9969100 0.9949704 Models build upon the poisson distribution (for uncountable, or uncoutably large number of trials \\(N\\)), are even simpler than binomial or Gaussian model, because there is only a single parameter that describes its shape: \\[ y_{i} \\sim \\] The conventional link function for a Poisson Model is the log link: \\[ \\begin{array}{rclr} y_{i} &amp; \\sim &amp; Poisson(\\lambda_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{log}(\\lambda_{i}) &amp; = &amp; \\alpha + \\beta(x_{i} - \\bar{x}) &amp; \\textrm{[linear model]} \\end{array} \\] 12.2.1 Oceanic Tool Complexity library(sf) data(Kline) data_kline &lt;- Kline %&gt;% as_tibble() %&gt;% mutate(pop_log_scl = scale(log(population))[,1], contact_idx = 3L - as.integer(factor(contact))) data_kline_sf &lt;- read_sf(&quot;data/pacific.gpkg&quot;) %&gt;% left_join(Kline) %&gt;% mutate(coords = st_centroid(geom) %&gt;% st_coordinates() %&gt;% as_tibble() ) %&gt;% mutate(x = coords$X, y = coords$Y) %&gt;% dplyr::select(-coords) library(rnaturalearth) crs_pac &lt;- 3832 long_lim &lt;- c(100,-140) lat_lim &lt;- c(-25, 25) bb &lt;- tibble(x = long_lim[c(1,1,2,2,1)], y = lat_lim[c(1,2,2,1,1)]) %&gt;% st_as_sf(coords = c(&quot;x&quot;,&quot;y&quot;), crs = 4326) %&gt;% st_transform(crs = crs_pac) %&gt;% st_combine() %&gt;% st_cast(&quot;POLYGON&quot;) dateline &lt;- tibble(x = c(180,180), y = lat_lim) %&gt;% st_as_sf(coords = c(&quot;x&quot;,&quot;y&quot;), crs = 4326) %&gt;% st_transform(crs = crs_pac) %&gt;% st_combine() %&gt;% st_cast(&quot;LINESTRING&quot;) equator &lt;- tibble(x = long_lim, y = c(0, 0)) %&gt;% st_as_sf(coords = c(&quot;x&quot;,&quot;y&quot;), crs = 4326) %&gt;% st_transform(crs = crs_pac) %&gt;% st_combine() %&gt;% st_cast(&quot;LINESTRING&quot;) coast &lt;- read_sf(&quot;~/work/geo_store/natural_earth/ne_110m_land.shp&quot;) %&gt;% st_transform(crs = crs_pac) %&gt;% st_intersection(bb) library(raster) library(ggspatial) # library(marmap) # bat_l &lt;- marmap::getNOAA.bathy(long_lim[1], 180, # lat_lim[1], lat_lim[2], resolution = 5) # # bat_r &lt;- marmap::getNOAA.bathy(-180, long_lim[2], # lat_lim[1], lat_lim[2], resolution = 5) # # bat_l_proj &lt;- projectRaster(marmap::as.raster(bat_l), crs = crs(bb)) # bat_r_proj &lt;- projectRaster(marmap::as.raster(bat_r), crs = crs(bb)) # bat &lt;- mosaic(bat_l_proj, bat_r_proj,tolerance = 0.1, fun = mean) bat &lt;- raster(&quot;data/pacific.tif&quot;) %&gt;% raster::crop(y = as_Spatial(bb)) %&gt;% raster::mask(mask = as_Spatial(bb)) ggplot() + geom_raster(data = df_spatial(bat), aes(x = x, y = y, fill = band1)) + geom_sf(data = bb, fill = &quot;transparent&quot;, color = clr0d) + geom_sf(data = dateline, size = .3, linetype = 3, color = clr_dark) + geom_sf(data = equator, size = .3, linetype = 3, color = clr_dark) + geom_sf(data = coast, color = clr0dd, fill = clr0, size = .2) + geom_sf(data = data_kline_sf, color = clr_current, fill = clr_lighten(clr_current), size = .4) + ggrepel::geom_text_repel(data = data_kline_sf, aes(x = x, y = y, label = culture), family = fnt_sel) + annotation_north_arrow(style = north_arrow_fancy_orienteering(text_family = fnt_sel)) + annotation_scale(style = &quot;ticks&quot;, text_family = fnt_sel) + scale_fill_gradientn(colours = c(&quot;white&quot;, &quot;black&quot;) %&gt;% clr_lighten(.35), guide = &quot;none&quot;) + coord_sf(crs = crs_pac, expand = 0) + theme(panel.grid = element_blank(), axis.title = element_blank()) data_kline #&gt; # A tibble: 10 × 7 #&gt; culture population contact total_tools mean_TU pop_log_scl contact_idx #&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 Malekula 1100 low 13 3.2 -1.29 1 #&gt; 2 Tikopia 1500 low 22 4.7 -1.09 1 #&gt; 3 Santa Cruz 3600 low 24 4 -0.516 1 #&gt; 4 Yap 4791 high 43 5 -0.329 2 #&gt; 5 Lau Fiji 7400 high 33 5 -0.0443 2 #&gt; 6 Trobriand 8000 high 19 4 0.00667 2 #&gt; 7 Chuuk 9200 high 40 3.8 0.0981 2 #&gt; 8 Manus 13000 low 28 6.6 0.324 1 #&gt; 9 Tonga 17500 high 55 5.4 0.519 2 #&gt; 10 Hawaii 275000 low 71 6.6 2.32 1 modeling the idea that: the number of tools increases with the log population size the number of tools increases with the contact rate among islands the impact of population on tool counts is moderated by high contact (looking for a positive interaction) \\[ \\begin{array}{rclr} T_{i} &amp; \\sim &amp; Poisson(\\lambda_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{log}(\\lambda_{i}) &amp; = &amp; \\alpha_{CID[i]} + \\beta_{CID[i]}~\\textrm{log}~P_{i}&amp; \\textrm{[linear model]}\\\\ \\alpha_{j}&amp; \\sim &amp; ... &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{j}&amp; \\sim &amp; ... &amp; \\textrm{[$\\beta$ prior]}\\\\ \\end{array} \\] Prior considerations (first for \\(\\alpha\\)): \\[ \\begin{array}{rclr} T_{i} &amp; \\sim &amp; Poisson(\\lambda_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{log}(\\lambda_{i}) &amp; = &amp; \\alpha &amp; \\textrm{[linear model]}\\\\ \\alpha&amp; \\sim &amp; Normal( 0, 10 ) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\end{array} \\] ggplot() + stat_function(fun = function(x){dlnorm(x, 3, .5)}, xlim = c(0, 100), n = 201, geom = &quot;area&quot;, color = clr_current, fill = fll_current())+ stat_function(fun = function(x){dlnorm(x, 0, 10)}, xlim = c(0, 100), n = 201, geom = &quot;area&quot;, color = clr0d, fill = fll0) + geom_richtext(data = tibble(x = c(12.5, 37.5), y = c(.065, .035), lab = c(glue(&quot;*{mth(&#39;\\U03B1&#39;)}* {mth(&#39;\\U007E&#39;)} dnorm(0, 10)&quot;), glue(&quot;*{mth(&#39;\\U03B1&#39;)}* {mth(&#39;\\U007E&#39;)} dnorm(3, 0.5)&quot;)), prior = c(&quot;wide&quot;, &quot;narrow&quot;)), aes(x = x, y = y, color = prior, label = lab ), hjust = -.05, size = 5, fill = NA, label.color = NA, label.padding = grid::unit(rep(0, 4), &quot;pt&quot;), family = fnt_sel) + scale_color_manual(values = c(narrow = clr_current, wide = clr0dd), guide = &quot;none&quot;) + labs(x = &quot;mean number of tools&quot;, y = &quot;density&quot;) Mean of the narrow curve (\\(exp(\\mu + \\sigma^2 / 2)\\)): alpha &lt;- rnorm(1e4, 0, 10) lambda &lt;- exp(alpha) mean(lambda) #&gt; [1] 7.778822e+13 tibble(prior = c( &quot;wide&quot;, &quot;narrow&quot;), mean = c(exp(0 + 10 ^2 /2), exp(3 + .5 ^2 /2))) #&gt; # A tibble: 2 × 2 #&gt; prior mean #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 wide 5.18e21 #&gt; 2 narrow 2.28e 1 mean_weight &lt;- function(mu, sigma){exp(mu + sigma ^2 /2)} (prior_tab &lt;- crossing(mu = c(0, 1.5, 3, 4), sigma = c(5, 1, .5, .1)) %&gt;% mutate(RN = row_number(), mean = map2_dbl(mu, sigma, mean_weight)) ) #&gt; # A tibble: 16 × 4 #&gt; mu sigma RN mean #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 0 0.1 1 1.01 #&gt; 2 0 0.5 2 1.13 #&gt; 3 0 1 3 1.65 #&gt; 4 0 5 4 268337. #&gt; 5 1.5 0.1 5 4.50 #&gt; 6 1.5 0.5 6 5.08 #&gt; 7 1.5 1 7 7.39 #&gt; 8 1.5 5 8 1202604. #&gt; 9 3 0.1 9 20.2 #&gt; 10 3 0.5 10 22.8 #&gt; 11 3 1 11 33.1 #&gt; 12 3 5 12 5389698. #&gt; 13 4 0.1 13 54.9 #&gt; 14 4 0.5 14 61.9 #&gt; 15 4 1 15 90.0 #&gt; 16 4 5 16 14650719. ggplot() + (pmap(prior_tab, .f = function(mu, sigma,...){ force(mu) force(sigma) return(geom_function(data = tibble(#x = 0:150, mu = mu), fun = function(x, mu, sigma,...){ dlnorm(x, mu, sigma)}, aes(color = factor(mu)), xlim = c(0, 150), n = 201, args = list(mu = mu, sigma = sigma))) })) + facet_wrap(mu ~ ., scales = &quot;free_y&quot;, labeller = label_both) + scale_color_manual(values = c(`0` = clr0d, `1.5` = clr1, `3` = clr2, `4` = clr3), guide = &quot;none&quot;) + labs(x = &quot;mean number of tools&quot;, y = &quot;density&quot;) Now for the \\(\\beta\\) prior: n &lt;- 100 label_strp &lt;- function(prior){glue(&quot;beta {mth(&#39;\\U007E&#39;)} Normal({str_remove(prior,&#39;b&#39;)})&quot;)} tibble(idx = 1:n, a = rnorm(n, mean = 3, sd = 0.5)) %&gt;% mutate(`b0, 10` = rnorm(n, mean = 0 , sd = 10), `b0, 0.2` = rnorm(n, mean = 0 , sd = 0.2)) %&gt;% pivot_longer(contains(&quot;b&quot;), values_to = &quot;b&quot;, names_to = &quot;prior&quot;) %&gt;% mutate(prior = label_strp(prior), prior = factor(prior, levels = label_strp(c(&quot;b0, 10&quot;, &quot;b0, 0.2&quot;))) ) %&gt;% expand(nesting(idx, a, b, prior), x = seq(from = -2, to = 2, length.out = 100)) %&gt;% ggplot(aes(x = x, y = exp(a + b * x), group = idx)) + geom_line(size = .2, color = clr_alpha(clr_current, .25)) + labs(x = &quot;log population (std)&quot;, y = &quot;total tools&quot;) + coord_cartesian(ylim = c(0, 100)) + facet_wrap(~ prior) + theme(strip.text = element_markdown()) prior &lt;- tibble(idx = 1:n, a = rnorm(n, mean = 3, sd = 0.5), b = rnorm(n, mean = 0, sd = 0.2)) %&gt;% expand(nesting(idx, a, b), x = seq(from = log(100), to = log(200000), length.out = 100)) p1 &lt;- prior %&gt;% ggplot(aes(x = x, y = exp(a + b * x), group = idx)) + geom_line(size = .2, color = clr_alpha(clr_current, .25)) + geom_hline(yintercept = 100, linetype = 3, size = .6, color = clr_dark) + labs(subtitle = glue(&quot;alpha {mth(&#39;\\U007E&#39;)} Normal(3, 0.5); beta {mth(&#39;\\U007E&#39;)} Normal(0, 0.2)&quot;), x = &quot;log population&quot;, y = &quot;total tools&quot;) + coord_cartesian(xlim = c(log(100), log(200000)), ylim = c(0, 500)) p2 &lt;- prior %&gt;% ggplot(aes(x = exp(x), y = exp(a + b * x), group = idx)) + geom_line(size = .2, color = clr_alpha(clr_current, .4)) + geom_hline(yintercept = 100, linetype = 3, size = .6, color = clr_dark) + labs(subtitle = glue(&quot;alpha {mth(&#39;\\U007E&#39;)} Normal(3, 0.5); beta {mth(&#39;\\U007E&#39;)} Normal(0, 0.2)&quot;), x = &quot;population&quot;, y = &quot;total tools&quot;) + coord_cartesian(xlim = c(100, 200000), ylim = c(0, 500)) p1 + p2 &amp; theme(plot.subtitle = element_markdown(size = 8, hjust = .5)) Using a log link function, induces diminishing returns due to establishing a log-linear relationship. Finally, we can create the model(s) - one with interaction and one simpler one that is intercept-only. data_kline_list &lt;- data_kline %&gt;% dplyr::select(total_tools, pop_log_scl, contact_idx) %&gt;% as.list() #&gt; intercept only model_ocean_intercept &lt;- ulam( flist = alist( total_tools ~ dpois(lambda), log(lambda) &lt;- alpha, alpha ~ dnorm(3, 0.5) ), data = data_kline_list, chains = 4, cores = 4, log_lik = TRUE ) #&gt; interaction model model_ocean_interact &lt;- ulam( flist = alist( total_tools ~ dpois(lambda), log(lambda) &lt;- alpha[contact_idx] + beta[contact_idx] * pop_log_scl, alpha[contact_idx] ~ dnorm(3, 0.5), beta[contact_idx] ~ dnorm(0, 0.2) ), data = data_kline_list, chains = 4, cores = 4, log_lik = TRUE ) compare(model_ocean_intercept, model_ocean_interact, func = PSIS) %&gt;% knit_precis(param_name = &quot;model&quot;) model PSIS SE dPSIS dSE pPSIS weight model_ocean_interact 84.62 13.05 0.00 NA 6.60 1 model_ocean_intercept 143.47 34.71 58.85 33.98 9.67 0 #&gt; Some Pareto k values are high (&gt;0.5). Set pointwise=TRUE to inspect individual points. #&gt; Some Pareto k values are high (&gt;0.5). Set pointwise=TRUE to inspect individual points. The effective number of parameters (pPSIS / pWAIC) is actually larger for the model with fewer parameters (model_ocean_intercept) 😲. \\(\\rightarrow\\) the clear relationship of overfitting and the number of parameters only holds for simple linear regressions with flat priors! Checking the influential points (with high Pareto k values) through posterior prediction plots: ocean_k_values &lt;- PSIS(model_ocean_interact, pointwise = TRUE) %&gt;% bind_cols(data_kline, .) n &lt;- 101 ocean_grid &lt;- crossing(pop_log_scl = seq(-1.4, 3, length.out = n), contact_idx = 1:2) ocean_posterior_predictions &lt;- link(model_ocean_interact, data = ocean_grid) %&gt;% as.matrix() %&gt;% t() %&gt;% as_tibble() %&gt;% bind_cols(ocean_grid, .) %&gt;% pivot_longer(-c(pop_log_scl, contact_idx), values_to = &quot;total_tools&quot;) %&gt;% dplyr::select(-name) %&gt;% group_by(pop_log_scl, contact_idx) %&gt;% summarise(p = list(quantile(total_tools, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) %&gt;% mutate(population = exp((pop_log_scl * sd(log(data_kline$population))) + mean(log(data_kline$population)))) p1 &lt;- ocean_posterior_predictions %&gt;% ggplot(aes(x = pop_log_scl, y = total_tools)) + geom_smooth(aes(ymin = ll, y = m, ymax = hh, group = factor(contact_idx), color = factor(contact_idx), fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, size = .3) + geom_point(data = ocean_k_values, aes(color = factor(contact_idx), fill = after_scale(clr_alpha(color)), size = k), shape = 21) + ggrepel::geom_text_repel(data = ocean_k_values %&gt;% filter(k &gt; .5), aes(label = str_c(culture, &quot; (&quot;, round(k, digits = 2), &quot;)&quot;)), family = fnt_sel,nudge_y = 6, min.segment.length = 15) + scale_color_manual(values = c(`1` = clr_current, `2` = clr0dd), guide = &quot;none&quot;) + scale_size_continuous(guide = &quot;none&quot;)+ coord_cartesian(ylim = c(0, 90), x = c(-1.4, 3), expand = 0) + labs(subtitle = &quot;on log scale&quot;) p2 &lt;- ocean_posterior_predictions %&gt;% ggplot(aes(x = population, y = total_tools)) + geom_smooth(aes(ymin = ll, y = m, ymax = hh, group = factor(contact_idx), color = factor(contact_idx), fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, size = .3) + geom_point(data = ocean_k_values, aes(color = factor(contact_idx), fill = after_scale(clr_alpha(color)), size = k), shape = 21) + scale_color_manual(values = c(`1` = clr_current, `2` = clr0dd), guide = &quot;none&quot;) + scale_size_continuous(guide = &quot;none&quot;) + scale_x_continuous(breaks = 1e5 * (0:3), labels = scales::comma) + coord_cartesian(ylim = c(0, 90), x = c(-1e3, 3e5), expand = 1) + labs(subtitle = &quot;on natural scale&quot;) p1 + p2 &amp; theme(plot.subtitle = element_text(hjust = .5)) Notice how the posterior predictions expect fewer tools for well connected cultures compared to less connected ones at high population sizes (after the mean lines cross). This asks for a better model that incorporates scientific knowledge (eg. forcing the model to go through the origin for bot culture types). 12.2.2 Modeling Tool Innovation \\[ \\Delta T = \\alpha P^{\\beta} - \\gamma T \\] Setting \\(\\Delta T\\) to 0 at the equilibrium: \\[ \\hat{T} = \\frac{\\alpha P^{\\beta}}{\\gamma} \\] Plugging this into a Poisson model: \\[ \\begin{array}{rclr} T_{i} &amp; \\sim &amp; Poisson(\\lambda_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{log}(\\lambda_{i}) &amp; = &amp; \\alpha P_{i}^{\\beta} / \\gamma &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] data_kline_list_sc &lt;- data_kline %&gt;% dplyr::select(total_tools, population, contact_idx) %&gt;% as.list() model_ocean_scientific &lt;- ulam( flist = alist( total_tools ~ dpois(lambda), lambda &lt;- exp(alpha[contact_idx]) * population^beta[contact_idx] / gamma, alpha[contact_idx] ~ dnorm(1, 1), beta[contact_idx] ~ dexp(1), gamma ~ dexp(1) ), data = data_kline_list_sc, chains = 4, cores = 4, log_lik = TRUE ) ocean_k_values_sc &lt;- PSIS(model_ocean_scientific, pointwise = TRUE) %&gt;% bind_cols(data_kline, .) n &lt;- 101 ocean_grid_sc &lt;- crossing(population = seq(0, 3e5, length.out = n), contact_idx = 1:2) ocean_posterior_predictions_sc &lt;- link(model_ocean_scientific, data = ocean_grid_sc) %&gt;% as.matrix() %&gt;% t() %&gt;% as_tibble() %&gt;% bind_cols(ocean_grid_sc, .) %&gt;% pivot_longer(-c(population, contact_idx), values_to = &quot;total_tools&quot;) %&gt;% dplyr::select(-name) %&gt;% group_by(population, contact_idx) %&gt;% summarise(p = list(quantile(total_tools, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) ocean_posterior_predictions_sc %&gt;% ggplot(aes(x = population, y = total_tools)) + geom_smooth(aes(ymin = ll, y = m, ymax = hh, group = factor(contact_idx), color = factor(contact_idx), fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, size = .3) + geom_point(data = ocean_k_values_sc, aes(color = factor(contact_idx), fill = after_scale(clr_alpha(color)), size = k), shape = 21) + scale_color_manual(values = c(`1` = clr_current, `2` = clr0dd), guide = &quot;none&quot;) + scale_size_continuous(guide = &quot;none&quot;) + scale_x_continuous(breaks = 1e5 * (0:3), labels = scales::comma) + coord_cartesian(ylim = c(0, 80), x = c(-1e3, 3e5), expand = 1) + labs(subtitle = &quot;the scientific model&quot;) + theme(plot.subtitle = element_text(hjust = .5)) 12.2.3 Negative Binomial (gamma-Poisson) models This relates to the Poisson distribution, like the Student-t distribution reates to the normal. The negative binomial (aka. gamma-Poisson) distribution is just a mixtrue of several different Poisson distributions 12.2.4 Exposure and the Offset In Poisson models \\(\\lambda\\) is both the expected value, as well as a rate: \\(\\lambda = \\mu / \\tau\\). \\[ \\begin{array}{rcl} y_{i} &amp; \\sim &amp; Poisson(\\lambda_{i}) \\\\ \\textrm{log}(\\lambda_{i}) &amp; = &amp; \\textrm{log} \\frac{\\mu_{i}}{\\tau_{i}}~=~\\alpha + \\beta x_{i} \\\\ \\textrm{log}(\\lambda_{i}) &amp; = &amp; \\textrm{log}~\\mu_{i} - \\textrm{log}~\\tau_{i}~=~\\alpha + \\beta x_{i} \\\\ \\end{array} \\] In here \\(\\tau\\) are the exposures - if different observations \\(i\\) do have different exposures: \\[ \\textrm{log}~\\mu_{i} = \\textrm{log}~\\tau_{i} + \\alpha + \\beta \\] We can use \\(\\tau\\) (needs to be a data column) to scale the expected number of events: \\[ \\begin{array}{rcl} y_{i} &amp; \\sim &amp; Poisson(\\mu_{i}) \\\\ \\textrm{log}(\\mu_{i}) &amp; = &amp; \\textrm{log}~\\tau_{i} + \\alpha + \\beta x_{i} \\\\ \\end{array} \\] The practical gain here is that we can introduce an offset (scaling of the rate), if our sampling is uneven over time (eg. varying lengths of observations, area of sampling, intensity of sampling). To test this we are modelling book-completions by different medieval monasteries (starting with \\(\\lambda_{M1} = 1.5\\) books/day and \\(\\lambda_{M2} = .5\\) books/day - but tallied in weeks). n_days &lt;- 30 n_weeks &lt;- 4 books_m1 &lt;- rpois(n_days, 1.5) books_m2 &lt;- rpois(n_weeks, .5 *7 ) data_books &lt;- tibble(books = c(books_m1, books_m2), monastery = rep(0:1, c(n_days,n_weeks)), days = rep(c(1,7), c(n_days,n_weeks))) %&gt;% # exposure mutate(days_log = log(days)) model_books &lt;- quap( flist = alist( books ~ dpois( lambda ), log(lambda) &lt;- days_log + alpha + beta * monastery, alpha ~ dnorm( 0, 1 ), beta ~ dnorm(0 ,1 ) ), data = data_books ) Computing the posterior distributions: books_posterior &lt;- extract.samples(model_books) %&gt;% as_tibble() %&gt;% mutate(lambda_old = exp(alpha), lambda_new = exp(alpha + beta)) precis(books_posterior %&gt;% dplyr::select(starts_with(&quot;lambda&quot;))) %&gt;% knit_precis(param_name = &quot;column&quot;) column mean sd 5.5% 94.5% histogram lambda_old 1.24 0.20 0.95 1.58 ▁▂▇▇▃▁▁▁▁ lambda_new 0.45 0.13 0.28 0.67 ▁▂▇▇▃▂▁▁▁▁▁▁▁ 12.3 Multinomial and Categorical Models The multinomial distribution is the maximum entropy distribution if more than two types of events can happen and if the probabilities for each event is constant. For \\(K\\) types of events, with the probabilities \\(p_{1}, ..., p_{K}\\), the probability of observing \\(y_{1}, ..., y_{K}\\) in \\(n\\) trials is: \\[ Pr(y_{1}, ..., y_{K}| n, p_{1}, ..., p_{K}) = \\frac{n!}{\\prod_{i} y_{i}!} \\prod_{i=1}^{K} p_{i}^{y_{i}} \\] The typical link function for multinomial models (categorical models) is the multinomial logit (aka. softmax) which takes a score vector of the \\(K\\) event types: \\[ Pr(k | s_{1},...,s_{K}) = \\frac{\\textrm{exp}(s_{K})}{\\sum_{i=1}^{K}\\textrm{exp}(s_{i})} \\] This link function produces a multinomial logistic regression. 12.3.1 Predictor Matches to Outcomes Simulating career choices based on expected income: n &lt;- 500 career_p &lt;- tibble(income = c(1, 2, 5), score = .5 * income, p = softmax(score)) set.seed(34302) career &lt;- 1:n %&gt;% map_dbl(.f = function(x){ sample(x = 1:3, size = 1, prob = career_p$p) }) Writing the Stan model by hand: code_model_career &lt;- &quot; data{ int N; // number of individuals int K; // number of possible careers int career[N]; // outcome vector[K] career_income; } parameters{ vector[K-1] alpha; // intercepts real&lt;lower=0&gt; beta; // association of income with slope } model{ vector[K] p; vector[K] s; alpha ~ normal( 0, 1 ); beta ~ normal( 0, 0.5 ); s[1] = alpha[1] + beta * career_income[1]; s[2] = alpha[2] + beta * career_income[2]; s[3] = 0; // pivot p = softmax( s ); career ~ categorical( p ); } &quot; data_career_list &lt;- list( N = n, K = 3, career = career, career_income = career_p$income ) model_career &lt;- stan( model_code = code_model_career, data = data_career_list, chains = 4, cores = 4 ) precis(model_career, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha[1] -2.16 0.20 -2.49 -1.86 421.69 1.01 alpha[2] -1.81 0.29 -2.33 -1.47 329.50 1.01 beta 0.15 0.13 0.01 0.39 304.44 1.01 Counterfactual simulations (doubling income #2): career_counterfactual &lt;- extract.samples(model_career) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% mutate(idx = row_number()) %&gt;% pivot_longer(cols = starts_with(&quot;alpha&quot;), names_to = &quot;case&quot;, values_to = &quot;alpha&quot;) %&gt;% mutate(case = str_remove(case, &quot;alpha.&quot;) %&gt;% as.integer(), score = alpha + beta * career_p$income[case], double_score = alpha + beta * career_p$income[case] * 2 ) %&gt;% pivot_wider(names_from = case, values_from = alpha:double_score,id_cols = c(idx, beta)) %&gt;% mutate(s2_orig = map2_dbl(score_1, score_2, .f = function(x,y){softmax(c(x,y,0))[2]}), s2_new = map2_dbl(score_1, double_score_2, .f = function(x,y){softmax(c(x,y,0))[2]}), p_two_dif = s2_new - s2_orig) career_counterfactual %&gt;% dplyr::select(p_two_dif) %&gt;% precis() %&gt;% knit_precis(param_name = &quot;column&quot;) column mean sd 5.5% 94.5% histogram p_two_dif 0.05 0.05 0 0.14 ▇▂▁▁▁▁▁ This section does get quite different values compared to the book - they are similar to the summary by Solomon Kurz though (and also raised an issue within the rethinking repo) 12.3.2 Predictors Matched to Observations n &lt;- 500 beta &lt;- c(-2, 0, 2) career_fam &lt;- tibble(family_income = runif(n)) %&gt;% mutate(score = purrr::map(family_income, function(x){.5 * 1:3 + beta * x}), p = purrr::map(score, function(x){softmax(x[1], x[2], x[3])}), career = map_int(p, function(x){sample(1:3, size = 1,prob = x)})) code_model_career_fam &lt;- &quot; data{ int N; // number of observations int K; // number of outcome values int career[N]; // outcome real family_income[N]; // } parameters{ vector[K-1] alpha; // intercepts vector[K-1] beta; // coefficients on family income } model{ vector[K] p; vector[K] s; alpha ~ normal( 0, 1.5 ); beta ~ normal( 0, 1 ); for( i in 1:N ) { for( j in 1:(K-1) ){ s[j] = alpha[j] + beta[j] * family_income[i]; } s[K] = 0; p = softmax( s ); career[i] ~ categorical( p ); } } &quot; data_career_fam_list &lt;- list( N = n, K = 3, career = career_fam$career, family_income = career_fam$family_income ) model_career_fam &lt;- stan( model_code = code_model_career_fam, data = data_career_fam_list, chains = 4, cores = 4 ) precis(model_career_fam, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha[1] -0.98 0.28 -1.43 -0.53 959.22 1 alpha[2] -0.89 0.22 -1.25 -0.53 765.97 1 beta[1] -3.18 0.61 -4.19 -2.25 1028.09 1 beta[2] -1.06 0.40 -1.68 -0.43 699.29 1 12.3.3 Multinomial in Disguise as Poisson model_ucb_binom &lt;- quap( flist = alist( admit ~ dbinom( applications, p), logit(p) &lt;- alpha, alpha ~ dnorm( 0, 1.5) ), data = data_ucb ) data_ucb_list_poisson &lt;- list( adm = data_ucb$admit, rej = data_ucb$reject ) model_ucb_poisson &lt;- ulam( flist = alist( adm ~ dpois( lambda_1 ), rej ~ dpois( lambda_2 ), log( lambda_1 ) &lt;- alpha_1, log( lambda_2 ) &lt;- alpha_2, c( alpha_1, alpha_2 ) ~ dnorm( 0, 1.5 ) ), data = data_ucb_list_poisson, cores = 4, chains = 4 ) inv_logit(coef(model_ucb_binom)) #&gt; alpha #&gt; 0.3878044 The implied probability of admission within the Poisson model: \\[ p_{ADMIT} = \\frac{\\lambda_{1}}{\\lambda_{1} + \\lambda_{2}} = \\frac{\\textrm{exp}(\\alpha_{1})}{\\textrm{exp}(\\alpha_{1}) + \\textrm{exp}(\\alpha_{2})} \\] k &lt;- coef(model_ucb_poisson) alpha_1 &lt;- k[&quot;alpha_1&quot;] alpha_2 &lt;- k[&quot;alpha_2&quot;] exp(alpha_1) / (exp(alpha_1) + exp(alpha_2)) #&gt; alpha_1 #&gt; 0.3874382 library(rlang) chapter11_models &lt;- env( data_chimp = data_chimp, model_omega10 = model_omega10, model_omega15 = model_omega15, model_beta10 = model_beta10, model_beta05 = model_beta05, model_chimp = model_chimp, data_chimp_list = data_chimp_list, model_chimp_interaction = model_chimp_interaction, data_chimp_list2 = data_chimp_list2, model_chimp_stan = model_chimp_stan, data_chimp_aggregated = data_chimp_aggregated, model_chimp_aggregated = model_chimp_aggregated, data_ucb = data_ucb, model_ucb = model_ucb, data_ucb_list2 = data_ucb_list2, model_ucb_dept = model_ucb_dept, data_kline = data_kline, data_kline_sf = data_kline_sf, data_kline_list = data_kline_list, model_ocean_intercept = model_ocean_intercept, model_ocean_interact = model_ocean_interact, data_kline_list_sc = data_kline_list_sc, model_ocean_scientific = model_ocean_scientific, data_books = data_books, model_books = model_books, data_career_list = data_career_list, model_career = model_career, data_career_fam_list = data_career_fam_list, model_career_fam = model_career_fam, model_ucb_binom = model_ucb_binom, data_ucb_list_poisson = data_ucb_list_poisson, model_ucb_poisson = model_ucb_poisson ) write_rds(chapter11_models, &quot;envs/chapter11_models.rds&quot;) 12.4 Homework E1 Based on the definition of log-odds: \\[ \\textrm{log-odds} = \\textrm{log}(\\frac{p}{1-p}) \\] log(.35 / (1 - .35)) #&gt; [1] -0.6190392 E2 Rearranging the log-odds formula to represent the logistic function (s. page 317): \\[ p_{i} = \\frac{e^\\textrm{log-odds}}{1 - e^\\textrm{log-odds}} \\] logistic(3.2) #&gt; [1] 0.9608343 exp(3.2) / (1 + exp(3.2)) #&gt; [1] 0.9608343 x &lt;- exp(3.2) / (1 + exp(3.2)) log(x / (1 - x)) #&gt; [1] 3.2 E3 A coefficient in a logistic regression with a value of 1.7 implies rate 70% increase for every unit change. E4 The offset is needed if rates are compared that are measured in different intervals. It provides a way of standardizing - in our example this was two monasteries tallying their manuscript productions over different time intervals (days vs. weeks). M1 This is likely an effect of the multiplicity that is inflated when using the dis-aggregated format (because the ordering for individual trials is flexible, while the aggregated form is just interpreted as a singular event). In a way, this also has to happen if the posteriors are the same, but the priors change (because we are not assuming a Bernoulli distribution as prior for the aggregated data). Therefore the likelihood has to balance this difference if we are still to arrive at the same posterior distribution. M2 An increase in 1.7 in a Poisson regression translates to an increase in 1.7 orders of magnitude (I think 🤔 - since a log link function is commonly used here). M3 The inverse-logit transforms all values from the \\(\\mathbb{R}\\) space (real numbers) to the range of probabilities, which is \\(p \\in [0, 1]\\). It is thus suited to transform variables that are far from any boundary to the probability scale. p1 &lt;- ggplot() + stat_function(fun = logit, xlim = c(0, 1), geom = &quot;line&quot;, color = clr2, fill = fll2) + geom_vline(xintercept = c(0, 1), linetype = 3, color = clr_dark) + labs(subtitle = &quot;the logit function&quot;, x = &quot;p&quot;, y = &quot;natural scale&quot;) p2 &lt;- ggplot() + stat_function(fun = inv_logit, xlim = c(-10, 10), geom = &quot;area&quot;, color = clr2, fill = fll2) + geom_hline(yintercept = c(0, 1), linetype = 3, color = clr_dark) + labs(subtitle = &quot;the inverse logit&quot;, y = &quot;p&quot;, x = &quot;natural scale&quot;) p1 + p2 &amp; theme(plot.subtitle = element_text(hjust = .5)) M4 The inverse-logit transforms all values from the \\(\\mathbb{R}\\) space (real numbers) to the space \\(\\mathbb{R}_{*}^{+}\\) (positive real numbers). It is thus suited to transform variables that are far from any boundary to the probability scale. p1 &lt;- ggplot() + stat_function(fun = log, xlim = c(0, 100), geom = &quot;line&quot;,n = 501, color = clr2, fill = fll2) + geom_vline(xintercept = 0, linetype = 3, color = clr_dark) + labs(subtitle = &quot;the log function&quot;, x = &quot;count&quot;, y = &quot;parameter scale&quot;) + coord_cartesian(ylim = c(-1.25, 4.5)) p2 &lt;- ggplot() + stat_function(fun = exp, xlim = c(-1.25, 4.6), geom = &quot;area&quot;, n = 501, color = clr2, fill = fll2) + geom_hline(yintercept = 0, linetype = 3, color = clr_dark) + labs(subtitle = &quot;the exponential function&quot;, y = &quot;count&quot;, x = &quot;parameter scale&quot;) p1 + p2 &amp; theme(plot.subtitle = element_text(hjust = .5)) M5 Using the logit link for a Poisson GLM would imply that the rate \\(\\lambda\\) also does have a theoretical upper bound that needs to be considered. M6 Constraints of the binomial distribution discrete (count distribution) \\(n\\) trials constant expected value \\(np\\) Constraints of the Poisson distribution (special case of the Binomial) discrete (count distribution) \\(n\\) is very large (even unknown) \\(p\\) is very small expected rate per unit time \\(\\lambda = np\\) The constraints are similar since the Poisson is a special case of the Binomial distribution. There could be an argument that knowing the magnitude of \\(n\\) and \\(p\\) implies further previous knowledge about the system. M7 model_chimp_quap &lt;- quap( flist = alist( pulled_left ~ dbinom( 1, p ), logit(p) &lt;- alpha[actor] + beta[treatment], alpha[actor] ~ dnorm( 0, 1.5 ), beta[treatment] ~ dnorm(0,0.5) ), data = data_chimp_list ) model_chimp_a10 &lt;- ulam( flist = alist( pulled_left ~ dbinom( 1, p ), logit(p) &lt;- alpha[actor] + beta[treatment], alpha[actor] ~ dnorm( 0, 10 ), beta[treatment] ~ dnorm(0,0.5) ), data = data_chimp_list, chains = 4, cores = 4, log_lik = TRUE ) model_chimp_a10_quap &lt;- quap( flist = alist( pulled_left ~ dbinom( 1, p ), logit(p) &lt;- alpha[actor] + beta[treatment], alpha[actor] ~ dnorm( 0, 10 ), beta[treatment] ~ dnorm(0,0.5) ), data = data_chimp_list ) plot_chimps &lt;- function(model, plot_title){ is_ulam &lt;- grepl(&quot;ulam&quot;, plot_title) clr_current &lt;- c(clr1, clr2)[1 + is_ulam] chimp_posterior &lt;- extract.samples(model) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% mutate(across(starts_with(&quot;alpha&quot;),.fns = inv_logit, .names = &quot;p_{.col}&quot;)) p1 &lt;- chimp_posterior %&gt;% dplyr::select(starts_with(&quot;p&quot;)) %&gt;% pivot_longer(everything(), names_to = &quot;actor&quot;, values_to = &quot;p&quot;) %&gt;% mutate(actor = str_remove(actor,&quot;p_alpha.&quot;) %&gt;% as.integer()) %&gt;% group_by(actor) %&gt;% summarise(p = list(quantile(p, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) %&gt;% ggplot(aes(y = factor(actor, levels = 7:1))) + geom_linerange(aes(xmin = ll, xmax = hh), lwd = .2, color = clr_current) + geom_linerange(aes(xmin = l, xmax = h), lwd = .7, color = clr_current) + geom_point(aes(x = m), size = 2, shape = 21, color = clr_current, fill = clr_lighten(clr_current, .2)) + labs(y = &quot;actor&quot;, x = &quot;alpha&quot;, subtitle = plot_title) + coord_cartesian(xlim = c(0, 1)) + theme(plot.subtitle = element_text(hjust = .5)) p2 &lt;- chimp_posterior %&gt;% dplyr::select(starts_with(&quot;beta&quot;)) %&gt;% mutate(`beta.5` = `beta.1` - `beta.3` , `beta.6` = `beta.2` - `beta.4` ) %&gt;% pivot_longer(everything(), names_to = &quot;param&quot;, values_to = &quot;p&quot;) %&gt;% mutate(treatment = treatment_labels[str_remove(param, &quot;beta.&quot;) %&gt;% as.integer()]) %&gt;% group_by(treatment) %&gt;% summarise(p = list(quantile(p, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) %&gt;% ggplot(aes(y = factor(treatment, levels = rev(treatment_labels)))) + geom_rect(data = tibble(x = 1),inherit.aes = FALSE, aes(xmin = -Inf, xmax = Inf, ymin = .5, ymax = 2.5), color = clr0d, fill = fll0) + geom_vline(xintercept = 0, linetype = 3, color = clr_dark) + geom_linerange(aes(xmin = ll, xmax = hh), lwd = .2, color = clr_current) + geom_linerange(aes(xmin = l, xmax = h), lwd = .7, color = clr_current) + geom_point(aes(x = m), size = 2, shape = 21, color = clr_current, fill = clr_lighten(clr_current, .2)) + scale_y_discrete() + labs(y = &quot;treatment&quot;, x = &quot;beta&quot;) p1 + p2 } list(model_chimp, model_chimp_quap, model_chimp_a10, model_chimp_a10_quap) %&gt;% map2(.y = c(&quot;ulam (0, 1.5)&quot;, &quot;quap (0, 1.5)&quot;, &quot;ulam (0, 10)&quot;, &quot;quap (0, 10)&quot;), plot_chimps) %&gt;% wrap_plots(ncol = 1) M8 data_kline_non_hawaii &lt;- data_kline %&gt;% filter(culture != &quot;Hawaii&quot;) data_kline_nH_list &lt;- data_kline_non_hawaii %&gt;% dplyr::select(total_tools, pop_log_scl, contact_idx) %&gt;% as.list() model_ocean_interact_non_hawaii &lt;- ulam( flist = alist( total_tools ~ dpois(lambda), log(lambda) &lt;- alpha[contact_idx] + beta[contact_idx] * pop_log_scl, alpha[contact_idx] ~ dnorm(3, 0.5), beta[contact_idx] ~ dnorm(0, 0.2) ), data = data_kline_nH_list, chains = 4, cores = 4, log_lik = TRUE ) data_kline_nH_list_sc &lt;- data_kline_non_hawaii %&gt;% dplyr::select(total_tools, population, contact_idx) %&gt;% as.list() model_ocean_non_hawaii_scientific &lt;- ulam( flist = alist( total_tools ~ dpois(lambda), lambda &lt;- exp(alpha[contact_idx]) * population ^ beta[contact_idx] / gamma, alpha[contact_idx] ~ dnorm(1, 1), beta[contact_idx] ~ dexp(1), gamma ~ dexp(1) ), data = data_kline_nH_list_sc, chains = 4, cores = 4, log_lik = TRUE ) ocean_nH_k_values &lt;- PSIS(model_ocean_interact_non_hawaii, pointwise = TRUE) %&gt;% bind_cols(data_kline_non_hawaii, .) ocean_nH_posterior_predictions &lt;- link(model_ocean_interact_non_hawaii, data = ocean_grid) %&gt;% as.matrix() %&gt;% t() %&gt;% as_tibble() %&gt;% bind_cols(ocean_grid, .) %&gt;% pivot_longer(-c(pop_log_scl, contact_idx), values_to = &quot;total_tools&quot;) %&gt;% dplyr::select(-name) %&gt;% group_by(pop_log_scl, contact_idx) %&gt;% summarise(p = list(quantile(total_tools, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) %&gt;% mutate(population = exp((pop_log_scl * sd(log(data_kline_non_hawaii$population))) + mean(log(data_kline_non_hawaii$population)))) p1 &lt;- ocean_nH_posterior_predictions %&gt;% ggplot(aes(x = pop_log_scl, y = total_tools)) + geom_smooth(aes(ymin = ll, y = m, ymax = hh, group = factor(contact_idx), color = factor(contact_idx), fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, size = .3) + geom_point(data = ocean_nH_k_values, aes(color = factor(contact_idx), fill = after_scale(clr_alpha(color)), size = k), shape = 21) + ggrepel::geom_text_repel(data = ocean_nH_k_values %&gt;% filter(k &gt; .5), aes(label = str_c(culture, &quot; (&quot;, round(k, digits = 2), &quot;)&quot;)), family = fnt_sel,nudge_y = 6, min.segment.length = 15) + scale_color_manual(values = c(`1` = clr_current, `2` = clr0dd), guide = &quot;none&quot;) + scale_size_continuous(guide = &quot;none&quot;)+ coord_cartesian(ylim = c(0, 90), x = c(-1.4, 3), expand = 0) + labs(subtitle = &quot;on log scale&quot;) p2 &lt;- ocean_nH_posterior_predictions %&gt;% ggplot(aes(x = population, y = total_tools)) + geom_smooth(aes(ymin = ll, y = m, ymax = hh, group = factor(contact_idx), color = factor(contact_idx), fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, size = .3) + geom_point(data = ocean_nH_k_values, aes(color = factor(contact_idx), fill = after_scale(clr_alpha(color)), size = k), shape = 21) + scale_color_manual(values = c(`1` = clr_current, `2` = clr0dd), guide = &quot;none&quot;) + scale_size_continuous(guide = &quot;none&quot;) + scale_x_continuous(breaks = 1e4 * (0:3), labels = scales::comma) + coord_cartesian(ylim = c(0, 90), x = c(-1e3, 3e4), expand = 1) + labs( subtitle = &quot;on natural scale&quot; ) p1 + p2 &amp; theme(plot.subtitle = element_text(hjust = .5)) ocean_nH_k_values_sc &lt;- PSIS(model_ocean_non_hawaii_scientific, pointwise = TRUE) %&gt;% bind_cols(data_kline_non_hawaii, .) n &lt;- 101 ocean_grid_nH_sc &lt;- crossing(population = seq(0, 3e4, length.out = n), contact_idx = 1:2) ocean_nH_posterior_predictions_sc &lt;- link(model_ocean_non_hawaii_scientific, data = ocean_grid_nH_sc) %&gt;% as.matrix() %&gt;% t() %&gt;% as_tibble() %&gt;% bind_cols(ocean_grid_nH_sc, .) %&gt;% pivot_longer(-c(population, contact_idx), values_to = &quot;total_tools&quot;) %&gt;% dplyr::select(-name) %&gt;% group_by(population, contact_idx) %&gt;% summarise(p = list(quantile(total_tools, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) ocean_nH_posterior_predictions_sc %&gt;% ggplot(aes(x = population, y = total_tools)) + geom_smooth(aes(ymin = ll, y = m, ymax = hh, group = factor(contact_idx), color = factor(contact_idx), fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, size = .3) + geom_point(data = ocean_nH_k_values_sc, aes(color = factor(contact_idx), fill = after_scale(clr_alpha(color)), size = k), shape = 21) + scale_color_manual(values = c(`1` = clr_current, `2` = clr0dd), guide = &quot;none&quot;) + scale_size_continuous(guide = &quot;none&quot;) + scale_x_continuous(breaks = 1e4 * (0:3), labels = scales::comma) + coord_cartesian(ylim = c(0, 80), x = c(-1e3, 3e4), expand = 1) + labs(subtitle = &quot;the scientific model&quot;) + theme(plot.subtitle = element_text(hjust = .5)) H1 model_beta05_ulam &lt;- ulam( flist = alist( pulled_left ~ dbinom( 1, p ), logit(p) &lt;- alpha + beta[treatment], alpha ~ dnorm( 0, 1.5 ), beta[treatment] ~ dnorm(0,0.5) ), data = data_chimp_list, chains = 4, cores = 4, log_lik = TRUE ) model_omega15_ulam &lt;- ulam( flist = alist( pulled_left ~ dbinom( 1, p ), logit(p) &lt;- alpha, alpha ~ dnorm( 0, 1.5 ) ), data = data_chimp_list, chains = 4, cores = 4, log_lik = TRUE ) compare(model_chimp, model_chimp_interaction, model_beta05_ulam, model_omega15_ulam) %&gt;% knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight model_chimp_interaction 530.66 19.15 0.00 NA 7.74 0.66 model_chimp 531.97 18.87 1.31 1.35 8.32 0.34 model_beta05_ulam 682.50 9.16 151.84 18.48 3.64 0.00 model_omega15_ulam 687.82 7.09 157.16 19.13 0.94 0.00 The models without the individual intercepts is doing a way worse job according to WAIC compared to the two models that do include individual intercepts. Including the intercepts is also a major improvement and more important than including a slope for the treatment (which has only a marginal effect). H2 data(eagles,package = &quot;MASS&quot;) data_eagles &lt;- eagles %&gt;% as_tibble() %&gt;% rename(sucesses = &quot;y&quot;, n_total = &quot;n&quot;, pirate_size = &quot;P&quot;, pirate_age = &quot;A&quot;, victim_size = &quot;V&quot;) %&gt;% mutate(p = 2L - as.integer(pirate_size), a = 2L - as.integer(pirate_age), v = 2L - as.integer(victim_size)) rm(eagles) a. \\[ \\begin{array}{rclr} y_{i} &amp; \\sim &amp; Binomial(n_{i}, p_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{logit}(p_{i}) &amp; = &amp; \\alpha + \\beta_{P} P_{i} + \\beta_{V} V_{i} + \\beta_{A} A_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp;\\sim &amp; Normal(0, 1.5) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{P}, \\beta_{V}, \\beta_{A} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta$ priors]}\\\\ \\end{array} \\] model_eagle_quap &lt;- quap( flist = alist( sucesses ~ dbinom(n_total, p_i), logit(p_i) &lt;- alpha + beta_p * p + beta_v *v + beta_a * a, alpha ~ dnorm( 0, 1.5 ), c( beta_p, beta_v, beta_a ) ~ dnorm( 0, 0.5 ) ), data = data_eagles ) model_eagle_ulam &lt;- ulam( flist = alist( sucesses ~ dbinom(n_total, p_i), logit(p_i) &lt;- alpha + beta_p * p + beta_v *v + beta_a * a, alpha ~ dnorm( 0, 1.5 ), c( beta_p, beta_v, beta_a ) ~ dnorm( 0, 0.5 ) ), data = data_eagles, chain = 4, cores = 4, log_lik = 4 ) precis(model_eagle_quap, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.29 0.37 -0.29 0.88 beta_p 1.62 0.31 1.13 2.11 beta_v -1.67 0.32 -2.18 -1.16 beta_a 0.65 0.31 0.16 1.14 precis(model_eagle_ulam, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha 0.30 0.37 -0.28 0.90 933.57 1 beta_a 0.67 0.31 0.17 1.17 1121.91 1 beta_v -1.69 0.33 -2.22 -1.18 1151.23 1 beta_p 1.64 0.31 1.16 2.16 1437.01 1 clr_current &lt;- clr2 eagle_posterior_quap &lt;- extract.samples(model_eagle_quap) %&gt;% as_tibble() %&gt;% pivot_longer(everything(), names_to = &quot;parameter&quot;) %&gt;% mutate(model = &quot;quap&quot;) eagle_posterior_ulam &lt;- extract.samples(model_eagle_ulam) %&gt;% as_tibble() %&gt;% pivot_longer(everything(), names_to = &quot;parameter&quot;) %&gt;% mutate(model = &quot;ulam&quot;) eagle_quap_params &lt;- precis(model_eagle_quap, depth = 2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% mutate(rowid = as.integer(factor(parameter))) add_quap_theory &lt;- function(rowid, mean, sd, scale = .3, ...){ stat_function(fun = function(x, mean, sd, rowid){ rowid + dnorm(x, mean, sd) * scale }, geom = &quot;line&quot;, args = list(rowid = rowid, mean = mean, sd = sd), color = clr0dd, linetype = 3) } bind_rows(eagle_posterior_quap, eagle_posterior_ulam) %&gt;% ggplot(aes(x = value, y = parameter, color = model)) + stat_slab( height = .5, size = .5, aes(side = c(quap = &quot;right&quot;, ulam = &quot;left&quot;)[model], fill = after_scale(clr_alpha(color)))) + (pmap(eagle_quap_params, add_quap_theory)) + scale_color_manual(values = c(quap = clr0d, ulam = clr_current)) + theme(legend.position = &quot;bottom&quot;) \\(\\rightarrow\\) yes, I would say the quadratic approximation does a very good job here. b. \\[ \\textrm{logit}(p_{i}) = \\alpha + \\beta_{P} P_{i} + \\beta_{V} V_{i} + \\beta_{A} A_{i} \\] # eagles_grid &lt;- data_eagles %&gt;% # distinct(p,a,v, n_total = 1e4) eagles_mod_val &lt;- link(model_eagle_quap) %&gt;% as_tibble() %&gt;% summarise(across(everything(), .fns = function(x){quantile(x, probs = c(.055, .5, .945))})) %&gt;% mutate(percentile = c(&quot;lower_89&quot;, &quot;median&quot;, &quot;upper_89&quot;)) %&gt;% pivot_longer(-percentile, names_to = &quot;rowid&quot;) %&gt;% pivot_wider(names_from = percentile) %&gt;% bind_cols(data_eagles,. ) %&gt;% mutate(rn = row_number()) eagles_sim &lt;- sim(model_eagle_quap) %&gt;% as_tibble() %&gt;% summarise(across(everything(), .fns = function(x){quantile(x, probs = c(.055, .5, .945))})) %&gt;% mutate(percentile = c(&quot;lower_89&quot;, &quot;median&quot;, &quot;upper_89&quot;)) %&gt;% pivot_longer(-percentile, names_to = &quot;rowid&quot;) %&gt;% pivot_wider(names_from = percentile) %&gt;% bind_cols(data_eagles,. ) %&gt;% mutate(across(c(lower_89, median, upper_89), .fns = function(x, n){x/n}, n = n_total, .names = c(&quot;{.col}_p&quot;)), rn = row_number()) eagles_mod_val %&gt;% ggplot(aes(y = glue(&quot;v {victim_size}, p {pirate_size}, a {pirate_age}, n {n_total}&quot;))) + geom_pointinterval(aes(xmin = lower_89, x = median, xmax = upper_89))+ geom_point(data = eagles_sim %&gt;% pivot_longer(ends_with(&quot;89_p&quot;)), aes(x = value), size = .4) + geom_point(aes(x = sucesses / n_total), color = clr_current, shape = 1, size = 2) + labs(y = &quot;case&quot;) + lims(x = 0:1) eagles_sim %&gt;% ggplot(aes(y = glue(&quot;v {victim_size}, p {pirate_size}, a {pirate_age}, n {n_total}&quot;))) + geom_pointinterval(aes(xmin = lower_89, x = median, xmax = upper_89))+ geom_point(aes(x = sucesses), color = clr_current, shape = 1, size = 2) + labs(y = &quot;case&quot;) c. model_eagle_interact_quap &lt;- quap( flist = alist( sucesses ~ dbinom(n_total, p_i), logit(p_i) &lt;- alpha + beta_p * p + beta_v *v + beta_a * a + beta_ap * a * p, alpha ~ dnorm( 0, 1.5 ), c( beta_p, beta_v, beta_a, beta_ap ) ~ dnorm( 0, 0.5 ) ), data = data_eagles ) sim(model_eagle_interact_quap) %&gt;% as_tibble() %&gt;% summarise(across(everything(), .fns = function(x){quantile(x, probs = c(.055, .5, .945))})) %&gt;% mutate(percentile = c(&quot;lower_89&quot;, &quot;median&quot;, &quot;upper_89&quot;)) %&gt;% pivot_longer(-percentile, names_to = &quot;rowid&quot;) %&gt;% pivot_wider(names_from = percentile) %&gt;% bind_cols(data_eagles,. ) %&gt;% ggplot(aes(y = glue(&quot;v {victim_size}, p {pirate_size}, a {pirate_age}&quot;))) + geom_pointinterval(aes(xmin = lower_89, x = median, xmax = upper_89))+ geom_point(aes(x = sucesses), color = clr_current, shape = 1, size = 2) + labs(y = &quot;case&quot;) compare(model_eagle_interact_quap, model_eagle_quap) #&gt; WAIC SE dWAIC dSE pWAIC #&gt; model_eagle_quap 58.88127 11.33705 0.000000 NA 8.015855 #&gt; model_eagle_interact_quap 61.03597 11.61914 2.154699 1.694545 9.028379 #&gt; weight #&gt; model_eagle_quap 0.7459921 #&gt; model_eagle_interact_quap 0.2540079 H3 data(salamanders) data_salamander &lt;- salamanders %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;site_idx&quot;, &quot;salamander_n&quot;, &quot;cover_pct&quot;, &quot;forest_age&quot;)) %&gt;% mutate(across(cover_pct:forest_age, standardize, .names = &quot;{.col}_std&quot;)) rm(salamanders) a. model_salamander_quap &lt;- quap( flist = alist( salamander_n ~ dpois(lambda), log(lambda) &lt;- alpha + beta_c * cover_pct_std, alpha ~ dnorm(1.5, .5), beta_c ~ dnorm(0, 0.2) ), data = data_salamander ) model_salamander_ulam &lt;- ulam( flist = alist( salamander_n ~ dpois(lambda), log(lambda) &lt;- alpha + beta_c * cover_pct_std, alpha ~ dnorm(1.5, .5), beta_c ~ dnorm(0, 0.2) ), data = data_salamander, chains = 4, cores = 4, log_lik = TRUE ) salamander_posterior_quap &lt;- extract.samples(model_salamander_quap) %&gt;% as_tibble() %&gt;% pivot_longer(everything(), names_to = &quot;parameter&quot;) %&gt;% mutate(model = &quot;quap&quot;) salamander_posterior_ulam &lt;- extract.samples(model_salamander_ulam) %&gt;% as_tibble() %&gt;% pivot_longer(everything(), names_to = &quot;parameter&quot;) %&gt;% mutate(model = &quot;ulam&quot;) salamander_quap_params &lt;- precis(model_salamander_quap, depth = 2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;parameter&quot;) %&gt;% mutate(rowid = as.integer(factor(parameter))) bind_rows(salamander_posterior_quap, salamander_posterior_ulam) %&gt;% ggplot(aes(x = value, y = parameter, color = model)) + stat_slab( height = .55, size = .5, aes(side = c(quap = &quot;right&quot;, ulam = &quot;left&quot;)[model], fill = after_scale(clr_alpha(color))), trim = FALSE, normalize = &quot;xy&quot;) + (pmap(salamander_quap_params, add_quap_theory, scale = .1)) + scale_color_manual(values = c(quap = clr0d, ulam = clr_current)) + theme(legend.position = &quot;bottom&quot;) salamander_mod_val &lt;- link(model_salamander_ulam) %&gt;% as_tibble() %&gt;% summarise(across(everything(), .fns = function(x){quantile(x, probs = c(.055, .5, .945))})) %&gt;% mutate(percentile = c(&quot;lower_89&quot;, &quot;median&quot;, &quot;upper_89&quot;)) %&gt;% pivot_longer(-percentile, names_to = &quot;rowid&quot;) %&gt;% pivot_wider(names_from = percentile) %&gt;% bind_cols(data_salamander,. ) %&gt;% mutate(rn = row_number()) salamander_sim &lt;- sim(model_salamander_ulam) %&gt;% as_tibble() %&gt;% summarise(across(everything(), .fns = function(x){quantile(x, probs = c(.055, .5, .945))})) %&gt;% mutate(percentile = c(&quot;lower_89&quot;, &quot;median&quot;, &quot;upper_89&quot;)) %&gt;% pivot_longer(-percentile, names_to = &quot;rowid&quot;) %&gt;% pivot_wider(names_from = percentile) %&gt;% bind_cols(data_salamander, . ) %&gt;% mutate(rn = row_number()) salamander_mod_val %&gt;% mutate(rn = fct_reorder(as.character(rn), -salamander_n)) %&gt;% ggplot(aes(x = rn, color = log10(forest_age))) + geom_pointinterval(aes(ymin = lower_89, y = median, ymax = upper_89)) + geom_point(data = salamander_sim %&gt;% pivot_longer(ends_with(&quot;89&quot;)), aes(y = value), size = .4) + geom_point(aes(y = salamander_n), color = clr_dark, shape = 1, size = 2) + scale_color_gradientn(colours = c(clr_lighten(clr_current), clr_dark)) + labs(x = &quot;case&quot;) + guides(color = guide_colorbar(title.position = &quot;top&quot;, barwidth = unit(.7, &quot;npc&quot;), barheight = unit(4, &quot;pt&quot;)))+ theme(legend.position = &quot;bottom&quot;) n &lt;- 301 new_salamander &lt;- tibble(cover_pct_std = seq(-1.8, 1.8, length.out = n)) salamander_posterior_predictions &lt;- link(model_salamander_ulam, data = new_salamander) %&gt;% as.matrix() %&gt;% t() %&gt;% as_tibble() %&gt;% bind_cols(new_salamander, .) %&gt;% pivot_longer(-cover_pct_std, values_to = &quot;salamander_n&quot;) %&gt;% dplyr::select(-name) %&gt;% group_by(cover_pct_std) %&gt;% summarise(p = list(quantile(salamander_n, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) salamander_posterior_predictions %&gt;% ggplot(aes(x = cover_pct_std, y = salamander_n)) + geom_smooth(aes(ymin = ll, y = m, ymax = hh), color = clr2, fill = fll2, stat = &quot;identity&quot;, size = .3) + geom_point(data = data_salamander, color = clr2, fill = fll2, shape = 21) b. model_salamander_age &lt;- ulam( flist = alist( salamander_n ~ dpois(lambda), log(lambda) &lt;- alpha + beta_c * cover_pct_std + beta_a * forest_age_std, alpha ~ dnorm(-.5, .5), beta_c ~ dnorm(0, 0.2), beta_a ~ dnorm(0, 0.2) ), data = data_salamander, chains = 4, cores = 4, log_lik = TRUE ) library(ggmcmc) clr_chains &lt;- function(n = 4, alpha = .7, col = clr2){ scales::colour_ramp(colors = c(clr0dd, col))(seq(0,1,length.out = n)) %&gt;% clr_lighten(.2) %&gt;% clr_alpha(alpha = alpha) } ggs(model_salamander_age@stanfit, inc_warmup = TRUE) %&gt;% mutate(chain = factor(Chain)) %&gt;% filter(Parameter %in% c(&quot;alpha&quot;, &quot;beta_c&quot;, &quot;beta_a&quot;)) %&gt;% ggplot(aes(x = Iteration, y = value)) + annotate(geom = &quot;rect&quot;, xmin = 0, xmax = 500, ymin = -Inf, ymax = Inf, fill = clr0d, alpha = .3, size = 0) + geom_line(aes(color = chain), size = .15) + scale_color_manual(values = clr_chains() ) + facet_wrap(~ Parameter, scales = &quot;free_y&quot;) + labs(x = NULL, y = NULL) + theme(legend.position = &quot;bottom&quot;) compare(model_salamander_ulam, model_salamander_age) %&gt;% knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight model_salamander_ulam 219.95 26.11 0.00 NA 4.03 0.83 model_salamander_age 223.19 28.78 3.24 5.14 7.16 0.17 salamander_age_mod_val &lt;- link(model_salamander_age) %&gt;% as_tibble() %&gt;% summarise(across(everything(), .fns = function(x){quantile(x, probs = c(.055, .5, .945))})) %&gt;% mutate(percentile = c(&quot;lower_89&quot;, &quot;median&quot;, &quot;upper_89&quot;)) %&gt;% pivot_longer(-percentile, names_to = &quot;rowid&quot;) %&gt;% pivot_wider(names_from = percentile) %&gt;% bind_cols(data_salamander,. ) %&gt;% mutate(rn = row_number()) salamander_age_sim &lt;- sim(model_salamander_age) %&gt;% as_tibble() %&gt;% summarise(across(everything(), .fns = function(x){quantile(x, probs = c(.055, .5, .945))})) %&gt;% mutate(percentile = c(&quot;lower_89&quot;, &quot;median&quot;, &quot;upper_89&quot;)) %&gt;% pivot_longer(-percentile, names_to = &quot;rowid&quot;) %&gt;% pivot_wider(names_from = percentile) %&gt;% bind_cols(data_salamander, . ) %&gt;% mutate(rn = row_number()) salamander_age_mod_val %&gt;% mutate(rn = fct_reorder(as.character(rn), -salamander_n)) %&gt;% ggplot(aes(x = rn, color = log10(forest_age))) + geom_pointinterval(aes(ymin = lower_89, y = median, ymax = upper_89)) + geom_point(data = salamander_age_sim %&gt;% pivot_longer(ends_with(&quot;89&quot;)), aes(y = value), size = .4) + geom_point(aes(y = salamander_n), color = clr_dark, shape = 1, size = 2) + scale_color_gradientn(colours = c(clr_lighten(clr_current), clr_dark)) + labs(x = &quot;case&quot;) + guides(color = guide_colorbar(title.position = &quot;top&quot;, barwidth = unit(.7, &quot;npc&quot;), barheight = unit(4, &quot;pt&quot;)))+ theme(legend.position = &quot;bottom&quot;) n &lt;- 301 salamander_grid &lt;- crossing(forest_age_std = seq(-1, 3, length.out = n), cover_pct_std = c(-1, 0, 1)) %&gt;% mutate(cover_class = cut(cover_pct_std, c(-2, -.5 , .5, 2))) salamander_age_posterior_predictions &lt;- link(model_salamander_age, data = salamander_grid) %&gt;% as.matrix() %&gt;% t() %&gt;% as_tibble() %&gt;% bind_cols(salamander_grid, .) %&gt;% pivot_longer(-c(forest_age_std:cover_class), values_to = &quot;salamander_n&quot;) %&gt;% dplyr::select(-name) %&gt;% group_by(forest_age_std, cover_pct_std, cover_class) %&gt;% summarise(p = list(quantile(salamander_n, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) salamander_age_posterior_predictions %&gt;% ggplot(aes(x = forest_age_std, y = salamander_n)) + geom_smooth(aes(ymin = ll, y = m, ymax = hh, color = cover_class, fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, size = .3) + geom_point(data = data_salamander %&gt;% mutate(cover_class = cut(cover_pct_std, c(-2, -.5 , .5, 2))) , aes(color = cover_class, fill = after_scale(clr_alpha(color))), shape = 21) + ggrepel::geom_text_repel(data = data_salamander %&gt;% filter(site_idx %in% 1:5) %&gt;% mutate(cover_class = cut(cover_pct_std, c(-2, -.5 , .5, 2))) , aes(label = site_idx), faily = fnt_sel)+ scale_color_manual(values = c(clr0dd, clr_dark, clr_current), guide = &quot;none&quot;) + facet_wrap(cover_class ~ . , labeller = label_both) H4 data(NWOGrants) data_nwo &lt;- NWOGrants %&gt;% as_tibble() %&gt;% mutate(gender_idx = as.integer(gender), discipline_idx = as.integer(discipline)) rm(NWOGrants) clr_current &lt;- clr3 data_nwo %&gt;% ggplot(aes(x = discipline, color = gender)) + geom_bar(position = position_dodge2(padding = .15), width = .9, stat = &#39;identity&#39;, aes(y = awards, fill = after_scale(clr_alpha(color,.8)))) + geom_bar(position = position_dodge2(padding = .15), width = .9, stat = &#39;identity&#39;, aes(y = -applications*.15, fill = after_scale(clr_alpha(color,.4))))+ geom_hline(yintercept = 0, lietype = 3, color = clr_dark) + scale_color_manual(values = c(f = clr_current, m = clr0dd)) dag_nwo &lt;- dagify( A ~ D + G, D ~ G, exposure = &quot;A&quot;, outcome = &quot;G&quot;, coords = tibble(name = c(&quot;G&quot;, &quot;D&quot;, &quot;A&quot;), x = c(0, .5, 1), y = c(0, 1, 0))) dag_nwo %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;A&quot;, &quot;response&quot;, if_else(name %in% c(&quot;G&quot;, &quot;D&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr_current) + coord_fixed(ratio = .5) + scale_y_continuous(limits = c(-.1, 1.1)) + scale_x_continuous(limits = c(-.1, 1.1)) + theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) model_nwo &lt;- ulam( flist = alist( awards ~ dbinom( applications, p ), logit(p) &lt;- alpha[gender_idx], alpha[gender_idx] ~ dnorm( 0, 1.5 ), delta[discipline_idx] ~ dnorm( 0, 1.5 ) ), data = data_nwo, iter = 4000, chains = 4, cores = 4, log_lik = TRUE ) model_nwo_dept &lt;- ulam( flist = alist( awards ~ dbinom( applications, p ), logit(p) &lt;- alpha[gender_idx] + delta[discipline_idx], alpha[gender_idx] ~ dnorm( 0, 1.5 ), delta[discipline_idx] ~ dnorm( 0, 1.5 ) ), data = data_nwo, iter = 4000, chains = 4, cores = 4, log_lik = TRUE ) precis(model_nwo, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha[1] -1.74 0.08 -1.87 -1.61 10602.88 1 alpha[2] -1.53 0.06 -1.64 -1.43 11267.82 1 delta[1] 0.00 1.49 -2.34 2.38 11217.19 1 delta[2] -0.02 1.50 -2.42 2.38 11389.37 1 delta[3] -0.01 1.48 -2.35 2.37 11046.01 1 delta[4] 0.01 1.51 -2.42 2.45 11694.70 1 delta[5] 0.00 1.48 -2.37 2.36 9851.27 1 delta[6] 0.01 1.51 -2.39 2.43 10631.04 1 delta[7] 0.00 1.50 -2.41 2.37 11894.85 1 delta[8] 0.01 1.51 -2.40 2.39 10507.24 1 delta[9] 0.00 1.51 -2.40 2.41 11570.85 1 precis(model_nwo_dept, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha[1] -1.30 0.46 -2.05 -0.57 673.41 1.01 alpha[2] -1.16 0.46 -1.90 -0.43 665.83 1.01 delta[1] 0.16 0.49 -0.62 0.95 771.30 1.01 delta[2] -0.18 0.47 -0.96 0.58 707.61 1.01 delta[3] -0.42 0.47 -1.17 0.35 716.84 1.01 delta[4] -0.46 0.49 -1.23 0.34 738.22 1.01 delta[5] -0.52 0.47 -1.26 0.25 697.43 1.01 delta[6] -0.20 0.49 -0.97 0.58 734.04 1.01 delta[7] 0.13 0.51 -0.69 0.96 847.88 1.01 delta[8] -0.64 0.47 -1.39 0.12 681.17 1.01 delta[9] -0.39 0.48 -1.16 0.39 719.68 1.01 compare(model_nwo, model_nwo_dept) #&gt; WAIC SE dWAIC dSE pWAIC weight #&gt; model_nwo_dept 128.1254 8.109053 0.000000 NA 12.438897 0.6664024 #&gt; model_nwo 129.5093 8.688161 1.383917 9.177237 4.624921 0.3335976 nwo_grid &lt;- distinct(data_nwo, discipline_idx, gender_idx, applications) nwo_posterior_predictions &lt;- sim(model_nwo, data = nwo_grid) %&gt;% as.matrix() %&gt;% t() %&gt;% as_tibble() %&gt;% bind_cols(nwo_grid, .) %&gt;% pivot_longer(-c(discipline_idx, gender_idx, applications), values_to = &quot;awards&quot;) %&gt;% dplyr::select(-name) %&gt;% group_by(discipline_idx, gender_idx, applications) %&gt;% summarise(p = quantile(awards/applications, probs = c(.055, .25, .5, .75, .955)), median = median(awards), breaks = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;)) %&gt;% ungroup() %&gt;% pivot_wider(names_from = breaks, values_from = p) %&gt;% mutate(type = &quot;post. pred.&quot;, x = 2 * (as.integer(as.factor(discipline_idx)) - 1) + gender_idx) data_nwo %&gt;% group_by(discipline_idx, gender_idx) %&gt;% summarise(mean_data = mean(awards/applications), type = &quot;data&quot;) %&gt;% mutate(x = 2 * (as.integer(discipline_idx) - 1) + gender_idx) %&gt;% ggplot(aes(x = x, y = mean_data)) + geom_segment(data = nwo_posterior_predictions, aes(x = x, xend = x, y = ll, yend = hh), color = clr_current) + geom_point(data = nwo_posterior_predictions, aes(y = m, shape = factor(gender_idx)), color = clr_current, fill = clr_lighten(clr_current), size = 1.8) + geom_textline(aes(label = discipline_idx, group = discipline_idx), color = clr0dd, family = fnt_sel) + geom_point(aes(shape = factor(gender_idx)), color = clr0dd, fill = clr0, size = 1.8) + scale_shape_manual(values = c(`2` = 21, `1` = 19)) + scale_x_continuous(breaks = 2 * (seq_along(levels(data_nwo$discipline))) - .5, labels = levels(data_nwo$discipline))+ labs(x = &quot;discipline&quot;, y = &quot;awards&quot;) + theme(panel.background = element_rect(color = clr0, fill = &quot;transparent&quot;), panel.grid.major.x = element_blank(), axis.text.x = element_text(angle = 90), legend.position = &quot;bottom&quot;) nwo_dept_posterior_predictions &lt;- sim(model_nwo_dept, data = nwo_grid) %&gt;% as.matrix() %&gt;% t() %&gt;% as_tibble() %&gt;% bind_cols(nwo_grid, .) %&gt;% pivot_longer(-c(discipline_idx, gender_idx, applications), values_to = &quot;awards&quot;) %&gt;% dplyr::select(-name) %&gt;% group_by(discipline_idx, gender_idx, applications) %&gt;% summarise(p = quantile(awards/applications, probs = c(.055, .25, .5, .75, .955)), median = median(awards), breaks = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;)) %&gt;% ungroup() %&gt;% pivot_wider(names_from = breaks, values_from = p) %&gt;% mutate(type = &quot;post. pred.&quot;, x = 2 * (as.integer(as.factor(discipline_idx)) - 1) + gender_idx) data_nwo %&gt;% group_by(discipline_idx, gender_idx) %&gt;% summarise(mean_data = mean(awards/applications), type = &quot;data&quot;) %&gt;% mutate(x = 2 * (as.integer(discipline_idx) - 1) + gender_idx) %&gt;% ggplot(aes(x = x, y = mean_data)) + geom_segment(data = nwo_dept_posterior_predictions, aes(x = x, xend = x, y = ll, yend = hh), color = clr_current) + geom_point(data = nwo_dept_posterior_predictions, aes(y = m, shape = factor(gender_idx)), color = clr_current, fill = clr_lighten(clr_current), size = 1.8) + geom_textline(aes(label = discipline_idx, group = discipline_idx), color = clr0dd, family = fnt_sel) + geom_point(aes(shape = factor(gender_idx)), color = clr0dd, fill = clr0, size = 1.8) + scale_shape_manual(values = c(`2` = 21, `1` = 19)) + scale_x_continuous(breaks = 2 * (seq_along(levels(data_nwo$discipline))) - .5, labels = levels(data_nwo$discipline))+ labs(x = &quot;discipline&quot;, y = &quot;awards&quot;) + theme(panel.background = element_rect(color = clr0, fill = &quot;transparent&quot;), panel.grid.major.x = element_blank(), axis.text.x = element_text(angle = 90), legend.position = &quot;bottom&quot;) H5 dag_nwo_u &lt;- dagify( A ~ D + G + U, D ~ G + U, exposure = &quot;A&quot;, outcome = &quot;G&quot;, coords = tibble(name = c(&quot;G&quot;, &quot;D&quot;, &quot;A&quot;, &quot;U&quot;), x = c(0, .5, 1, 1), y = c(0, 1, 0, 1))) dag_nwo_u %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;A&quot;, &quot;response&quot;, if_else(name %in% c(&quot;G&quot;, &quot;D&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr_current) + coord_fixed(ratio = .5) + scale_y_continuous(limits = c(-.1, 1.1)) + scale_x_continuous(limits = c(-.1, 1.1)) + theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) Conditioning on department opens the collider between G and U - so this is not closing the backdoor path. H6 data(&quot;Primates301&quot;) data_primates &lt;- Primates301 %&gt;% as_tibble() %&gt;% mutate(brain_size_log = log(brain), research_effort_log = log(research_effort), across(c(brain_size_log, research_effort_log), standardize, .names = &quot;{.col}_std&quot;)) %&gt;% dplyr::select(genus, species, spp_id, social_learning, research_effort, brain, brain_size_log:research_effort_log_std) rm(Primates301) data_primates_complete &lt;- data_primates %&gt;% filter(complete.cases(brain, social_learning)) a. range(data_primates$social_learning,na.rm = TRUE) #&gt; [1] 0 214 ggplot() + stat_function(fun = function(x){dlnorm(x, 4, .5)}, xlim = c(0, 250), n = 201, geom = &quot;area&quot;, color = clr_current, fill = fll_current()) data_primates_list &lt;- data_primates_complete %&gt;% dplyr::select(spp_id, social_learning, brain_size_log_std) %&gt;% as.list() model_primates &lt;- ulam( flist = alist( social_learning ~ dpois(lambda), log(lambda) &lt;- alpha + beta * brain_size_log_std, alpha ~ dnorm(4, 0.5), beta ~ dnorm(0, 0.2) ), data = data_primates_list, chains = 4, cores = 4, log_lik = TRUE ) precis(model_primates) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha -0.65 0.09 -0.80 -0.50 254.16 1.01 beta 2.41 0.07 2.31 2.52 244.40 1.01 n &lt;- 101 new_primates &lt;- tibble(brain_size_log_std = seq(-3, 2.5, length.out = n)) primates_posterior_predictions &lt;- link(model_primates, data = new_primates) %&gt;% as.matrix() %&gt;% t() %&gt;% as_tibble() %&gt;% bind_cols(new_primates, .) %&gt;% pivot_longer(-brain_size_log_std, values_to = &quot;social_learning&quot;) %&gt;% dplyr::select(-name) %&gt;% group_by(brain_size_log_std) %&gt;% summarise(p = list(quantile(social_learning, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) %&gt;% mutate(brain_size_log = brain_size_log_std * sd(data_primates$brain_size_log, na.rm = TRUE) + mean(data_primates$brain_size_log, na.rm = TRUE), brain = exp(brain_size_log)) p1 &lt;- primates_posterior_predictions %&gt;% ggplot(aes(x = brain_size_log_std, y = social_learning)) + geom_smooth(aes(ymin = ll, y = m, ymax = hh), stat = &quot;identity&quot;, size = .3, color = clr_current, fill = clr_alpha(clr_current)) + geom_point(data = data_primates_complete, color = clr_dark, fill = clr_alpha(clr_dark), shape = 21) + labs(subtitle = &quot;on log scale&quot;) p2 &lt;- primates_posterior_predictions %&gt;% ggplot(aes(x = brain, y = social_learning)) + geom_smooth(aes(ymin = ll, y = m, ymax = hh), stat = &quot;identity&quot;, size = .3, color = clr_current, fill = clr_alpha(clr_current)) + geom_point(data = data_primates_complete, color = clr_dark, fill = clr_alpha(clr_dark), shape = 21) + labs(subtitle = &quot;on natural scale&quot;) p1 + p2 b. data_primates_effort_list &lt;- data_primates_complete %&gt;% filter(complete.cases(research_effort_log_std)) %&gt;% dplyr::select(spp_id, social_learning, brain_size_log_std, research_effort_log_std) %&gt;% as.list() model_primates_effort &lt;- ulam( flist = alist( social_learning ~ dpois(lambda), log(lambda) &lt;- alpha + beta_b * brain_size_log_std + beta_e * research_effort_log_std, alpha ~ dnorm(4, 0.5), beta_b ~ dnorm(0, 0.2), beta_e ~ dnorm(0, 0.2) ), data = data_primates_effort_list, chains = 4, cores = 4, log_lik = TRUE ) precis(model_primates_effort) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha -1.24 0.12 -1.42 -1.05 698.07 1 beta_b 0.55 0.07 0.43 0.67 786.15 1 beta_e 1.74 0.07 1.63 1.86 599.34 1 \\(\\rightarrow\\) the more research is conducted on a species, the more likely it is to observe social learning events. compare(model_primates, model_primates_effort) %&gt;% # ! differnet sample sizes.... knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight model_primates_effort 601.24 174.58 0.00 NA 58.45 1 model_primates 1482.76 530.23 881.52 559.57 149.12 0 n &lt;- 101 grid_primates &lt;- crossing(brain_size_log_std = c(-1.5, 0, 1.5), research_effort_log_std = seq(-3, 3, length.out = n)) %&gt;% mutate(brain_class = cut(brain_size_log_std, c(-3.1, -1 , 1, 3.1))) primates_posterior_predictions &lt;- link(model_primates_effort, data = grid_primates) %&gt;% as.matrix() %&gt;% t() %&gt;% as_tibble() %&gt;% bind_cols(grid_primates, .) %&gt;% pivot_longer(-c(brain_size_log_std, research_effort_log_std, brain_class), values_to = &quot;social_learning&quot;) %&gt;% dplyr::select(-name) %&gt;% group_by(brain_size_log_std, research_effort_log_std, brain_class) %&gt;% summarise(p = list(quantile(social_learning, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) %&gt;% mutate(brain_size_log = brain_size_log_std * sd(data_primates$brain_size_log, na.rm = TRUE) + mean(data_primates$brain_size_log, na.rm = TRUE), brain = exp(brain_size_log)) primates_posterior_predictions %&gt;% ggplot(aes(x = research_effort_log_std, y = social_learning)) + geom_smooth(aes(ymin = ll, y = m, ymax = hh), stat = &quot;identity&quot;, size = .3, color = clr_current, fill = clr_alpha(clr_current)) + geom_point(data = data_primates_complete %&gt;% mutate(brain_class = cut(brain_size_log_std, c(-3.1, -1 , 1, 3.1))), color = clr_dark, fill = clr_alpha(clr_dark), shape = 21) + facet_wrap(brain_class ~ . , labeller = label_both) + labs(subtitle = &quot;on log scale&quot;) c. data_primates %&gt;% dplyr::select(social_learning, brain_size_log_std, research_effort_log_std) %&gt;% ggpairs( lower = list(continuous = wrap(ggally_points, colour = clr_current, size = .9, alpha = .7)), diag = list(continuous = wrap(my_diag, fill = fll0, col = clr_darken(clr_current), color = clr0d, adjust = .7)), upper = list(continuous = wrap(my_upper , size = 5, color = &quot;black&quot;, family = fnt_sel)) ) dag_primates &lt;- dagify( S ~ B + R, R ~ B, exposure = &quot;B&quot;, outcome = &quot;S&quot;, coords = tibble(name = c(&quot;B&quot;, &quot;R&quot;, &quot;S&quot;), x = c(0, .5, 1), y = c(0, 1, 0))) dag_primates %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;S&quot;, &quot;response&quot;, if_else(name %in% c(&quot;B&quot;, &quot;R&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr_current) + coord_fixed(ratio = .5) + scale_y_continuous(limits = c(-.1, 1.1)) + scale_x_continuous(limits = c(-.1, 1.1)) + theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) 12.5 {brms} section 12.5.1 Binomial Regression In the brm() formula syntax, including a | bar on the left side of a formula indicates we have extra supplementary information about our criterion. In this case, that information is that each pulled_left value corresponds to a single trial (i.e., trials(1)), which itself corresponds to the \\(n = 1\\) portion of the statistical formula, above. brms_c11_model_omega10 &lt;- brm( data = data_chimp, family = binomial, pulled_left | trials(1) ~ 1, prior(normal(0, 10), class = Intercept), seed = 42, sample_prior = TRUE, file = &quot;brms/brms_c11_model_omega10&quot;) brms_c11_model_omega15 &lt;- brm( data = data_chimp, family = binomial, pulled_left | trials(1) ~ 1, prior(normal(0, 1.5), class = Intercept), seed = 42, sample_prior = TRUE, file = &quot;brms/brms_c11_model_omega15&quot;) p1 &lt;- bind_rows(prior_draws(brms_c11_model_omega10), prior_draws(brms_c11_model_omega15)) %&gt;% mutate(p = inv_logit_scaled(Intercept), w = factor(rep(c(10, 1.5), each = n() / 2), levels = c(10, 1.5))) %&gt;% as_tibble() %&gt;% ggplot(aes(x = p, color = w)) + geom_density(size = .3, adjust = 0.1, aes(fill = after_scale(clr_alpha(color)))) + scale_color_manual(expression(italic(w)), values = c(`1.5` = clr0d, `10` = clr_dark)) + scale_y_continuous(&quot;denstiy&quot;) + labs(title = expression(alpha%~%Normal(0*&quot;, &quot;*italic(w))), x = &quot;prior prob pull left&quot;) data_chimp_f &lt;- data_chimp %&gt;% mutate(actor = factor(actor), treatment = factor(treatment), side_idx = factor(side_idx), condition_idx = factor(condition_idx)) brms_c11_model_beta10 &lt;- brm( data = data_chimp_f, family = binomial, bf(pulled_left | trials(1) ~ a + b, a ~ 1, b ~ 0 + treatment, nl = TRUE), prior = c(prior(normal(0, 1.5), nlpar = a), prior(normal(0, 10), nlpar = b, coef = treatment1), prior(normal(0, 10), nlpar = b, coef = treatment2), prior(normal(0, 10), nlpar = b, coef = treatment3), prior(normal(0, 10), nlpar = b, coef = treatment4)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, sample_prior = TRUE, file = &quot;brms/brms_c11_model_beta10&quot;) brms_c11_model_beta05 &lt;- brm( data = data_chimp_f, family = binomial, bf(pulled_left | trials(1) ~ a + b, a ~ 1, b ~ 0 + treatment, nl = TRUE), prior = c(prior(normal(0, 1.5), nlpar = a), prior(normal(0, 0.5), nlpar = b, coef = treatment1), prior(normal(0, 0.5), nlpar = b, coef = treatment2), prior(normal(0, 0.5), nlpar = b, coef = treatment3), prior(normal(0, 0.5), nlpar = b, coef = treatment4)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, sample_prior = TRUE, file = &quot;brms/brms_c11_model_beta05&quot;) You could have just included a single line, prior(normal(0, 0.5), nlpar = b), that did not include a coef argument. The problem with this approach is we’d only get one column for treatment when using the prior_samples() function to retrieve the prior samples. To get separate columns for the prior samples of each of the levels of treatment, you need to take the verbose approach, above. prior_beta &lt;- bind_rows(prior_draws(brms_c11_model_beta10), prior_draws(brms_c11_model_beta05)) %&gt;% mutate(w = factor(rep(c(10, 0.5), each = n() / 2), levels = c(10, 0.5)), p1 = inv_logit_scaled(b_a + b_b_treatment1), p2 = inv_logit_scaled(b_a + b_b_treatment2)) %&gt;% mutate(diff = abs(p1 - p2)) p2 &lt;- prior_beta %&gt;% ggplot(aes(x = diff, color = w)) + geom_density(size = .3, adjust = 0.1, aes(fill = after_scale(clr_alpha(color)))) + scale_color_manual(expression(italic(beta)), values = c(`0.5` = clr0d, `10` = clr_dark)) + labs(title = expression(alpha%~%Normal(0*&quot;, &quot;*italic(w))), x = &quot;prior diff between treatments&quot;) p1 + p2 &amp; theme(legend.position = &quot;bottom&quot;) prior_beta %&gt;% group_by(w) %&gt;% summarise(mean = mean(diff)) #&gt; # A tibble: 2 × 2 #&gt; w mean #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 10 0.465 #&gt; 2 0.5 0.0965 as_draws_df(brms_c11_model_beta05) %&gt;% as_tibble() %&gt;% transmute(alpha = inv_logit_scaled(b_a_Intercept)) %&gt;% mean_qi() #&gt; # A tibble: 1 × 6 #&gt; alpha .lower .upper .width .point .interval #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 0.577 0.453 0.698 0.95 mean qi Empirical treatment means: data_chimp_f %&gt;% group_by(treatment) %&gt;% summarise(mean = mean(pulled_left)) #&gt; # A tibble: 4 × 2 #&gt; treatment mean #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 1 0.548 #&gt; 2 2 0.659 #&gt; 3 3 0.476 #&gt; 4 4 0.635 Posterior: posterior_samples(brms_c11_model_beta05) %&gt;% pivot_longer(b_b_treatment1:b_b_treatment4) %&gt;% mutate(treatment = str_remove(name, &quot;b_b_treatment&quot;), mean = inv_logit_scaled(b_a_Intercept + value)) %&gt;% group_by(treatment) %&gt;% mean_qi(mean) #&gt; # A tibble: 4 × 7 #&gt; treatment mean .lower .upper .width .point .interval #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 0.551 0.465 0.636 0.95 mean qi #&gt; 2 2 0.650 0.566 0.726 0.95 mean qi #&gt; 3 3 0.487 0.404 0.571 0.95 mean qi #&gt; 4 4 0.629 0.546 0.706 0.95 mean qi brms_c11_model_chimp &lt;- brm( data = data_chimp_f, family = binomial, bf(pulled_left | trials(1) ~ a + b, a ~ 0 + actor, b ~ 0 + treatment, nl = TRUE), prior = c(prior(normal(0, 1.5), nlpar = a), prior(normal(0, 0.5), nlpar = b)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c11_model_chimp&quot;) chimp_post &lt;- as_draws_df(brms_c11_model_chimp) p1 &lt;- chimp_post %&gt;% pivot_longer(contains(&quot;actor&quot;)) %&gt;% mutate(probability = inv_logit_scaled(value), actor = factor(str_remove(name, &quot;b_a_actor&quot;), levels = 7:1)) %&gt;% ggplot(aes(x = probability, y = actor)) + geom_vline(xintercept = .5, color = clr_dark, linetype = 3) + stat_pointinterval(.width = .95, size = 1/2, color = clr_dark) + scale_x_continuous(expression(alpha[actor]), limits = 0:1) + ylab(NULL) + theme(axis.ticks.y = element_blank()) p2 &lt;- chimp_post %&gt;% dplyr::select(contains(&quot;treatment&quot;)) %&gt;% set_names(&quot;R|N&quot;,&quot;L|N&quot;,&quot;R|P&quot;,&quot;L|P&quot;) %&gt;% mutate(`R|diff`= `R|N` - `R|P`, `L|diff`= `L|N` - `L|P`) %&gt;% pivot_longer(everything()) %&gt;% mutate(probability = inv_logit_scaled(value), treatment = factor(name, levels = treatment_labels)) %&gt;% mutate(treatment = fct_rev(treatment)) %&gt;% ggplot(aes(x = value, y = treatment)) + geom_rect(data = tibble(x = 1),inherit.aes = FALSE, aes(xmin = -Inf, xmax = Inf, ymin = .5, ymax = 2.5), color = clr0d, fill = fll0) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_pointinterval(.width = .95, size = 1/2, color = clr_dark) + labs(x = expression(beta[treatment]), y = NULL) + scale_y_discrete() + theme(axis.ticks.y = element_blank()) p1 + p2 p1 &lt;- data_chimp_f %&gt;% group_by(actor, treatment) %&gt;% summarise(proportion = mean(pulled_left)) %&gt;% left_join(data_chimp_f %&gt;% mutate(labels = treatment_labels[treatment]) %&gt;% distinct(actor, treatment, labels, condition, prosoc_left), by = c(&quot;actor&quot;, &quot;treatment&quot;)) %&gt;% mutate(condition = factor(condition)) %&gt;% ggplot(aes(x = labels, y = proportion)) + geom_hline(yintercept = .5, color = clr_dark, linetype = 3) + geom_line(aes(group = prosoc_left), size = 1/4, color = clr_dark) + geom_point(aes(color = condition), size = 2.5, show.legend = F) + labs(subtitle = &quot;observed proportions&quot;) new_chimp &lt;- data_chimp_f %&gt;% distinct(actor, treatment, condition_idx, prosoc_left) p2 &lt;- fitted(brms_c11_model_chimp, newdata = new_chimp) %&gt;% as_tibble() %&gt;% bind_cols(new_chimp) %&gt;% mutate(labels = treatment_labels[treatment]) %&gt;% ggplot(aes(x = labels, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_hline(yintercept = .5, color = clr_dark, linetype = 3) + geom_line(aes(group = prosoc_left), size = 1/4, color = clr_dark) + geom_pointrange(aes(color = condition_idx), fatten = 2.5, show.legend = F) + labs(subtitle = &quot;posterior predictions&quot;) p1 + p2 + plot_layout( ncol = 1 ) &amp; scale_color_manual(values = c(clr0d, clr_dark)) &amp; scale_y_continuous(&quot;proportion left lever&quot;, breaks = c(0, .5, 1), limits = c(0, 1)) &amp; facet_wrap(~ actor, nrow = 1, labeller = label_both) &amp; xlab(NULL) &amp; theme(axis.ticks.x = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), panel.background = element_rect(fill = &quot;transparent&quot;, color = clr0)) brms_c11_model_chimp_no_interaction &lt;- brm( data = data_chimp_f, family = binomial, bf(pulled_left | trials(1) ~ a + bs + bc, a ~ 0 + actor, bs ~ 0 + side_idx, bc ~ 0 + condition_idx, nl = TRUE), prior = c(prior(normal(0, 1.5), nlpar = a), prior(normal(0, 0.5), nlpar = bs), prior(normal(0, 0.5), nlpar = bc)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c11_model_chimp_no_interaction&quot;) brms_c11_model_chimp &lt;- add_criterion(brms_c11_model_chimp, c(&quot;loo&quot;, &quot;waic&quot;)) brms_c11_model_chimp_no_interaction &lt;- add_criterion(brms_c11_model_chimp_no_interaction, c(&quot;loo&quot;, &quot;waic&quot;)) loo_compare(brms_c11_model_chimp, brms_c11_model_chimp_no_interaction, criterion = &quot;loo&quot;) %&gt;% print(simplify = FALSE) #&gt; elpd_diff se_diff elpd_loo se_elpd_loo #&gt; brms_c11_model_chimp_no_interaction 0.0 0.0 -265.4 9.6 #&gt; brms_c11_model_chimp -0.6 0.6 -266.0 9.5 #&gt; p_loo se_p_loo looic se_looic #&gt; brms_c11_model_chimp_no_interaction 7.7 0.4 530.7 19.2 #&gt; brms_c11_model_chimp 8.3 0.4 532.0 18.9 model_weights(brms_c11_model_chimp, brms_c11_model_chimp_no_interaction, weights = &quot;loo&quot;) %&gt;% round(digits = 2) #&gt; brms_c11_model_chimp brms_c11_model_chimp_no_interaction #&gt; 0.35 0.65 new_chimp &lt;- data_chimp_f %&gt;% distinct(actor, treatment, condition_idx, side_idx, prosoc_left) fitted(brms_c11_model_chimp_no_interaction, newdata = new_chimp) %&gt;% data.frame() %&gt;% bind_cols(new_chimp) %&gt;% mutate(labels = treatment_labels[treatment]) %&gt;% ggplot(aes(x = labels, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_hline(yintercept = .5, color = clr_dark) + geom_line(aes(group = side_idx), size = 1/4, color = clr_dark) + geom_pointrange(aes(color = condition_idx), fatten = 2.5, show.legend = FALSE) + scale_color_manual(values = c(clr0, clr_dark)) + scale_y_continuous(&quot;proportion left lever&quot;, breaks = c(0, .5, 1), limits = c(0, 1)) + labs(subtitle = &quot;posterior predictions for b11.5&quot;, x = NULL) + facet_wrap(~ actor, nrow = 1, labeller = label_both)+ theme(axis.ticks.x = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), panel.background = element_rect(fill = &quot;transparent&quot;, color = clr0)) log_lik(brms_c11_model_chimp) %&gt;% str() #&gt; num [1:4000, 1:504] -0.442 -0.517 -0.51 -0.423 -0.439 ... #&gt; - attr(*, &quot;dimnames&quot;)=List of 2 #&gt; ..$ : NULL #&gt; ..$ : NULL 12.5.1.1 Relative Shark and Absolute Deer as_draws_df(brms_c11_model_chimp) %&gt;% mutate(proportional_odds = exp(b_b_treatment4 - b_b_treatment2)) %&gt;% mean_qi(proportional_odds) #&gt; # A tibble: 1 × 6 #&gt; proportional_odds .lower .upper .width .point .interval #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 0.927 0.523 1.54 0.95 mean qi 12.5.1.2 Aggregated Binomial: Chimpanzees Again, Condensed data_chimp_aggregated_f &lt;- data_chimp_f %&gt;% group_by(treatment, actor, side_idx, condition_idx) %&gt;% summarise(left_pulls = sum(pulled_left)) %&gt;% ungroup() brms_c11_model_chimp_aggregated &lt;- brm( data = data_chimp_aggregated_f, family = binomial, bf(left_pulls | trials(18) ~ a + b, a ~ 0 + actor, b ~ 0 + treatment, nl = TRUE), prior = c(prior(normal(0, 1.5), nlpar = a), prior(normal(0, 0.5), nlpar = b)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c11_model_chimp_aggregated&quot;) bind_rows(as_draws_df(brms_c11_model_chimp), as_draws_df(brms_c11_model_chimp_aggregated)) %&gt;% mutate(fit = rep(c(&quot;brms_c11_model_chimp&quot;, &quot;brms_c11_model_chimp_aggregated&quot;), each = n() / 2)) %&gt;% pivot_longer(b_a_actor1:b_b_treatment4) %&gt;% # plot ggplot(aes(x = value, y = name, color = fit)) + stat_pointinterval(.width = .95, size = 2/3, position = position_dodge(width = .8)) + scale_color_manual(values = c(clr0d, clr_dark)) + labs(x = &quot;posterior (log-odds scale)&quot;, y = NULL) + theme(axis.ticks.y = element_blank(), legend.position = &quot;bottom&quot;) brms_c11_model_chimp &lt;- add_criterion(brms_c11_model_chimp, &quot;loo&quot;) brms_c11_model_chimp_aggregated &lt;- add_criterion(brms_c11_model_chimp_aggregated, &quot;loo&quot;) loo_compare(brms_c11_model_chimp, brms_c11_model_chimp_aggregated, criterion = &quot;loo&quot;) %&gt;% # does not work on models based on different data - that is the point here... print(simplify = FALSE) #&gt; Error in h(simpleError(msg, call)): error in evaluating the argument &#39;x&#39; in selecting a method for function &#39;print&#39;: Not all models have the same number of data points. loo(brms_c11_model_chimp_aggregated) %&gt;% loo::pareto_k_table() %&gt;% as.data.frame() %&gt;% knitr::kable() Count Proportion Min. n_eff (-Inf, 0.5] 20 0.7142857 1512.1786 (0.5, 0.7] 7 0.2500000 513.1781 (0.7, 1] 1 0.0357143 747.3307 (1, Inf) 0 0.0000000 NA 12.5.1.3 Aggregated Binomial: Graduate School Admissions data_ucb_f &lt;- data_ucb %&gt;% mutate(gid = factor(applicant.gender, levels = c(&quot;male&quot;, &quot;female&quot;)), case = factor(1:n())) brms_c11_model_ucb &lt;- brm( data = data_ucb_f, family = binomial, admit | trials(applications) ~ 0 + gid, prior(normal(0, 1.5), class = b), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 42, file = &quot;brms/brms_c11_model_ucb&quot;) as_draws_df(brms_c11_model_ucb) %&gt;% mutate(diff_a = b_gidmale - b_gidfemale, diff_p = inv_logit_scaled(b_gidmale) - inv_logit_scaled(b_gidfemale)) %&gt;% pivot_longer(contains(&quot;diff&quot;)) %&gt;% group_by(name) %&gt;% mean_qi(value, .width = .89) #&gt; # A tibble: 2 × 7 #&gt; name value .lower .upper .width .point .interval #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 diff_a 0.611 0.505 0.715 0.89 mean qi #&gt; 2 diff_p 0.142 0.118 0.165 0.89 mean qi ucb_predict &lt;- predict(brms_c11_model_ucb) %&gt;% data.frame() %&gt;% bind_cols(data_ucb_f) ucb_predict %&gt;% ggplot(aes(x = case, y = admit / applications)) + geom_pointrange(aes(y = Estimate / applications, ymin = Q2.5 / applications , ymax = Q97.5 / applications), color = clr0d, fill = clr0, shape = 21) + geom_point(color = clr_dark) + geom_textpath(aes(group = dept, label = dept), color = clr_dark, family = fnt_sel) + scale_y_continuous(&quot;Proportion admitted&quot;, limits = 0:1) + ggtitle(&quot;Posterior validation check&quot;) + theme(axis.ticks.x = element_blank()) model_ucb_dept #&gt; Hamiltonian Monte Carlo approximation #&gt; 8000 samples from 4 chains #&gt; #&gt; Sampling durations (seconds): #&gt; warmup sample total #&gt; chain:1 0.25 0.25 0.49 #&gt; chain:2 0.22 0.23 0.44 #&gt; chain:3 0.23 0.27 0.50 #&gt; chain:4 0.30 0.20 0.49 #&gt; #&gt; Formula: #&gt; admit ~ dbinom(applications, p) #&gt; logit(p) &lt;- alpha[gid] + delta[dept_idx] #&gt; alpha[gid] ~ dnorm(0, 1.5) #&gt; delta[dept_idx] ~ dnorm(0, 1.5) brms_c11_model_ucb_dept &lt;- brm( data = data_ucb_f, family = binomial, bf(admit | trials(applications) ~ a + d, a ~ 0 + gid, d ~ 0 + dept, nl = TRUE), prior = c(prior(normal(0, 1.5), nlpar = a), prior(normal(0, 1.5), nlpar = d)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 42, file = &quot;brms/brms_c11_model_ucb_dept&quot;) ucb_dept_predict &lt;- predict(brms_c11_model_ucb_dept) %&gt;% data.frame() %&gt;% bind_cols(data_ucb_f) ucb_dept_predict %&gt;% ggplot(aes(x = case, y = admit / applications)) + geom_pointrange(aes(y = Estimate / applications, ymin = Q2.5 / applications , ymax = Q97.5 / applications), color = clr0d, fill = clr0, shape = 21) + geom_point(color = clr_dark) + geom_textpath(aes(group = dept, label = dept), color = clr_dark, family = fnt_sel) + scale_y_continuous(&quot;Proportion admitted&quot;, limits = 0:1) + ggtitle(&quot;Posterior validation check&quot;) + theme(axis.ticks.x = element_blank()) as_draws_df(brms_c11_model_ucb_dept) %&gt;% as_tibble() %&gt;% dplyr::select(starts_with(&quot;b_a&quot;) | starts_with(&quot;b_d&quot;)) %&gt;% set_names(c(&quot;alpha[male]&quot;, &quot;alpha[female]&quot;, str_c(&quot;delta[&quot;, LETTERS[1:6], &quot;]&quot;))) %&gt;% ggpairs( lower = list(continuous = wrap(my_lower, col = clr_dark)), diag = list(continuous = wrap(my_diag, fill = fll0, col = clr_dark, adjust = .7)), upper = list(continuous = wrap(my_upper , size = 3, col = &quot;black&quot;, family = fnt_sel)) ) + theme(panel.border = element_rect(color = clr_dark, fill = &quot;transparent&quot;)) 12.5.2 Poisson Regression data_kline_f &lt;- data_kline %&gt;% mutate(contact_idx = factor(contact_idx)) # intercept only brms_c11_model_ocean_intercept &lt;- brm( data = data_kline_f, family = poisson, total_tools ~ 1, prior(normal(3, 0.5), class = Intercept), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c11_model_ocean_intercept&quot;) # interaction model brms_c11_model_ocean_interact &lt;- brm( data = data_kline_f, family = poisson, bf(total_tools ~ a + b * pop_log_scl, a + b ~ 0 + contact_idx, nl = TRUE), prior = c(prior(normal(3, 0.5), nlpar = a), prior(normal(0, 0.2), nlpar = b)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c11_model_ocean_interact&quot;) brms_c11_model_ocean_intercept &lt;- add_criterion(brms_c11_model_ocean_intercept, &quot;loo&quot;) brms_c11_model_ocean_interact &lt;- add_criterion(brms_c11_model_ocean_interact, &quot;loo&quot;) loo_compare(brms_c11_model_ocean_intercept, brms_c11_model_ocean_interact, criterion = &quot;loo&quot;) %&gt;% print(simplify = FALSE) #&gt; elpd_diff se_diff elpd_loo se_elpd_loo p_loo #&gt; brms_c11_model_ocean_interact 0.0 0.0 -42.7 6.6 7.1 #&gt; brms_c11_model_ocean_intercept -27.7 16.4 -70.4 16.7 7.8 #&gt; se_p_loo looic se_looic #&gt; brms_c11_model_ocean_interact 2.7 85.4 13.2 #&gt; brms_c11_model_ocean_intercept 3.4 140.9 33.4 model_weights(brms_c11_model_ocean_intercept, brms_c11_model_ocean_interact, weights = &quot;loo&quot;) %&gt;% round(digits = 2) #&gt; brms_c11_model_ocean_intercept brms_c11_model_ocean_interact #&gt; 0 1 loo(brms_c11_model_ocean_interact) %&gt;% loo::pareto_k_table() %&gt;% as.data.frame()%&gt;% knitr::kable() Count Proportion Min. n_eff (-Inf, 0.5] 5 0.5 1909.98609 (0.5, 0.7] 3 0.3 331.31839 (0.7, 1] 1 0.1 36.61212 (1, Inf) 1 0.1 32.51158 tibble(culture = data_kline_f$culture, k = brms_c11_model_ocean_interact$criteria$loo$diagnostics$pareto_k) %&gt;% arrange(desc(k)) %&gt;% mutate_if(is.double, round, digits = 2) #&gt; # A tibble: 10 × 2 #&gt; culture k #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Hawaii 1.16 #&gt; 2 Tonga 0.87 #&gt; 3 Yap 0.56 #&gt; 4 Trobriand 0.54 #&gt; 5 Malekula 0.51 #&gt; 6 Manus 0.43 #&gt; 7 Lau Fiji 0.32 #&gt; 8 Chuuk 0.31 #&gt; 9 Tikopia 0.28 #&gt; 10 Santa Cruz 0.18 high_k_cultures &lt;- tibble(culture = data_kline_f$culture, k = brms_c11_model_ocean_interact$criteria$loo$diagnostics$pareto_k) %&gt;% arrange(desc(k)) %&gt;% filter(k &gt; .5) %&gt;% transmute(culture) %&gt;% unlist() new_ocean &lt;- distinct(data_kline_f, contact_idx) %&gt;% expand(contact_idx, pop_log_scl = seq(from = -2, to = 2.5, length.out = 100)) ocean_fitted &lt;- fitted(brms_c11_model_ocean_interact, newdata = new_ocean, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(new_ocean) p1 &lt;- ocean_fitted %&gt;% ggplot(aes(x = pop_log_scl, group = contact_idx, color = contact_idx)) + geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(data = bind_cols(data_kline_f, brms_c11_model_ocean_interact$criteria$loo$diagnostics), aes(y = total_tools, size = pareto_k, fill = after_scale(clr_alpha(color))), shape = 21) + ggrepel::geom_text_repel(data = bind_cols(data_kline_f, brms_c11_model_ocean_interact$criteria$loo$diagnostics) %&gt;% filter(culture %in% high_k_cultures) %&gt;% mutate(label = str_c(culture, &quot; (&quot;, round(pareto_k, digits = 2), &quot;)&quot;)), aes(y = total_tools, label = label), size = 3, seed = 11, color = &quot;black&quot;, family = fnt_sel) + labs(x = &quot;log population (std)&quot;, y = &quot;total tools&quot;) + coord_cartesian(xlim = range(brms_c11_model_ocean_interact$data$pop_log_scl), ylim = c(0, 80)) p2 &lt;- ocean_fitted %&gt;% mutate(population = exp((pop_log_scl * sd(log(data_kline_f$population))) + mean(log(data_kline_f$population)))) %&gt;% ggplot(aes(x = population, group = contact_idx, color = contact_idx)) + geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(data = bind_cols(data_kline_f, brms_c11_model_ocean_interact$criteria$loo$diagnostics), aes(y = total_tools, size = pareto_k, fill = after_scale(clr_alpha(color))), shape = 21) + scale_x_continuous(&quot;population&quot;, breaks = c(0, 50000, 150000, 250000)) + ylab(&quot;total tools&quot;) + coord_cartesian(xlim = range(data_kline_f$population), ylim = c(0, 80)) p1 + p2 &amp; scale_color_manual(values = c(clr_dark, clr0dd)) &amp; scale_size(range = c(2, 5)) &amp; theme(legend.position = &quot;none&quot;) 12.5.2.1 Modeling Tool Innovation brms_c11_model_ocean_scientific &lt;- brm( data = data_kline_f, family = poisson(link = &quot;identity&quot;), bf(total_tools ~ exp(a) * population^b / g, a + b ~ 0 + contact_idx, g ~ 1, nl = TRUE), prior = c(prior(normal(1, 1), nlpar = a), prior(exponential(1), nlpar = b, lb = 0), prior(exponential(1), nlpar = g, lb = 0)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, control = list(adapt_delta = .95), file = &quot;brms/brms_c11_model_ocean_scientific&quot;) brms_c11_model_ocean_scientific &lt;- add_criterion(brms_c11_model_ocean_scientific, criterion = &quot;loo&quot;, moment_match = TRUE) loo(brms_c11_model_ocean_scientific) #&gt; #&gt; Computed from 4000 by 10 log-likelihood matrix #&gt; #&gt; Estimate SE #&gt; elpd_loo -40.8 6.0 #&gt; p_loo 4.9 1.6 #&gt; looic 81.5 12.0 #&gt; ------ #&gt; Monte Carlo SE of elpd_loo is 0.1. #&gt; #&gt; Pareto k diagnostic values: #&gt; Count Pct. Min. n_eff #&gt; (-Inf, 0.5] (good) 7 70.0% 361 #&gt; (0.5, 0.7] (ok) 3 30.0% 191 #&gt; (0.7, 1] (bad) 0 0.0% &lt;NA&gt; #&gt; (1, Inf) (very bad) 0 0.0% &lt;NA&gt; #&gt; #&gt; All Pareto k estimates are ok (k &lt; 0.7). #&gt; See help(&#39;pareto-k-diagnostic&#39;) for details. new_oceans &lt;- distinct(data_kline_f, contact_idx) %&gt;% expand(contact_idx, population = seq(from = 0, to = 300000, length.out = 100)) # compute the poster predictions for lambda fitted(brms_c11_model_ocean_scientific, newdata = new_oceans, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(new_oceans) %&gt;% ggplot(aes(x = population, group = contact_idx, color = contact_idx)) + geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(data = bind_cols(data_kline_f, brms_c11_model_ocean_scientific$criteria$loo$diagnostics), aes(y = total_tools, size = pareto_k, fill = after_scale(clr_alpha(color))), shape = 21) + scale_color_manual(values = c(clr_dark, clr0dd)) + scale_size(range = c(2, 5)) + scale_x_continuous(&quot;population&quot;, breaks = c(0, 50000, 150000, 250000)) + ylab(&quot;total tools&quot;) + coord_cartesian(xlim = range(data_kline_f$population), ylim = range(data_kline_f$total_tools)) + theme(legend.position = &quot;none&quot;) 12.5.2.2 Exposure and the Offset data_books_f &lt;- data_books %&gt;% mutate(monastery = factor(monastery)) brms_c11_model_books &lt;- brm( data = data_books_f, family = poisson, books ~ 1 + offset(days_log) + monastery, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = b)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 42, file = &quot;brms/brms_c11_model_books&quot;) as_draws_df(brms_c11_model_books) %&gt;% mutate(lambda_old = exp(b_Intercept), lambda_new = exp(b_Intercept + b_monastery1)) %&gt;% pivot_longer(contains(&quot;lambda&quot;)) %&gt;% mutate(name = factor(name, levels = c(&quot;lambda_old&quot;, &quot;lambda_new&quot;))) %&gt;% group_by(name) %&gt;% mean_hdi(value, .width = .89) %&gt;% mutate_if(is.double, round, digits = 2) #&gt; # A tibble: 2 × 7 #&gt; name value .lower .upper .width .point .interval #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 lambda_old 1.39 1.05 1.7 0.89 mean hdi #&gt; 2 lambda_new 0.53 0.32 0.73 0.89 mean hdi 12.5.3 Multinomial and Categorical Models 12.5.3.1 Predictors Matched to Outcomes data_career &lt;- tibble(career = factor(career)) %&gt;% mutate(career_income = ifelse(career == 3, 5, career)) data_career %&gt;% ggplot(aes(y = career)) + ggstance::geom_barh(size = 0, fill = clr_dark) brms_c11_model_career &lt;- brm( data = data_career, family = categorical(link = logit, refcat = 3), career ~ 1, prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1), prior(normal(0, 1), class = Intercept, dpar = mu2)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 42, file = &quot;brms/brms_c11_model_career&quot;) fitted(brms_c11_model_career)[1, , ] %&gt;% round(digits = 2) %&gt;% t() %&gt;% as.data.frame() %&gt;% knitr::kable() Estimate Est.Error Q2.5 Q97.5 P(Y = 1) 0.10 0.01 0.08 0.13 P(Y = 2) 0.16 0.02 0.13 0.19 P(Y = 3) 0.74 0.02 0.70 0.78 tibble(income = c(1, 2, 5)) %&gt;% mutate(score = 0.5 * income) %&gt;% mutate(p = exp(score) / sum(exp(score))) #&gt; # A tibble: 3 × 3 #&gt; income score p #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 0.5 0.0996 #&gt; 2 2 1 0.164 #&gt; 3 5 2.5 0.736 as_draws_df(brms_c11_model_career) %&gt;% mutate(b_mu3_Intercept = 0) %&gt;% mutate(p1 = exp(b_mu1_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept)), p2 = exp(b_mu2_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept)), p3 = exp(b_mu3_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept))) %&gt;% pivot_longer(p1:p3) %&gt;% group_by(name) %&gt;% mean_qi(value) %&gt;% mutate_if(is.double, round, digits = 2) #&gt; # A tibble: 3 × 7 #&gt; name value .lower .upper .width .point .interval #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 p1 0.1 0.08 0.13 0.95 mean qi #&gt; 2 p2 0.16 0.13 0.19 0.95 mean qi #&gt; 3 p3 0.74 0.7 0.78 0.95 mean qi Two alternative ways to fit multinomial models within {brms}: # verbose syntax brms_c11_model_career_verbose &lt;- brm( data = data_career, family = categorical(link = logit, refcat = 3), bf(career ~ 1, mu1 ~ 1, mu2 ~ 1), prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1), prior(normal(0, 1), class = Intercept, dpar = mu2)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 42, file = &quot;brms/brms_c11_model_career_verbose&quot;) # nonlinear syntax brms_c11_model_career_nonlinear &lt;- brm( data = data_career, family = categorical(link = logit, refcat = 3), bf(career ~ 1, nlf(mu1 ~ a1), nlf(mu2 ~ a2), a1 + a2 ~ 1), prior = c(prior(normal(0, 1), class = b, nlpar = a1), prior(normal(0, 1), class = b, nlpar = a2)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 42, file = &quot;brms/brms_c11_model_career_nonlinear&quot;) brms_c11_model_career_d &lt;- brm( data = data_career, family = categorical(link = logit, refcat = 3), bf(career ~ 1, nlf(mu1 ~ a1 + b * 1), nlf(mu2 ~ a2 + b * 2), a1 + a2 + b ~ 1), prior = c(prior(normal(0, 1), class = b, nlpar = a1), prior(normal(0, 1), class = b, nlpar = a2), prior(normal(0, 0.5), class = b, nlpar = b, lb = 0)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 42, control = list(adapt_delta = .99), file = &quot;brms/brms_c11_model_career_d&quot;) income &lt;- c(1, 2, 5) as_draws_df(brms_c11_model_career_d) %&gt;% transmute(s1 = b_a1_Intercept + b_b_Intercept * income[1], s2_orig = b_a2_Intercept + b_b_Intercept * income[2], s2_new = b_a2_Intercept + b_b_Intercept * income[2] * 2) %&gt;% mutate(p_orig = purrr::map2_dbl(s1, s2_orig, ~softmax(.x, .y, 0)[2]), p_new = purrr::map2_dbl(s1, s2_new, ~softmax(.x, .y, 0)[2])) %&gt;% mutate(p_diff = p_new - p_orig) %&gt;% mean_qi(p_diff) %&gt;% mutate_if(is.double, round, digits = 2) #&gt; # A tibble: 1 × 6 #&gt; p_diff .lower .upper .width .point .interval #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 0.05 0 0.16 0.95 mean qi 12.5.3.2 Predictors Matched to Observations p1 &lt;- career_fam %&gt;% mutate(career = as.factor(career)) %&gt;% ggplot(aes(x = family_income, color = career)) + geom_density(size = .5, aes(fill = after_scale(clr_alpha(color)))) + scale_color_manual(values = c(clr_dark, clr0dd, clr0)) + theme(legend.position = &quot;none&quot;) p2 &lt;- career_fam %&gt;% mutate(career = as.factor(career)) %&gt;% mutate(fi = santoku::chop_width(family_income, width = .1, start = 0, labels = 1:10)) %&gt;% count(fi, career) %&gt;% group_by(fi) %&gt;% mutate(proportion = n / sum(n)) %&gt;% mutate(f = as.double(fi)) %&gt;% ggplot(aes(x = (f - 1) / 9, y = proportion, color = career)) + geom_area(size = .5, aes(fill = after_scale(clr_alpha(color)))) + scale_color_manual(values = c(clr_dark, clr0dd, clr0)) + xlab(&quot;family_income, descritized&quot;) p1 + p2 + plot_layout(guides = &quot;collect&quot;) &amp; theme(legend.position = &quot;bottom&quot;) data_career_fam &lt;- career_fam %&gt;% dplyr::select(family_income, career) %&gt;% mutate(career = factor(career)) brms_c11_model_career_fam &lt;- brm( data = data_career_fam, family = categorical(link = logit, refcat = 3), bf(career ~ 1, nlf(mu1 ~ a1 + b1 * family_income), nlf(mu2 ~ a2 + b2 * family_income), a1 + a2 + b1 + b2 ~ 1), prior = c(prior(normal(0, 1.5), class = b, nlpar = a1), prior(normal(0, 1.5), class = b, nlpar = a2), prior(normal(0, 1), class = b, nlpar = b1), prior(normal(0, 1), class = b, nlpar = b2)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 42, file = &quot;brms/brms_c11_model_career_fam&quot;) brms_c11_model_career_fam &lt;- add_criterion(brms_c11_model_career_fam, &quot;loo&quot;) loo(brms_c11_model_career_fam) #&gt; #&gt; Computed from 4000 by 500 log-likelihood matrix #&gt; #&gt; Estimate SE #&gt; elpd_loo -337.8 15.8 #&gt; p_loo 3.1 0.2 #&gt; looic 675.6 31.6 #&gt; ------ #&gt; Monte Carlo SE of elpd_loo is 0.0. #&gt; #&gt; All Pareto k estimates are good (k &lt; 0.5). #&gt; See help(&#39;pareto-k-diagnostic&#39;) for details. new_careers &lt;- tibble(family_income = seq(from = 0, to = 1, length.out = 60)) careers_fitted &lt;- fitted(brms_c11_model_career_fam, newdata = new_careers) p1 &lt;- rbind(careers_fitted[, , 1], careers_fitted[, , 2], careers_fitted[, , 3]) %&gt;% as_tibble() %&gt;% bind_cols(new_careers %&gt;% expand(career = 1:3, family_income)) %&gt;% mutate(career = str_c(&quot;career: &quot;, career)) %&gt;% ggplot(aes(x = family_income, y = Estimate, ymin = Q2.5, ymax = Q97.5, color = career)) + geom_smooth(size = .5, stat = &quot;identity&quot;, aes(fill = after_scale(clr_alpha(color)))) + scale_color_manual(values = c(clr_dark, clr0dd, clr0)) + scale_x_continuous(breaks = 0:2 / 2) + scale_y_continuous(&quot;probability&quot;, limits = c(0, 1), breaks = 0:3 / 3, labels = c(&quot;0&quot;, &quot;.33&quot;, &quot;.67&quot;, &quot;1&quot;)) + theme(axis.text.y = element_text(hjust = 0), legend.position = &quot;none&quot;) p2 &lt;- rbind(careers_fitted[, , 1], careers_fitted[, , 2], careers_fitted[, , 3]) %&gt;% as_tibble() %&gt;% bind_cols(new_careers %&gt;% expand(career = 1:3, family_income)) %&gt;% group_by(family_income) %&gt;% mutate(proportion = Estimate / sum(Estimate), career = factor(career)) %&gt;% ggplot(aes(x = family_income, y = proportion)) + geom_area(aes(color = career, fill = after_scale(clr_alpha(color))), size = .5) + scale_color_manual(values = c(clr_dark, clr0dd, clr0)) p1 + p2 + plot_layout(guides = &quot;collect&quot;) &amp; theme(legend.position = &quot;bottom&quot;) 12.5.3.3 Multinomial in Disguise as Poisson data_ucb_f &lt;- data_ucb %&gt;% rename(rej = &quot;reject&quot;) # &#39;reject&#39; is a reserved word brms_c11_model_ucb_binom &lt;- brm( data = data_ucb_f, family = binomial, admit | trials(applications) ~ 1, prior(normal(0, 1.5), class = Intercept), iter = 2000, warmup = 1000, cores = 3, chains = 3, seed = 42, file = &quot;brms/brms_c11_model_ucb_binom&quot;) # Poisson model of overall admission rate and rejection rate brms_c11_model_ucb_poisson &lt;- brm( data = data_ucb_f, family = poisson, mvbind(admit, rej) ~ 1, prior(normal(0, 1.5), class = Intercept), iter = 2000, warmup = 1000, cores = 3, chains = 3, seed = 42, file = &quot;brms/brms_c11_model_ucb_poisson&quot;) ucb_posterior &lt;- as_draws_df(brms_c11_model_ucb_poisson) ucb_posterior %&gt;% mutate(admit = exp(b_admit_Intercept), reject = exp(b_rej_Intercept)) %&gt;% pivot_longer(admit:reject) %&gt;% ggplot(aes(x = value, y = name, fill = name)) + stat_halfeye(point_interval = median_qi, .width = .95, color = &quot;black&quot;) + scale_fill_manual(values = c(clr0d, clr_dark)) + labs(title = &quot; Mean admit/reject rates across departments&quot;, x = &quot;# applications&quot;, y = NULL) + theme(axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) summary(brms_c11_model_ucb_binom)$fixed #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS #&gt; Intercept -0.4565042 0.03053728 -0.5157323 -0.3961243 1.002774 1038.096 #&gt; Tail_ESS #&gt; Intercept 1536.365 summary(brms_c11_model_ucb_poisson)$fixed #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS #&gt; admit_Intercept 4.984195 0.02339115 4.938030 5.029932 1.000916 2445.350 #&gt; rej_Intercept 5.440590 0.01933551 5.402334 5.478048 1.000355 2500.151 #&gt; Tail_ESS #&gt; admit_Intercept 1960.293 #&gt; rej_Intercept 1984.288 fixef(brms_c11_model_ucb_binom)[, &quot;Estimate&quot;] %&gt;% inv_logit_scaled() #&gt; [1] 0.3878155 k &lt;- fixef(brms_c11_model_ucb_poisson) %&gt;% as.numeric() exp(k[1]) / (exp(k[1]) + exp(k[2])) #&gt; [1] 0.3878414 bind_cols( as_draws_df(brms_c11_model_ucb_poisson) %&gt;% mutate(`the Poisson` = exp(b_admit_Intercept) / (exp(b_admit_Intercept) + exp(b_rej_Intercept))), as_draws_df(brms_c11_model_ucb_binom) %&gt;% mutate(`the binomial` = inv_logit_scaled(b_Intercept)) ) %&gt;% pivot_longer(starts_with(&quot;the&quot;)) %&gt;% ggplot(aes(x = value, y = name, fill = name)) + stat_halfeye(point_interval = median_qi, .width = c(.89, .5), color = &quot;black&quot;) + scale_fill_manual(values = c(clr0d, clr_dark)) + labs(title = &quot;Two models, same marginal posterior&quot;, x = &quot;admissions probability&quot;, y = NULL) + coord_cartesian(ylim = c(1.5, 2.35)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) 12.5.4 Survival analysis data(AustinCats, package = &quot;rethinking&quot;) data_cats &lt;- AustinCats %&gt;% as_tibble() %&gt;% mutate(black = ifelse(color == &quot;Black&quot;, &quot;black&quot;, &quot;other&quot;), adopted = ifelse(out_event == &quot;Adoption&quot;, 1, 0), censored = ifelse(out_event != &quot;Adoption&quot;, 1, 0)) data_cats %&gt;% count(color) %&gt;% arrange(-n) %&gt;% slice(1:10) #&gt; # A tibble: 10 × 2 #&gt; color n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Brown Tabby 3401 #&gt; 2 Black 2965 #&gt; 3 Black/White 2113 #&gt; 4 Brown Tabby/White 1794 #&gt; 5 Orange Tabby 1562 #&gt; 6 Tortie 1057 #&gt; 7 Calico 994 #&gt; 8 Blue Tabby 865 #&gt; 9 Orange Tabby/White 858 #&gt; 10 Blue 813 data_cats %&gt;% count(black) %&gt;% mutate(percent = 100 * n / sum(n)) %&gt;% mutate(label = str_c(round(percent, digits = 1), &quot;%&quot;)) %&gt;% ggplot(aes(y = black)) + geom_col(aes(x = n, color = black, fill = after_scale(clr_alpha(color,.65))), size = .5) + geom_text(aes(x = n - 250, label = label), color = &quot;white&quot;, family = fnt_sel, hjust = 1.1) + scale_color_manual(values = c(&quot;black&quot;, clr0dd), breaks = NULL) + scale_x_continuous(expression(italic(n)), breaks = c(0, count(data_cats, black) %&gt;% pull(n))) + labs(title = &quot;Cat color&quot;, y = NULL) + theme(axis.ticks.y = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.major.y = element_blank()) data_cats %&gt;% count(out_event) #&gt; # A tibble: 7 × 2 #&gt; out_event n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Adoption 11351 #&gt; 2 Censored 549 #&gt; 3 Died 369 #&gt; 4 Disposal 9 #&gt; 5 Euthanasia 636 #&gt; 6 Missing 28 #&gt; 7 Transfer 9414 data_cats %&gt;% mutate(censored = factor(censored)) %&gt;% filter(days_to_event &lt; 300) %&gt;% ggplot(aes(x = days_to_event, y = censored)) + # let&#39;s just mark off the 50% intervals stat_halfeye(.width = .5, fill = fll0dd, height = 4) + scale_y_discrete(NULL, labels = c(&quot;censored == 0&quot;, &quot;censored == 1&quot;)) + coord_cartesian(ylim = c(1.5, 2.6)) + theme(axis.ticks.y = element_blank()) McElreath fit his survival model using the exponential likelihood. […] If we let \\(y\\) be a non-negative continuous variable, the probability density function for the exponential distribution is \\[ f(y) = \\lambda e^{-\\lambda y} \\] where \\(\\lambda\\) is called the rate. The mean of the exponential distribution is the inverse of the rate: \\[ E[y] =\\frac{1}{\\lambda} \\] we can write our continuous-time survival model as: \\[ \\begin{align*} \\text{days_to_event}_i | \\text{censored}_i = 0 &amp; \\sim \\operatorname{Exponential}(\\lambda_i) \\\\ \\text{days_to_event}_i | \\text{censored}_i = 1 &amp; \\sim \\operatorname{Exponential-CCDF}(\\lambda_i) \\\\ \\lambda_i &amp; = 1 / \\mu_i \\\\ \\log \\mu_i &amp; = \\alpha_{\\text{black}[i]} \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0, 1). \\end{align*} \\] When you fit a continuous-time survival analysis with brm(), you’ll want to tell the software about how the data have been censored with help from the cens() function. For many of the models in this chapter, we used the trials() function to include the \\(n_{i}\\) information into our binomial models. Both trials() and cens() are members of a class of functions designed to provide supplemental information about our criterion variables to brm(). The cens() function lets us add in information about censoring. […] We will feed this information into the model with the formula code days_to_event | cens(censored), where censored is the name of the variable in our data that indexes the censoring. brms_c11_model_cats &lt;- brm( data = data_cats, family = exponential, days_to_event | cens(censored) ~ 0 + black, prior(normal(0, 1), class = b), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c11_model_cats&quot;) Since we modeled \\(\\log \\mu_{i}\\), we need to transform our \\(\\alpha\\) parameters back into the \\(\\lambda\\) metric using the formula: \\[ \\begin{align*} \\log \\mu &amp; = \\alpha_\\text{black}, &amp;&amp; \\text{and} \\\\ \\lambda &amp; = 1 / \\mu, &amp;&amp; \\text{therefore} \\\\ \\lambda_\\text{black} &amp; = 1 / \\exp(\\alpha_\\text{black}). \\end{align*} \\] (1 / exp(fixef(brms_c11_model_cats)[, -2])) %&gt;% as.data.frame() %&gt;% knitr::kable() Estimate Q2.5 Q97.5 blackblack 0.0174122 0.0183289 0.0165367 blackother 0.0206480 0.0210554 0.0202256 cats_fixed &lt;- fixef(brms_c11_model_cats) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% as_tibble() %&gt;% mutate(color = str_remove(rowname, &quot;black&quot;)) %&gt;% expand(nesting(Estimate, Q2.5, Q97.5, color), days = 0:100) %&gt;% mutate(m = 1 - pexp(days, rate = 1 / exp(Estimate)), ll = 1 - pexp(days, rate = 1 / exp(Q2.5)), ul = 1 - pexp(days, rate = 1 / exp(Q97.5))) p1 &lt;- cats_fixed %&gt;% ggplot(aes(x = days)) + geom_hline(yintercept = .5, linetype = 3, color = clr_dark) + geom_smooth(aes(ymin = ll, y = m, ymax = ul, color = color, fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(black = &quot;black&quot;, other = clr0d)) + scale_y_continuous(&quot;proportion remaining&quot;, breaks = c(0, .5, 1), limits = 0:1) + xlab(&quot;days to adoption&quot;) About how many days would it take for half of the cats of a given color to be adopted? qexp(p = .5, rate = 1 / exp(fixef(brms_c11_model_cats)[1, 1])) #&gt; [1] 39.80819 cats_posterior &lt;- as_draws_df(brms_c11_model_cats) %&gt;% pivot_longer(starts_with(&quot;b_&quot;)) %&gt;% mutate(color = str_remove(name, &quot;b_black&quot;), days = qexp(p = .5, rate = 1 / exp(value))) # axis breaks cats_medians &lt;- group_by(cats_posterior, color) %&gt;% summarise(med = median(days)) %&gt;% pull(med) %&gt;% round(., digits = 1) p2 &lt;- cats_posterior %&gt;% ggplot(aes(x = days, y = color)) + stat_halfeye(.width = .95, aes(fill = color), height = 4) + scale_x_continuous(&quot;days untill 50% are adopted&quot;, breaks = c(30, cats_medians, 45), labels = c(&quot;30&quot;, cats_medians, &quot;45&quot;), limits = c(30, 45)) + scale_fill_manual(values = c(&quot;black&quot;, clr0d) %&gt;% clr_alpha() %&gt;% set_names(nm = c(&quot;black&quot;, &quot;other&quot;)), guide = &quot;none&quot;) + ylab(NULL) + coord_cartesian(ylim = c(1.5, 5.1)) + theme(axis.ticks.y = element_blank(), panel.grid.minor.x = element_blank()) p1 + p2 + plot_layout(guides = &quot;collect&quot;) &amp; theme(legend.position = &quot;bottom&quot;) 12.6 pymc3 section × "],["rethinking-chapter-12.html", "13 Rethinking: Chapter 12 13.1 Over-Dispersed Counts 13.2 Zero-Inflated Outcomes 13.3 Ordered Categorical Outcomes 13.4 Ordered Categorical Predictors 13.5 Homework 13.6 {brms} section 13.7 pymc3 section", " 13 Rethinking: Chapter 12 Monsters and Mixtures by Richard McElreath, building on the Summaries by Solomon Kurz. 13.1 Over-Dispersed Counts Continuous mixture models to cope with unmeasured sources of variation for count data. 13.1.1 Beta-Binomial This is a mixture of binomial distributions which estimates the distribution of success (beta-distribution; instead of a single probability). The beta distribution has two parameters (\\(\\bar{p}\\), or \\(\\mu\\) the average probability and \\(\\theta\\), or \\(\\kappa\\) the shape). It is often alternatively defined in terms of \\(\\alpha\\) and \\(\\beta\\) (eg. in R): \\[ Beta(y | \\alpha, \\beta) = \\frac{y^{\\alpha - 1} (1 - y)^{\\beta - 1}}{B(\\alpha, \\beta)} \\] were \\(B()\\) is the Beta function that computes the normalization. The connection to \\(\\bar{p}\\)/\\(\\mu\\) and \\(\\theta\\)/\\(\\kappa\\) arises from: \\[ \\mu = \\frac{\\alpha}{\\alpha + \\beta} \\] and \\[ \\kappa = \\alpha + \\beta \\] With these parameters, the standard deviation of the beta distribution is \\[ \\sigma = \\sqrt{\\mu(1 - \\mu) / (\\kappa + 1)} \\] library(rethinking) draw_beta &lt;- function(theta = 2, prob = .5){ ggplot() + stat_function(fun = function(x){dbeta2(x,prob = prob, theta = theta)}, geom = &quot;area&quot;, color = clr_current, fill = fll_current()) + coord_cartesian(xlim = 0:1, ylim = c(0, 2)) + labs(subtitle = glue(&quot;p: {prob}; theta: {theta}&quot;), y = &#39;density&#39;, x = &quot;probability&quot;) + theme(plot.subtitle = element_text(hjust = .5)) } draw_beta() + draw_beta(5) + draw_beta(.5) + draw_beta(prob = .25) + draw_beta(5, .25) + draw_beta(.5, .25) data(UCBadmit) data_ucb &lt;- UCBadmit %&gt;% as_tibble() %&gt;% mutate(gid = 3L - as.integer(factor(applicant.gender))) rm(UCBadmit) \\[ \\begin{array}{rclr} A_{i} &amp; \\sim &amp; BetaBinomial(N_{i}, \\bar{p}_{i}, \\theta) &amp; \\textrm{[likelihood]}\\\\ \\textrm{logit}(\\bar{p}_{i}) &amp; = &amp; \\alpha_{GID[i]} &amp; \\textrm{[linear model]}\\\\ \\alpha_{j} &amp; \\sim &amp; Normal(0, 1.5) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\theta &amp; = &amp; \\phi + 2 &amp; \\textrm{[$\\theta$ prior, forcing $\\gt 2$]}\\\\ \\phi &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\phi$ prior]}\\\\ \\end{array} \\] data_ucb_list &lt;- data_ucb %&gt;% dplyr::select(admit, applications, gid) %&gt;% as.list() model_ucb_beta &lt;- ulam( flist = alist( admit ~ dbetabinom( applications, p_bar, theta ), logit(p_bar) &lt;- alpha[gid], alpha[gid] ~ dnorm( 0, 1.5 ), transpars&gt; theta &lt;&lt;- phi + 2.0, phi ~ dexp(1) ), data = data_ucb_list, chains = 4, cores = 4, log_lik = TRUE ) ucb_posterior_means &lt;- extract.samples(model_ucb_beta) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% mutate(alpha.contrast = alpha.1 - alpha.2, across(c(alpha.1, alpha.2), .fns = logistic,.names = &quot;p_{.col}&quot;)) precis(ucb_posterior_means) %&gt;% knit_precis(param_name = &quot;column&quot;) column mean sd 5.5% 94.5% histogram alpha.1 -0.44 0.39 -1.07 0.19 ▁▁▁▇▇▂▁ alpha.2 -0.31 0.43 -0.98 0.36 ▁▁▅▇▃▁▁ phi 0.99 0.79 0.09 2.46 ▇▇▅▂▁▁▁▁▁ theta 2.99 0.79 2.09 4.46 ▇▇▅▂▁▁▁▁▁ alpha.contrast -0.13 0.59 -1.09 0.81 ▁▁▁▅▇▇▂▁▁ p_alpha.1 0.40 0.09 0.26 0.55 ▁▁▁▁▃▅▇▇▅▃▁▁▁▁ p_alpha.2 0.43 0.10 0.27 0.59 ▁▁▁▃▅▇▇▇▅▂▁▁▁▁▁ p1 &lt;- ggplot() + (pmap( ucb_posterior_means %&gt;% filter(row_number() &lt; 51) %&gt;% rename(p = &#39;p_alpha.2&#39;), function(p, theta, ...){ stat_function(fun = function(x){dbeta2(x, prob = p,theta = theta)}, color = clr0dd, alpha = .3, n = 301, xlim = 0:1) } )) + stat_function(fun = function(x){ dbeta2(x, prob = mean(ucb_posterior_means$p_alpha.1), theta = mean(ucb_posterior_means$theta)) }, color = clr_current, n = 301, xlim = 0:1, geom = &quot;line&quot;, size = 1) + coord_cartesian(ylim = c(0, 3)) + labs(x = &quot;pobability admit&quot;, y = &#39;density&#39;, subtitle = &quot;distribution of female admission rates&quot;) ucb_grid &lt;- distinct(data_ucb, applications, gid, dept) ucb_mod_val &lt;- link(model_ucb_beta) %&gt;% as_tibble() %&gt;% summarise(across(everything(), .fns = function(x){quantile(x, probs = c(.055, .5, .945))})) %&gt;% mutate(percentile = c(&quot;lower_89&quot;, &quot;median&quot;, &quot;upper_89&quot;)) %&gt;% pivot_longer(-percentile, names_to = &quot;rowid&quot;) %&gt;% pivot_wider(names_from = percentile) %&gt;% bind_cols(ucb_grid, . ) %&gt;% mutate(rn = row_number()) ucb_posterior_predictions &lt;- sim(model_ucb_beta, data = ucb_grid) %&gt;% as.matrix() %&gt;% t() %&gt;% as_tibble() %&gt;% bind_cols(ucb_grid, .) %&gt;% pivot_longer(-c(dept, gid, applications), values_to = &quot;admit&quot;) %&gt;% dplyr::select(-name) %&gt;% group_by(dept, gid, applications) %&gt;% summarise(p = quantile(admit/applications, probs = c(.055, .25, .5, .75, .955)), median = median(admit), mean = mean(admit/applications), breaks = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;)) %&gt;% ungroup() %&gt;% pivot_wider(names_from = breaks, values_from = p) %&gt;% mutate(type = &quot;post. pred.&quot;, x = 2 * (as.integer(as.factor(dept)) - 1) + gid) p2 &lt;- data_ucb %&gt;% group_by(dept, gid) %&gt;% summarise(mean_data = mean(admit/applications), type = &quot;data&quot;) %&gt;% mutate(x = 2 * (as.integer(dept) - 1) + gid) %&gt;% ggplot(aes(x = x, y = mean_data)) + geom_segment(data = ucb_posterior_predictions, aes(xend = x, y = ll, yend = hh), size = 3, color = clr_alpha(clr0dd, .2)) + geom_segment(data = ucb_mod_val, aes(x = rn, xend = rn, y = lower_89, yend = upper_89), color = clr_current) + geom_point(data = ucb_posterior_predictions, aes(y = mean, shape = factor(gid)), color = clr_current, fill = clr_lighten(clr_current), size = 1.8) + geom_point(aes(shape = factor(gid)), color = clr0dd, fill = clr0, size = 1.8) + scale_shape_manual(values = c(`2` = 21, `1` = 19), guide = &quot;none&quot;) + scale_x_continuous(breaks = 1:12)+ labs(x = &quot;case&quot;, y = &quot;admission rate&quot;, subtitle = &quot;posterior validation check&quot;) + lims(y = c(0,1)) + theme(panel.background = element_rect(color = clr0, fill = &quot;transparent&quot;), panel.grid.major.x = element_blank()) p1 + p2 13.1.2 Negative-Binomial or gamma-Poisson This equivalent to the beta-binomial, but for Poisson processes. The gamma-Poisson distribution has two parameters which are recruited from: shape (\\(\\alpha\\)) scale (\\(\\theta\\)) rate (\\(\\beta\\)) mean (\\(\\mu\\)) There are several equivalent ways to define the gamma distribution itself. \\[ Gamma(y | \\alpha, \\beta) = \\frac{\\beta^{\\alpha} y^{\\alpha - 1} e^{-\\beta y}}{\\Gamma (\\alpha)} \\] where \\(\\Gamma\\) is the Gamma function. The rate and shape are simply reciprocals (\\(\\phi = 1 / \\beta\\)): \\[ Gamma(y | \\alpha, \\phi) = \\frac{y^{\\alpha - 1} e^{-y/\\phi}}{\\phi^{\\alpha} \\Gamma (\\alpha)} \\] It can also be defined in terms of \\(\\mu\\) \\[ Gamma(y | \\mu, \\alpha ) = \\frac{\\left(\\frac{\\alpha}{\\mu}\\right)^{\\alpha}}{\\Gamma(\\alpha)} y ^{\\alpha - 1} exp(- \\frac{\\alpha y}{y}) \\] Based on the gamma distribution, the gamma-Poisson (aka. negative binomial) can be expressed as \\[ y_{i} \\sim Gamma-Poisson(\\mu, \\alpha) \\] where \\(\\mu\\) is the mean or rate, taking the place of \\(\\lambda\\) from the Poisson distribution. In the rethinking book, this is noted as: \\[ y_{i} \\sim Gamma-Poisson(\\lambda_{i}, \\phi) \\] Note, that \\(\\phi\\) controls the variance and must thus be positive - it controls how closely the distribution matches a pure Poisson process. The variance of the gamma-Poisson is \\(\\lambda + \\lambda ^ 2 / \\phi\\)) data(Kline) data_kline &lt;- Kline %&gt;% as_tibble() %&gt;% mutate(pop_log_std = standardize(log(population)), contact_idx = 3L - as.integer(factor(contact))) rm(Kline) data_kline_list &lt;- data_kline %&gt;% dplyr::select(total_tools, population, contact_idx) %&gt;% as.list() model_ocean_sci_gamma &lt;- ulam( flist = alist( total_tools ~ dgampois( lambda, phi ), lambda &lt;- exp(alpha[contact_idx]) * population^beta[contact_idx] / gamma, alpha[contact_idx] ~ dnorm(1, 1), beta[contact_idx] ~ dexp(1), gamma ~ dexp(1), phi ~ dexp(1) ), data = data_kline_list, chains = 4, cores = 4, log_lik = TRUE ) plot_ocean &lt;- function(model, title){ ocean_k_values_sc &lt;- PSIS(model, pointwise = TRUE) %&gt;% bind_cols(data_kline, .) n &lt;- 101 ocean_grid_sc &lt;- crossing(population = seq(0, 3e5, length.out = n), contact_idx = 1:2) ocean_posterior_predictions_sc &lt;- link(model, data = ocean_grid_sc) %&gt;% as.matrix() %&gt;% t() %&gt;% as_tibble() %&gt;% bind_cols(ocean_grid_sc, .) %&gt;% pivot_longer(-c(population, contact_idx), values_to = &quot;total_tools&quot;) %&gt;% dplyr::select(-name) %&gt;% group_by(population, contact_idx) %&gt;% summarise(p = list(quantile(total_tools, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) ocean_posterior_predictions_sc %&gt;% ggplot(aes(x = population, y = total_tools)) + geom_smooth(aes(ymin = ll, y = m, ymax = hh, group = factor(contact_idx), color = factor(contact_idx), fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, size = .3) + geom_point(data = ocean_k_values_sc, aes(color = factor(contact_idx), fill = after_scale(clr_alpha(color)), size = k), shape = 21) + scale_color_manual(values = c(`1` = clr_current, `2` = clr0dd), guide = &quot;none&quot;) + scale_size_continuous(guide = &quot;none&quot;) + scale_x_continuous(breaks = 1e5 * (0:3), labels = scales::comma) + coord_cartesian(ylim = c(0, 80), x = c(-1e3, 3e5), expand = 1) + labs(subtitle = title) + theme(plot.subtitle = element_text(hjust = .5)) } chapter11_models &lt;- read_rds(&quot;envs/chapter11_models.rds&quot;) plot_ocean(chapter11_models$model_ocean_scientific, title = &quot;the scientific model&quot;) + plot_ocean(model_ocean_sci_gamma, title = &quot;the scientific gamma-poisson model&quot;) 13.1.3 Over-dispersion, Entropy and Information Criteria Generally it’s not advised to apply WAIC or PSIS to over-dispersed models (beta-Binomial and gamma-Poisson), ‘’unless you are very sure of what you are doing’’. That is because in these model types can not easily be dis-aggregated row-wise without changing causal assumptions. 13.2 Zero-Inflated Outcomes Whenever there are different causes for the same observation, then a mixture model may be useful. This is used e.g. when several ways can prevent a count from happening (we count zero scrub jays in the forest either because there are none or because we scared them all away before starting the count.) 13.2.1 Zero-Inflated Poisson Revisiting the monastery example - this time there are also drinking days, which prevent monks from producing new manuscripts. library(ggraph) library(tidygraph) rstat_nodes &lt;- data.frame(name = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;)) rstat_edges &lt;- data.frame(from = c(1, 1, 2, 3, 3), to = c(2, 3, 4, 4, 5)) flow_grph &lt;- tbl_graph(nodes = rstat_nodes, edges = rstat_edges) %E&gt;% mutate(label = c(&quot;p&quot;, &quot;1 - p&quot;, &quot;&quot;, &quot;&quot; ,&quot;&quot;)) %N&gt;% mutate(label = c(&quot;&quot;, &quot;Drink&quot;, &quot;Work&quot;, &quot;observe\\ny = 0&quot; ,&quot;observe\\ny &gt; 0&quot;), x_offset = c(0, -.13, .13, 0, 0), show_point = row_number() &lt; 4) %N&gt;% create_layout(layout = tibble( x = c(.5, 0, 1, 0, 1), y = c(1, .5, .5, 0, 0))) edge_labels &lt;- ggraph::get_edges(format = &quot;long&quot;)(flow_grph) %&gt;% mutate(node_start = c(&quot;from&quot;, &quot;to&quot;)[1 + (node == from)]) %&gt;% dplyr::select(edge.id, node_start, x, y, label) %&gt;% filter(label != &quot;&quot;) %&gt;% mutate(order = c(1,2, 4,3)) %&gt;% arrange(order) p1 &lt;- flow_grph %&gt;% ggraph() + geom_edge_link(end_cap = ggraph::rectangle(1, .15, &quot;npc&quot;), arrow = arrow(type = &quot;closed&quot;, length = unit(4, &quot;pt&quot;)), color = clr0dd) + geom_node_point(aes(filter = show_point), size = 10, shape = 21, color = clr0dd, fill = clr0) + geom_node_text(aes(x = x + x_offset, label = label), family = fnt_sel) + geomtextpath::geom_textpath(data = edge_labels, aes(x = x, y = y, label = label, group = edge.id), text_only = TRUE, vjust = 1.8, family = fnt_sel) + coord_fixed(ratio = .6, ylim = c(-.2, 1.1)) The likelihood of observing zero manuscripts is: \\[ \\begin{array}{rcl} Pr(0 | p, \\lambda) &amp; = &amp; Pr(\\textrm{drink} | p) + Pr(\\textrm{work} | p) \\times Pr(0 | \\lambda) \\\\ &amp; = &amp; p + (1 - p)~\\textrm{exp}(-\\lambda) \\end{array} \\] and the likelihood for non-zero values is: \\[ Pr(y | y &gt; 0, p, \\lambda) = Pr(\\textrm{drink} | p)(0) + Pr(\\textrm{work}|p) Pr(y | \\lambda) = (1 - p) \\frac{\\lambda^{y}\\textrm{exp}(-\\lambda)}{y} \\] We are defining those distributions as \\(ZIPoisson\\) (zero-inflated Poisson, with the parameters \\(p\\) probability of 0, and \\(\\lambda\\) the mean of the Poisson) to use in the model: \\[ \\begin{array}{rclr} y_{i} &amp; \\sim &amp; ZIPoisson(p_{i}, \\lambda_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{logit}(p_{i}) &amp; = &amp; \\alpha_{p} + \\beta_{p} x_{i} &amp; \\textrm{[linear model #1 ]}\\\\ \\textrm{log}(\\lambda_{i}) &amp; = &amp; \\alpha_{\\lambda} + \\beta_{\\lambda} x_{i} &amp; \\textrm{[linear model #2 ]}\\\\ \\end{array} \\] \\(\\rightarrow\\) we need two linear models and two link functions for the two processes within the ZIPoisson. p_drink &lt;- .2 # 20% of days rate_work &lt;- 1 # ~ 1 ms / day n &lt;- 365 set.seed(42) data_books &lt;- tibble(drink = rbinom(n , 1, p_drink), books = as.integer((1 - drink) * rpois(n , rate_work))) p2 &lt;- data_books %&gt;% group_by(drink, books) %&gt;% count() %&gt;% ggplot(aes(x = books, y = n)) + geom_bar(stat = &quot;identity&quot;, width = .66, aes(color = factor(drink, levels = 1:0), fill = after_scale(clr_alpha(color, .7)))) + scale_color_manual(&quot;drunk?&quot;, values = c(`1` = clr_current, `0` = clr0dd)) + theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank()) p1 + p2 model_books_drunk &lt;- ulam( flist = alist( books ~ dzipois(p, lambda), logit(p) &lt;- alpha_p, log(lambda) &lt;- alpha_l, alpha_p ~ dnorm(-1.5, 1), alpha_l ~ dnorm(1, .5) ), data = data_books, chains = 4, cores = 4 ) precis(model_books_drunk) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_p -1.37 0.41 -2.05 -0.84 458.49 1.01 alpha_l -0.06 0.09 -0.21 0.08 470.42 1.01 books_posterior &lt;- extract.samples(model_books_drunk) %&gt;% as_tibble() books_posterior %&gt;% summarise(alpha_p = mean(inv_logit(alpha_p)), alpha_l = mean(exp(alpha_l))) #&gt; # A tibble: 1 × 2 #&gt; alpha_p alpha_l #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.209 0.946 \\(\\rightarrow\\) we can get an accurate estimate of share of days when the monks are drunk, but we don’t know which exact days those are. 13.2.2 Zero-Inflated Model in Stan Here is how to translate the rethinking::dzipois() function into stan: model_books_drunk_alt &lt;- ulam( flist = alist( books | books &gt; 0 ~ custom( log1m(p) + poisson_lpmf(books | lambda) ), books | books == 0 ~ custom( log_mix( p, 0, poisson_lpmf(0 | lambda) ) ), logit(p) &lt;- alpha_p, log(lambda) &lt;- alpha_l, alpha_p ~ dnorm(-1.5, 1), alpha_l ~ dnorm(1, .5) ), data = data_books, chains = 4, cores = 4 ) precis(model_books_drunk_alt) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_p -1.39 0.40 -2.09 -0.86 519.90 1.01 alpha_l -0.06 0.09 -0.21 0.08 616.36 1.00 stancode(model_books_drunk_alt) #&gt; data{ #&gt; int drink[365]; #&gt; int books[365]; #&gt; } #&gt; parameters{ #&gt; real alpha_p; #&gt; real alpha_l; #&gt; } #&gt; model{ #&gt; real p; #&gt; real lambda; #&gt; alpha_l ~ normal( 1 , 0.5 ); #&gt; alpha_p ~ normal( -1.5 , 1 ); #&gt; lambda = alpha_l; #&gt; lambda = exp(lambda); #&gt; p = alpha_p; #&gt; p = inv_logit(p); #&gt; for ( i in 1:365 ) #&gt; if ( books[i] == 0 ) target += log_mix(p, 0, poisson_lpmf(0 | lambda)); #&gt; for ( i in 1:365 ) #&gt; if ( books[i] &gt; 0 ) target += log1m(p) + poisson_lpmf(books[i] | lambda); #&gt; } 13.3 Ordered Categorical Outcomes [For ordered categories, unlike counts], the differences in value are not necessarily equal. […] Just treating ordered categories as continuous measures is not a good idea. The general approach is a multinomial prediction problem. But, we are using a cumulative link function to move predictions progressively through the categories in sequence. 13.3.1 Moral Intuition The example works on data for the trolley problem. data(Trolley) data_trolley &lt;- Trolley %&gt;% as_tibble() %&gt;% mutate(education = factor(as.character(edu), levels = levels(edu)[c(2, 6, 8, 4, 7, 1, 5, 3)]), education_idx = as.integer(education), education_norm = normalize(education_idx), sex = 1L + male, age_scl = age / max(age)) rm(Trolley) p1 &lt;- data_trolley %&gt;% ggplot(aes(x = response)) + geom_bar(width = .6, color = clr0dd, fill = clr_alpha(clr0dd,.7)) + theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank()) table(data_trolley$response) / nrow(data_trolley) #&gt; #&gt; 1 2 3 4 5 6 7 #&gt; 0.12829809 0.09154079 0.10785498 0.23393756 0.14723061 0.14551863 0.14561934 logit &lt;- function(x){ log(x / (1 - x)) } data_trolley_cumulative &lt;- data_trolley %&gt;% group_by(response) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(pr_k = n / sum(n), cum_pr_k = cumsum(pr_k), log_cumulative_odds = logit(cum_pr_k), cum_pr_k_m1 = lag(cum_pr_k, default = 0)) p2 &lt;- data_trolley_cumulative %&gt;% ggplot(aes(x = response, y = cum_pr_k)) + # as_reference(geom_point(size = 5), id = &quot;pnts&quot;) + # with_blend( geom_line(color = clr0dd) + # ,bg_layer = &quot;pnts&quot;, blend_type = &quot;out&quot;) + geom_point(color = clr0dd, fill = clr0, shape = 21, size = 2) + lims(y = c(0, 1)) The log-cumulative-odds, that a response value \\(y_{i}\\) is \\(\\geqslant\\) some possible outcome: \\[ \\textrm{log} \\frac{Pr(y_{i} \\leqslant k)}{1 - Pr(y_{i} \\leqslant k)} = \\alpha_{k} \\] with an unique intercept \\(\\alpha_{k}\\) for each possible outcome value \\(k\\). p3 &lt;- data_trolley_cumulative %&gt;% filter(log_cumulative_odds != Inf) %&gt;% ggplot(aes(x = response, y = log_cumulative_odds)) + # as_reference(geom_point(size = 5), id = &quot;pnts&quot;) + # with_blend( geom_line(color = clr0dd) + #,bg_layer = &quot;pnts&quot;, blend_type = &quot;out&quot;) + geom_point(color = clr0dd, fill = clr0, shape = 21, size = 2) p1 + p2 + p3 &amp; scale_x_discrete(limits = factor(1:7)) After observing \\(k\\), we can get its likelihood by subtraction: \\[ p_{k} = Pr(y_{i} = k) = Pr(y_{i} \\leqslant k) - Pr(y_{i} \\leqslant k - 1) \\] data_trolley_cumulative %&gt;% ggplot(aes(x = response, y = cum_pr_k)) + geom_linerange(aes(ymin = 0, ymax = cum_pr_k), color = clr_dark, linetype = 3) + geom_linerange(aes(ymin = cum_pr_k_m1, ymax = cum_pr_k), color = clr_current, size = 2, alpha = .5) + geom_text(aes(y = .5 * (cum_pr_k_m1 + cum_pr_k), label = response, x = response + .2), color = clr_current, family = fnt_sel) + geom_line(color = clr0dd) + geom_point(color = clr0dd, fill = clr0, shape = 21, size = 1.5) + lims(y = c(0, 1)) From the 2019 Lecture, at 25:32: And I know at this point you are saying: ‘But I already had that when I started.’ Yes, but you would not be able to model the whole thing with a linear model. The whole reason that this literature uses the cumulative link is because it manages the fact that they are all bound together in an ordered way. You establish the order by using a cumulative link. So here is the mathematical notation of the ordered logit: \\[ \\begin{array}{rclr} R_{i} &amp; \\sim &amp; Ordered-logit(\\phi_{i}, \\kappa) &amp; \\textrm{[probability of the data]}\\\\ \\phi_{i} &amp; = &amp; 0 &amp; \\textrm{[linear model]}\\\\ \\kappa_{k} &amp; \\sim &amp; Normal(0, 1.5) &amp; \\textrm{[common prior for each intercept]}\\\\ \\end{array} \\] But the model can also be expressed more literally: \\[ \\begin{array}{rclcr} R_{i} &amp; \\sim &amp; Categorical(p) &amp; &amp;\\textrm{[probability of the data]}\\\\ p_{1} &amp; = &amp; q_{1} &amp; &amp;\\textrm{[probabilities of each value $k$]}\\\\ p_{k} &amp; = &amp; q_{k} - q_{k-1} &amp; \\textrm{for}~K \\gt k \\gt 1 &amp;\\\\ p_{K} &amp; = &amp; 1 - q_{k-1} &amp; &amp;\\\\ \\textrm{logit}(q_{k}) &amp; = &amp; \\kappa_{k} - \\phi_{i}&amp; &amp;\\textrm{[cumulative logit link]}\\\\ \\phi_{i} &amp; = &amp; \\textrm{trems of linear model} &amp; &amp;\\textrm{[linear model]}\\\\ \\kappa_{k} &amp; \\sim &amp; Normal(0, 1.5) &amp; &amp;\\textrm{[common prior for each intercept]}\\\\ \\end{array} \\] model_trolley &lt;- ulam( flist = alist( response ~ dordlogit( 0, cutpoints ), cutpoints ~ dnorm( 0, 1.5 ) ), data = list(response = data_trolley$response), cores = 4, chains = 4 ) model_trolley_quap &lt;- quap( flist = alist( response ~ dordlogit( 0, c(alpha_1, alpha_2, alpha_3, alpha_4, alpha_5, alpha_6) ), c(alpha_1, alpha_2, alpha_3, alpha_4, alpha_5, alpha_6) ~ dnorm( 0, 1.5 ) ), data = data_trolley_cumulative, start = list(alpha_1 = -2, alpha_2 = -1, alpha_3 = 0, alpha_4 = 1, alpha_5 = 2, alpha_6 = 2.5) ) precis(model_trolley, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 cutpoints[1] -1.92 0.03 -1.97 -1.86 1691.39 1 cutpoints[2] -1.27 0.02 -1.31 -1.23 2039.57 1 cutpoints[3] -0.72 0.02 -0.75 -0.68 2617.99 1 cutpoints[4] 0.25 0.02 0.22 0.28 2675.98 1 cutpoints[5] 0.89 0.02 0.86 0.93 2608.58 1 cutpoints[6] 1.77 0.03 1.73 1.82 2424.11 1 To get the cumulative probabilities back round(inv_logit(coef(model_trolley)), 3) %&gt;% as.matrix() %&gt;% as.data.frame() %&gt;% knitr::kable() V1 cutpoints[1] 0.128 cutpoints[2] 0.220 cutpoints[3] 0.328 cutpoints[4] 0.562 cutpoints[5] 0.709 cutpoints[6] 0.855 compare to data_trolley_cumulative$cum_pr_k: data_trolley_cumulative #&gt; # A tibble: 7 × 6 #&gt; response n pr_k cum_pr_k log_cumulative_odds cum_pr_k_m1 #&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1274 0.128 0.128 -1.92 0 #&gt; 2 2 909 0.0915 0.220 -1.27 0.128 #&gt; 3 3 1071 0.108 0.328 -0.719 0.220 #&gt; 4 4 2323 0.234 0.562 0.248 0.328 #&gt; 5 5 1462 0.147 0.709 0.890 0.562 #&gt; 6 6 1445 0.146 0.854 1.77 0.709 #&gt; 7 7 1446 0.146 1 Inf 0.854 But now, we also have a posterior distribution around these values, which provides a measure of uncertainty. extract.samples(model_trolley) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% mutate(across(everything(), inv_logit), cutpoints.7 = 1, .draw = row_number()) %&gt;% pivot_longer(-.draw, values_to = &quot;p_cum&quot;) %&gt;% group_by(.draw) %&gt;% arrange(.draw, name) %&gt;% mutate(p = p_cum - lag(p_cum, default = 0), name = str_sub(name,-1, -1) %&gt;% as.integer()) %&gt;% group_by(name) %&gt;% summarise(quantiles = list(tibble(p = c(quantile(p, probs = c(.055, .25, .5, .75, .955)), mean(p)), quantile = c(&quot;ll&quot;,&quot;l&quot;,&quot;m&quot;,&quot;h&quot;,&quot;hh&quot;, &quot;mean&quot;)))) %&gt;% unnest(quantiles) %&gt;% pivot_wider(names_from = &quot;quantile&quot;, values_from = &quot;p&quot;) %&gt;% ggplot(aes(y = factor(name))) + geom_linerange(aes(xmin = l, xmax = h), size = 1, color = clr0dd) + geom_linerange(aes(xmin = ll, xmax = hh), size = .2, color = clr0dd) + geom_point(aes(x = m), shape = 21, color = clr0dd, fill = clr0) + geom_point(data = data_trolley_cumulative, aes(y = response, x = pr_k), color = clr_current, shape = 1, size = 3.5) + labs(y = &quot;response&quot;, x = &quot;p&quot;, subtitle = &quot;posterior distribution for pr_k&quot;) Adding predictor variables We define a linear model \\(\\phi_{i} = \\beta x_{i}\\) to express the cumulative logit as: \\[ \\begin{array}{rcl} \\textrm{log}\\frac{Pr(y_{i} -\\leqslant k)}{1 - Pr(y_{i} \\leqslant k)} &amp; = &amp;\\alpha_k - \\phi_i\\\\ \\phi_{i} &amp; = &amp; \\beta x_{i} \\end{array} \\] We need to subtract \\(\\phi\\) from \\(\\alpha\\), since the log-cumulative odds will shift the sign: (pk &lt;- dordlogit( 1:7, 0, coef(model_trolley))) %&gt;% round(digits = 2) #&gt; [1] 0.13 0.09 0.11 0.23 0.15 0.15 0.15 sum(pk * 1:7) #&gt; [1] 4.198875 (pk &lt;- dordlogit( 1:7, 0, coef(model_trolley) - .5)) %&gt;% round(digits = 2) #&gt; [1] 0.08 0.06 0.08 0.21 0.16 0.18 0.22 sum(pk * 1:7) #&gt; [1] 4.729279 Now, to specify the predictors (action, intention, contact) within the model (including an interaction between intention and the other two), we write: \\[ \\begin{array}{rcl} \\textrm{log}\\frac{Pr(y_{i} -\\leqslant k)}{1 - Pr(y_{i} \\leqslant k)} &amp; = &amp;\\alpha_k - \\phi_i\\\\ \\phi_{i} &amp; = &amp; \\beta_{A} A_{i} + \\beta_{C} C_{i} + B_{I,i} I_{i}\\\\ B_{I,i} &amp; = &amp; \\beta_{I} + \\beta_{IA} A_{i} + \\beta_{IC} C_{i} \\end{array} \\] data_trolley_list &lt;- data_trolley %&gt;% dplyr::select(response, action, intention, contact) %&gt;% as.list() model_trolley_predict &lt;- ulam( flist = alist( response ~ dordlogit( phi, cutpoints), phi &lt;- beta_a * action + beta_c * contact + B_i * intention, B_i &lt;- beta_i + beta_ia * action + beta_ic * contact, c(beta_a, beta_c, beta_i, beta_ia, beta_ic) ~ dnorm(0, .5), cutpoints ~ dnorm(0, 1.5) ), data = data_trolley_list, chains = 4, cores = 4, log_lik = TRUE ) library(tidybayes) trolley_posterior &lt;- extract.samples(model_trolley_predict) %&gt;% as.data.frame() %&gt;% as_tibble() trolley_posterior %&gt;% dplyr::select(starts_with(&quot;beta&quot;)) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;beta_a&quot;, &quot;beta_i&quot;, &quot;beta_c&quot;, &quot;beta_ia&quot;, &quot;beta_ic&quot;))) %&gt;% ggplot(aes(x = value, y = name)) + geom_vline(xintercept = 0, linetype = 3, color = clr_dark) + stat_gradientinterval(.width = c(.5, .89), size = 1, point_size = 1.5, shape = 21, point_fill = clr_lighten(clr_current), fill = clr_lighten(clr_current), color = clr_dark) + scale_x_continuous(&quot;marginal posterior&quot;, breaks = -5:0 / 4) + scale_y_discrete(NULL) + coord_cartesian(xlim = c(-1.4, 0)) new_trolley &lt;- data_trolley %&gt;% distinct(action, contact, intention) %&gt;% mutate(combination = str_c(action, contact, intention, sep = &quot;_&quot;), data_config = row_number()) label_levels &lt;- map2_chr(c(0,1,0), c(0, 0, 1), function(action, contact){ glue(&quot;action: {action}; contact: {contact}&quot;) }) trolley_posterior_prediction &lt;- link(model_trolley_predict, data = new_trolley) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% mutate(.iter = row_number()) %&gt;% bind_cols(trolley_posterior) %&gt;% pivot_longer(-c(.iter, starts_with(&quot;beta&quot;), starts_with(&quot;cutpoints&quot;))) %&gt;% separate(name, into = c(&quot;param&quot;, &quot;data_config&quot;), sep = &quot;\\\\.&quot;) %&gt;% mutate(data_config = as.integer(data_config)) %&gt;% pivot_wider(names_from = param, values_from = value) %&gt;% filter(.iter &lt; 51) %&gt;% left_join(new_trolley) %&gt;% mutate(p_k = pmap(cur_data(), .f = function(phi, cutpoints.1, cutpoints.2, cutpoints.3, cutpoints.4, cutpoints.5, cutpoints.6, ...){ ct &lt;- c(cutpoints.1, cutpoints.2, cutpoints.3, cutpoints.4, cutpoints.5, cutpoints.6) tibble(pk = pordlogit(1:6, phi, ct), idx = 1:6)} )) %&gt;% unnest(p_k) %&gt;% mutate(label = factor(glue(&quot;action: {action}; contact: {contact}&quot;), levels = label_levels)) p1 &lt;- trolley_posterior_prediction %&gt;% ggplot(aes(x = intention, y = pk)) + geom_line(aes(group = str_c(.iter,&quot;_&quot;, idx)), color = clr_alpha(clr0d, .25)) + facet_wrap(label ~ .) p2 &lt;- sim(model_trolley_predict, data = new_trolley) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% set_names(nm = new_trolley$combination) %&gt;% pivot_longer(everything(), names_to = &quot;combination&quot;) %&gt;% group_by(combination, value) %&gt;% count() %&gt;% ungroup() %&gt;% separate(combination, into = c(&quot;action&quot;, &quot;contact&quot;, &quot;intention&quot;), sep = &quot;_&quot;) %&gt;% mutate(label = factor(glue(&quot;action: {action}; contact: {contact}&quot;), levels = label_levels)) %&gt;% ggplot(aes(x = value, y = n)) + geom_bar(aes(fill = after_scale(clr_alpha(color)), color = intention), position = position_dodge2(padding = .3) , width = .8, stat = &quot;identity&quot;) + facet_wrap(label ~ .) + scale_color_manual(values = c(`0` = clr0d, `1` = clr_current)) + theme(legend.position = &quot;bottom&quot;) p1 / p2 13.4 Ordered Categorical Predictors Adding education as ordered categorical predictor for the trolley model: levels(data_trolley$education) #&gt; [1] &quot;Elementary School&quot; &quot;Middle School&quot; &quot;Some High School&quot; #&gt; [4] &quot;High School Graduate&quot; &quot;Some College&quot; &quot;Bachelor&#39;s Degree&quot; #&gt; [7] &quot;Master&#39;s Degree&quot; &quot;Graduate Degree&quot; We will include the new predictors within the model as series of \\(\\delta\\) parameters (excluding the first level, which will be present the intercept): \\[ \\begin{array}{rclr} \\phi_{i} &amp; = &amp; \\delta_{1} + \\textrm{other stuff} &amp; \\textrm{[effect of `Middle School`]}\\\\ \\phi_{i} &amp; = &amp; \\delta_{1} + \\delta_{2} + \\textrm{other stuff} &amp; \\textrm{[effect of `some High School`]}\\\\ \\phi_{i} &amp; = &amp; \\sum_{j=1}^{7} \\delta_{j} + \\textrm{other stuff} &amp; \\textrm{[effect of highest education]}\\\\ \\phi_{i} &amp; = &amp; \\beta_{e} \\sum_{j=0}^{E_{i}-1} \\delta_{j} + \\textrm{other stuff} &amp; \\textrm{[using $\\beta = 1$ for max education effect ]} \\end{array} \\] Where \\(E_{i}\\) is the completed education of individual \\(i\\). For individuals with \\(E_{i} = 1\\), \\(\\beta_{E}\\) (the education effect) is ignored, since \\(\\beta_{E}~\\delta_{0} = 0\\). Now, we can include the new predictor term in the model: \\[ \\begin{array}{rclr} R_{i} &amp; \\sim &amp; Ordered-logit( \\phi_{i}, \\kappa) &amp; \\textrm{[likelihood]}\\\\ \\phi_{i} &amp; = &amp; \\beta_{E} \\sum_{j=0}^{E_{i}-1} \\delta_{j} + \\beta_{A} A_{i} + \\beta_{I} I_{i} + \\beta_{C} C_{i} &amp; \\textrm{[model]}\\\\ \\beta_{A},\\beta_{I},\\beta_{C},\\beta_{E} &amp; \\sim &amp; Normal(0,1) &amp; \\textrm{[$\\beta$ priors]}\\\\ \\delta &amp; \\sim &amp; Dirichlet(\\alpha) &amp; \\textrm{[$\\delta$ prior]} \\end{array} \\] Introducing the Dirichlet distribution The Dirichlet distribution is a multivariate extension of the beta distribution (it produces probabilities for multiple event with multiple different outcomes that together sum up to one). library(gtools) set.seed(42) delta &lt;- rdirichlet( 10, alpha = rep(2, 7)) %&gt;% as_tibble() %&gt;% mutate(rn = row_number()) %&gt;% pivot_longer(-rn) %&gt;% mutate(x = str_sub(name, -1, -1) %&gt;% as.integer()) p1 &lt;- delta %&gt;% ggplot(aes(x = x, y = value, group = rn)) + geom_line(data = . %&gt;% filter(rn &lt; 10), color = clr0d) + geom_line(data = . %&gt;% filter(rn == 10), color = clr_current) + geom_point(data = . %&gt;% filter(rn &lt; 10), color = clr0d, fill = clr0, size = 2, shape = 21) + geom_point(data = . %&gt;% filter(rn == 10), color = clr_current, fill = clr_lighten(clr_current), size = 2, shape = 21) + labs(subtitle = &quot;10 draws from a 7 class dirichlet distribution&quot;, x = &quot;outcome&quot;, y = &quot;probability&quot;) delta3 &lt;- rdirichlet( 500, alpha = c(3, 4, 6)) %&gt;% as_tibble() %&gt;% mutate(rn = row_number()) library(ggtern) source(&quot;bayesian_settings.R&quot;) clr_current &lt;- clr1 p2 &lt;- delta3 %&gt;% ggtern(mapping = aes(x = V1, y = V2, z = V3)) + stat_density_tern(aes(color = ..level.., fill = after_scale(clr_alpha(color))), geom = &#39;polygon&#39;) + scale_color_gradientn(colours = c(clr0, clr_current), guide = &quot;none&quot;) + scale_T_continuous(breaks = c(.25, .5 ,.75, 1), minor_breaks = c(.125, .375, .625, .875), label = c(0.25, 0.5, .75, 1) %&gt;% sprintf(&quot;%.2f&quot;,.)) + scale_L_continuous(breaks = c(.25, .5 ,.75, 1), minor_breaks = c(.125, .375, .625, .875), label = c(0.25, 0.5, .75, 1) %&gt;% sprintf(&quot;%.2f&quot;,.)) + scale_R_continuous(breaks = c(.25, .5 ,.75, 1), minor_breaks = c(.125, .375, .625, .875), label = c(0.25, 0.5, .75, 1) %&gt;% sprintf(&quot;%.2f&quot;,.)) + labs(subtitle = &quot;density for a 3 class dirichlet distribution&quot;) + theme_minimal(base_family = fnt_sel) + theme(panel.border = element_rect(size = .5, color = clr0d)) list(ggplotGrob(p1), ggplotGrob(p2)) %&gt;% wrap_plots() data_trolley_list2 &lt;- data_trolley %&gt;% dplyr::select(response, action, intention, contact, education_idx) %&gt;% as.list() %&gt;% c(., list(alpha = rep(2, 7))) model_trolley_education &lt;- ulam( flist = alist( response ~ ordered_logistic( phi, kappa ), phi &lt;- beta_e * sum(delta_j[1:education_idx]) + beta_a * action + beta_i * intention + beta_c * contact, kappa ~ normal( 0, 1.5 ), c( beta_a, beta_i, beta_c, beta_e ) ~ normal( 0, 1 ), vector[8]: delta_j &lt;&lt;- append_row( 0, delta ), simplex[7]: delta ~ dirichlet( alpha ) ), data = data_trolley_list2, chains = 4, cores = 4 ) write_rds(model_trolley_education, file = &quot;brms/ulam_c12_model_trolley_education.rds&quot;) model_trolley_education &lt;- read_rds(&quot;brms/ulam_c12_model_trolley_education.rds&quot;) precis(model_trolley_education, depth = 2, omit = &quot;kappa&quot;) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 beta_e -0.32 0.17 -0.59 -0.07 955.42 1 beta_c -0.96 0.05 -1.03 -0.88 2336.49 1 beta_i -0.72 0.04 -0.78 -0.66 2848.95 1 beta_a -0.70 0.04 -0.77 -0.64 2281.89 1 delta[1] 0.23 0.14 0.05 0.48 1478.39 1 delta[2] 0.14 0.08 0.03 0.30 2423.27 1 delta[3] 0.20 0.11 0.05 0.38 2112.40 1 delta[4] 0.17 0.10 0.04 0.34 2282.96 1 delta[5] 0.04 0.05 0.01 0.11 945.10 1 delta[6] 0.10 0.06 0.02 0.21 2489.65 1 delta[7] 0.13 0.08 0.03 0.27 2097.86 1 extract.samples(model_trolley_education) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% dplyr::select(starts_with(&quot;delta&quot;)) %&gt;% set_names(nm = levels(data_trolley$education)[2:8]) %&gt;% ggpairs( lower = list(continuous = wrap(my_lower, col = clr_current)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll0, color = clr_current, fill = clr_alpha(clr_current), adjust = .7)), upper = list(continuous = wrap(my_upper , size = 4, color = &quot;black&quot;, family = fnt_sel)) ) + theme(panel.border = element_rect(color = clr_dark, fill = &quot;transparent&quot;)) Comparing how simply throwing the ordered categorical predictor into a linear model underestimates the education effect: data_trolley_list3 &lt;- data_trolley %&gt;% dplyr::select(response, action, intention, contact, education_norm) %&gt;% as.list() model_trolley_education_linear &lt;- ulam( flist = alist( response ~ ordered_logistic( mu, cutpoints ), mu &lt;- beta_e * education_norm + beta_a * action + beta_i * intention + beta_c * contact, c( beta_a, beta_i, beta_c, beta_e ) ~ normal( 0, 15 ), cutpoints ~ normal( 0, 1.5 ) ), data = data_trolley_list3, chains = 4, cores = 4 ) precis(model_trolley_education_linear) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 beta_e -0.10 0.09 -0.25 0.04 1723.48 1 beta_c -0.96 0.05 -1.04 -0.88 2115.57 1 beta_i -0.72 0.04 -0.78 -0.66 2385.87 1 beta_a -0.71 0.04 -0.77 -0.64 1934.02 1 library(rlang) chapter12_models &lt;- env( data_ucb_list = data_ucb_list, model_ucb_beta = model_ucb_beta, data_kline = data_kline, data_kline_list = data_kline_list, model_ocean_sci_gamma = model_ocean_sci_gamma, data_books = data_books, model_books_drunk = model_books_drunk, model_books_drunk_alt = model_books_drunk_alt, data_trolley = data_trolley, data_trolley_cumulative = data_trolley_cumulative, model_trolley = model_trolley, model_trolley_quap = model_trolley_quap, data_trolley_list = data_trolley_list, model_trolley_predict = model_trolley_predict, data_trolley_list2 = data_trolley_list2, model_trolley_education = model_trolley_education, data_trolley_list3 = data_trolley_list3, model_trolley_education_linear = model_trolley_education_linear ) write_rds(chapter12_models, &quot;envs/chapter12_models.rds&quot;) 13.5 Homework E1 In an un-ordered categorical variable the ranking of levels is arbitrary (eg. types of fruit: apple, banana, oranges), while in an ordered categorical variable the order of categories is meaningful but not even across levels (eg. developmental stages in fish: egg, larvae, juvenile, adult). E2 It uses a cumulative link function to encode the order of levels. Here the probability of witnessing the current event or an event of a lover level is considered. E3 It will tend to underestimate the rate \\(\\lambda\\) of the modeled Poisson process. E4 Over-dispersed counts can occur when an event can be the result of two different processes. This could be eg. fruit-fall below trees which could depend on the ripening process as well as on animal intervention. M1 data_uni &lt;- tibble( rating = 1:4, n = c(12L, 36L, 7L, 41L), pr = n / sum(n), n_cumulative = cumsum(n), pr_cumulative = n_cumulative / sum(n), cumulative_odds = pr_cumulative / (1 - pr_cumulative), log_cumulative_odds = log(cumulative_odds)) M2 data_uni %&gt;% ggplot(aes(x = rating, y = pr_cumulative)) + geom_linerange(aes(ymin = 0, ymax = pr_cumulative), linetype = 3, color = clr_dark) + geom_linerange(aes(ymin = pr_cumulative - pr, ymax = pr_cumulative), size = 2, color = clr_alpha(clr_dark, .4)) + geom_line(color = clr0dd) + geom_point(shape = 21, size = 2, color = clr0dd, fill = clr0) + coord_cartesian(ylim = 0:1) M3 \\[ \\begin{array}{rclr} y_{i} &amp; \\sim &amp; ZIBinomial(p_{0,i}, n, p_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{logit}(p_{0,i}) &amp; = &amp; \\alpha_{p_{0}} + \\beta_{p_{0}} x_{i} &amp; \\textrm{[linear model #1 ]}\\\\ \\textrm{logit}(p_{i}) &amp; = &amp; \\alpha_{p} + \\beta_{p} x_{i} &amp; \\textrm{[linear model #2 ]}\\\\ \\end{array} \\] H1 data(Hurricanes) data_hurricane &lt;- Hurricanes %&gt;% as_tibble() %&gt;% mutate(damage_norm_log = log(damage_norm), across(c(femininity, damage_norm, damage_norm_log, min_pressure), .fns = standardize, .names = &quot;{.col}_std&quot;)) rm(Hurricanes) model_hurricane_intercept &lt;- ulam( flist = alist( deaths ~ dpois( lambda ), log(lambda) &lt;- alpha, alpha ~ dnorm(3, 0.5) ), data = data_hurricane, chains = 4, cores = 4, log_lik = TRUE ) model_hurricane_fem &lt;- ulam( flist = alist( deaths ~ dpois( lambda ), log(lambda) &lt;- alpha + beta_f * femininity_std, alpha ~ dnorm(3, 0.5), beta_f ~ dnorm(0, 0.2) ), data = data_hurricane, chains = 4, cores = 4, log_lik = TRUE ) precis(model_hurricane_intercept) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha 3.03 0.02 2.99 3.06 687.29 1 precis(model_hurricane_fem) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha 3.00 0.02 2.96 3.04 1134.56 1.01 beta_f 0.23 0.03 0.19 0.27 1093.41 1.00 hurricane_k_values &lt;- PSIS(model_hurricane_fem, pointwise = TRUE) %&gt;% bind_cols(data_hurricane, .) n &lt;- 101 new_hurricane &lt;- tibble(femininity_std = seq(-2, 1.5, length.out = n)) hurricane_posterior_predictions &lt;- link(model_hurricane_fem, data = new_hurricane) %&gt;% as.matrix() %&gt;% t() %&gt;% as_tibble() %&gt;% bind_cols(new_hurricane, .) %&gt;% pivot_longer(-femininity_std, values_to = &quot;deaths&quot;) %&gt;% dplyr::select(-name) %&gt;% group_by(femininity_std) %&gt;% summarise(p = list(quantile(deaths, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) hurricane_posterior_predictions %&gt;% ggplot(aes(x = femininity_std, y = deaths)) + geom_smooth(aes(ymin = ll, y = m, ymax = hh), stat = &quot;identity&quot;, size = .5, color = clr0dd, fill = fll0dd) + geom_point(data = hurricane_k_values, aes(size = k), color = clr_dark, fill = clr0, shape = 21) + ggrepel::geom_text_repel(data = hurricane_k_values %&gt;% filter(k &gt; .5), aes(label = str_c(name, &quot; (&quot;, round(k, digits = 2), &quot;)&quot;)), family = fnt_sel, nudge_y = 8, min.segment.length = 15) + scale_size_continuous(guide = &quot;none&quot;) H2 model_hurricane_fem_gamma &lt;- ulam( flist = alist( deaths ~ dgampois( lambda, phi ), log(lambda) &lt;- alpha + beta_f * femininity_std, alpha ~ dnorm(3, 0.5), beta_f ~ dnorm(0, 0.2), phi ~ dexp(1) ), data = data_hurricane, chains = 4, cores = 4, log_lik = TRUE ) precis(model_hurricane_fem) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha 3.00 0.02 2.96 3.04 1134.56 1.01 beta_f 0.23 0.03 0.19 0.27 1093.41 1.00 precis(model_hurricane_fem_gamma) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha 3.03 0.15 2.79 3.28 1965.30 1 beta_f 0.13 0.12 -0.07 0.33 1916.78 1 phi 0.45 0.07 0.36 0.56 1770.07 1 H3 model_hurricane_fem_dam &lt;- ulam( flist = alist( deaths ~ dgampois( lambda, phi ), log(lambda) &lt;- alpha + beta_f * femininity_std + beta_d * damage_norm_std, alpha ~ dnorm(3, 0.5), c(beta_f, beta_d) ~ dnorm(0, 0.2), phi ~ dexp(1) ), data = data_hurricane, chains = 4, cores = 4, log_lik = TRUE ) model_hurricane_fem_dam_i &lt;- ulam( flist = alist( deaths ~ dgampois( lambda, phi ), log(lambda) &lt;- alpha + beta_f * femininity_std + beta_d * damage_norm_std + beta_fd * femininity_std * damage_norm_std, alpha ~ dnorm(3, 0.5), c(beta_f, beta_d, beta_fd) ~ dnorm(0, 0.2), phi ~ dexp(1) ), data = data_hurricane, chains = 4, cores = 4, log_lik = TRUE ) model_hurricane_fem_pres &lt;- ulam( flist = alist( deaths ~ dgampois( lambda, phi ), log(lambda) &lt;- alpha + beta_f * femininity_std + beta_p * min_pressure_std, alpha ~ dnorm(3, 0.5), c(beta_f, beta_p) ~ dnorm(0, 0.2), phi ~ dexp(1) ), data = data_hurricane, chains = 4, cores = 4, log_lik = TRUE ) model_hurricane_fem_pres_i &lt;- ulam( flist = alist( deaths ~ dgampois( lambda, phi ), log(lambda) &lt;- alpha + beta_f * femininity_std + beta_p * min_pressure_std + beta_fp * femininity_std * min_pressure_std, alpha ~ dnorm(3, 0.5), c(beta_f, beta_p, beta_fp) ~ dnorm(0, 0.2), phi ~ dexp(1) ), data = data_hurricane, chains = 4, cores = 4, log_lik = TRUE ) model_hurricane_fem_dam_pres &lt;- ulam( flist = alist( deaths ~ dgampois( lambda, phi ), log(lambda) &lt;- alpha + beta_f * femininity_std + beta_d * damage_norm_std + beta_p * min_pressure_std, alpha ~ dnorm(3, 0.5), c( beta_f, beta_d, beta_p) ~ dnorm(0, 0.2), phi ~ dexp(1) ), data = data_hurricane, chains = 4, cores = 4, log_lik = TRUE ) model_hurricane_fem_dam_pres_i &lt;- ulam( flist = alist( deaths ~ dgampois( lambda, phi ), log(lambda) &lt;- alpha + beta_f * femininity_std + beta_d * damage_norm_std + beta_p * min_pressure_std + beta_fd * femininity_std * damage_norm_std + beta_fp * femininity_std * min_pressure_std + beta_dp * damage_norm_std * min_pressure_std + beta_fdp * femininity_std * damage_norm_std * min_pressure_std , alpha ~ dnorm(3, 0.5), c( beta_f, beta_d, beta_p, beta_fd, beta_fp, beta_dp, beta_fdp) ~ dnorm(0, 0.2), phi ~ dexp(1) ), data = data_hurricane, chains = 4, cores = 4, log_lik = TRUE ) compare(model_hurricane_fem_gamma, model_hurricane_fem_dam, model_hurricane_fem_pres, model_hurricane_fem_dam_i, model_hurricane_fem_pres_i, model_hurricane_fem_dam_pres, model_hurricane_fem_dam_pres_i) %&gt;% knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight model_hurricane_fem_dam_pres_i 663.90 34.36 0.00 NA 7.86 0.99 model_hurricane_fem_dam_pres 674.80 36.09 10.90 5.61 6.71 0.00 model_hurricane_fem_dam_i 676.62 31.40 12.72 7.29 4.16 0.00 model_hurricane_fem_dam 677.18 31.61 13.28 7.07 3.58 0.00 model_hurricane_fem_pres_i 695.80 36.14 31.90 6.55 6.94 0.00 model_hurricane_fem_pres 696.36 36.26 32.46 7.10 5.96 0.00 model_hurricane_fem_gamma 708.78 31.96 44.88 10.11 2.99 0.00 precis(model_hurricane_fem_dam_pres_i) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha 2.64 0.13 2.44 2.85 1622.29 1 beta_fdp 0.09 0.10 -0.07 0.25 1341.85 1 beta_dp 0.16 0.11 -0.02 0.32 1344.51 1 beta_fp 0.11 0.13 -0.10 0.31 2114.63 1 beta_fd 0.25 0.16 0.00 0.50 1617.36 1 beta_p -0.44 0.12 -0.64 -0.24 1833.15 1 beta_d 0.62 0.15 0.38 0.85 1593.65 1 beta_f 0.11 0.11 -0.07 0.29 2518.47 1 phi 0.75 0.13 0.56 0.97 1723.51 1 new_hurricane &lt;- crossing(femininity_std = seq(-2, 2, length.out = 15), damage_norm_std = seq(-2, 2, length.out = 5), min_pressure_std = seq(-2, 2, length.out = 5)) hurricne_posterior_pred &lt;- link(model_hurricane_fem_dam_pres_i, data = new_hurricane) %&gt;% t() %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% bind_cols(new_hurricane,. ) %&gt;% pivot_longer(-(c(femininity_std,damage_norm_std,min_pressure_std)), names_to = &quot;.draw&quot;) %&gt;% mutate(.draw = str_remove(.draw, &quot;V&quot;) %&gt;% as.integer()) %&gt;% group_by(femininity_std, damage_norm_std, min_pressure_std) %&gt;% summarise(probs = list(tibble(p = c(.055, .25, .5, .75, .945), deaths = quantile(value, c(.055, .25, .5, .75, .945)), label = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;)))) %&gt;% ungroup() %&gt;% unnest(probs) p1 &lt;- hurricne_posterior_pred %&gt;% dplyr::select(-p) %&gt;% pivot_wider(names_from = &quot;label&quot;, values_from = &quot;deaths&quot;) %&gt;% ggplot(aes(x = femininity_std, y = m)) + geom_ribbon(aes(ymin = ll, ymax = hh), fill = fll0dd) + geom_smooth(aes(ymin = l, ymax = h), stat = &quot;identity&quot;, size = .7, color = clr_dark, fill = fll0dd) + facet_grid(damage_norm_std ~ min_pressure_std, labeller = label_both) p1 + scale_y_continuous(labels = scales::label_comma()) + p1 + scale_y_log10() H4 model_hurricane_fem_damlog_pres_i &lt;- ulam( flist = alist( deaths ~ dgampois( lambda, phi ), log(lambda) &lt;- alpha + beta_f * femininity_std + beta_d * damage_norm_log_std + beta_p * min_pressure_std + beta_fd * femininity_std * damage_norm_log_std + beta_fp * femininity_std * min_pressure_std + beta_dp * damage_norm_log_std * min_pressure_std + beta_fdp * femininity_std * damage_norm_log_std * min_pressure_std , alpha ~ dnorm(3, 0.5), c( beta_f, beta_d, beta_p, beta_fd, beta_fp, beta_dp, beta_fdp) ~ dnorm(0, 0.2), phi ~ dexp(1) ), data = data_hurricane, chains = 4, cores = 4, log_lik = TRUE ) compare(model_hurricane_fem_dam_pres_i, model_hurricane_fem_damlog_pres_i) %&gt;% knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight model_hurricane_fem_damlog_pres_i 646.03 33.33 0.00 NA 7.97 1 model_hurricane_fem_dam_pres_i 663.90 34.36 17.87 4.86 7.86 0 new_hurricane &lt;- crossing(femininity_std = seq(-2, 2, length.out = 15), damage_norm_log_std = seq(-2, 2, length.out = 5), min_pressure_std = seq(-2, 2, length.out = 5)) hurricne_posterior_pred &lt;- link(model_hurricane_fem_damlog_pres_i, data = new_hurricane) %&gt;% t() %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% bind_cols(new_hurricane,. ) %&gt;% pivot_longer(-(c(femininity_std,damage_norm_log_std,min_pressure_std)), names_to = &quot;.draw&quot;) %&gt;% mutate(.draw = str_remove(.draw, &quot;V&quot;) %&gt;% as.integer()) %&gt;% group_by(femininity_std, damage_norm_log_std, min_pressure_std) %&gt;% summarise(probs = list(tibble(p = c(.055, .25, .5, .75, .945), deaths = quantile(value, c(.055, .25, .5, .75, .945)), label = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;)))) %&gt;% ungroup() %&gt;% unnest(probs) p1 &lt;- hurricne_posterior_pred %&gt;% dplyr::select(-p) %&gt;% pivot_wider(names_from = &quot;label&quot;, values_from = &quot;deaths&quot;) %&gt;% ggplot(aes(x = femininity_std, y = m)) + geom_ribbon(aes(ymin = ll, ymax = hh), fill = fll0dd) + geom_smooth(aes(ymin = l, ymax = h), stat = &quot;identity&quot;, size = .7, color = clr_dark, fill = fll0dd) + facet_grid(damage_norm_log_std ~ min_pressure_std, labeller = label_both) p1 + scale_y_continuous(labels = scales::label_comma()) + p1 + scale_y_log10() H5 data_trolley_list_harm &lt;- data_trolley %&gt;% dplyr::select(response, contact, sex) %&gt;% as.list() model_trolley_harm &lt;- ulam( flist = alist( response ~ dordlogit( phi, cutpoints), phi &lt;- beta_c[sex] * contact, beta_c[sex] ~ dnorm(0, .5), cutpoints ~ dnorm(0, 1.5) ), data = data_trolley_list_harm, chains = 4, cores = 4) precis(model_trolley_harm, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 beta_c[1] -0.77 0.06 -0.87 -0.67 2599.91 1 beta_c[2] -0.41 0.06 -0.50 -0.31 2324.70 1 cutpoints[1] -2.05 0.03 -2.10 -2.00 1461.24 1 cutpoints[2] -1.39 0.03 -1.44 -1.35 1785.28 1 cutpoints[3] -0.84 0.02 -0.87 -0.80 1957.18 1 cutpoints[4] 0.14 0.02 0.11 0.18 2034.32 1 cutpoints[5] 0.79 0.02 0.75 0.83 1862.68 1 cutpoints[6] 1.68 0.03 1.63 1.72 2406.00 1 H6 data(Fish) data_fish &lt;- Fish %&gt;% as_tibble() %&gt;% mutate(hours_log = log(hours)) model_fish &lt;- ulam( flist = alist( fish_caught | fish_caught &gt; 0 ~ custom( log1m(p) + poisson_lpmf(fish_caught | lambda) ), fish_caught | fish_caught == 0 ~ custom( log_mix( p, 0, poisson_lpmf(0 | lambda) ) ), logit(p) &lt;- alpha_p, log(lambda) &lt;- hours_log + alpha_l, alpha_p ~ dnorm(-1.5, 1), alpha_l ~ dnorm(1, .5) ), data = data_fish, chains = 4, cores = 4 ) precis(model_fish) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_p -0.78 0.18 -1.06 -0.48 1263.09 1.00 alpha_l -0.14 0.03 -0.19 -0.09 1154.07 1.01 H7 dag &lt;- dagify( R ~ E + A, E ~ A, exposure = &quot;E&quot;, outcome = &quot;R&quot;) %&gt;% tidy_dagitty(.dagitty = ., layout = tibble(x = c(0,.5,1), y = c(1,0, 1))) %&gt;% mutate(stage = if_else(name == &quot;R&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;E&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag, clr_in = clr_current) + coord_fixed(ratio = .6, ylim = c(-.05,1.05)) data_trolley_list_age &lt;- data_trolley %&gt;% dplyr::select(response, education_idx, age_scl) %&gt;% as.list() %&gt;% c(., list(alpha = rep(2, 7))) model_trolley_education_age &lt;- ulam( flist = alist( response ~ ordered_logistic( phi, kappa ), phi &lt;- beta_e * sum(delta_j[1:education_idx]) + beta_a * age_scl, kappa ~ normal( 0, 1.5 ), c(beta_e, beta_a) ~ normal( 0, 1 ), vector[8]: delta_j &lt;&lt;- append_row( 0, delta ), simplex[7]: delta ~ dirichlet( alpha ) ), data = data_trolley_list_age, chains = 4, cores = 4 ) write_rds(model_trolley_education_age, file = &quot;brms/model_trolley_education_age.rds&quot;) model_trolley_education_age &lt;- readRDS(&quot;brms/model_trolley_education_age.rds&quot;) precis(model_trolley_education_age) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 beta_a -0.42 0.12 -0.6 -0.22 570.84 1.01 beta_e 0.16 0.17 -0.2 0.34 264.85 1.02 H8 data_trolley_list_age_gender &lt;- data_trolley %&gt;% dplyr::select(response, education_idx, age_scl, sex) %&gt;% as.list() %&gt;% c(., list(alpha = rep(2, 7))) dag &lt;- dagify( R ~ E + A + G, E ~ A + G, exposure = &quot;E&quot;, outcome = &quot;R&quot;) %&gt;% tidy_dagitty(.dagitty = ., layout = tibble(x = c(0,.5,1, .5), y = c(1,1, 1, 0))) %&gt;% mutate(stage = if_else(name == &quot;R&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;E&quot;, &quot;G&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag, clr_in = clr_current) + coord_fixed(ratio = .6, ylim = c(-.05,1.05)) model_trolley_education_age_gender &lt;- ulam( flist = alist( response ~ ordered_logistic( phi, kappa ), phi &lt;- beta_e[sex] * sum(delta_j[1:education_idx]) + beta_a[sex] * age_scl, kappa ~ normal( 0, 1.5 ), beta_e[sex] ~ normal( 0, 1 ), beta_a[sex] ~ normal( 0, 1 ), vector[8]: delta_j &lt;&lt;- append_row( 0, delta ), simplex[7]: delta ~ dirichlet( alpha ) ), data = data_trolley_list_age_gender, chains = 4, cores = 4 ) write_rds(model_trolley_education_age_gender, file = &quot;brms/model_trolley_education_age_gender.rds&quot;) model_trolley_education_age_gender &lt;- readRDS(&quot;brms/model_trolley_education_age_gender.rds&quot;) precis(model_trolley_education_age_gender, depth = 2 ) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 kappa[1] -2.20 0.09 -2.35 -2.07 986.01 1.00 kappa[2] -1.55 0.09 -1.70 -1.42 998.19 1.00 kappa[3] -1.00 0.09 -1.14 -0.86 1045.09 1.00 kappa[4] -0.01 0.09 -0.15 0.13 1057.49 1.00 kappa[5] 0.65 0.09 0.51 0.78 1083.92 1.00 kappa[6] 1.54 0.09 1.40 1.68 1075.90 1.00 beta_e[1] -0.70 0.14 -0.93 -0.49 869.26 1.01 beta_e[2] 0.30 0.14 0.06 0.53 1042.38 1.00 beta_a[1] -0.06 0.14 -0.29 0.17 1495.12 1.00 beta_a[2] -0.46 0.14 -0.69 -0.24 1842.19 1.00 delta[1] 0.18 0.09 0.05 0.34 1837.60 1.00 delta[2] 0.15 0.09 0.03 0.31 1795.78 1.00 delta[3] 0.27 0.11 0.10 0.45 1370.80 1.00 delta[4] 0.09 0.06 0.02 0.19 1908.61 1.00 delta[5] 0.04 0.03 0.01 0.10 1739.86 1.00 delta[6] 0.23 0.07 0.12 0.34 1930.22 1.00 delta[7] 0.04 0.03 0.01 0.09 2817.29 1.00 13.6 {brms} section 13.6.1 Over-dispersed counts 13.6.1.1 Beta-binomial First, we need to use the custom_family() function to define the name and parameters of the beta-binomial distribution for use in brm(). Second, we have to define some functions for Stan which are not defined in Stan itself. We’ll save them as stan_funs. Third, we’ll make a stanvar() statement which will allow us to pass our stan_funs to brm(). beta_binomial2 &lt;- custom_family( &quot;beta_binomial2&quot;, dpars = c(&quot;mu&quot;, &quot;phi&quot;), links = c(&quot;logit&quot;, &quot;log&quot;), lb = c(NA, 2), type = &quot;int&quot;, vars = &quot;vint1[n]&quot; ) stan_funs &lt;- &quot; real beta_binomial2_lpmf(int y, real mu, real phi, int T) { return beta_binomial_lpmf(y | T, mu * phi, (1 - mu) * phi); } int beta_binomial2_rng(real mu, real phi, int T) { return beta_binomial_rng(T, mu * phi, (1 - mu) * phi); } &quot; stanvars &lt;- stanvar(scode = stan_funs, block = &quot;functions&quot;) Did you notice the lb = c(NA, 2) portion of the code defining beta_binomial2()? In Bürkner’s vignette, he set the lower bound of phi to zero. Since McElreath wanted the lower bound for \\(\\phi\\) to be 2, we just set that as the default in the likelihood. We should clarify two more points: First, what McElreath referred to as the shape parameter, \\(\\theta\\), Bürkner called the precision parameter, \\(\\phi\\). In our exposition, above, we followed Kruschke’s convention and called it \\(\\kappa\\). These are all the same thing: \\(\\theta\\), \\(\\phi\\), and \\(\\kappa\\) are all the same thing. Perhaps less confusingly, what McElreath called the pbar parameter, \\(\\bar{p}\\), Bürkner simply refers to as \\(\\mu\\). Second, we’ve become accustomed to using the y | trials() ~ ... syntax when defining our formula arguments for binomial models. Here we are replacing trials() with vint(). From Bürkner’s Define custom response distributions with brms vignette, we read: To provide information about the number of trials (an integer variable), we are going to use the addition argument vint(), which can only be used in custom families. Similarly, if we needed to include additional vectors of real data, we would use vreal(). Actually, for this particular example, we could more elegantly apply the addition argument trials() instead of vint() as in the basic binomial model. However, since the present vignette is meant to give a general overview of the topic, we will go with the more general method. We now have all components together to fit our custom beta-binomial model: data_ucb_tib &lt;- data_ucb %&gt;% dplyr::select(admit, applications, gid) %&gt;% mutate(gid = factor(gid)) brms_c12_model_ucb_beta &lt;- brm( data = data_ucb_tib, family = beta_binomial2, # here&#39;s our custom likelihood admit | vint(applications) ~ 0 + gid, prior = c(prior(normal(0, 1.5), class = b), prior(exponential(1), class = phi)), iter = 2000, warmup = 1000, cores = 4, chains = 4, stanvars = stanvars, # note our `stanvars` seed = 42, file = &quot;brms/brms_c12_model_ucb_beta&quot;) posterior_ucb &lt;- as_draws_df(brms_c12_model_ucb_beta) posterior_ucb %&gt;% transmute(da = b_gid1 - b_gid2) %&gt;% mean_qi(.width = .89) %&gt;% mutate_if(is.double, round, digits = 3) #&gt; # A tibble: 1 × 6 #&gt; da .lower .upper .width .point .interval #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 -0.119 -1.04 0.81 0.89 mean qi p1 &lt;- posterior_ucb %&gt;% mutate(iter = 1:n(), p_bar = inv_logit_scaled(b_gid2)) %&gt;% slice_sample(n = 100) %&gt;% expand(nesting(iter, p_bar, phi), x = seq(from = 0, to = 1, by = .005)) %&gt;% mutate(density = pmap_dbl(list(x, p_bar, phi), rethinking::dbeta2)) %&gt;% ggplot(aes(x = x, y = density)) + geom_line(aes(group = iter), alpha = .4, color = clr0d) + stat_function(fun = rethinking::dbeta2, args = list(prob = mean(inv_logit_scaled(posterior_ucb$b_gid2)), theta = mean(posterior_ucb$phi)), size = .5, linetype = 2, color = clr_dark, xlim = c(0,1))+ coord_cartesian(ylim = c(0, 3)) + labs(subtitle = &quot;distribution of female admission rates&quot;, x = &quot;probability admit&quot;) Before we can do our variant of Figure 12.1.b, we’ll need to define a few more custom functions. The log_lik_beta_binomial2() and posterior_predict_beta_binomial2() functions are required for brms::predict() to work with our family = beta_binomial2 brmfit object. Similarly, posterior_epred_beta_binomial2() is required for brms::fitted() to work properly. And before all that, we need to throw in a line with the expose_functions() function. Just go with it. expose_functions(brms_c12_model_ucb_beta, vectorize = TRUE) # required to use `predict()` log_lik_beta_binomial2 &lt;- function(i, prep) { mu &lt;- prep$dpars$mu[, i] phi &lt;- prep$dpars$phi trials &lt;- prep$data$vint1[i] y &lt;- prep$data$Y[i] beta_binomial2_lpmf(y, mu, phi, trials) } posterior_predict_beta_binomial2 &lt;- function(i, prep, ...) { mu &lt;- prep$dpars$mu[, i] phi &lt;- prep$dpars$phi trials &lt;- prep$data$vint1[i] beta_binomial2_rng(mu, phi, trials) } # required to use `fitted()` posterior_epred_beta_binomial2 &lt;- function(prep) { mu &lt;- prep$dpars$mu trials &lt;- prep$data$vint1 trials &lt;- matrix(trials, nrow = nrow(mu), ncol = ncol(mu), byrow = TRUE) mu * trials } p2 &lt;- predict(brms_c12_model_ucb_beta) %&gt;% as_tibble() %&gt;% transmute(ll = Q2.5, ul = Q97.5) %&gt;% bind_cols( # the fitted intervals fitted(brms_c12_model_ucb_beta) %&gt;% as_tibble(), # the original data used to fit the model) %&gt;% brms_c12_model_ucb_beta$data ) %&gt;% mutate(case = 1:12) %&gt;% # plot! ggplot(aes(x = case)) + geom_linerange(aes(ymin = ll / applications, ymax = ul / applications), color = clr0, size = 2.5) + geom_pointrange(aes(ymin = Q2.5 / applications, ymax = Q97.5 / applications, y = Estimate/applications), color = clr0dd, size = 1/2, shape = 1) + geom_point(aes(y = admit/applications), color = clr_dark, size = 2) + scale_x_continuous(breaks = 1:12) + scale_y_continuous(breaks = 0:5 / 5, limits = c(0, 1)) + labs(subtitle = &quot;Posterior validation check&quot;, caption = expression(italic(Note.)*&quot; A = admittance probability&quot;), y = &quot;A&quot;) + theme(axis.ticks.x = element_blank(), legend.position = &quot;none&quot;) p1 + p2 13.6.1.2 Negative-binomial or gamma-Poisson We have deviated from McElreath’s convention of using \\(\\alpha\\) for the model in favor \\(\\beta_{0}\\). This is because brms parameterizes the gamma likelihood in terms of \\(\\mu\\) and shape and, as we discussed above, shape is typically denoted as \\(\\alpha\\). get_prior(data = data_kline, family = negbinomial, total_tools ~ 1) #&gt; prior class coef group resp dpar nlpar bound source #&gt; student_t(3, 3.4, 2.5) Intercept default #&gt; gamma(0.01, 0.01) shape default gamma(0.01, 0.01) is the brms default for the shape parameter in this model. Within brms, priors using the gamma distribution are based on the shape-rate \\((\\alpha - \\phi)\\) parameterization. ggplot(data = tibble(x = seq(from = 0, to = 60, by = .1)), aes(x = x, y = dgamma(x, 0.01, 0.01))) + geom_area(fill = clr0d, color = clr0dd) + scale_x_continuous(NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 50)) + ggtitle(expression(brms~default~gamma(0.01*&quot;, &quot;*0.01)~shape~prior)) brms_c12_model_ocean_sci_gamma_prep &lt;- brm( data = data_kline, family = negbinomial, total_tools ~ 1, prior = c(prior(normal(3, 0.5), class = Intercept), # beta_0 prior(gamma(0.01, 0.01), class = shape)), # alpha iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 42, file = &quot;brms/brms_c12_model_ocean_sci_gamma_prep&quot;) ocean_predictions &lt;- predict(brms_c12_model_ocean_sci_gamma_prep, summary = FALSE) ocean_predictions %&gt;% as_tibble() %&gt;% set_names(data_kline$culture) %&gt;% pivot_longer(everything(), names_to = &quot;culture&quot;, values_to = &quot;lambda&quot;) %&gt;% ggplot(aes(x = lambda)) + geom_density(color = clr0d, fill = fll0) + scale_x_continuous(&quot;lambda_culture&quot;, breaks = 0:2 * 100) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 210)) + facet_wrap(~ culture, nrow = 2) ocean_posterior &lt;- as_draws_df(brms_c12_model_ocean_sci_gamma_prep) %&gt;% as_tibble() ocean_posterior %&gt;% mutate(mu = exp(b_Intercept), alpha = shape) %&gt;% pivot_longer(mu:alpha, names_to = &quot;parameter&quot;) %&gt;% ggplot(aes(x = value)) + geom_density(color = clr0d, fill = fll0) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Behold our gamma parameters!&quot;, x = &quot;posterior&quot;) + facet_wrap(~ parameter, scales = &quot;free&quot;) \\[ \\theta = \\frac{\\mu}{\\alpha} \\] p1 &lt;- ocean_posterior %&gt;% mutate(mu = exp(b_Intercept), alpha = shape) %&gt;% mutate(theta = mu / alpha) %&gt;% ggplot(aes(x = theta)) + geom_density(color = clr0d, fill = fll0) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = glue(&quot;We define the scale as *{mth(&#39;\\U03B8&#39;)} = {mth(&#39;\\U03BC&#39;)} / {mth(&#39;\\U03B1&#39;)}*&quot;), x = &quot;posterior&quot;) + coord_cartesian(xlim = c(0, 40)) + theme(plot.subtitle = element_markdown()) set.seed(42) # wrangle to get 200 draws p2 &lt;- ocean_posterior %&gt;% mutate(iter = 1:n(), alpha = shape, theta = exp(b_Intercept) / shape) %&gt;% slice_sample(n = 200) %&gt;% expand(nesting(iter, alpha, theta), x = 0:250) %&gt;% mutate(density = dgamma(x, shape = alpha, scale = theta)) %&gt;% # plot ggplot(aes(x = x, y = density)) + geom_line(aes(group = iter), alpha = .1, color = clr0dd) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = glue(&quot;200 credible gamma densities for {mth(&#39;\\U03BB&#39;)}&quot;), x = mth(&#39;\\U03BB&#39;)) + coord_cartesian(xlim = c(0, 170), ylim = c(0, 0.045)) + theme(plot.subtitle = element_markdown(), axis.title.x = element_markdown()) p1 + p2 data_kline_f &lt;- data_kline %&gt;% mutate(contact_idx = factor(contact_idx)) brms_c12_model_ocean_sci_gamma &lt;- brm( data = data_kline_f, family = negbinomial(link = &quot;identity&quot;), bf(total_tools ~ exp(b0) * population^b1 / g, b0 + b1 ~ 0 + contact_idx, g ~ 1, nl = TRUE), prior = c(prior(normal(1, 1), nlpar = b0), prior(exponential(1), nlpar = b1, lb = 0), prior(exponential(1), nlpar = g, lb = 0), prior(exponential(1), class = shape)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, control = list(adapt_delta = .95), file = &quot;brms/brms_c12_model_ocean_sci_gamma&quot;) brms_c12_model_ocean_sci_gamma &lt;- add_criterion(brms_c12_model_ocean_sci_gamma, &quot;loo&quot;) loo(brms_c12_model_ocean_sci_gamma) #&gt; #&gt; Computed from 4000 by 10 log-likelihood matrix #&gt; #&gt; Estimate SE #&gt; elpd_loo -41.3 1.7 #&gt; p_loo 1.2 0.2 #&gt; looic 82.6 3.3 #&gt; ------ #&gt; Monte Carlo SE of elpd_loo is 0.0. #&gt; #&gt; Pareto k diagnostic values: #&gt; Count Pct. Min. n_eff #&gt; (-Inf, 0.5] (good) 9 90.0% 1674 #&gt; (0.5, 0.7] (ok) 1 10.0% 822 #&gt; (0.7, 1] (bad) 0 0.0% &lt;NA&gt; #&gt; (1, Inf) (very bad) 0 0.0% &lt;NA&gt; #&gt; #&gt; All Pareto k estimates are ok (k &lt; 0.7). #&gt; See help(&#39;pareto-k-diagnostic&#39;) for details. brms_c11_model_ocean_scientific &lt;- readRDS(&quot;brms/brms_c11_model_ocean_scientific.rds&quot;) new_ocean &lt;- distinct(data_kline_f, contact_idx) %&gt;% expand(contact_idx, population = seq(from = 0, to = 300000, length.out = 100)) p1 &lt;- fitted(brms_c11_model_ocean_scientific, newdata = new_ocean, probs = c(.055, .945)) %&gt;% as_tibble() %&gt;% bind_cols(new_ocean) %&gt;% ggplot(aes(x = population, group = contact_idx, color = contact_idx)) + geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(data = bind_cols(data_kline_f, brms_c11_model_ocean_scientific$criteria$loo$diagnostics), aes(y = total_tools, size = pareto_k), alpha = 4/5) + labs(subtitle = &quot;pure Poisson model&quot;, y = &quot;total tools&quot;) p2 &lt;- fitted(brms_c12_model_ocean_sci_gamma, newdata = new_ocean, probs = c(.055, .945)) %&gt;% as_tibble() %&gt;% bind_cols(new_ocean) %&gt;% ggplot(aes(x = population, group = contact_idx, color = contact_idx)) + geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(data = bind_cols(data_kline_f, brms_c12_model_ocean_sci_gamma$criteria$loo$diagnostics), aes(y = total_tools, size = pareto_k), alpha = 4/5) + scale_y_continuous(NULL, labels = NULL) + labs(subtitle = &quot;gamma-Poisson model&quot;) p1 + p2 &amp; scale_color_manual(values = c(`1` = clr0dd, `2` = &quot;black&quot;), guide = &quot;none&quot;) &amp; scale_size_continuous(guide = &quot;none&quot;,range = c(2, 5)) &amp; coord_cartesian(xlim = range(data_kline_f$population), ylim = range(data_kline_f$total_tools)) predict(brms_c12_model_ocean_sci_gamma, summary = FALSE) %&gt;% as_tibble() %&gt;% set_names(data_kline_f$culture) %&gt;% pivot_longer(everything(), names_to = &quot;culture&quot;, values_to = &quot;lambda&quot;) %&gt;% left_join(data_kline_f) %&gt;% ggplot(aes(x = lambda, y = 0)) + stat_halfeye(point_interval = mean_qi, .width = .5, fill = fll0dd, color = &quot;black&quot;) + geom_vline(aes(xintercept = total_tools), color = clr_dark, linetype = 3) + scale_x_continuous(&quot;lambda_culture&quot;, breaks = 0:2 * 100) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 210)) + facet_wrap(~ culture, nrow = 2) 13.6.2 Zero-inflated outcomes brms_c12_model_books_drunk &lt;- brm( data = data_books, family = zero_inflated_poisson, books ~ 1, prior = c(prior(normal(1, 0.5), class = Intercept), prior(beta(2, 6), class = zi)), # the brms default is beta(1, 1) iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c12_model_books_drunk&quot;) priors &lt;- c(prior(beta(1, 1), class = zi), prior(beta(2, 6), class = zi)) priors %&gt;% parse_dist(prior) #&gt; prior class coef group resp dpar nlpar bound source .dist .args #&gt; 1 beta(1, 1) zi user beta 1, 1 #&gt; 2 beta(2, 6) zi user beta 2, 6 priors %&gt;% parse_dist(prior) %&gt;% ggplot(aes(y = prior, dist = .dist, args = .args, fill = prior)) + stat_dist_halfeye(.width = c(.89, .5), shape = 21, point_fill = clr0d) + scale_fill_manual(values = c(clr0d, clr_dark)) + scale_x_continuous(&quot;zi&quot;, breaks = c(0, .5, 1)) + ylab(NULL) + theme(legend.position = &quot;none&quot;) fixef(brms_c12_model_books_drunk)[1, ] %&gt;% exp() #&gt; Estimate Est.Error Q2.5 Q97.5 #&gt; 0.9496241 1.0937009 0.7959164 1.1318121 The Stan code from the brms model: brms_c12_model_books_drunk$model #&gt; // generated with brms 2.16.1 #&gt; functions { #&gt; /* zero-inflated poisson log-PDF of a single response #&gt; * Args: #&gt; * y: the response value #&gt; * lambda: mean parameter of the poisson distribution #&gt; * zi: zero-inflation probability #&gt; * Returns: #&gt; * a scalar to be added to the log posterior #&gt; */ #&gt; real zero_inflated_poisson_lpmf(int y, real lambda, real zi) { #&gt; if (y == 0) { #&gt; return log_sum_exp(bernoulli_lpmf(1 | zi), #&gt; bernoulli_lpmf(0 | zi) + #&gt; poisson_lpmf(0 | lambda)); #&gt; } else { #&gt; return bernoulli_lpmf(0 | zi) + #&gt; poisson_lpmf(y | lambda); #&gt; } #&gt; } #&gt; /* zero-inflated poisson log-PDF of a single response #&gt; * logit parameterization of the zero-inflation part #&gt; * Args: #&gt; * y: the response value #&gt; * lambda: mean parameter of the poisson distribution #&gt; * zi: linear predictor for zero-inflation part #&gt; * Returns: #&gt; * a scalar to be added to the log posterior #&gt; */ #&gt; real zero_inflated_poisson_logit_lpmf(int y, real lambda, real zi) { #&gt; if (y == 0) { #&gt; return log_sum_exp(bernoulli_logit_lpmf(1 | zi), #&gt; bernoulli_logit_lpmf(0 | zi) + #&gt; poisson_lpmf(0 | lambda)); #&gt; } else { #&gt; return bernoulli_logit_lpmf(0 | zi) + #&gt; poisson_lpmf(y | lambda); #&gt; } #&gt; } #&gt; /* zero-inflated poisson log-PDF of a single response #&gt; * log parameterization for the poisson part #&gt; * Args: #&gt; * y: the response value #&gt; * eta: linear predictor for poisson distribution #&gt; * zi: zero-inflation probability #&gt; * Returns: #&gt; * a scalar to be added to the log posterior #&gt; */ #&gt; real zero_inflated_poisson_log_lpmf(int y, real eta, real zi) { #&gt; if (y == 0) { #&gt; return log_sum_exp(bernoulli_lpmf(1 | zi), #&gt; bernoulli_lpmf(0 | zi) + #&gt; poisson_log_lpmf(0 | eta)); #&gt; } else { #&gt; return bernoulli_lpmf(0 | zi) + #&gt; poisson_log_lpmf(y | eta); #&gt; } #&gt; } #&gt; /* zero-inflated poisson log-PDF of a single response #&gt; * log parameterization for the poisson part #&gt; * logit parameterization of the zero-inflation part #&gt; * Args: #&gt; * y: the response value #&gt; * eta: linear predictor for poisson distribution #&gt; * zi: linear predictor for zero-inflation part #&gt; * Returns: #&gt; * a scalar to be added to the log posterior #&gt; */ #&gt; real zero_inflated_poisson_log_logit_lpmf(int y, real eta, real zi) { #&gt; if (y == 0) { #&gt; return log_sum_exp(bernoulli_logit_lpmf(1 | zi), #&gt; bernoulli_logit_lpmf(0 | zi) + #&gt; poisson_log_lpmf(0 | eta)); #&gt; } else { #&gt; return bernoulli_logit_lpmf(0 | zi) + #&gt; poisson_log_lpmf(y | eta); #&gt; } #&gt; } #&gt; // zero-inflated poisson log-CCDF and log-CDF functions #&gt; real zero_inflated_poisson_lccdf(int y, real lambda, real zi) { #&gt; return bernoulli_lpmf(0 | zi) + poisson_lccdf(y | lambda); #&gt; } #&gt; real zero_inflated_poisson_lcdf(int y, real lambda, real zi) { #&gt; return log1m_exp(zero_inflated_poisson_lccdf(y | lambda, zi)); #&gt; } #&gt; } #&gt; data { #&gt; int&lt;lower=1&gt; N; // total number of observations #&gt; int Y[N]; // response variable #&gt; int prior_only; // should the likelihood be ignored? #&gt; } #&gt; transformed data { #&gt; } #&gt; parameters { #&gt; real Intercept; // temporary intercept for centered predictors #&gt; real&lt;lower=0,upper=1&gt; zi; // zero-inflation probability #&gt; } #&gt; transformed parameters { #&gt; } #&gt; model { #&gt; // likelihood including constants #&gt; if (!prior_only) { #&gt; // initialize linear predictor term #&gt; vector[N] mu = Intercept + rep_vector(0.0, N); #&gt; for (n in 1:N) { #&gt; target += zero_inflated_poisson_log_lpmf(Y[n] | mu[n], zi); #&gt; } #&gt; } #&gt; // priors including constants #&gt; target += normal_lpdf(Intercept | 1, 0.5); #&gt; target += beta_lpdf(zi | 2, 6); #&gt; } #&gt; generated quantities { #&gt; // actual population-level intercept #&gt; real b_Intercept = Intercept; #&gt; } 13.6.3 Ordered categorical outcomes Whereas in rethinking::ulam() you indicate the likelihood by &lt;criterion&gt; ~ dordlogit(0 , c(&lt;the thresholds&gt;), in brms::brm() you code family = cumulative. Here’s how to fit the intercepts-only model. 13.6.3.1 Describing an ordered distribution with intercepts # define the start values inits &lt;- list(`Intercept[1]` = -2, `Intercept[2]` = -1, `Intercept[3]` = 0, `Intercept[4]` = 1, `Intercept[5]` = 2, `Intercept[6]` = 2.5) inits_list &lt;- list(inits, inits, inits, inits) brms_c12_model_trolley &lt;- brm( data = data_trolley, family = cumulative, response ~ 1, prior(normal(0, 1.5), class = Intercept), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 42, inits = inits_list, # here we add our start values file = &quot;brms/brms_c12_model_trolley&quot;) brms_c12_model_trolley %&gt;% fixef() %&gt;% inv_logit_scaled() %&gt;% round(digits = 3) %&gt;% as.data.frame() %&gt;% knit_precis() param Estimate Est.Error Q2.5 Q97.5 Intercept[1] 0.13 0.51 0.12 0.14 Intercept[2] 0.22 0.51 0.21 0.23 Intercept[3] 0.33 0.50 0.32 0.34 Intercept[4] 0.56 0.50 0.55 0.57 Intercept[5] 0.71 0.50 0.70 0.72 Intercept[6] 0.85 0.51 0.85 0.86 But recall that the posterior \\(SD\\) (i.e., the ‘Est.Error’ values) are not valid using that approach. If you really care about them, you’ll need to work with the as_draws_df(). as_draws_df(brms_c12_model_trolley) %&gt;% mutate_all(inv_logit_scaled) %&gt;% pivot_longer(starts_with(&quot;b_&quot;), names_to = &quot;param&quot;) %&gt;% group_by(param) %&gt;% summarise(mean = mean(value), sd = sd(value), ll = quantile(value, probs = .025), ul = quantile(value, probs = .975)) %&gt;% knitr::kable() param mean sd ll ul b_Intercept[1] 0.1282508 0.0033712 0.1215212 0.1348983 b_Intercept[2] 0.2198645 0.0041508 0.2117238 0.2278593 b_Intercept[3] 0.3277496 0.0046961 0.3184706 0.3368620 b_Intercept[4] 0.5616384 0.0049927 0.5516282 0.5713468 b_Intercept[5] 0.7088398 0.0045180 0.6999152 0.7175442 b_Intercept[6] 0.8543232 0.0035578 0.8474255 0.8611575 fixef(brms_c12_model_trolley) %&gt;% as_tibble() %&gt;% rownames_to_column(&quot;intercept&quot;) %&gt;% mutate(response = str_extract(intercept, &quot;\\\\d&quot;) %&gt;% as.double()) %&gt;% ggplot(aes(x = response, y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = response)) + geom_line(color = clr0dd) + geom_point(shape = 21, colour = clr0dd, fill = clr0, size = 1.5, stroke = 1) + geom_linerange(color = clr0dd) + scale_x_continuous(breaks = 1:7, limits = c(1, 7)) + ylab(&quot;log-cumulative-odds&quot;) 13.6.3.2 Adding predictor variables brms_c12_model_trolley_predict &lt;- brm( data = data_trolley, family = cumulative, response ~ 1 + action + contact + intention + intention:action + intention:contact, prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 0.5), class = b)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 42, file = &quot;brms/brms_c12_model_trolley_predict&quot;) as_draws_df(brms_c12_model_trolley_predict) %&gt;% as_tibble() %&gt;% select(b_action:`b_contact:intention`) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = name)) + geom_vline(xintercept = 0, alpha = 1/5, linetype = 3) + stat_gradientinterval(.width = .5, size = 1, point_size = 3/2, shape = 21, point_fill = clr0, fill = clr0dd, color = clr_dark) + scale_x_continuous(&quot;marginal posterior&quot;, breaks = -5:0 / 4) + scale_y_discrete(NULL) + coord_cartesian(xlim = c(-1.4, 0)) # new_trolley &lt;- d %&gt;% # distinct(action, contact, intention) %&gt;% # mutate(combination = str_c(action, contact, intention, sep = &quot;_&quot;)) trolley_fitted_prep &lt;- fitted(brms_c12_model_trolley_predict, newdata = new_trolley, summary = FALSE) trolley_fitted &lt;- rbind(trolley_fitted_prep[, , 1], trolley_fitted_prep[, , 2], trolley_fitted_prep[, , 3], trolley_fitted_prep[, , 4], trolley_fitted_prep[, , 5], trolley_fitted_prep[, , 6], trolley_fitted_prep[, , 7]) %&gt;% as_tibble() %&gt;% set_names(pull(new_trolley, combination)) %&gt;% mutate(response = rep(1:7, each = n() / 7), iter = rep(1:4000, times = 7)) %&gt;% pivot_longer(-c(iter, response), names_to = c(&quot;action&quot;, &quot;contact&quot;, &quot;intention&quot;), names_sep = &quot;_&quot;, values_to = &quot;pk&quot;) %&gt;% mutate(intention = intention %&gt;% as.integer()) p1 &lt;- trolley_fitted %&gt;% # unnecessary for these plots filter(response &lt; 7) %&gt;% # this will help us define the three panels of the triptych mutate(facet = factor(glue(&quot;action: {action}; contact: {contact}&quot;), levels = label_levels)) %&gt;% # these next three lines allow us to compute the cumulative probabilities group_by(iter, facet, intention) %&gt;% arrange(iter, facet, intention, response) %&gt;% mutate(probability = cumsum(pk)) %&gt;% ungroup() %&gt;% # these next three lines are how we randomly selected 50 posterior draws nest(data = -iter) %&gt;% slice_sample(n = 50) %&gt;% unnest(data) %&gt;% # plot! ggplot(aes(x = intention, y = probability)) + geom_line(aes(group = interaction(iter, response), color = probability), alpha = 1/10) + geom_point(data = data_trolley %&gt;% # wrangle the original data to make the dots group_by(intention, contact, action) %&gt;% count(response) %&gt;% mutate(probability = cumsum(n / sum(n)), facet = factor(glue(&quot;action: {action}; contact: {contact}&quot;), levels = label_levels)) %&gt;% filter(response &lt; 7), color = clr_dark) + scale_color_gradient(low = clr0, high = clr_dark) + scale_x_continuous(&quot;intention&quot;, breaks = 0:1) + scale_y_continuous(breaks = c(0, .5, 1), limits = 0:1) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ facet) trolley_perdictions &lt;- predict(brms_c12_model_trolley_predict, newdata = new_trolley, ndraws = 1000, scale = &quot;response&quot;, summary = FALSE) p2 &lt;- trolley_perdictions %&gt;% as_tibble() %&gt;% set_names(pull(new_trolley, combination)) %&gt;% pivot_longer(everything(), names_to = c(&quot;action&quot;, &quot;contact&quot;, &quot;intention&quot;), names_sep = &quot;_&quot;, values_to = &quot;response&quot;) %&gt;% mutate(facet = factor(glue(&quot;action: {action}; contact: {contact}&quot;), levels = label_levels)) %&gt;% ggplot(aes(x = response, fill = intention)) + geom_bar(width = 1/3, position = position_dodge(width = .4)) + scale_fill_manual(values = c(clr0d, clr_dark)) + scale_x_continuous(&quot;response&quot;, breaks = 1:7) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ facet) p1 + p2 + plot_layout(ncol = 1) p1 &lt;- trolley_fitted %&gt;% mutate(facet = factor(glue(&quot;action: {action}; contact: {contact}&quot;), levels = label_levels)) %&gt;% group_by(iter, facet, intention) %&gt;% summarise(mean_response = sum(pk * response)) %&gt;% ungroup() %&gt;% nest(data = -iter) %&gt;% slice_sample(n = 50) %&gt;% unnest(data) %&gt;% ggplot(aes(x = intention, y = mean_response)) + geom_line(aes(group = iter, color = mean_response), alpha = 1/10) + scale_color_gradient(low = clr0, high = clr_dark) + scale_x_continuous(&quot;intention&quot;, breaks = 0:1) + scale_y_continuous(&quot;resopnse&quot;, breaks = 1:7, limits = c(1, 7)) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ facet) p2 &lt;- trolley_fitted %&gt;% mutate(facet = factor(glue(&quot;action: {action}; contact: {contact}&quot;), levels = label_levels)) %&gt;% ggplot(aes(x = response, y = pk, fill = factor(intention))) + stat_ccdfinterval(.width = .95, justification = 1, size = 1/4, shape = 21, point_fill = clr0, point_size = 1/3, position = &quot;dodge&quot;, width = .75) + scale_fill_manual(values = c(clr0d, clr_dark)) + scale_x_continuous(&quot;resopnse&quot;, breaks = 1:7) + scale_y_continuous(&quot;count&quot;, breaks = 0:3 / 10, labels = 0:3 * 100, limits = c(0, NA)) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ facet) p1 + p2 + plot_layout(ncol = 1) 13.6.4 Ordered categorical predictors The brms package has a rdirichlet() function, too. Here we use that to make an alternative version of the plot, above. set.seed(12) brms::rdirichlet(n = 1e4, alpha = rep(2, 7)) %&gt;% data.frame() %&gt;% set_names(1:7) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = name %&gt;% as.double(), alpha = str_c(&quot;alpha[&quot;, name, &quot;]&quot;)) %&gt;% ggplot(aes(x = value, color = name, group = name, fill= name)) + geom_density(alpha = .8) + scale_fill_gradient(low = clr0, high = clr_dark) + scale_color_gradient(low = clr0, high = clr_dark) + scale_x_continuous(&quot;probability&quot;, limits = c(0, 1), breaks = c(0, .5, 1), labels = c(&quot;0&quot;, &quot;.5&quot;, &quot;1&quot;), ) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(&quot;Dirichlet&quot;*(2*&quot;, &quot;*2*&quot;, &quot;*2*&quot;, &quot;*2*&quot;, &quot;*2*&quot;, &quot;*2*&quot;, &quot;*2))) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ alpha, nrow = 2) When using brms, the issue of treating a predictor in the way McElreath covered in this section is referred to as monotonic effects. Bürkner outlined the issue in his (2021c) vignette, Estimating monotonic effects with with brms, and in his (2020) article with Emmanuel Charpentier, Modelling monotonic effects of ordinal predictors in Bayesian regression models (click here for the freely-available preprint). For a single monotonic predictor, \\(x\\), the linear predictor term of observation \\(n\\) looks as follows: \\[\\eta_{n} = bD\\sum_{i = 1}^{x_{n}} \\zeta_{i}\\] The parameter \\(b\\) can take on any real value, while \\(\\zeta\\) is a simplex, which means that it satisfies \\(\\zeta_{i} \\in [0, 1]\\) and \\(\\sum_{i = 1}^{D} \\zeta_{i} = 1\\) with \\(D\\) being the number of elements of \\(\\zeta\\). Equivalently, \\(D\\) is the number of categories (or highest integer in the data) minus 1, since we start counting categories from zero to simplify the notation. In this context, \\(N\\) indexes the observations in the hypothetical data and \\(\\eta\\) denotes the linear model for some outcome \\(y\\). Unlike with rethinking, the brms syntax for fitting models with monotonic predictors is fairly simple. Just place your monotonic predictors within the mo() function and enter them into the formula. # first run-time: ~ 20 min brms_c12_model_trolley_education &lt;- brm( data = data_trolley, family = cumulative, response ~ 1 + action + contact + intention + mo(education_idx), # note the `mo()` syntax prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = b), # note the new kinds of prior statements prior(normal(0, 0.143), class = b, coef = moeducation_idx), prior(dirichlet(2, 2, 2, 2, 2, 2, 2), class = simo, coef = moeducation_idx1)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 42, file = &quot;brms/brms_c12_model_trolley_education&quot;) If you compare our results to those in the text, you may be surprised by how small our summary values are for moeducation_idx. brms and rethinking have an important difference in how they parameterize \\(\\beta_{Education}\\). From page 392 in the text, McElreath explained the sum of all the \\(\\delta\\) parameters is the maximum education effect. It will be very convenient for interpretation if we call this maximum sum an ordinary coefficient like \\(\\beta_{E}\\) and then let the \\(\\delta\\) parameters be fractions of it. If we also make a dummy \\(\\delta_0 = 0\\) then we can write it all very compactly. Like this: \\[\\phi_{i} = \\beta_{E} \\sum_{j = 1}^{E_{i} - 1} \\delta_{j} + \\textrm{other stuff}\\] where \\(E_{i}\\) is the completed education level of individual \\(i\\). Now the sum of every \\(\\delta_{j}\\) is 1, and we can interpret the maximum education effect by looking at \\(\\beta_{E}\\). The current version of brms takes expresses \\(\\beta_{E}\\) as an average effect. From Bürkner &amp; Charpentier (2020), we read: If the monotonic effect is used in a linear model, \\(b\\) can be interpreted as the expected average difference between two adjacent categories of \\(x\\), while \\(\\zeta_{i}\\) describes the expected difference between the categories \\(i\\) and \\(i − 1\\) in the form of a proportion of the overall difference between lowest and highest categories. Thus, this parameterization has an intuitive interpretation while guaranteeing the monotonicity of the effect (p. 6) To clarify, the \\(b\\) in this section is what we’re calling \\(\\beta_{E}\\) in the current example and Bürkner and Charpentier used \\(\\zeta_{i}\\) in place of McElreath’s \\(\\delta_{j}\\). The upshot of all this is that if we’d like to compare the summary of our brms_c12_model_trolley_education to the results McElreath reported for his m12.6, we’ll need to multiply our moeducation_idx by 7. as_draws_df(brms_c12_model_trolley_education) %&gt;% transmute(bE = bsp_moeducation_idx * 7) %&gt;% median_qi(.width = .89) %&gt;% mutate_if(is.double, round, digits = 2) #&gt; # A tibble: 1 × 6 #&gt; bE .lower .upper .width .point .interval #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 -0.35 -0.69 -0.12 0.89 median qi This parameterization difference between brms and rethinking is also the reason why we set prior(normal(0, 0.143), class = b, coef = moedu_new) within brms_c12_model_trolley_education where as McElreath used a Normal(0,1) prior for all his \\(\\beta\\) coefficients, including for his \\(\\beta_{E}\\). Because our moedu_new (i.e., \\(\\beta_{E}\\)) is parameterized as the average of seven \\(\\delta\\) parameters, it made sense to divide our hyperparameter for \\(\\sigma\\) by 7. That is, \\(1/7 \\approx 0.143\\). \\[ \\begin{array}{rcl} \\textrm{response} &amp; \\sim &amp; \\textrm{Categorical}(p) \\\\ \\textrm{logit}(p_{k}) &amp; = &amp; \\alpha_{k} - \\phi_{i} \\\\ \\phi_{i} &amp; = &amp; \\beta_{a}~action_{i} + \\beta_{c}~contact_{i} + \\beta_{i}~intetion_{i} + \\beta_{e} \\textrm{mo}(education\\_idx_{i}, \\delta)\\\\ \\alpha_{k} &amp; \\sim &amp; Normal(0, 1.5) \\\\ \\beta_{a}, \\beta_{c}, \\beta_{i} &amp; \\sim &amp; Normal(0, 1)\\\\ \\beta_{e} &amp; \\sim &amp; Normal(0, 0.143) \\\\ \\delta &amp; \\sim &amp; Dirichlet(2,2,2,2,2,2,2) \\end{array} \\] where \\(mo(x,\\delta)\\) is an operator indicating some predictor \\(x\\) is has undergone the monotonic transform and \\(\\delta\\) is our vector of simplex parameters \\(\\delta_{1}, .., \\delta_{7}\\). That is, \\(\\beta_{e}~mo(education\\_idx_{i}, \\delta)\\) is our alternative brms-oriented way of expressing McElreath’s \\(\\beta_{e} \\sum_{j = 0}^{E_{i}-1} \\delta_{j}\\). I don’t know that one is better. 🤷 At first contact, I found them both confusing. as_draws_df(brms_c12_model_trolley_education) %&gt;% as_tibble() %&gt;% select(contains(&quot;simo_moeducation_idx1&quot;)) %&gt;% ggpairs( lower = list(continuous = wrap(my_lower, col = &quot;black&quot;)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll0, color = &quot;black&quot;, fill = clr_alpha(&quot;black&quot;), adjust = .7)), upper = list(continuous = wrap(my_upper , size = 4, color = &quot;black&quot;, family = fnt_sel)) ) + theme(panel.border = element_rect(color = clr_dark, fill = &quot;transparent&quot;)) brms_c12_model_trolley_education_linear &lt;- brm( data = data_trolley, family = cumulative, response ~ 1 + action + contact + intention + education_norm, prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = b)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 42, file = &quot;brms/model_trolley_education_linear&quot;) fixef(brms_c12_model_trolley_education_linear)[7:10, ] %&gt;% as.data.frame() %&gt;% knitr::kable() Estimate Est.Error Q2.5 Q97.5 action -0.7050471 0.0401382 -0.7800204 -0.6257275 contact -0.9569504 0.0496886 -1.0559838 -0.8603990 intention -0.7183681 0.0356500 -0.7882778 -0.6483887 education_norm -0.1151032 0.0920943 -0.2956388 0.0688995 new_trolley2 &lt;- tibble(education_idx = 1:8, action = 0, contact = 0, intention = 0) trolley_education_fitted &lt;- fitted(brms_c12_model_trolley_education, newdata = new_trolley2) trolley_education_fitted_tib &lt;- rbind(trolley_education_fitted[, , 1], trolley_education_fitted[, , 2], trolley_education_fitted[, , 3], trolley_education_fitted[, , 4], trolley_education_fitted[, , 5], trolley_education_fitted[, , 6], trolley_education_fitted[, , 7]) %&gt;% as_tibble %&gt;% mutate(edu = factor(rep(1:8, times = 7)), response = rep(1:7, each = 8)) new_trolley3 &lt;- new_trolley2 %&gt;% mutate(education_norm = 1:8) trolley_education_linear_fitted &lt;- fitted(brms_c12_model_trolley_education_linear, newdata = new_trolley3) trolley_education_linear_fitted_tib &lt;- rbind(trolley_education_linear_fitted[, , 1], trolley_education_linear_fitted[, , 2], trolley_education_linear_fitted[, , 3], trolley_education_linear_fitted[, , 4], trolley_education_linear_fitted[, , 5], trolley_education_linear_fitted[, , 6], trolley_education_linear_fitted[, , 7]) %&gt;% as_tibble() %&gt;% mutate(edu = factor(rep(1:8, times = 7)), response = rep(1:7, each = 8)) bind_rows(trolley_education_fitted_tib, trolley_education_linear_fitted_tib) %&gt;% mutate(fit = rep(c(&quot;`mo()` syntax&quot;, &quot;conventional syntax&quot;), each = n() / 2)) %&gt;% ggplot(aes(x = response, y = Estimate, ymin = Q2.5, ymax = Q97.5, color = edu, group = edu)) + geom_pointrange(fatten = 3/2, position = position_dodge(width = 3/4)) + scale_color_manual(&quot;education&quot;, values = scales::colour_ramp(colors = c(clr0d, clr_current))(seq(0,1, length.out = 8))) + scale_x_continuous(breaks = 1:7) + scale_y_continuous(&quot;probability&quot;, limits = c(0, .43)) + theme(legend.background = element_blank(), legend.position = &quot;right&quot;) + facet_wrap(~ fit) brms_c12_model_trolley_education &lt;- add_criterion(brms_c12_model_trolley_education, &quot;loo&quot;) brms_c12_model_trolley_education_linear &lt;- add_criterion(brms_c12_model_trolley_education_linear, &quot;loo&quot;) loo_compare(brms_c12_model_trolley_education, brms_c12_model_trolley_education_linear, criterion = &quot;loo&quot;) %&gt;% print(simplify = FALSE) #&gt; elpd_diff se_diff elpd_loo se_elpd_loo #&gt; brms_c12_model_trolley_education 0.0 0.0 -18540.6 38.1 #&gt; brms_c12_model_trolley_education_linear -4.4 1.7 -18545.0 38.0 #&gt; p_loo se_p_loo looic se_looic #&gt; brms_c12_model_trolley_education 11.0 0.1 37081.3 76.2 #&gt; brms_c12_model_trolley_education_linear 9.9 0.1 37090.1 76.1 model_weights(brms_c12_model_trolley_education, brms_c12_model_trolley_education_linear, weights = &quot;loo&quot;) %&gt;% round(digits = 2) #&gt; brms_c12_model_trolley_education brms_c12_model_trolley_education_linear #&gt; 0.99 0.01 We might explore the monotonic effects of brms_c12_model_trolley_education in one more way. If you were reading closely along in the text, you may have noticed that “the sum of every \\(\\delta_{j}\\) is 1” (p. 392). When using HMC, this is true for each posterior draw. We can exploit that information to visualize the \\(\\delta_{j}\\) parameters in a cumulative fashion. as_draws_df(brms_c12_model_trolley_education) %&gt;% select(contains(&quot;idx1&quot;)) %&gt;% set_names(1:7) %&gt;% mutate(iter = 1:n(), `0` = 0) %&gt;% pivot_longer(-iter, names_to = &quot;delta&quot;) %&gt;% arrange(delta) %&gt;% group_by(iter) %&gt;% mutate(cum_sum = cumsum(value)) %&gt;% ggplot(aes(x = delta, y = cum_sum)) + stat_pointinterval(.width = .89, size = 1, color = clr0dd) + stat_pointinterval(.width = .5, shape = 21, color = clr0dd, point_color = clr0dd, fill = clr0) + scale_x_discrete(NULL, labels = str_c(&quot;delta[&quot;, 0:7 , &quot;]&quot;)) + ylab(&quot;cumulative sum&quot;) This is another way to show that the largest effects of education are when going from Elementary School to Middle School \\((\\delta_{0} \\rightarrow \\delta_{1})\\) and when going from Some High School to High School Graduate \\((\\delta_{2} \\rightarrow \\delta_{3})\\). data_trolley %&gt;% distinct(edu, education_idx) %&gt;% arrange(education_idx) %&gt;% mutate(`delta[j]` = education_idx - 1) #&gt; # A tibble: 8 × 3 #&gt; edu education_idx `delta[j]` #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Elementary School 1 0 #&gt; 2 Middle School 2 1 #&gt; 3 Some High School 3 2 #&gt; 4 High School Graduate 4 3 #&gt; 5 Some College 5 4 #&gt; 6 Bachelor&#39;s Degree 6 5 #&gt; 7 Master&#39;s Degree 7 6 #&gt; 8 Graduate Degree 8 7 13.7 pymc3 section × "],["rethinking-chapter-13.html", "14 Rethinking: Chapter 13 14.1 Multilevel tadpoles 14.2 Varying Effects and the Underfitting/Overfitting Tade-Off 14.3 More than One Type of Cluster 14.4 Divergent Transitions and Non-Centered Priors 14.5 Multilevel Posterior Predictions 14.6 Homework 14.7 {brms} section 14.8 pymc3 section", " 14 Rethinking: Chapter 13 Models with Memory by Richard McElreath, building on the Summaries by Solomon Kurz and Erik Kusch. Advantages of multilevel models: Improved estimates for repeated sampling Improved estimates for imbalanced sampling Estimates of variation Avoid averaging, retain variation When it comes to regression, multilevel regression deserves to be the default approach. There are certainly contexts in which it would be better to use an old-fashioned single-level model. But the contexts in which multilevel models are superior are much more numerous. Better to begin to build a multilevel analysis and then realize it’s unnecessary, than to overlook it. The most common synonyms for ‘multilevel’ are hierarchical and mixed effects. The type of parameters that appear in multilevel models are most commonly known as random effects, which itself can mean very different things to different analysts and in different contexts. 14.1 Multilevel tadpoles library(rethinking) data(reedfrogs) data_frogs &lt;- reedfrogs %&gt;% as_tibble() %&gt;% mutate(tank = row_number()) rm(reedfrogs) Old fashioned Single-level model: \\[ \\begin{array}{rcllr} S_{i} &amp; \\sim &amp; Binomial(N_{i}, p_{i}) &amp; &amp; \\textrm{[likelihood]}\\\\ \\textrm{logit}(p_{i}) &amp; = &amp; \\alpha_{TANK[i]} &amp; &amp; \\textrm{[unique log-odds for each tank]}\\\\ \\alpha_{j} &amp; \\sim &amp; Normal(0, 1.5) &amp; \\textrm{for}~j = 1..48 &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\end{array} \\] data_frogs_list_1 &lt;- data_frogs %&gt;% dplyr::select(surv, density, tank) %&gt;% as.list() model_frog_single &lt;- ulam( flist = alist( surv ~ dbinom( density, p ), logit(p) &lt;- alpha[tank], alpha[tank] ~ dnorm(0, 1.5) ), data = data_frogs, cores = 4, chains = 4, log_lik = TRUE ) # not executed because to verbose precis(model_frog_single, depth = 2) %&gt;% knit_precis() Update to multilevel model: \\[ \\begin{array}{rclr} S_{i} &amp; \\sim &amp; Binomial(N_{i}, p_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{logit}(p_{i}) &amp; = &amp; \\alpha_{TANK[i]} &amp; \\textrm{[unique log-odds for each tank]}\\\\ \\alpha_{j} &amp; \\sim &amp; Normal(\\color{#B35136}{\\bar{\\alpha}}, \\color{#B35136}{\\sigma}) &amp; \\textrm{[adaptive prior]}\\\\ \\color{#B35136}{\\bar{\\alpha}} &amp; \\sim &amp; \\color{#B35136}{Normal(0, 1.5)} &amp;\\textrm{[prior for average tank]}\\\\ \\color{#B35136}{\\sigma} &amp; \\sim &amp; \\color{#B35136}{Exponential(1)} &amp; \\textrm{[prior for standard deviation of tanks]}\\\\ \\end{array} \\] The parameters \\(\\bar{\\alpha}\\) and \\(\\sigma\\) represent hyperparameters which define the second level of the multilevel model. They come with hyperpriors. model_frog_multi &lt;- ulam( flist = alist( surv ~ dbinom( density, p ), logit(p) &lt;- alpha[tank], alpha[tank] ~ dnorm(alpha_bar, sigma), alpha_bar ~ dnorm(0, 1.5), sigma ~ dexp(1) ), data = data_frogs, cores = 4, chains = 4, log_lik = TRUE ) compare(model_frog_single, model_frog_multi) %&gt;% knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight model_frog_multi 201.75 7.38 0.00 NA 21.68 1 model_frog_single 214.91 4.41 13.16 3.97 25.71 0 🤓 There are only 21 effective parameters (pWAIC) in the multilevel model, despite the 48 different tanks. This is because we used a regularizing prior that shrinks the alpha intercepts towards the mean \\(\\bar{\\alpha}\\). frogs_posterior &lt;- extract.samples(model_frog_multi) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% pivot_longer(everything(), names_to = &quot;param&quot;) %&gt;% filter(!(param %in% c(&quot;alpha_bar&quot;, &quot;sigma&quot;))) %&gt;% group_by(param) %&gt;% summarize(posterior = list(tibble(value = c(quantile(value, probs = c(.055, .25, .5, .75,.945)), mean(value)), label = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;, &quot;mean&quot;)))) %&gt;% ungroup() %&gt;% unnest(posterior) %&gt;% mutate(value = logistic(value)) %&gt;% pivot_wider(names_from = label, values_from = value ) %&gt;% mutate(tank = str_remove( param, &quot;alpha.&quot;) %&gt;% as.integer()) %&gt;% arrange(tank) %&gt;% left_join(data_frogs, .) posterior_median &lt;- extract.samples(model_frog_multi) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% pivot_longer(everything(), names_to = &quot;param&quot;) %&gt;% summarize(median = median(value)) %&gt;% .$median %&gt;% logistic() frogs_posterior %&gt;% ggplot(aes(x = tank, color = factor(density))) + geom_hline(yintercept = posterior_median, color = clr_dark, linetype = 3) + geom_linerange(aes(ymin = ll, ymax = hh), size = .2)+ geom_linerange(aes(ymin = l, ymax = h), size = .75) + geom_point(aes(y = m, fill = after_scale(clr_lighten(color))), shape = 21) + geom_point(aes(y = propsurv), shape = 1, size = 2) + facet_wrap(density ~ ., scale = &quot;free_x&quot;, labeller = label_both) + scale_color_manual(values = c(clr0d, clr_dark, clr_current), guide = &quot;none&quot;) + labs(y = &#39;survival_prop&#39;) + theme(panel.border = element_rect(color = clr0d, fill = &quot;transparent&quot;)) The mean posterior is always closer to the estimated median survival proportion of the population than the raw data. This is an effect called shrinkage, resulting from the regularizing priors as an effect of pooling information across clusters. The inferred population distribution of survival: frogs_hyperposterior &lt;- extract.samples(model_frog_multi) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% dplyr::select(alpha_bar, sigma) p1 &lt;- ggplot() + pmap(frogs_hyperposterior %&gt;% filter(row_number() &lt; 100), function(alpha_bar, sigma){ stat_function(fun = dnorm, args = list(mean = alpha_bar, sd = sigma), xlim = c(-3, 4), geom = &quot;line&quot;, color = clr0dd, alpha = .4) }) + labs(y = &quot;density&quot;, x = &quot;log-odds survival&quot;, subtitle = &quot;population distribution of survival&quot;) sim_taks &lt;- tibble(logit_p = rnorm(8000, mean = frogs_hyperposterior$alpha_bar, sd = frogs_hyperposterior$sigma), p = inv_logit(logit_p)) p2 &lt;- sim_taks %&gt;% ggplot(aes(x = p)) + geom_density(adjust = .1, color = clr0d, fill = fll0) + labs(subtitle = &quot;variation of survival in population of tanks&quot;) p1 + p2 There can be certain issues with the weakly regularizing priors for the variance components (\\(\\sigma\\)) in multilevel models: if there only few clusters (e.g. ~ 5), estimating the variance is difficult and a more informative prior might be necessary the edge-effects of \\(log\\) and \\(logit\\) link functions can lead to ineffective sampling (lots of divergent transitions and small n_eff - visible as swings in the trace plot for \\(\\sigma\\)). Using a distribution with less mass in the tail compared to the \\(exponential\\) can help here: \\[ \\begin{array}{rclr} S_{i} &amp; \\sim &amp; Binomial(N_{i}, p_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{logit}(p_{i}) &amp; = &amp; \\alpha_{TANK[i]} &amp; \\textrm{[linear model]}\\\\ \\alpha_{j} &amp; \\sim &amp; Normal(\\bar{\\alpha}, \\sigma) &amp; \\textrm{[}~\\alpha~\\textrm{prior]}\\\\ \\bar{\\alpha} &amp; \\sim &amp; Normal(0, 1.5) &amp; \\textrm{[}~\\bar{\\alpha}~\\textrm{prior]}\\\\ \\sigma &amp; \\sim &amp; Half\\textrm{-}Normal(0, 1) &amp; \\textrm{[}~\\sigma~\\textrm{prior]}\\\\ \\end{array} \\] Use dhalfnorm() in {rethinking} and lower = 0 in regular Stan. 14.2 Varying Effects and the Underfitting/Overfitting Tade-Off Simulating data from a multilevel model to compare the estimates from a no-pooling, complete pooling and partial-pooling modeling approach. The Model \\[ \\begin{array}{rclr} S_{i} &amp; \\sim &amp; Binomial(N_{i}, p_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{logit}(p_{i}) &amp; = &amp; \\alpha_{\\color{#B35136}{POND[i]}} &amp; \\textrm{[linear model]}\\\\ \\alpha_{j} &amp; \\sim &amp; Normal(\\bar{\\alpha}, \\sigma) &amp; \\textrm{[}~\\alpha~\\textrm{prior]}\\\\ \\bar{\\alpha} &amp; \\sim &amp; Normal(0, 1.5) &amp; \\textrm{[}~\\bar{\\alpha}~\\textrm{prior]}\\\\ \\sigma &amp; \\sim &amp; Half\\textrm{-}Normal(0, 1) &amp; \\textrm{[}~\\sigma~\\textrm{prior]}\\\\ \\end{array} \\] Assigning values to the parameters and simulating the non-pooled estimates. set.seed(42) alpha_bar &lt;- 1.5 sigma &lt;- 1.5 n_ponds &lt;- 60 pond_levels &lt;- c(&quot;tiny&quot;, &quot;small&quot;, &quot;medium&quot;, &quot;large&quot;) data_sim &lt;- tibble(pond_idx = 1:n_ponds, n_i = rep(c(5, 10, 25, 35), each = 15), pond_size = rep(pond_levels, each = 15) %&gt;% factor(levels = pond_levels), true_alpha = rnorm(n = n_ponds, mean = alpha_bar, sd = sigma), surv_i = rbinom(n_ponds, prob = logistic(true_alpha), size = n_i), p_true = inv_logit(true_alpha), p_no_pool = surv_i / n_i) Where true_alpha is the true log-odds survival for each pond, surv_i are the simulated survivors and p_no_pool is the non-pooling estimate of survival. Remember that we used the logit link, so the probability is defined by the logistic function (\\(p_i = \\frac{exp(\\alpha_i)}{1 - exp(\\alpha_{i})}\\)) Computing the partial-pooling estimates. data_sim_list &lt;- data_sim %&gt;% dplyr::select(surv_i, n_i, pond_idx) model_sim_partial &lt;- ulam( flist = alist( surv_i ~ dbinom(n_i, p), logit(p) &lt;- alpha_pond[pond_idx], alpha_pond[pond_idx] ~ dnorm(alpha_bar, sigma), alpha_bar ~ dnorm(0, 1.5), sigma ~ dexp( 1 ) ), data = data_sim_list, cores = 4, chains = 4, log_lik = TRUE ) precis(model_sim_partial, depth = 2) %&gt;% tail() %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_pond[57] 2.09 0.52 1.31 2.98 2382.97 1 alpha_pond[58] 1.61 0.43 0.95 2.33 4025.40 1 alpha_pond[59] -2.04 0.52 -2.91 -1.26 2461.46 1 alpha_pond[60] 1.60 0.44 0.93 2.33 3172.11 1 alpha_bar 1.36 0.23 0.99 1.74 2211.66 1 sigma 1.58 0.21 1.26 1.95 1026.87 1 sim_posterior &lt;- extract.samples(model_sim_partial) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% pivot_longer(everything(), names_to = &quot;param&quot;) %&gt;% filter(!(param %in% c(&quot;alpha_bar&quot;, &quot;sigma&quot;))) %&gt;% group_by(param) %&gt;% summarize(posterior = list(tibble(value = c(quantile(value, probs = c(.055, .25, .5, .75,.945)), mean(value)), label = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;, &quot;mean&quot;)))) %&gt;% ungroup() %&gt;% unnest(posterior) %&gt;% mutate(value = logistic(value)) %&gt;% pivot_wider(names_from = label, values_from = value ) %&gt;% mutate(pond_idx = str_remove( param, &quot;alpha_pond.&quot;) %&gt;% as.integer()) %&gt;% arrange(pond_idx) %&gt;% left_join(data_sim, .) %&gt;% mutate(error_no_pool = abs(p_no_pool - p_true), error_part_pool = abs(m - p_true)) error_means &lt;- sim_posterior %&gt;% dplyr::select(pond_size, starts_with(&quot;error&quot;)) %&gt;% pivot_longer(starts_with(&quot;error&quot;), names_to = &quot;model&quot;) %&gt;% group_by(pond_size, model) %&gt;% summarise(mean = mean(value)) %&gt;% ungroup() %&gt;% mutate(model = str_remove(model, &quot;error_&quot;)) sim_posterior %&gt;% dplyr::select(pond_idx, pond_size, starts_with(&quot;error&quot;)) %&gt;% pivot_longer(starts_with(&quot;error&quot;), names_to = &quot;model&quot;) %&gt;% mutate(model = str_remove(model, &quot;error_&quot;)) %&gt;% ggplot(aes(x = pond_idx, color = model)) + geom_hline(data = error_means, aes(yintercept = mean, color = model),linetype = 3) + geom_point(aes(y = value, fill = after_scale(clr_lighten(color))), shape = 21) + facet_wrap(pond_size ~ ., scale = &quot;free_x&quot;, labeller = label_both, nrow = 1) + scale_color_manual(values = c(clr0dd,clr_current), guide = &quot;none&quot;) + labs(y = &#39;absolute error&#39;) + theme(panel.border = element_rect(color = clr0d, fill = &quot;transparent&quot;)) 14.3 More than One Type of Cluster \\[ \\begin{array}{rclr} L_{i} &amp; \\sim &amp; Binomial(1, p_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{logit}(p_{i}) &amp; = &amp; \\alpha_{ACTOR[i]} + \\color{#B35136}{\\gamma_{BLOCK[i]}} + \\beta_{TREATMENT[i]} &amp; \\textrm{[linear model]}\\\\ \\beta_{j} &amp; \\sim &amp; Normal(0, 0.5) ~~ , \\textrm{for}~j= 1..4 &amp; \\textrm{[}~\\beta~\\textrm{prior]}\\\\ \\alpha_{j} &amp; \\sim &amp; Normal(\\bar{\\alpha}, \\sigma_{\\alpha}) ~~ , \\textrm{for}~j= 1..7 &amp; \\textrm{[}~\\alpha~\\textrm{prior]} \\\\ \\color{#B35136}{\\gamma_{j}} &amp; \\sim &amp; \\color{#B35136}{Normal(0, \\sigma_{\\gamma}) ~~ , \\textrm{for}~j= 1..6} &amp; \\textrm{[}~\\gamma~\\textrm{prior]} \\\\ \\bar{\\alpha} &amp; \\sim &amp; Normal(0, 1.5) &amp; \\textrm{[}~\\bar{\\alpha}~\\textrm{prior]} \\\\ \\sigma_{\\alpha} &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[}~\\sigma_{\\alpha}~\\textrm{prior]} \\\\ \\color{#B35136}{\\sigma_{\\gamma}} &amp; \\sim &amp; \\color{#B35136}{Exponential(1)} &amp; \\textrm{[}~\\sigma_{\\gamma}~\\textrm{prior]} \\end{array} \\] data(chimpanzees) data_chimp &lt;- chimpanzees %&gt;% as_tibble() %&gt;% mutate(treatment = 1L + prosoc_left + 2L * condition, side_idx = prosoc_left + 1L, # right 1, left 2 condition_idx = condition + 1L) # no partner 1, partner 2 rm(chimpanzees) data_chimp_list &lt;- data_chimp %&gt;% dplyr::select(pulled_left, actor, block, treatment) %&gt;% as.list() model_chimp_multicluster &lt;- ulam( flist = alist( pulled_left ~ dbinom( 1, p ), logit(p) &lt;- alpha[actor] + gamma[block] + beta[treatment], beta[treatment] ~ dnorm(0,.5), ## adaptive priors alpha[actor] ~ dnorm( alpha_bar, sigma_alpha ), gamma[block] ~ dnorm( 0, sigma_gamma ), ## hyper-priors alpha_bar ~ dnorm( 0, 1.5 ), sigma_alpha ~ dexp(1), sigma_gamma ~ dexp(1) ), data = data_chimp_list, cores = 4, chains = 4, log_lik = TRUE ) precis(model_chimp_multicluster, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 beta[1] -0.13 0.29 -0.61 0.32 239.03 1.02 beta[2] 0.36 0.29 -0.12 0.81 373.42 1.03 beta[3] -0.50 0.29 -0.99 -0.06 435.05 1.02 beta[4] 0.26 0.28 -0.22 0.72 453.49 1.02 alpha[1] -0.31 0.35 -0.89 0.21 374.70 1.02 alpha[2] 4.86 1.25 3.11 6.90 270.93 1.03 alpha[3] -0.59 0.37 -1.21 -0.05 73.39 1.05 alpha[4] -0.64 0.34 -1.17 -0.07 502.17 1.01 alpha[5] -0.34 0.34 -0.89 0.22 519.78 1.01 alpha[6] 0.57 0.36 0.07 1.18 163.07 1.03 alpha[7] 2.13 0.43 1.45 2.84 642.20 1.01 gamma[1] -0.15 0.22 -0.56 0.06 149.78 1.03 gamma[2] 0.03 0.17 -0.23 0.33 1100.80 1.00 gamma[3] 0.04 0.18 -0.21 0.35 942.22 1.01 gamma[4] 0.01 0.17 -0.27 0.30 991.59 1.00 gamma[5] -0.03 0.18 -0.33 0.22 1106.90 1.01 gamma[6] 0.10 0.18 -0.12 0.44 400.37 1.02 alpha_bar 0.77 0.84 -0.50 2.23 30.19 1.09 sigma_alpha 2.05 0.65 1.22 3.28 943.63 1.00 sigma_gamma 0.19 0.17 0.01 0.50 84.28 1.05 p1 &lt;- precis(model_chimp_multicluster, depth = 2) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% as_tibble() %&gt;% ggplot(aes(y = rowname)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_linerange(aes(xmin = `X5.5.`, xmax = `X94.5.`), color = clr0dd) + geom_point(aes(x = mean), shape = 21, color = clr0dd, fill = clr0) + labs(y = &quot;param&quot;, x = &quot;posterior (mean and 89 percentile)&quot;) p2 &lt;- extract.samples(model_chimp_multicluster) %&gt;% as_tibble() %&gt;% dplyr::select(sigma_alpha, sigma_gamma) %&gt;% pivot_longer(everything(), names_sep = &quot;_&quot;, names_to = c(&quot;prefix&quot;, &quot;param&quot;)) %&gt;% ggplot(aes(x = value, color = param)) + geom_density(adjust = .5, aes(fill = after_scale(clr_alpha(color)))) + scale_color_manual(values = c(clr0dd, clr_current)) + theme(legend.position = c(1,1), legend.justification = c(1,1)) p1 + p2 model_chimp_singlecluster &lt;- ulam( flist = alist( pulled_left ~ dbinom( 1, p ), logit(p) &lt;- alpha[actor] + beta[treatment], beta[treatment] ~ dnorm(0,.5), ## adaptive priors alpha[actor] ~ dnorm( alpha_bar, sigma_alpha ), ## hyper-priors alpha_bar ~ dnorm( 0, 1.5 ), sigma_alpha ~ dexp(1) ), data = data_chimp_list, cores = 4, chains = 4, log_lik = TRUE ) compare(model_chimp_singlecluster, model_chimp_multicluster) %&gt;% knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight model_chimp_singlecluster 530.68 19.18 0.00 NA 8.35 0.63 model_chimp_multicluster 531.72 19.11 1.04 1.58 10.31 0.37 Even more clusters (partial pooling on the treatment) model_chimp_treatment &lt;- ulam( flist = alist( pulled_left ~ dbinom( 1, p ), logit(p) &lt;- alpha[actor] + gamma[block] + beta[treatment], ## adaptive priors beta[treatment] ~ dnorm(0, sigma_beta), alpha[actor] ~ dnorm( alpha_bar, sigma_alpha ), gamma[block] ~ dnorm( 0, sigma_gamma ), ## hyper-priors alpha_bar ~ dnorm( 0, 1.5 ), sigma_beta ~ dexp(1), sigma_alpha ~ dexp(1), sigma_gamma ~ dexp(1) ), data = data_chimp_list, cores = 4, chains = 4, log_lik = TRUE ) library(ggdist) extract.samples(model_chimp_treatment) %&gt;% as_tibble() %&gt;% dplyr::select(starts_with(&quot;sigma&quot;)) %&gt;% pivot_longer(everything(), names_sep = &quot;_&quot;, names_to = c(&quot;prefix&quot;, &quot;sigma&quot;)) %&gt;% ggplot(aes(x = value, y = sigma, color = sigma)) + stat_slab(adjust = .75, size = .5, normalize = &quot;xy&quot;, aes(fill = after_scale(clr_alpha(color)))) + scale_color_manual(values = c(clr0dd, clr_dark, clr_current), guide = &quot;none&quot;) + coord_cartesian(ylim = c(.95, 4.1), expand = 0) + theme(legend.position = c(1,1), legend.justification = c(1,1)) coeftab(model_chimp_multicluster, model_chimp_treatment)@coefs %&gt;% as.data.frame() %&gt;% head() %&gt;% knit_precis() param model_chimp_multicluster model_chimp_treatment beta[1] -0.13 -0.11 beta[2] 0.36 0.37 beta[3] -0.50 -0.44 beta[4] 0.26 0.26 alpha[1] -0.31 -0.36 alpha[2] 4.86 4.74 14.4 Divergent Transitions and Non-Centered Priors Divergent transitions arise from steep gradients within the posterior that leads to issues in the exploration on the basis of a discretized physical simulation. It can help to reparameterizethe model - that is re-writing the model in a equivalent notation that allows an alternative numerical approximation. 14.4.1 The Devil’s Funnel Given the joint distribution of the two variable \\(v\\) and \\(x\\): Model with centered parameterization (definition of \\(x\\) is conditional on another parameter). \\[ \\begin{array}{rcl} v &amp; \\sim &amp; Normal(0, 3)\\\\ x &amp; \\sim &amp; Normal(0, \\textrm{exp}(v)) \\end{array} \\] model_devils_funnel_centered &lt;- ulam( flist = alist( v ~ normal( 0, 3 ), x ~ normal( 0, exp(v) ) ), data = list(N = 1), chains = 4 ) #&gt; Warning: 78 of 2000 (4.0%) transitions ended with a divergence. #&gt; This may indicate insufficient exploration of the posterior distribution. #&gt; Possible remedies include: #&gt; * Increasing adapt_delta closer to 1 (default is 0.8) #&gt; * Reparameterizing the model (e.g. using a non-centered parameterization) #&gt; * Using informative or weakly informative prior distributions #&gt; #&gt; 14 of 2000 (1.0%) transitions hit the maximum treedepth limit of 10 or 2^10-1 leapfrog steps. #&gt; Trajectories that are prematurely terminated due to this limit will result in slow exploration. #&gt; Increasing the max_treedepth limit can avoid this at the expense of more computation. #&gt; If increasing max_treedepth does not remove warnings, try to reparameterize the model. precis(model_devils_funnel_centered) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 v 1.63 1.57 -0.42 4.42 68.06 1.05 x 1.67 51.26 -23.70 27.89 348.14 1.01 \\(\\rightarrow\\) note the low n_eff (part of 2000 - 4 chains X 500 samples) and high Rhat4 😟` # define the parameter space parameter_space &lt;- seq(from = -4, to = 4, length.out = 200) # simulate p1 &lt;- crossing(v = parameter_space, x = parameter_space) %&gt;% mutate(likelihood_v = dnorm(v, mean = 0, sd = 3), likelihood_x = dnorm(x, mean = 0, sd = exp(v))) %&gt;% mutate(joint_likelihood = likelihood_v * likelihood_x) %&gt;% # plot! ggplot(aes(x = x, y = v, color = joint_likelihood)) + ggisoband::geom_isobands(aes(z = joint_likelihood, fill = after_scale(clr_alpha(color,.1))), bins = 51, size = .4) + scale_color_gradientn(colours = c(clr0, clr_dark, clr_current)) + # geom_raster(interpolate = TRUE) + # scale_color_viridis_c(option = &quot;B&quot;) + labs(subtitle = &quot;Centered parameterization&quot;) + theme(legend.position = &quot;none&quot;) The same model with a non-centered parameterization (the embedded parameter is moved out of the definition of \\(x\\)) - this is similar to standardizing a variable. \\[ \\begin{array}{rcl} v &amp; \\sim &amp; Normal(0, 3) \\\\ z &amp; \\sim &amp; Normal(0, 1) \\\\ x &amp; = &amp; z~\\textrm{exp}(v) \\end{array} \\] p2 &lt;- crossing(v = parameter_space, z = parameter_space / 2) %&gt;% mutate(likelihood_v = dnorm(v, mean = 0, sd = 3), likelihood_z = dnorm(z, mean = 0, sd = 1)) %&gt;% mutate(joint_likelihood = likelihood_v * likelihood_z) %&gt;% # plot! ggplot(aes(x = z, y = v, color = joint_likelihood)) + ggisoband::geom_isobands(aes(z = joint_likelihood, fill = after_scale(clr_alpha(color,.1))), bins = 15, size = .4) + scale_color_gradientn(colours = c(clr0, clr_dark, clr_current)) + # scale_fill_viridis_c(option = &quot;B&quot;) + labs(subtitle = &quot;Non-centered parameterization&quot;) + theme(legend.position = &quot;none&quot;) p1 + p2 model_devils_funnel_non_centered &lt;- ulam( flist = alist( v ~ normal( 0, 3 ), z ~ normal( 0, 1), gq&gt; real[1]:x &lt;&lt;- z * exp(v) ), data = list(N = 1), chains = 4 ) precis(model_devils_funnel_non_centered) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 v -0.01 3.00 -4.88 4.59 1140.87 1 z 0.04 0.98 -1.55 1.59 1412.03 1 x 14.83 414.92 -21.11 33.61 1227.91 1 extract.samples(model_devils_funnel_non_centered) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% ggplot(aes(x = x, y = v)) + ggpointdensity::geom_pointdensity(size = .75)+ scale_color_gradientn(colours = c(clr0, clr_dark, clr_current), guide = &#39;none&#39;) + coord_cartesian(xlim = c(-4, 4), ylim = c(-4, 4)) + labs(subtitle = &quot;posterior sampled with HMC&quot;) 14.4.2 Non-Centered Chimpanzees The quick and easy way to help with divergent transitions is to increase the stringency of acceptance by selecting an adapt_delta that is closer to 1: set.seed(42) model_chimp_multicluster_fine &lt;- ulam( model_chimp_multicluster, chains = 4, cores = 4, control = list(adapt_delta = 0.99) ) divergent(model_chimp_multicluster) #&gt; [1] 51 divergent(model_chimp_multicluster_fine) #&gt; [1] 0 precis(model_chimp_multicluster_fine) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_bar 0.61 0.72 -0.49 1.79 1053.49 1.00 sigma_alpha 2.03 0.67 1.19 3.23 852.96 1.00 sigma_gamma 0.22 0.17 0.03 0.52 211.50 1.02 This still produces bad (low n_eff) though. The more serious approach is to re-parameterize the model. For the current model, we need to transform two adaptive priors to move their paramters into a linear model: \\[ \\begin{array}{rclr} L_{i} &amp; \\sim &amp; Binomial(1, p_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{logit}(p_{i}) &amp; = &amp; \\alpha_{ACTOR[i]} + \\gamma_{BLOCK[i]} + \\beta_{TREATMENT[i]} &amp; \\textrm{[linear model]}\\\\ \\beta_{j} &amp; \\sim &amp; Normal(0, 0.5) ~~ , \\textrm{for}~j= 1..4 &amp; \\textrm{[}~\\beta~\\textrm{prior]}\\\\ \\color{#B35136}{\\alpha_{j}} &amp; \\sim &amp; \\color{#B35136}{Normal(\\bar{\\alpha}, \\sigma_{\\alpha}) ~~ , \\textrm{for}~j= 1..7} &amp; \\textrm{[Intercepts for actors]} \\\\ \\color{#B35136}{\\gamma_{j}} &amp; \\sim &amp; \\color{#B35136}{Normal(0, \\sigma_{\\gamma}) ~~ , \\textrm{for}~j= 1..6} &amp; \\textrm{[Intercepts for blocks]} \\\\ \\bar{\\alpha} &amp; \\sim &amp; Normal(0, 1.5) &amp; \\textrm{[}~\\bar{\\alpha}~\\textrm{prior]} \\\\ \\sigma_{\\alpha} &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[}~\\sigma_{\\alpha}~\\textrm{prior]} \\\\ \\sigma_{\\gamma} &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[}~\\sigma_{\\gamma}~\\textrm{prior]} \\end{array} \\] Our task is to reconfigure remove three embedded parameters (\\(\\bar{\\alpha}\\), \\(\\sigma_{\\alpha}\\) and \\(\\sigma_{\\gamma}\\)). To do this we define new variables with \\(Normal\\) distributions. To reconstruct the original values by reversing the transformations. The reparamterized model then reads \\[ \\begin{array}{rclr} L_{i} &amp; \\sim &amp; Binomial(1, p_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{logit}(p_{i}) &amp; = &amp; \\color{#B35136}{\\underbrace{\\bar{\\alpha} + z_{ACTOR[i]} \\sigma_{\\alpha}}_{\\alpha_{ACTOR[i]}}} + \\color{#B35136}{\\underbrace{x_{BLOCK[i]} \\sigma_{\\gamma}}_{\\gamma_{BLOCK[i]}}}+ \\beta_{TREATMENT[i]} &amp; \\textrm{[linear model]}\\\\ \\beta_{j} &amp; \\sim &amp; Normal(0, 0.5) ~~ , \\textrm{for}~j= 1..4 &amp; \\textrm{[}~\\beta~\\textrm{prior]}\\\\ \\color{#B35136}{z_{j}} &amp; \\sim &amp; \\color{#B35136}{Normal(0, 1)} &amp; \\textrm{[standardized actor intercepts]} \\\\ \\color{#B35136}{x_{j}} &amp; \\sim &amp; \\color{#B35136}{Normal(0, 1)} &amp; \\textrm{[standardized block intercepts]} \\\\ \\bar{\\alpha} &amp; \\sim &amp; Normal(0, 1.5) &amp; \\textrm{[}~\\bar{\\alpha}~\\textrm{prior]} \\\\ \\sigma_{\\alpha} &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[}~\\sigma_{\\alpha}~\\textrm{prior]} \\\\ \\sigma_{\\gamma} &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[}~\\sigma_{\\gamma}~\\textrm{prior]} \\end{array} \\] model_chimp_multicluster_reparam &lt;- ulam( flist = alist( pulled_left ~ dbinom( 1, p ), logit(p) &lt;- alpha_bar + z[actor] * sigma_alpha + x[block] * sigma_gamma + beta[treatment], beta[treatment] ~ dnorm(0,.5), ## adaptive priors z[actor] ~ dnorm( 0, 1 ), x[block] ~ dnorm( 0, 1 ), ## hyper-priors alpha_bar ~ dnorm( 0, 1.5 ), sigma_alpha ~ dexp(1), sigma_gamma ~ dexp(1), gq&gt; vector[actor]:alpha &lt;&lt;- alpha_bar + z * sigma_alpha, gq&gt; vector[block]:gamma &lt;&lt;- x * sigma_gamma ), data = data_chimp_list, cores = 4, chains = 4, log_lik = TRUE ) get_n_eff &lt;- function(model, model_lab){ precis(model, depth = 2) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% as_tibble() %&gt;% dplyr::select(rowname, n_eff) %&gt;% set_names(nm = c(&quot;param&quot;, str_c(&quot;n_eff_&quot;, model_lab))) } get_n_eff(model_chimp_multicluster, &quot;centered&quot;) %&gt;% left_join(get_n_eff(model_chimp_multicluster_reparam, &quot;non-centered&quot;)) %&gt;% ggplot(aes(x = n_eff_centered, y = `n_eff_non-centered`)) + geom_abline(slope = 1, intercept = 0, color = clr_dark, linetype = 3) + geom_point(shape = 21, color = clr0dd, fill = clr0) + coord_cartesian(xlim = c(0, 2250), ylim = c(0, 2250)) Reparameterization does not need to use a gaussian distribution. For example the an exponential distribution would look like this: \\[ \\begin{array}{rcl} x &amp; = &amp; x\\lambda\\\\ z &amp; \\sim &amp; Exponential(1) \\end{array} \\] which replaces \\(x \\sim Exponential(\\lambda)\\). 14.5 Multilevel Posterior Predictions 14.5.1 Posterior Prediction for Same Clusters Using link() (quick approach): chimp &lt;- 2L data_predict &lt;- list( actor = rep(chimp, 4), treatment = 1:4, block = rep(1L, 4) ) link(model_chimp_multicluster, data = data_predict) %&gt;% as_tibble() %&gt;% set_names(nm = str_c(&quot;treatment&quot;, 1:4)) %&gt;% summarise(across(everything(), function(x){list(tibble(mean = mean(x), pi_lower = PI(x)[1], pi_upper = PI(x)[2]))})) %&gt;% pivot_longer(everything()) %&gt;% unnest(value) #&gt; # A tibble: 4 × 4 #&gt; name mean pi_lower pi_upper #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 treatment1 0.981 0.944 0.999 #&gt; 2 treatment2 0.989 0.966 0.999 #&gt; 3 treatment3 0.974 0.927 0.998 #&gt; 4 treatment4 0.987 0.962 0.999 Using extract.samples() (generic approach) chimp_posterior &lt;- extract.samples(model_chimp_multicluster) %&gt;% as.data.frame() %&gt;% as_tibble() chimp_posterior %&gt;% ggplot(aes(x = alpha.5)) + geom_density(adjust = .5, color = clr0dd, fill = fll0) chimp_posterior %&gt;% mutate(across(starts_with(&quot;beta&quot;), .fns = function(x){inv_logit(alpha.2 + gamma.1 + x)}, .names = &quot;{.col}_raw&quot;)) %&gt;% dplyr::select(ends_with(&quot;raw&quot;))%&gt;% summarise(across(everything(), function(x){list(tibble(mean = mean(x), pi_lower = PI(x)[1], pi_upper = PI(x)[2]))})) %&gt;% pivot_longer(everything()) %&gt;% unnest(value) #&gt; # A tibble: 4 × 4 #&gt; name mean pi_lower pi_upper #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 beta.1_raw 0.981 0.944 0.999 #&gt; 2 beta.2_raw 0.989 0.966 0.999 #&gt; 3 beta.3_raw 0.974 0.927 0.998 #&gt; 4 beta.4_raw 0.987 0.962 0.999 14.5.2 Posterior Predictions for New Clusters The average chimp p1 &lt;- chimp_posterior %&gt;% mutate(across(starts_with(&quot;beta&quot;), .fns = function(x){inv_logit(alpha_bar + x)}, .names = &quot;{.col}_raw&quot;)) %&gt;% dplyr::select(ends_with(&quot;raw&quot;))%&gt;% summarise(across(everything(), function(x){ list(tibble(mean = mean(x), pi_lower = PI(x)[1], pi_upper = PI(x)[2])) })) %&gt;% pivot_longer(everything()) %&gt;% mutate(treatment = str_extract(name, pattern = &quot;[1-4]&quot;) %&gt;% as.integer()) %&gt;% unnest(value) %&gt;% ggplot(aes(x = treatment)) + geom_smooth(aes(ymin = pi_lower, y = mean, ymax = pi_upper), stat = &#39;identity&#39;, color = clr0dd, fill = fll0dd, size = .5) + labs(y = &quot;prop_pulled_left&quot;, x = &quot;treatment&quot;, subtitle = &quot;average_actor&quot;) chimp_sim &lt;- chimp_posterior %&gt;% mutate(sim = rnorm(n = n(), mean = alpha_bar, sd = sigma_alpha)) %&gt;% mutate(across(starts_with(&quot;beta&quot;), .fns = function(x){inv_logit(sim + x)}, .names = &quot;{.col}_sim&quot;)) p2 &lt;- chimp_sim %&gt;% dplyr::select(ends_with(&quot;_sim&quot;)) %&gt;% summarise(across(everything(), function(x){ list(tibble(mean = mean(x), pi_lower = PI(x)[1], pi_upper = PI(x)[2])) })) %&gt;% pivot_longer(everything()) %&gt;% mutate(treatment = str_extract(name, pattern = &quot;[1-4]&quot;) %&gt;% as.integer()) %&gt;% unnest(value) %&gt;% ggplot(aes(x = treatment)) + geom_smooth(aes(ymin = pi_lower, y = mean, ymax = pi_upper), stat = &#39;identity&#39;, color = clr0dd, fill = fll0dd, size = .5) + labs(y = &quot;prop_pulled_left&quot;, x = &quot;treatment&quot;, subtitle = &quot;marginal_of_actor&quot;) + coord_cartesian(ylim = 0:1) + theme(panel.grid.minor.x = element_blank()) p3 &lt;- chimp_sim %&gt;% dplyr::select(ends_with(&quot;_sim&quot;)) %&gt;% mutate(.draw = row_number()) %&gt;% filter(row_number() &lt; 101) %&gt;% pivot_longer(ends_with(&quot;_sim&quot;)) %&gt;% mutate(treatment = str_extract(name, pattern = &quot;[1-4]&quot;) %&gt;% as.integer()) %&gt;% ggplot(aes(x = treatment, y = value, group = .draw)) + geom_line(color = clr_alpha(clr_dark, .4))+ labs(y = &quot;prop_pulled_left&quot;, x = &quot;treatment&quot;, subtitle = &quot;simulated_actors&quot;) p1 + p2 + p3 &amp; scale_x_continuous(breaks = 1:4, labels = c(&quot;R|N&quot;, &quot;L|N&quot;, &quot;R|P&quot;, &quot;L|P&quot;)) &amp; coord_cartesian(ylim = 0:1) &amp; theme(panel.grid.minor.x = element_blank()) library(rlang) chapter13_models &lt;- env( ) write_rds(chapter13_models, &quot;envs/chapter13_models.rds&quot;) 14.6 Homework E1 \\(\\alpha_{TANK} \\sim \\textrm{Normal}(0,1)\\) \\(\\cancel{\\alpha_{TANK} \\sim \\textrm{Normal}(0,2)}\\) E2 \\[ \\begin{array}{rclr} y_{i} &amp; \\sim &amp; \\textrm{Binomial}(1, p_{i}) &amp; \\\\ \\textrm{logit}(p_{i}) &amp; = &amp; \\alpha_{GROUP[i]} + \\beta x_{i} &amp;\\\\ \\alpha_{GROUP} &amp; \\sim &amp; \\textrm{Normal}(\\bar{\\alpha}, \\sigma_{\\alpha}) &amp;\\\\ \\beta &amp; \\sim &amp; \\textrm{Normal}(0, 0.5) &amp;\\\\ \\bar{\\alpha} &amp; \\sim &amp; \\textrm{Normal}(0, 1.5) &amp; \\textrm{[hyperprior]}\\\\ \\sigma_{\\alpha} &amp; \\sim &amp; \\textrm{Exponential}(1) &amp; \\textrm{[hyperprior]} \\end{array} \\] E3 \\[ \\begin{array}{rclr} y_{i} &amp; \\sim &amp; \\textrm{Normal}(\\mu_{i}, \\sigma) &amp; \\\\ \\textrm{logit}(\\mu_{i}) &amp; = &amp; \\alpha_{GROUP[i]} + \\beta x_{i} &amp;\\\\ \\alpha_{GROUP} &amp; \\sim &amp; \\textrm{Normal}(\\bar{\\alpha }, \\sigma_{\\alpha}) &amp;\\\\ \\beta &amp; \\sim &amp; \\textrm{Normal}(0, 1) &amp;\\\\ \\sigma &amp; \\sim &amp; \\textrm{Exponential}(1) &amp; \\\\ \\bar{\\alpha} &amp; \\sim &amp; \\textrm{Normal}(0, 5) &amp; \\textrm{[hyperprior]}\\\\ \\sigma_{\\alpha} &amp; \\sim &amp; \\textrm{Exponential}(1) &amp; \\textrm{[hyperprior]} \\end{array} \\] E4 \\[ \\begin{array}{rclr} y_{i} &amp; \\sim &amp; \\textrm{Poisson}(\\lambda_{i}) &amp; \\\\ \\textrm{log}(\\lambda_{i}) &amp; = &amp; \\alpha_{GROUP[i]} + \\beta x_{i} &amp;\\\\ \\alpha_{GROUP} &amp; \\sim &amp; \\textrm{Normal}(\\bar{\\alpha }, \\sigma_{\\alpha}) &amp;\\\\ \\beta &amp; \\sim &amp; \\textrm{Normal}(0, 1) &amp;\\\\ \\bar{\\alpha} &amp; \\sim &amp; \\textrm{Normal}(3, .5) &amp; \\textrm{[hyperprior]}\\\\ \\sigma_{\\alpha} &amp; \\sim &amp; \\textrm{Exponential}(1) &amp; \\textrm{[hyperprior]} \\end{array} \\] E5 \\[ \\begin{array}{rclr} y_{i} &amp; \\sim &amp; \\textrm{Poisson}(\\lambda_{i}) &amp; \\\\ \\textrm{log}(\\lambda_{i}) &amp; = &amp; \\alpha_{GROUP[i]} + \\gamma_{BLOCK[i]} + \\beta x_{i} &amp;\\\\ \\alpha_{GROUP} &amp; \\sim &amp; \\textrm{Normal}(\\bar{\\alpha}, \\sigma_{\\alpha}) &amp;\\\\ \\gamma_{BLOCK} &amp; \\sim &amp; \\textrm{Normal}(0, \\sigma_{\\gamma}) &amp; \\textrm{[ 0 to avoid the &#39;&#39;legs issue&#39;&#39;]}\\\\ \\beta &amp; \\sim &amp; \\textrm{Normal}(0, 1) &amp;\\\\ \\bar{\\alpha} &amp; \\sim &amp; \\textrm{Normal}(3, .5) &amp; \\textrm{[hyperprior]}\\\\ \\sigma_{\\alpha} &amp; \\sim &amp; \\textrm{Exponential}(1) &amp; \\textrm{[hyperprior]}\\\\ \\sigma_{\\gamma} &amp; \\sim &amp; \\textrm{Exponential}(1) &amp; \\textrm{[hyperprior]} \\end{array} \\] M1 Now also include predation and size as clusters within the model. We will be using the non-centered model notation: model_frog_tank &lt;- ulam( flist = alist( surv ~ dbinom( density, p ), logit(p) &lt;- alpha_bar + z_alpha[tank] * sigma_alpha, z_alpha[tank] ~ dnorm(0, 1), alpha_bar ~ dnorm(0, 1.5), sigma_alpha ~ dexp(1), gq&gt; vector[tank]:alpha &lt;&lt;- alpha_bar + z_alpha * sigma_alpha ), data = data_frogs, cores = 4, chains = 4, log_lik = TRUE ) model_frog_pred &lt;- ulam( flist = alist( surv ~ dbinom( density, p ), logit(p) &lt;- alpha_bar + z_alpha[tank] * sigma_alpha + z_gamma[pred] * sigma_gamma, z_alpha[tank] ~ dnorm(0, 1), z_gamma[pred] ~ dnorm(0, 1), alpha_bar ~ dnorm(0, 1.5), c(sigma_alpha, sigma_gamma) ~ dexp(1), gq&gt; vector[tank]:alpha &lt;&lt;- alpha_bar + z_alpha * sigma_alpha, gq&gt; vector[pred]:gamma &lt;&lt;- z_gamma * sigma_gamma ), data = data_frogs, cores = 4, chains = 4, log_lik = TRUE ) model_frog_size &lt;- ulam( flist = alist( surv ~ dbinom( density, p ), logit(p) &lt;- alpha_bar + z_alpha[tank] * sigma_alpha + z_delta[size] * sigma_delta, z_alpha[tank] ~ dnorm(0, 1), z_delta[size] ~ dnorm(0, 1), alpha_bar ~ dnorm(0, 1.5), c(sigma_alpha, sigma_delta) ~ dexp(1), gq&gt; vector[tank]:alpha &lt;&lt;- alpha_bar + z_alpha * sigma_alpha, gq&gt; vector[size]:delta &lt;&lt;- z_delta * sigma_delta ), data = data_frogs, cores = 4, chains = 4, log_lik = TRUE ) model_frog_combined &lt;- ulam( flist = alist( surv ~ dbinom( density, p ), logit(p) &lt;- alpha_bar + z_alpha[tank] * sigma_alpha + z_gamma[pred] * sigma_gamma + z_delta[size] * sigma_delta, z_alpha[tank] ~ dnorm(0, 1), z_gamma[pred] ~ dnorm(0, 1), z_delta[size] ~ dnorm(0, 1), alpha_bar ~ dnorm(0, 1.5), c(sigma_alpha, sigma_gamma, sigma_delta) ~ dexp(1), gq&gt; vector[tank]:alpha &lt;&lt;- alpha_bar + z_alpha * sigma_alpha, gq&gt; vector[pred]:gamma &lt;&lt;- z_gamma * sigma_gamma, gq&gt; vector[size]:delta &lt;&lt;- z_delta * sigma_delta ), data = data_frogs, cores = 4, chains = 4, log_lik = TRUE ) data_frogs_ind &lt;- data_frogs %&gt;% mutate(pred_ind = abs(1L - as.integer(pred))) model_frog_interact &lt;- ulam( flist = alist( surv ~ dbinom( density, p ), logit(p) &lt;- alpha_bar + z_alpha[tank] * sigma_alpha + z_gamma[pred] * sigma_gamma + z_delta[size] * sigma_delta + z_epsilon[size] * pred_ind * sigma_epsilon, z_alpha[tank] ~ dnorm(0, 1), z_gamma[pred] ~ dnorm(0, 1), z_delta[size] ~ dnorm(0, 1), z_epsilon[size] ~ dnorm(0, 1), alpha_bar ~ dnorm(0, 1.5), c(sigma_alpha, sigma_gamma, sigma_delta, sigma_epsilon) ~ dexp(1), gq&gt; vector[tank]:alpha &lt;&lt;- alpha_bar + z_alpha * sigma_alpha, gq&gt; vector[pred]:gamma &lt;&lt;- z_gamma * sigma_gamma, gq&gt; vector[size]:delta &lt;&lt;- z_delta * sigma_delta, gq&gt; vector[size]:epsilon &lt;&lt;- z_epsilon * sigma_epsilon ), data = data_frogs_ind, cores = 4, chains = 4, log_lik = TRUE ) get_sigma_alpha &lt;- function(model, mod_lab){ model %&gt;% precis() %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% filter(rowname == &quot;sigma_alpha&quot;) %&gt;% dplyr::select(-rowname) %&gt;% pivot_longer(everything(), names_to = &quot;param&quot;) %&gt;% mutate(model = mod_lab) } list(model_frog_tank, model_frog_pred, model_frog_size, model_frog_combined, model_frog_interact) %&gt;% map2_dfr(.y = c(&quot;tank&quot;,&quot;pred&quot;,&quot;size&quot;,&quot;combine&quot;, &quot;interact&quot;), get_sigma_alpha) %&gt;% pivot_wider(names_from = &quot;param&quot;, values_from = &quot;value&quot;) %&gt;% ggplot(aes(y = model)) + geom_vline(xintercept = 0, linetype = 3, color = clr_dark) + geom_linerange(aes(xmin = `X5.5.`, xmax = `X94.5.`), color = clr0dd) + geom_point(aes(x = mean), color = clr0dd, fill = clr0, shape = 21, size = 2) + coord_cartesian(xlim = c(0, 2)) + labs(subtitle = &quot;estimates of sigma_alpha&quot;, x = &quot;sigma_alpha&quot;) M2 compare(model_frog_tank, model_frog_pred, model_frog_size, model_frog_combined, model_frog_interact) %&gt;% knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight model_frog_combined 199.97 8.98 0.00 NA 19.31 0.29 model_frog_interact 200.08 9.32 0.12 1.95 19.11 0.27 model_frog_tank 200.33 7.18 0.36 5.97 20.99 0.24 model_frog_pred 202.10 9.61 2.14 1.76 20.43 0.10 model_frog_size 202.28 7.50 2.31 5.98 21.83 0.09 p1 &lt;- link(model_frog_pred) %&gt;% as_tibble() %&gt;% summarise(across(everything(), .fns = function(x){list(tibble(val = c(quantile(x, probs = c(.055,.25,.5,.75,.945)), mean(x)), label = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;, &quot;mean&quot;)))})) %&gt;% pivot_longer(everything()) %&gt;% bind_cols(data_frogs,.) %&gt;% unnest(value) %&gt;% pivot_wider(names_from = label, values_from = val) %&gt;% ggplot(aes(x = tank)) + labs(subtitle = &quot;predation&quot;) p2 &lt;- link(model_frog_interact) %&gt;% as_tibble() %&gt;% summarise(across(everything(), .fns = function(x){list(tibble(val = c(quantile(x, probs = c(.055,.25,.5,.75,.945)), mean(x)), label = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;, &quot;mean&quot;)))})) %&gt;% pivot_longer(everything()) %&gt;% bind_cols(data_frogs,.) %&gt;% unnest(value) %&gt;% pivot_wider(names_from = label, values_from = val) %&gt;% ggplot(aes(x = tank)) + labs(subtitle = &quot;interaction model&quot;) p1 + p2 &amp; geom_linerange(aes(ymin = ll, ymax = hh), color = clr0dd) &amp; geom_point(aes(y = m), color = clr0dd, fill = clr0, shape = 21, size = 2) &amp; geom_point(aes(y = propsurv), color = clr_current, shape = 1, size = 4) # prep_pred &lt;- link(model_frog_pred) %&gt;% # as_tibble() %&gt;% # summarise(across(everything(), # .fns = function(x){list(tibble(val_p = c(quantile(x, # probs = c(.055,.25,.5,.75,.945)), # mean(x)), # label = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;, &quot;mean&quot;) %&gt;% # str_c(.,&quot;_pred&quot;)))})) %&gt;% # pivot_longer(everything(), # values_to = &quot;t_pred&quot;) # # prep_interct &lt;- link(model_frog_interact) %&gt;% # as_tibble() %&gt;% # summarise(across(everything(), # .fns = function(x){list(tibble(val_i = c(quantile(x, # probs = c(.055,.25,.5,.75,.945)), # mean(x)), # label = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;, &quot;mean&quot;) %&gt;% # str_c(.,&quot;_interact&quot;)))})) %&gt;% # pivot_longer(everything(), # values_to = &quot;t_interact&quot;) # # prep_size &lt;- link(model_frog_size) %&gt;% # as_tibble() %&gt;% # summarise(across(everything(), # .fns = function(x){list(tibble(val_s = c(quantile(x, # probs = c(.055,.25,.5,.75,.945)), # mean(x)), # label = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;, &quot;mean&quot;) %&gt;% # str_c(.,&quot;_size&quot;)))})) %&gt;% # pivot_longer(everything(), # values_to = &quot;t_size&quot;) # # posterior_frogs_tripple &lt;- list(prep_pred, # prep_size, # prep_interct) %&gt;% # reduce(left_join) %&gt;% # bind_cols(data_frogs,. ) %&gt;% # unnest(t_pred) %&gt;% # pivot_wider(names_from = label, values_from = val_p) %&gt;% # unnest(t_size) %&gt;% # pivot_wider(names_from = label, values_from = val_s) %&gt;% # unnest(t_interact) %&gt;% # pivot_wider(names_from = label, values_from = val_i) # # p1 &lt;- posterior_frogs_tripple %&gt;% # ggplot(aes(x = m_pred, y = m_interact)) + # geom_linerange(aes(xmin = ll_pred, xmax = hh_pred), # color = clr0dd)+ # geom_linerange(aes(ymin = ll_interact, ymax = hh_interact), # color = clr0dd) + # geom_point(color = clr0dd, fill = clr0, # shape = 21, size = 2) # # p2 &lt;- posterior_frogs_tripple %&gt;% # ggplot(aes(x = m_pred, y = m_size)) + # geom_linerange(aes(xmin = ll_pred, xmax = hh_pred), # color = clr0dd)+ # geom_linerange(aes(ymin = ll_size, ymax = hh_size), # color = clr0dd) + # geom_point(color = clr0dd, fill = clr0, # shape = 21, size = 2) # # p1 + p2 &amp; # geom_abline(slope = 1, intercept = 0, linetype = 3, color = clr_current) plot_posterior &lt;- function(model){ data_prep &lt;- extract.samples(model) %&gt;% data.frame() %&gt;% as_tibble() %&gt;% dplyr::select(c(alpha_bar, starts_with(&quot;sigma&quot;))) data_prep %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(y = name, x = value)) + geom_vline(xintercept = 0, linetype = 3, color = clr_dark)+ stat_slab(adjust = .75, size = .5, normalize = &quot;xy&quot;, color = clr0dd, fill = fll0) + coord_cartesian(ylim = c(.9, ncol(data_prep)+1.1), xlim = c(-2.5, 5), expand = 0)+ labs(subtitle = deparse(substitute(model)) %&gt;% str_remove(&quot;model_frog_&quot;), y = NULL, x = NULL) } ((plot_posterior(model_frog_tank) + theme(axis.text.x = element_blank())) / (plot_posterior(model_frog_pred) + theme(axis.text.x = element_blank())) / plot_posterior(model_frog_size)) | ((plot_posterior(model_frog_combined) + theme(axis.text.x = element_blank())) / plot_posterior(model_frog_interact)) M3 model_frog_cauchy &lt;- ulam( flist = alist( surv ~ dbinom( density, p ), logit(p) &lt;- alpha_bar + z_alpha[tank] * sigma_alpha, z_alpha[tank] ~ dcauchy(0, 1), alpha_bar ~ dnorm(0, 1.5), sigma_alpha ~ dexp(1), gq&gt; vector[tank]:alpha &lt;&lt;- alpha_bar + z_alpha * sigma_alpha ), data = data_frogs, control = list(adapt_delta = 0.99), cores = 4, chains = 4, max_treedepth = 15, log_lik = TRUE ) Cauchy fit precis(model_frog_cauchy, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 z_alpha[1] 0.54 0.90 -0.71 2.10 1276.94 1.00 z_alpha[2] 3.52 5.91 -0.05 11.56 135.29 1.03 z_alpha[3] -0.38 0.65 -1.39 0.63 1210.19 1.00 z_alpha[4] 4.17 6.65 0.05 16.75 200.48 1.01 z_alpha[5] 0.52 0.86 -0.63 1.94 1198.49 1.00 z_alpha[6] 0.53 0.91 -0.66 2.07 933.13 1.00 z_alpha[7] 11.30 48.65 0.00 17.05 49.16 1.08 z_alpha[8] 0.60 1.09 -0.63 2.35 339.69 1.01 z_alpha[9] -1.67 0.95 -3.38 -0.40 243.20 1.02 z_alpha[10] 0.53 0.91 -0.60 1.93 843.89 1.00 z_alpha[11] -0.36 0.72 -1.50 0.69 751.96 1.00 z_alpha[12] -0.78 0.73 -2.05 0.27 589.85 1.01 z_alpha[13] -0.41 0.70 -1.58 0.65 1080.07 1.00 z_alpha[14] -1.21 0.81 -2.59 -0.08 380.78 1.01 z_alpha[15] 0.56 0.94 -0.69 2.13 954.72 1.00 z_alpha[16] 0.52 0.88 -0.63 1.99 1230.22 1.00 z_alpha[17] 1.40 1.02 0.09 3.12 1045.03 1.00 z_alpha[18] 0.80 0.75 -0.17 2.10 811.23 1.00 z_alpha[19] 0.45 0.63 -0.45 1.51 919.59 1.00 z_alpha[20] 8.42 13.35 0.76 32.90 193.79 1.02 z_alpha[21] 0.81 0.77 -0.23 2.10 817.38 1.01 z_alpha[22] 0.81 0.76 -0.21 2.08 1144.96 1.00 z_alpha[23] 0.80 0.75 -0.19 2.04 1139.41 1.00 z_alpha[24] 0.16 0.57 -0.69 1.10 819.55 1.00 z_alpha[25] -2.75 1.01 -4.54 -1.46 126.32 1.04 z_alpha[26] -1.36 0.69 -2.55 -0.44 175.42 1.03 z_alpha[27] -3.30 1.18 -5.37 -1.81 122.51 1.05 z_alpha[28] -2.10 0.85 -3.64 -0.98 131.11 1.04 z_alpha[29] -1.36 0.71 -2.59 -0.44 169.72 1.03 z_alpha[30] -0.04 0.53 -0.87 0.85 1098.81 1.00 z_alpha[31] -2.30 0.92 -3.83 -1.16 119.19 1.04 z_alpha[32] -1.92 0.80 -3.35 -0.86 134.78 1.04 z_alpha[33] 1.79 1.10 0.42 3.83 894.87 1.00 z_alpha[34] 1.12 0.79 0.10 2.46 495.15 1.01 z_alpha[35] 1.15 0.79 0.12 2.56 907.07 1.00 z_alpha[36] 0.50 0.55 -0.31 1.38 822.24 1.00 z_alpha[37] 0.51 0.58 -0.30 1.47 981.42 1.00 z_alpha[38] 11.39 26.01 1.15 41.99 133.00 1.02 z_alpha[39] 1.13 0.78 0.09 2.47 1276.52 1.00 z_alpha[40] 0.78 0.68 -0.12 1.92 994.64 1.00 z_alpha[41] -3.77 1.27 -5.86 -2.14 121.82 1.04 z_alpha[42] -2.23 0.85 -3.71 -1.17 123.56 1.04 z_alpha[43] -2.09 0.81 -3.46 -1.06 123.75 1.04 z_alpha[44] -1.95 0.77 -3.30 -0.96 126.32 1.04 z_alpha[45] -0.90 0.57 -1.86 -0.09 221.86 1.02 z_alpha[46] -2.23 0.86 -3.69 -1.15 118.63 1.05 z_alpha[47] 0.49 0.55 -0.33 1.43 1069.80 1.00 z_alpha[48] -1.56 0.70 -2.77 -0.67 138.37 1.03 alpha_bar 1.49 0.29 1.02 1.95 224.20 1.02 sigma_alpha 1.00 0.25 0.64 1.42 101.59 1.06 alpha[1] 2.04 0.82 0.91 3.50 1431.46 1.00 alpha[2] 4.92 5.28 1.46 13.44 180.91 1.02 alpha[3] 1.14 0.59 0.22 2.09 1227.47 1.00 alpha[4] 5.63 6.96 1.56 16.74 205.68 1.02 alpha[5] 2.01 0.80 0.94 3.41 1577.47 1.00 alpha[6] 2.02 0.84 0.87 3.52 1267.89 1.00 alpha[7] 12.04 42.78 1.50 18.39 49.18 1.08 alpha[8] 2.07 0.92 0.94 3.68 584.52 1.00 alpha[9] -0.06 0.70 -1.19 1.06 2418.94 1.00 alpha[10] 2.02 0.84 0.94 3.42 1005.65 1.00 alpha[11] 1.16 0.64 0.13 2.17 1579.65 1.00 alpha[12] 0.77 0.62 -0.21 1.74 2058.63 1.00 alpha[13] 1.11 0.64 0.10 2.10 1736.01 1.00 alpha[14] 0.36 0.64 -0.67 1.36 2475.54 1.00 alpha[15] 2.04 0.85 0.89 3.53 1118.71 1.00 alpha[16] 2.02 0.82 0.89 3.38 1398.31 1.00 alpha[17] 2.86 0.92 1.65 4.52 1353.29 1.00 alpha[18] 2.27 0.65 1.39 3.43 1537.40 1.00 alpha[19] 1.94 0.56 1.10 2.91 2591.07 1.00 alpha[20] 9.77 13.46 2.29 32.70 194.36 1.02 alpha[21] 2.28 0.65 1.38 3.43 1402.91 1.00 alpha[22] 2.29 0.67 1.34 3.45 1525.88 1.00 alpha[23] 2.27 0.67 1.38 3.41 1417.97 1.00 alpha[24] 1.66 0.49 0.92 2.49 2208.92 1.00 alpha[25] -1.06 0.48 -1.85 -0.30 3826.69 1.00 alpha[26] 0.23 0.41 -0.44 0.89 3298.78 1.00 alpha[27] -1.57 0.54 -2.47 -0.78 2958.41 1.00 alpha[28] -0.46 0.42 -1.15 0.20 3327.90 1.00 alpha[29] 0.24 0.42 -0.45 0.91 3811.62 1.00 alpha[30] 1.47 0.46 0.76 2.23 2526.40 1.00 alpha[31] -0.64 0.42 -1.32 0.03 3017.64 1.00 alpha[32] -0.28 0.40 -0.93 0.35 3732.62 1.00 alpha[33] 3.21 0.94 1.99 4.90 1298.74 1.00 alpha[34] 2.58 0.64 1.69 3.70 1186.97 1.00 alpha[35] 2.61 0.66 1.68 3.79 1683.87 1.00 alpha[36] 1.98 0.46 1.29 2.77 2180.86 1.00 alpha[37] 1.99 0.49 1.26 2.85 1885.35 1.00 alpha[38] 12.46 27.29 2.72 38.05 144.99 1.01 alpha[39] 2.59 0.67 1.66 3.76 2120.85 1.00 alpha[40] 2.25 0.57 1.45 3.23 1897.47 1.00 alpha[41] -2.02 0.56 -2.98 -1.18 2994.60 1.00 alpha[42] -0.58 0.37 -1.19 0.01 3170.95 1.00 alpha[43] -0.45 0.36 -1.02 0.11 4132.21 1.00 alpha[44] -0.32 0.34 -0.86 0.22 4135.91 1.00 alpha[45] 0.67 0.36 0.12 1.22 3093.76 1.00 alpha[46] -0.57 0.36 -1.17 0.00 3838.30 1.00 alpha[47] 1.98 0.46 1.29 2.77 2494.72 1.00 alpha[48] 0.05 0.35 -0.51 0.61 3303.74 1.00 Gaussian fit precis(model_frog_tank, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 z_alpha[1] 0.49 0.57 -0.41 1.45 1870.38 1.00 z_alpha[2] 1.06 0.69 0.05 2.26 2000.23 1.00 z_alpha[3] -0.23 0.44 -0.90 0.50 1991.04 1.00 z_alpha[4] 1.07 0.67 0.10 2.23 1893.13 1.00 z_alpha[5] 0.50 0.56 -0.33 1.47 1592.06 1.00 z_alpha[6] 0.47 0.55 -0.34 1.40 2138.12 1.00 z_alpha[7] 1.06 0.68 0.08 2.19 1489.88 1.00 z_alpha[8] 0.50 0.58 -0.36 1.48 1601.97 1.00 z_alpha[9] -0.97 0.41 -1.62 -0.35 1105.24 1.00 z_alpha[10] 0.49 0.56 -0.34 1.47 1731.87 1.00 z_alpha[11] -0.22 0.45 -0.91 0.51 1473.10 1.00 z_alpha[12] -0.50 0.42 -1.15 0.19 1447.45 1.00 z_alpha[13] -0.23 0.44 -0.89 0.49 1237.91 1.00 z_alpha[14] -0.73 0.43 -1.43 -0.02 1022.57 1.00 z_alpha[15] 0.50 0.56 -0.36 1.44 1870.68 1.00 z_alpha[16] 0.50 0.55 -0.34 1.43 1805.64 1.00 z_alpha[17] 0.98 0.52 0.23 1.89 1676.62 1.00 z_alpha[18] 0.65 0.43 0.04 1.38 1158.41 1.00 z_alpha[19] 0.41 0.38 -0.18 1.06 1078.99 1.00 z_alpha[20] 1.43 0.62 0.52 2.50 1760.28 1.00 z_alpha[21] 0.65 0.43 0.03 1.39 1756.53 1.00 z_alpha[22] 0.67 0.47 -0.03 1.46 1312.81 1.00 z_alpha[23] 0.65 0.43 0.00 1.37 1349.87 1.00 z_alpha[24] 0.22 0.37 -0.33 0.85 1059.37 1.00 z_alpha[25] -1.49 0.35 -2.07 -0.96 708.91 1.00 z_alpha[26] -0.75 0.30 -1.24 -0.28 678.36 1.00 z_alpha[27] -1.76 0.36 -2.35 -1.19 680.12 1.00 z_alpha[28] -1.15 0.31 -1.67 -0.67 578.92 1.01 z_alpha[29] -0.76 0.30 -1.25 -0.29 642.74 1.00 z_alpha[30] 0.06 0.34 -0.48 0.62 899.51 1.00 z_alpha[31] -1.26 0.31 -1.80 -0.80 641.76 1.00 z_alpha[32] -1.05 0.31 -1.56 -0.58 598.01 1.00 z_alpha[33] 1.14 0.50 0.42 2.00 1546.38 1.00 z_alpha[34] 0.84 0.44 0.19 1.61 1029.88 1.00 z_alpha[35] 0.85 0.44 0.19 1.58 1224.98 1.00 z_alpha[36] 0.45 0.35 -0.08 1.02 1229.36 1.00 z_alpha[37] 0.45 0.37 -0.11 1.06 766.67 1.00 z_alpha[38] 1.58 0.58 0.71 2.58 1654.66 1.00 z_alpha[39] 0.86 0.45 0.20 1.61 1364.17 1.00 z_alpha[40] 0.62 0.40 0.03 1.28 1126.64 1.00 z_alpha[41] -1.99 0.38 -2.64 -1.43 606.72 1.00 z_alpha[42] -1.22 0.28 -1.68 -0.78 529.53 1.00 z_alpha[43] -1.15 0.29 -1.63 -0.70 572.59 1.00 z_alpha[44] -1.07 0.28 -1.53 -0.64 533.62 1.00 z_alpha[45] -0.49 0.28 -0.95 -0.07 582.37 1.00 z_alpha[46] -1.22 0.29 -1.69 -0.76 491.84 1.01 z_alpha[47] 0.44 0.36 -0.11 1.04 687.16 1.00 z_alpha[48] -0.86 0.26 -1.30 -0.47 515.20 1.00 alpha_bar 1.36 0.25 0.97 1.77 254.02 1.02 sigma_alpha 1.61 0.21 1.30 1.97 487.86 1.00 alpha[1] 2.14 0.89 0.78 3.67 2251.43 1.00 alpha[2] 3.06 1.13 1.43 5.04 2049.97 1.00 alpha[3] 1.00 0.67 -0.03 2.13 3233.64 1.00 alpha[4] 3.08 1.09 1.54 5.02 1817.81 1.00 alpha[5] 2.16 0.87 0.87 3.64 1949.71 1.00 alpha[6] 2.12 0.85 0.89 3.56 2576.70 1.00 alpha[7] 3.06 1.12 1.52 4.92 1360.12 1.00 alpha[8] 2.15 0.91 0.84 3.69 2001.80 1.00 alpha[9] -0.18 0.60 -1.17 0.74 2376.60 1.00 alpha[10] 2.15 0.87 0.84 3.65 2072.54 1.00 alpha[11] 1.00 0.68 -0.02 2.12 3102.50 1.00 alpha[12] 0.56 0.64 -0.42 1.61 2703.44 1.00 alpha[13] 0.99 0.66 -0.04 2.09 2489.94 1.00 alpha[14] 0.20 0.65 -0.81 1.27 2403.15 1.00 alpha[15] 2.15 0.89 0.78 3.63 1975.27 1.00 alpha[16] 2.16 0.87 0.86 3.63 1987.94 1.00 alpha[17] 2.93 0.83 1.74 4.33 2267.23 1.00 alpha[18] 2.39 0.64 1.47 3.51 2108.60 1.00 alpha[19] 2.01 0.56 1.17 2.93 2544.19 1.00 alpha[20] 3.65 1.01 2.22 5.40 1846.68 1.00 alpha[21] 2.40 0.64 1.43 3.48 2515.23 1.00 alpha[22] 2.43 0.72 1.36 3.70 1963.71 1.00 alpha[23] 2.39 0.65 1.41 3.52 2089.73 1.00 alpha[24] 1.71 0.54 0.89 2.59 2633.50 1.00 alpha[25] -1.01 0.46 -1.76 -0.32 3538.34 1.00 alpha[26] 0.17 0.40 -0.47 0.80 3542.02 1.00 alpha[27] -1.43 0.48 -2.20 -0.69 3041.17 1.00 alpha[28] -0.46 0.41 -1.12 0.16 3521.79 1.00 alpha[29] 0.15 0.39 -0.50 0.79 3189.08 1.00 alpha[30] 1.46 0.49 0.71 2.29 2432.75 1.00 alpha[31] -0.64 0.41 -1.31 -0.01 2760.34 1.00 alpha[32] -0.30 0.40 -0.92 0.33 2986.85 1.00 alpha[33] 3.17 0.76 2.09 4.51 2773.68 1.00 alpha[34] 2.70 0.66 1.75 3.78 2240.81 1.00 alpha[35] 2.70 0.64 1.77 3.81 2145.08 1.00 alpha[36] 2.07 0.50 1.33 2.95 2937.14 1.00 alpha[37] 2.06 0.53 1.26 2.96 2684.41 1.00 alpha[38] 3.89 0.96 2.53 5.58 1751.60 1.00 alpha[39] 2.73 0.68 1.74 3.90 2693.78 1.00 alpha[40] 2.35 0.56 1.51 3.28 2141.12 1.00 alpha[41] -1.80 0.48 -2.64 -1.08 3187.34 1.00 alpha[42] -0.57 0.34 -1.11 -0.02 2881.39 1.00 alpha[43] -0.46 0.34 -1.00 0.09 3305.89 1.00 alpha[44] -0.34 0.34 -0.88 0.18 2982.16 1.00 alpha[45] 0.59 0.36 0.03 1.18 3413.59 1.00 alpha[46] -0.57 0.35 -1.17 -0.03 3121.54 1.00 alpha[47] 2.06 0.51 1.29 2.92 2469.40 1.00 alpha[48] 0.00 0.33 -0.53 0.53 2517.00 1.00 precis(model_frog_cauchy, depth = 2) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% dplyr::select(rowname, mean_c = mean, ll_c = `X5.5.`, hh_c = X94.5.) %&gt;% as_tibble() %&gt;% left_join(precis(model_frog_tank, depth = 2) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% dplyr::select(rowname, mean_g = mean, ll_g = `X5.5.`, hh_g = X94.5.) %&gt;% as_tibble()) %&gt;% filter(grepl(&quot;^alpha&quot;, rowname)) %&gt;% ggplot(aes(x = mean_c, y = mean_g)) + geom_abline(slope = 1, intercept = 0, color = clr_current, linetype = 3) + geom_linerange(aes(xmin = ll_c, xmax = hh_c), color = clr0dd)+ geom_linerange(aes(ymin = ll_g, ymax = hh_g), color = clr0dd) + geom_point(color = clr0dd, fill = clr0, shape = 21, size = 2) + coord_cartesian(xlim = c(-5, 50)) M4 model_frog_student &lt;- ulam( flist = alist( surv ~ dbinom( density, p ), logit(p) &lt;- alpha_bar + z_alpha[tank] * sigma_alpha, z_alpha[tank] ~ dstudent(nu = 2, 0, 1), alpha_bar ~ dnorm(0, 1.5), sigma_alpha ~ dexp(1), gq&gt; vector[tank]:alpha &lt;&lt;- alpha_bar + z_alpha * sigma_alpha ), data = data_frogs, control = list(adapt_delta = 0.99), cores = 4, chains = 4, max_treedepth = 15, log_lik = TRUE ) p1 &lt;- precis(model_frog_student, depth = 2) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% dplyr::select(rowname, mean_s = mean, ll_s = `X5.5.`, hh_s = X94.5.) %&gt;% as_tibble() %&gt;% left_join(precis(model_frog_tank, depth = 2) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% dplyr::select(rowname, mean_g = mean, ll_g = `X5.5.`, hh_g = X94.5.) %&gt;% as_tibble()) %&gt;% filter(grepl(&quot;^alpha&quot;, rowname)) %&gt;% ggplot(aes(x = mean_s, y = mean_g)) + geom_abline(slope = 1, intercept = 0, color = clr_current, linetype = 3) + geom_linerange(aes(xmin = ll_s, xmax = hh_s), color = clr0dd)+ geom_linerange(aes(ymin = ll_g, ymax = hh_g), color = clr0dd) + geom_point(color = clr0dd, fill = clr0, shape = 21, size = 2) p2 &lt;- precis(model_frog_student, depth = 2) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% dplyr::select(rowname, mean_s = mean, ll_s = `X5.5.`, hh_s = X94.5.) %&gt;% as_tibble() %&gt;% left_join(precis(model_frog_cauchy, depth = 2) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% dplyr::select(rowname, mean_c = mean, ll_c = `X5.5.`, hh_c = X94.5.) %&gt;% as_tibble()) %&gt;% filter(grepl(&quot;^alpha&quot;, rowname)) %&gt;% ggplot(aes(x = mean_s, y = mean_c)) + geom_abline(slope = 1, intercept = 0, color = clr_current, linetype = 3) + geom_linerange(aes(xmin = ll_s, xmax = hh_s), color = clr0dd)+ geom_linerange(aes(ymin = ll_c, ymax = hh_c), color = clr0dd) + geom_point(color = clr0dd, fill = clr0, shape = 21, size = 2) + coord_cartesian(ylim = c(-5, 50), xlim = c(-7, 10)) p1 + p2 M5 Adding a mean \\(\\gamma\\) to the cimpanzee model \\[ \\begin{array}{rcl} \\gamma_{j} &amp; \\sim &amp; \\textrm{Normal}(\\bar{\\gamma}, \\sigma_{\\gamma})\\\\ \\bar{\\gamma} &amp; \\sim &amp; \\textrm{Normal}(0, 1.5) \\end{array} \\] model_chimp_multicluster_adapt &lt;- ulam( flist = alist( pulled_left ~ dbinom( 1, p ), logit(p) &lt;- alpha[actor] + gamma[block] + beta[treatment], beta[treatment] ~ dnorm(0,.5), ## adaptive priors alpha[actor] ~ dnorm( alpha_bar, sigma_alpha ), gamma[block] ~ dnorm( gamma_bar, sigma_gamma ), ## hyper-priors alpha_bar ~ dnorm( 0, 1.5 ), gamma_bar ~ dnorm( 0, 1.5 ), sigma_alpha ~ dexp(1), sigma_gamma ~ dexp(1) ), data = data_chimp_list, cores = 4, chains = 4, log_lik = TRUE ) compare(model_chimp_multicluster, model_chimp_multicluster_adapt) %&gt;% knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight model_chimp_multicluster 531.72 19.11 0.00 NA 10.31 0.58 model_chimp_multicluster_adapt 532.36 19.35 0.64 0.57 10.72 0.42 precis(model_chimp_multicluster) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_bar 0.77 0.84 -0.50 2.23 30.19 1.09 sigma_alpha 2.05 0.65 1.22 3.28 943.63 1.00 sigma_gamma 0.19 0.17 0.01 0.50 84.28 1.05 precis(model_chimp_multicluster_adapt) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_bar 0.32 1.10 -1.39 2.07 268.26 1.02 gamma_bar 0.36 1.09 -1.41 2.12 189.44 1.03 sigma_alpha 2.02 0.66 1.20 3.26 766.25 1.00 sigma_gamma 0.22 0.16 0.05 0.52 407.83 1.00 bar_alpha_single &lt;- extract.samples(model_chimp_multicluster) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% .$alpha_bar %&gt;% quantile(prob = c(.055, .25, .5, .75, .955)) %&gt;% tibble(alpha_bar = ., prob = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;)) extract.samples(model_chimp_multicluster_adapt) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% dplyr::select(ends_with(&quot;bar&quot;)) %&gt;% ggplot(aes(x = alpha_bar, y = gamma_bar)) + geom_rect(data = bar_alpha_single %&gt;% pivot_wider(names_from = prob, values_from = alpha_bar), inherit.aes = FALSE, aes(xmin = ll, xmax = hh, ymin = -Inf, ymax = Inf), color = &quot;transparent&quot;, fill = fll0) + geom_rect(data = bar_alpha_single %&gt;% pivot_wider(names_from = prob, values_from = alpha_bar), inherit.aes = FALSE, aes(xmin = l, xmax = h, ymin = -Inf, ymax = Inf), color = &quot;transparent&quot;, fill = fll0) + geom_hex(aes(color = ..count.., fill = after_scale(clr_alpha(color,.85))), bins = 40) + geom_vline(data = bar_alpha_single %&gt;% pivot_wider(names_from = prob, values_from = alpha_bar), aes(xintercept = m), color = clr_dark, linetype = 3) + scale_color_gradientn(colours = c(clr_dark, clr0, clr_current)) + guides(color = guide_colorbar(title.position = &quot;top&quot;, barwidth = unit(.9, &quot;npc&quot;), barheight = unit(5, &quot;pt&quot;))) + theme(legend.position = &quot;bottom&quot;) M6 data_y &lt;- list(y = 0) model_nn &lt;- quap( flist = alist( y ~ dnorm(mean = mu, sd = 1), mu ~ dnorm(mean = 10, sd = 1) ), data = data_y ) model_nt &lt;- quap( flist = alist( y ~ dnorm(mean = mu, sd = 1), mu ~ dstudent(nu = 2, mu = 10, sigma = 1) ), data = data_y ) model_tn &lt;- quap( flist = alist( y ~ dstudent(nu = 2, mu = mu, sigma = 1), mu ~ dnorm(mean = 10, sd = 1) ), data = data_y ) model_tt &lt;- quap( flist = alist( y ~ dstudent(nu = 2, mu = mu, sigma = 1), mu ~ dstudent(nu = 2, mu = 10, sigma = 1) ), data = data_y ) p1 &lt;- tibble_precis(model_nn) %&gt;% bind_rows(tibble_precis(model_nt)) %&gt;% bind_rows(tibble_precis(model_tn)) %&gt;% bind_rows(tibble_precis(model_tt)) %&gt;% ggplot(aes(y = model, x = mean)) + geom_linerange(aes(xmin = `X5.5.`, xmax = `X94.5.`), color = clr0dd) + geom_point(shape = 21, fill = clr0, color = clr0dd) + labs(y = NULL, x = &quot;mu&quot;) # x_vect &lt;- seq(-5, 20, length.out = 101) n &lt;- 1e3 dens_fun &lt;- function(type, x = x_vect, mean = 10, sd = 1){ switch(type, &quot;normal&quot; = rnorm(n = n, mean = mean, sd), &quot;student&quot; = rstudent(n = n, nu = 2, mu = mean,sigma = sd) ) } p2 &lt;- crossing(y = c(&quot;normal&quot;, &quot;student&quot;), mu = c(&quot;normal&quot;, &quot;student&quot;)) %&gt;% mutate(model = str_c(&quot;model_&quot;, str_sub(y,1,1),str_sub(mu, 1,1)), dist = purrr::map2(y, mu, function(y, mu){ tibble(y_dens = dens_fun(type = y, mean = 0), mu_dens = dens_fun(type = mu))})) %&gt;% unnest(dist) %&gt;% pivot_longer(cols = y_dens:mu_dens) %&gt;% mutate(dist = if_else(name == &quot;y_dens&quot;, y, mu)) %&gt;% ggplot(aes(y = model)) + stat_slab(aes(x = value, color = dist, fill = after_scale(clr_alpha(color))), size = .5, normalize = &quot;xy&quot;, height = .85, trim = FALSE) + coord_cartesian(xlim = c(-5, 15)) + scale_color_manual(values = c(clr_dark, clr0d)) + labs(y = NULL) p1 + p2 If the likelihood is \\(y \\sim Normal()\\), it is less likely to be pulled from 0 by the prior for \\(\\mu\\) compared to when \\(y \\sim Student()\\) When the prior is \\(\\mu \\sim Normal()\\), then it is more regularizing than with \\(\\mu \\sim Student()\\) (\\(\\rightarrow\\) it pulls harder) H1 data(&quot;bangladesh&quot;) data_bangaldesh &lt;- bangladesh %&gt;% as_tibble() %&gt;% mutate(district_idx = as.integer(as.factor(district))) %&gt;% rename(contraception = use.contraception) rm(bangladesh) tibble(district_idx = sort(unique(data_bangaldesh$district))) %&gt;% mutate(check_increment = district_idx - lag(district_idx,default = 0)) %&gt;% arrange(-check_increment) #&gt; # A tibble: 60 × 2 #&gt; district_idx check_increment #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 55 2 #&gt; 2 1 1 #&gt; 3 2 1 #&gt; 4 3 1 #&gt; 5 4 1 #&gt; 6 5 1 #&gt; 7 6 1 #&gt; 8 7 1 #&gt; 9 8 1 #&gt; 10 9 1 #&gt; # … with 50 more rows sort(unique(data_bangaldesh$district)) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #&gt; [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 #&gt; [51] 51 52 53 55 56 57 58 59 60 61 sort(unique(data_bangaldesh$district_idx)) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #&gt; [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 #&gt; [51] 51 52 53 54 55 56 57 58 59 60 data_bangaldesh_list &lt;- data_bangaldesh %&gt;% dplyr::select(woman, district_idx, contraception) %&gt;% as.list() model_bangladesh_classic &lt;- ulam( flist = alist( contraception ~ dbinom(1, p), logit(p) &lt;- alpha[district_idx], alpha[district_idx] ~ dnorm(0, 1.5) ), data = data_bangaldesh_list, cores = 4, chain = 4, log_lik = TRUE ) model_bangladesh_multilevel &lt;- ulam( flist = alist( contraception ~ dbinom(1, p), logit(p) &lt;- alpha_bar + z_alpha[district_idx] * sigma_alpha, z_alpha[district_idx] ~ dnorm(0, 1), alpha_bar ~ dnorm(0, 1.5), sigma_alpha ~ dexp(1), gq&gt; vector[district_idx]:alpha &lt;&lt;- alpha_bar + z_alpha * sigma_alpha ), data = data_bangaldesh_list, cores = 4, chain = 4, log_lik = TRUE ) new_districts &lt;- distinct(data_bangaldesh, district_idx) get_district_posterior &lt;- function(model){ mod_name &lt;- deparse(substitute(model)) link(model, data = new_districts) %&gt;% data.frame() %&gt;% as_tibble() %&gt;% summarise(across(everything(), function(x){ list(tibble(value = quantile(x, prob = c(.055, .25, .5, .75, .955)), lab = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) })) %&gt;% pivot_longer(everything()) %&gt;% bind_cols(new_districts, .) %&gt;% unnest(value) %&gt;% pivot_wider(names_from = lab, values_from = value) %&gt;% mutate(model = mod_name) } get_district_posterior(model_bangladesh_classic) %&gt;% bind_rows(get_district_posterior(model_bangladesh_multilevel)) %&gt;% mutate(model_short = str_remove( model, &quot;.*_&quot;), xshift = -.33 * (1.5 - as.numeric(as.factor(model_short))), x = as.numeric(factor(district_idx)) + xshift) %&gt;% ggplot(aes(x = x, color = model_short)) + geom_linerange(aes(ymin = ll, ymax = hh), size = .2) + geom_linerange(aes(ymin = l, ymax = h), size = .5) + geom_point(aes(y = m, fill = after_scale(clr_lighten(color))), shape = 21) + scale_color_manual(values = c(clr_dark, clr_current)) + scale_x_continuous(breaks = 2 * (1:30) - 1 ) + coord_cartesian(ylim = 0:1) + labs(y = &quot;probability of using contraception&quot;, x = &quot;district&quot;) + theme(legend.position = &quot;bottom&quot;, panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) \\(\\rightarrow\\) the model_bangladesh_multilevel regresses to the mean (eg. district no. 3) H2 chapter12_models &lt;- read_rds(&quot;envs/chapter12_models.rds&quot;) data_trolley_list3 &lt;- chapter12_models$data_trolley %&gt;% mutate(id_idx = as.integer(id)) %&gt;% dplyr::select(response, action, intention, contact, id_idx) %&gt;% as.list() model_trolley_predict_multi &lt;- ulam( flist = alist( response ~ dordlogit( phi, cutpoints ), phi &lt;- alpha_bar + z_alpha[id_idx] * sigma_alpha + beta_a * action + beta_c * contact + B_i * intention, B_i &lt;- beta_i + beta_ia * action + beta_ic * contact, c(beta_a, beta_c, beta_i, beta_ia, beta_ic) ~ dnorm(0, 0.5), cutpoints ~ dnorm(0, 1.5), alpha_bar ~ dnorm(0, 1), z_alpha[id_idx] ~ dnorm(0, 1), sigma_alpha ~ dexp(1) ), data = data_trolley_list3, chains = 4, cores = 4, log_lik = TRUE ) write_rds(model_trolley_predict_multi, file = &quot;brms/ulam_c13_model_trolley_predict_multi.rds&quot;) model_trolley_predict_multi &lt;- read_rds(&quot;brms/ulam_c13_model_trolley_predict_multi.rds&quot;) precis(model_trolley_predict_multi) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 beta_ic -1.66 0.10 -1.82 -1.51 1513.39 1.00 beta_ia -0.55 0.08 -0.68 -0.42 1271.30 1.00 beta_i -0.39 0.06 -0.48 -0.29 1243.37 1.00 beta_c -0.46 0.07 -0.57 -0.34 1471.66 1.00 beta_a -0.65 0.06 -0.74 -0.57 1326.95 1.00 alpha_bar 0.78 0.56 -0.10 1.69 1204.08 1.00 sigma_alpha 1.92 0.08 1.78 2.06 270.67 1.03 precis(chapter12_models$model_trolley_predict) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 beta_ic -1.24 0.10 -1.39 -1.08 1115.74 1 beta_ia -0.43 0.08 -0.56 -0.31 1266.84 1 beta_i -0.29 0.06 -0.39 -0.20 1097.48 1 beta_c -0.34 0.07 -0.45 -0.24 1174.36 1 beta_a -0.47 0.05 -0.56 -0.39 1232.07 1 compare(chapter12_models$model_trolley_predict, model_trolley_predict_multi) #&gt; WAIC SE dWAIC dSE #&gt; model_trolley_predict_multi 31055.82 179.48037 0.000 NA #&gt; chapter12_models$model_trolley_predict 36928.89 80.74662 5873.076 173.6475 #&gt; pWAIC weight #&gt; model_trolley_predict_multi 356.03065 1 #&gt; chapter12_models$model_trolley_predict 10.79849 0 posterior_comparison &lt;- tibble_precis(chapter12_models$model_trolley_predict) %&gt;% bind_rows(tibble_precis(model_trolley_predict_multi)) posterior_comparison %&gt;% ggplot(aes(y = rowname)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_linerange(aes(xmin = `X5.5.`, xmax = `X94.5.`), color = clr0dd) + geom_point(aes(x = mean), color = clr0dd, fill = clr0, shape = 21, size = 1.5) + facet_wrap(model ~ ., ncol = 1, scales = &quot;free_y&quot;)+ labs(y = NULL) H3 data_trolley_list4 &lt;- chapter12_models$data_trolley %&gt;% mutate(id_idx = as.integer(id), story_idx = as.integer(story)) %&gt;% dplyr::select(response, action, intention, contact, id_idx, story_idx) %&gt;% as.list() model_trolley_predict_story &lt;- ulam( flist = alist( response ~ dordlogit( phi, cutpoints ), phi &lt;- alpha_bar + z_alpha[id_idx] * sigma_alpha + z_gamma[story_idx] * sigma_gamma + beta_a * action + beta_c * contact + B_i * intention, B_i &lt;- beta_i + beta_ia * action + beta_ic * contact, c(beta_a, beta_c, beta_i, beta_ia, beta_ic) ~ dnorm(0, 0.5), cutpoints ~ dnorm(0, 1.5), alpha_bar ~ dnorm(0, 1), z_alpha[id_idx] ~ dnorm(0, 1), sigma_alpha ~ dexp(1), z_gamma[story_idx] ~ dnorm(0, 1), sigma_gamma ~ dexp(1) ), data = data_trolley_list4, chains = 4, cores = 4, log_lik = TRUE ) write_rds(model_trolley_predict_story, file = &quot;brms/ulam_c13_model_trolley_predict_story.rds&quot;) model_trolley_predict_story &lt;- read_rds(&quot;brms/ulam_c13_model_trolley_predict_story.rds&quot;) compare(model_trolley_predict_story, model_trolley_predict_multi) #&gt; WAIC SE dWAIC dSE pWAIC #&gt; model_trolley_predict_story 30567.72 180.3282 0.0000 NA 366.6173 #&gt; model_trolley_predict_multi 31055.82 179.4804 488.0974 42.62895 356.0306 #&gt; weight #&gt; model_trolley_predict_story 1.000000e+00 #&gt; model_trolley_predict_multi 1.025663e-106 posterior_comparison &lt;- tibble_precis(model_trolley_predict_story) %&gt;% bind_rows(tibble_precis(model_trolley_predict_multi)) posterior_comparison %&gt;% ggplot(aes(y = rowname)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_linerange(aes(xmin = `X5.5.`, xmax = `X94.5.`), color = clr0dd) + geom_point(aes(x = mean), color = clr0dd, fill = clr0, shape = 21, size = 1.5) + facet_wrap(model ~ ., ncol = 1, scales = &quot;free_y&quot;)+ labs(y = NULL) H4 data_frogs_list &lt;- data_frogs %&gt;% mutate(pred_size_inter = str_c(pred, size, sep = &quot;_&quot;) %&gt;% factor(), across(c(pred, size, pred_size_inter), as.integer, .names = &quot;{.col}_idx&quot;)) %&gt;% dplyr::select(density, surv, tank, pred_idx, size_idx, pred_size_inter_idx) %&gt;% as.list() model_frog_pred &lt;- ulam( flist = alist( surv ~ dbinom( density, p ), logit(p) &lt;- alpha_bar + z_tank[tank] * sigma_tank + z_pred[pred_idx] * sigma_pred, ## adaptive priors z_tank[tank] ~ dnorm( 0, 1 ), z_pred[pred_idx] ~ dnorm( 0, 1 ), ## hyper-priors alpha_bar ~ dnorm(0, 1.5), sigma_tank ~ dexp(1), sigma_pred ~ dexp(1), gq&gt; vector[tank]:alpha_tank &lt;&lt;- alpha_bar + z_tank * sigma_tank, gq&gt; vector[pred_idx]:alpha_pred &lt;&lt;- alpha_bar + z_pred * sigma_pred ), data = data_frogs_list, cores = 4, chains = 4, log_lik = TRUE ) model_frog_size &lt;- ulam( flist = alist( surv ~ dbinom( density, p ), logit(p) &lt;- alpha_bar + z_tank[tank] * sigma_tank + z_size[size_idx] * sigma_size, ## adaptive priors z_tank[tank] ~ dnorm( 0, 1 ), z_size[size_idx] ~ dnorm( 0, 1 ), ## hyper-priors alpha_bar ~ dnorm(0, 1.5), sigma_tank ~ dexp(1), sigma_size ~ dexp(1), gq&gt; vector[tank]:alpha_tank &lt;&lt;- alpha_bar + z_tank * sigma_tank, gq&gt; vector[size_idx]:alpha_size &lt;&lt;- z_size * sigma_size ), data = data_frogs_list, cores = 4, chains = 4, log_lik = TRUE ) model_frog_size_pred &lt;- ulam( flist = alist( surv ~ dbinom( density, p ), logit(p) &lt;- alpha_bar + z_tank[tank] * sigma_tank + z_pred[pred_idx] * sigma_pred + z_size[size_idx] * sigma_size, ## adaptive priors z_tank[tank] ~ dnorm( 0, 1 ), z_size[size_idx] ~ dnorm( 0, 1 ), z_pred[pred_idx] ~ dnorm( 0, 1 ), ## hyper-priors alpha_bar ~ dnorm(0, 1.5), sigma_size ~ dexp(1), sigma_tank ~ dexp(1), sigma_pred ~ dexp(1), gq&gt; vector[tank]:alpha_tank &lt;&lt;- alpha_bar + z_tank * sigma_tank, gq&gt; vector[size_idx]:alpha_size &lt;&lt;- z_size * sigma_size, gq&gt; vector[pred_idx]:alpha_pred &lt;&lt;- z_pred * sigma_pred ), data = data_frogs_list, cores = 4, chains = 4, log_lik = TRUE ) model_frog_size_pred_inter &lt;- ulam( flist = alist( surv ~ dbinom( density, p ), logit(p) &lt;- alpha_bar + z_tank[tank] * sigma_tank + z_pred[pred_idx] * sigma_pred + z_size[size_idx] * sigma_size + z_inter[pred_size_inter_idx] * sigma_inter, ## adaptive priors z_tank[tank] ~ dnorm( 0, 1 ), z_size[size_idx] ~ dnorm( 0, 1 ), z_pred[pred_idx] ~ dnorm( 0, 1 ), z_inter[pred_size_inter_idx] ~ dnorm( 0, 1 ), ## hyper-priors alpha_bar ~ dnorm(0, 1.5), sigma_size ~ dexp(1), sigma_tank ~ dexp(1), sigma_pred ~ dexp(1), sigma_inter ~ dexp(1), gq&gt; vector[tank]:alpha_tank &lt;&lt;- alpha_bar + z_tank * sigma_tank, gq&gt; vector[size_idx]:alpha_size &lt;&lt;- z_size * sigma_size, gq&gt; vector[pred_idx]:alpha_pred &lt;&lt;- z_pred * sigma_pred, gq&gt; vector[pred_size_inter_idx]:alpha_inter &lt;&lt;- z_inter * sigma_inter ), data = data_frogs_list, cores = 4, chains = 4, log_lik = TRUE ) compare(model_frog_pred, model_frog_size, model_frog_size_pred, model_frog_size_pred_inter) #&gt; WAIC SE dWAIC dSE pWAIC #&gt; model_frog_size_pred 199.4890 8.983376 0.0000000 NA 19.01785 #&gt; model_frog_size_pred_inter 199.7571 9.286128 0.2681206 2.018589 19.08583 #&gt; model_frog_pred 200.4490 9.387933 0.9599531 2.017005 19.70884 #&gt; model_frog_size 201.8626 7.213050 2.3735980 5.750028 21.76818 #&gt; weight #&gt; model_frog_size_pred 0.3573302 #&gt; model_frog_size_pred_inter 0.3124986 #&gt; model_frog_pred 0.2211152 #&gt; model_frog_size 0.1090560 tibble_precisposterior_comparison &lt;- tibble_precis(model_frog_pred, depth = 2) %&gt;% bind_rows(tibble_precis(model_frog_size, depth = 2)) %&gt;% bind_rows(tibble_precis(model_frog_size_pred, depth = 2)) %&gt;% bind_rows(tibble_precis(model_frog_size_pred_inter, depth = 2)) %&gt;% mutate(model = str_remove(model, &quot;model_frog_&quot;)) level_cpt &lt;- data_frogs_list %&gt;% as_tibble() %&gt;% mutate(lab = glue(&quot;{pred_size_inter_idx}: ({levels(data_frogs$size)[size_idx]} | {levels(data_frogs$pred)[pred_idx]})&quot;)) %&gt;% dplyr::select(lab) %&gt;% distinct() %&gt;% .$lab %&gt;% str_c(collapse = &quot;; &quot;) %&gt;% str_c(&quot;Interaction Levels: &quot;,.) posterior_comparison %&gt;% filter(grepl(&quot;sigma|alpha&quot;, rowname) &amp; !grepl(&quot;tank&quot;, rowname)) %&gt;% ggplot(aes(y = model, color = model)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_linerange(aes(xmin = `X5.5.`, xmax = `X94.5.`)#, #color = clr0dd ) + geom_point(aes(x = mean, color = model, fill = after_scale(clr_lighten(color))), # color = clr0dd, fill = clr0, shape = 21, size = 1.5) + facet_wrap(rowname ~ .,# ncol = 1, scales = &quot;free_y&quot;,dir = &quot;v&quot;) + scale_color_manual(values = c(clr_dark, clr1, clr2, clr3), guide = &#39;none&#39;) + labs(y = NULL, x = NULL, caption = level_cpt) pair_model &lt;- function(model, clr_current = clr_dark){ p &lt;- extract.samples(model) %&gt;% data.frame() %&gt;% as_tibble() %&gt;% dplyr::select(contains(&quot;alpha_pred&quot;), contains(&quot;alpha_size&quot;), #contains(&quot;sigma&quot;) sigma_tank#, # contains(&quot;alpha_inter&quot;) ) %&gt;% set_names(x = ., nm = names(.) %&gt;% str_remove(pattern = &quot;alpha_&quot;)) %&gt;% ggpairs( lower = list(continuous = wrap(my_lower, col = clr_current, bins = 25)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll0, color = clr_current, fill = clr_alpha(clr_current), adjust = .7)), upper = list(continuous = wrap(my_upper , size = 4, color = &quot;black&quot;, family = fnt_sel)) ) + theme(panel.border = element_rect(color = clr_dark, fill = &quot;transparent&quot;)) ggmatrix_gtable(p) } cowplot::plot_grid( cowplot::plot_grid(pair_model(model_frog_size, clr_dark), pair_model(model_frog_pred, clr1), ncol = 1), pair_model(model_frog_size_pred, clr2), pair_model(model_frog_size_pred_inter, clr3), nrow = 1, rel_widths = c(.75, 1, 1)) 14.7 {brms} section 14.7.1 Multilevel Tadpoles brms_c13_model_frog_single &lt;- brm( data = data_frogs, family = binomial, surv | trials(density) ~ 0 + factor(tank), prior(normal(0, 1.5), class = b), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c13_model_frog_single&quot;) library(tidybayes) fixef(brms_c13_model_frog_single) %&gt;% as_tibble() %&gt;% mutate(p = inv_logit_scaled(Estimate)) %&gt;% pivot_longer(Estimate:p) %&gt;% mutate(name = if_else(name == &quot;p&quot;, &quot;expected survival probability&quot;, &quot;expected survival log-odds&quot;)) %&gt;% ggplot(aes(x = value, color = name)) + stat_slab(slab_type = &quot;pdf&quot;, aes(fill_ramp = stat(cut_cdf_qi(cdf, .width = c(1, .95, 0.66)))), color = clr0d, size = .5, adjust = .75, normalize = &quot;xy&quot;, trim = FALSE, n = 301) + scale_colour_ramp_discrete(from = clr_dark, aesthetics = &quot;fill_ramp&quot;, guide = &quot;none&quot;)+ facet_wrap(~ name, scales = &quot;free_x&quot;) + scale_y_continuous(breaks = NULL) + labs(title = &quot;Tank-level intercepts from the no-pooling model&quot;, subtitle = &quot;Notice now inspecting the distributions of the posterior means can offer\\ninsights you might not get if you looked at them one at a time&quot;, y = &quot;density&quot;, x = NULL) The syntax for the varying effects follows the lme4 style, ( &lt;varying parameter(s)&gt; | &lt;grouping variable(s)&gt; ). In this case (1 | tank) indicates only the intercept, 1, varies by tank. The extent to which parameters vary is controlled by the prior, prior(exponential(1), class = sd), which is parameterized in the standard deviation metric. Do note that last part. It’s common in multilevel software to model in the variance metric, instead. brms_c13_model_frog_multi &lt;- brm( data = data_frogs, family = binomial, surv | trials(density) ~ 1 + (1 | tank), prior = c(prior(normal(0, 1.5), class = Intercept), # bar alpha prior(exponential(1), class = sd)), # sigma iter = 5000, warmup = 1000, chains = 4, cores = 4, sample_prior = &quot;yes&quot;, seed = 42, file = &quot;brms/brms_c13_model_frog_multi&quot;) brms_c13_model_frog_single &lt;- add_criterion(brms_c13_model_frog_single, &quot;waic&quot;) brms_c13_model_frog_multi &lt;- add_criterion(brms_c13_model_frog_multi, &quot;waic&quot;) (w &lt;- loo_compare(brms_c13_model_frog_single, brms_c13_model_frog_multi, criterion = &quot;waic&quot;)) \\[\\begin{bmatrix} 0 &amp;0 \\\\-100.080713925655 &amp;3.65719797675975 \\\\20.9937640102151 &amp;0.806322918256096 \\\\200.16142785131 &amp;7.3143959535195 \\\\-7.0516701236133 &amp;1.85830328721338 \\\\-107.132384049268 &amp;2.30310130945441 \\\\25.4020567497694 &amp;1.27920271752106 \\\\214.264768098536 &amp;4.60620261890881 \\\\ \\end{bmatrix}\\] The se_diff is small relative to the elpd_diff. If we convert the \\(elpd\\) difference to the WAIC metric, the message stays the same. cbind(waic_diff = w[, 1] * -2, se = w[, 2] * 2) \\[\\begin{bmatrix} 0 &amp;0 \\\\14.1033402472266 &amp;3.71660657442677 \\\\ \\end{bmatrix}\\] model_weights(brms_c13_model_frog_single, brms_c13_model_frog_multi, weights = &quot;waic&quot;) %&gt;% round(digits = 2) #&gt; brms_c13_model_frog_single brms_c13_model_frog_multi #&gt; 0 1 …the number of effective parameters for the two models. This, recall, is listed in the column for p_waic. w[, &quot;p_waic&quot;] #&gt; brms_c13_model_frog_multi brms_c13_model_frog_single #&gt; 20.99376 25.40206 brms_frogs_posterior &lt;- as_draws_df(brms_c13_model_frog_multi) %&gt;% as_tibble() posterior_median &lt;- coef(brms_c13_model_frog_multi, robust = TRUE)$tank[, , ] %&gt;% data.frame() %&gt;% bind_cols(data_frogs, .) %&gt;% mutate(posterior_median = inv_logit_scaled(Estimate)) posterior_median %&gt;% ggplot(aes(x = tank)) + geom_hline(yintercept = inv_logit_scaled(median(brms_frogs_posterior$b_Intercept)), linetype = 3, size = .5, color = clr_dark) + geom_point(aes(y = propsurv), color = clr_dark, shape = 19, size = 2) + geom_point(aes(y = posterior_median), shape = 1, size = 2) + facet_wrap(density ~ ., labeller = label_both) + scale_x_continuous(breaks = c(1, 16, 32, 48)) + scale_y_continuous(breaks = 0:5 / 5, limits = c(0, 1)) + labs(title = &quot;Multilevel shrinkage!&quot;, subtitle = &quot;The empirical proportions are in gray while the model-implied proportions are\\nthe black circles. The dashed line is the model-implied average survival proportion.&quot;) + theme(panel.grid.major = element_blank(), panel.border = element_rect(fill = &quot;transparent&quot;, color = clr0d)) p1 &lt;- brms_frogs_posterior %&gt;% mutate(iter = 1:n()) %&gt;% slice_sample(n = 100) %&gt;% expand(nesting(iter, b_Intercept, sd_tank__Intercept), x = seq(from = -4, to = 5, length.out = 100)) %&gt;% mutate(density = dnorm(x, mean = b_Intercept, sd = sd_tank__Intercept)) %&gt;% ggplot(aes(x = x, y = density, group = iter)) + geom_line(alpha = .2, color = clr_dark) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Population survival distribution&quot;, subtitle = &quot;log-odds scale&quot;, x = &quot;log-ods survival&quot;) + coord_cartesian(xlim = c(-3, 4)) p2 &lt;- brms_frogs_posterior %&gt;% slice_sample(n = 8000, replace = TRUE) %&gt;% mutate(sim_tanks = rnorm(n(), mean = b_Intercept, sd = sd_tank__Intercept)) %&gt;% ggplot(aes(x = inv_logit_scaled(sim_tanks))) + geom_density(size = .5, fill = fll0, color = clr0dd, adjust = 0.1) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Probability of survival&quot;, subtitle = &quot;transformed by the inverse-logit function&quot;, x = &quot;p&quot;) p1 + p2 Priors for variance components brms_c13_model_frog_multi_update &lt;- update( brms_c13_model_frog_multi, prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 5000, warmup = 1000, chains = 4, cores = 4, sample_prior = &quot;yes&quot;, seed = 42, file = &quot;brms/brms_c13_model_frog_multi_update&quot;) McElreath mentioned how one might set a lower bound at zero for the half-Normal prior when using rethinking::ulam(). There’s no need to do so when using brms::brm(). The lower bounds for priors of class = sd are already set to zero by default. If you’re curious how the exponential and half-Normal priors compare to one another and to their posteriors, you might just plot. tibble(`prior_Exponential(1)` = prior_draws(brms_c13_model_frog_multi) %&gt;% pull(sd_tank), `posterior_Exponential(1)` = as_draws_df(brms_c13_model_frog_multi) %&gt;% pull(sd_tank__Intercept), `prior_Half-Normal(0, 1)` = prior_draws(brms_c13_model_frog_multi_update) %&gt;% pull(sd_tank), `posterior_Half-Normal(0, 1)` = as_draws_df(brms_c13_model_frog_multi_update) %&gt;% pull(sd_tank__Intercept)) %&gt;% pivot_longer(everything(), names_sep = &quot;_&quot;, names_to = c(&quot;distribution&quot;, &quot;prior&quot;)) %&gt;% mutate(distribution = factor(distribution, levels = c(&quot;prior&quot;, &quot;posterior&quot;))) %&gt;% ggplot(aes(x = value, color = distribution)) + geom_density(size = .5, adjust = 0.5, aes(fill = after_scale(clr_alpha(color)))) + scale_color_manual(NULL, values = c(clr0d, clr_dark)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Hierarchical sigma parameter&quot;) + coord_cartesian(xlim = c(0, 4)) + facet_wrap(~ prior) + theme(legend.position = &quot;bottom&quot;, axis.title.x = element_blank(), panel.border = element_rect(fill = &quot;transparent&quot;, color = clr0d)) 14.7.2 Varying Effects and the Underfitting/Overfitting Trade-Off Compute the partial-pooling estimates The multi-level partial-pooling model: brms_c13_model_sim_partial &lt;- brm( data = data_sim, family = binomial, surv_i | trials(n_i) ~ 1 + ( 1 | pond_idx ), prior = c(prior(normal(0, 1.5), class = Intercept), prior(exponential(1), class = sd)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c13_model_sim_partial&quot;) brms_sim_posterior &lt;- coef(brms_c13_model_sim_partial)$pond[, , ] %&gt;% data.frame() %&gt;% transmute(p_partpool = inv_logit_scaled(Estimate)) %&gt;% bind_cols(data_sim, .) %&gt;% mutate(p_true = inv_logit_scaled(true_alpha), no_pool_error = abs(p_no_pool - p_true), part_pool_error = abs(p_partpool - p_true)) dfline &lt;- brms_sim_posterior %&gt;% select(n_i, no_pool_error:part_pool_error) %&gt;% pivot_longer(-n_i) %&gt;% group_by(name, n_i) %&gt;% summarise(mean_error = mean(value)) %&gt;% mutate(x = c( 1, 16, 31, 46), xend = c(15, 30, 45, 60)) brms_sim_posterior %&gt;% ggplot(aes(x = pond_idx)) + geom_point(aes(y = no_pool_error), shape = 19, color = clr0dd, size = 2) + geom_point(aes(y = part_pool_error), shape = 1, size = 2) + geom_segment(data = dfline, aes(x = x, xend = xend, y = mean_error, yend = mean_error), color = rep(c(clr0dd, clr_dark), each = 4), linetype = rep(c(1,3), each = 4)) + scale_x_continuous(breaks = c(1, 10, 20, 30, 40, 50, 60)) + facet_wrap(n_i ~ . , scales = &quot;free_x&quot;, nrow = 1, labeller = label_both) + labs(title = &quot;Estimate error by model type&quot;, subtitle = &quot;The horizontal axis displays pond number. The vertical axis measures the absolute error\\n in the predicted proportion of survivors, compared to the true value used in the simulation.\\nThe higher the point, the worse the estimate. No-pooling shown in gray, Partial pooling\\nshown in black. The gray and dashed black lines show the average error for each\\nkind of estimate, across each initial density of tadpoles (pond size).&quot;, y = &quot;absolute error&quot;) + theme(panel.grid.major = element_blank(), plot.subtitle = element_text(size = 10), panel.border = element_rect(fill = &quot;transparent&quot;, color = clr0d)) If you wanted to quantify the difference in simple summaries, you might execute something like this. brms_sim_posterior %&gt;% select(n_i, no_pool_error:part_pool_error) %&gt;% pivot_longer(-n_i) %&gt;% group_by(name) %&gt;% summarise(mean_error = mean(value) %&gt;% round(digits = 3), median_error = median(value) %&gt;% round(digits = 3)) #&gt; # A tibble: 2 × 3 #&gt; name mean_error median_error #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 no_pool_error 0.077 0.046 #&gt; 2 part_pool_error 0.073 0.043 Repeating the pond simulation Within the brms workflow, we can reuse a compiled model with update(). But first, we’ll simulate new data. set.seed(23) alpha_bar &lt;- 1.5 sigma &lt;- 1.5 n_ponds &lt;- 60 pond_levels &lt;- c(&quot;tiny&quot;, &quot;small&quot;, &quot;medium&quot;, &quot;large&quot;) data_sim_new &lt;- tibble(pond_idx = 1:n_ponds, n_i = rep(c(5, 10, 25, 35), each = 15), pond_size = rep(pond_levels, each = 15) %&gt;% factor(levels = pond_levels), true_alpha = rnorm(n = n_ponds, mean = alpha_bar, sd = sigma), surv_i = rbinom(n_ponds, prob = logistic(true_alpha), size = n_i), p_true = inv_logit(true_alpha), p_no_pool = surv_i / n_i) brms_c13_model_sim_partial_new &lt;- update( brms_c13_model_sim_partial, newdata = data_sim_new, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c13_model_sim_partial_new&quot;) Why not plot the first simulation versus the second one? bind_rows(as_draws_df(brms_c13_model_sim_partial), as_draws_df(brms_c13_model_sim_partial_new)) %&gt;% mutate(model = rep(c(&quot;original_data&quot;, &quot;new_data&quot;), each = n() / 2)) %&gt;% ggplot(aes(x = b_Intercept, y = sd_pond_idx__Intercept)) + stat_density_2d(geom = &quot;raster&quot;, aes(fill = stat(density)), contour = FALSE, n = 200, alpha = .9) + geom_density2d(color = rgb(1,1,1,.75), size = .1) + geom_vline(xintercept = alpha_bar, color = clr_current, linetype = 3) + geom_hline(yintercept = sigma, color = clr_current, linetype = 3) + scale_fill_gradient(low = clr0, high = clr_dark) + ggtitle(&quot;Our simulation posteriors contrast a bit&quot;, subtitle = &quot;alpha is on the x and sigma is on the y, both in log-odds.\\nThe dotted lines intersect at the true values.&quot;) + coord_cartesian(xlim = c(.7, 2), ylim = c(.9, 1.9), expand = 0) + theme(legend.position = &quot;none&quot;, panel.grid.major = element_blank()) + facet_wrap(~ model, ncol = 2) If you’d like the stanfit portion of your brm() object, subset with $fit. brms_c13_model_sim_partial$fit@stanmodel #&gt; S4 class stanmodel &#39;bbd5b3b656d9a26bdac13a9dd363a8cd&#39; coded as follows: #&gt; // generated with brms 2.16.1 #&gt; functions { #&gt; } #&gt; data { #&gt; int&lt;lower=1&gt; N; // total number of observations #&gt; int Y[N]; // response variable #&gt; int trials[N]; // number of trials #&gt; // data for group-level effects of ID 1 #&gt; int&lt;lower=1&gt; N_1; // number of grouping levels #&gt; int&lt;lower=1&gt; M_1; // number of coefficients per level #&gt; int&lt;lower=1&gt; J_1[N]; // grouping indicator per observation #&gt; // group-level predictor values #&gt; vector[N] Z_1_1; #&gt; int prior_only; // should the likelihood be ignored? #&gt; } #&gt; transformed data { #&gt; } #&gt; parameters { #&gt; real Intercept; // temporary intercept for centered predictors #&gt; vector&lt;lower=0&gt;[M_1] sd_1; // group-level standard deviations #&gt; vector[N_1] z_1[M_1]; // standardized group-level effects #&gt; } #&gt; transformed parameters { #&gt; vector[N_1] r_1_1; // actual group-level effects #&gt; r_1_1 = (sd_1[1] * (z_1[1])); #&gt; } #&gt; model { #&gt; // likelihood including constants #&gt; if (!prior_only) { #&gt; // initialize linear predictor term #&gt; vector[N] mu = Intercept + rep_vector(0.0, N); #&gt; for (n in 1:N) { #&gt; // add more terms to the linear predictor #&gt; mu[n] += r_1_1[J_1[n]] * Z_1_1[n]; #&gt; } #&gt; target += binomial_logit_lpmf(Y | trials, mu); #&gt; } #&gt; // priors including constants #&gt; target += normal_lpdf(Intercept | 0, 1.5); #&gt; target += exponential_lpdf(sd_1 | 1); #&gt; target += std_normal_lpdf(z_1[1]); #&gt; } #&gt; generated quantities { #&gt; // actual population-level intercept #&gt; real b_Intercept = Intercept; #&gt; } #&gt; 14.7.3 More than one Type of Cluster ⚠️ WARNING ⚠️ I am so sorry, but we are about to head straight into a load of confusion. If you follow along linearly in the text, we won’t have the language to parse this all out until [later] In short, our difficulties will have to do with what are called the centered and the non-centered parameterizations for multilevel models. For the next several models in the text, McElreath used the centered parameterization. As we’ll learn [later], this often causes problems when you use Stan to fit your multilevel models. Happily, the solution to those problems is often the non-centered parameterization, which is well known among the Stan team. This issue is so well known, in fact, that Bürkner only supports the non-centered parameterization with brms (see here). To my knowledge, there is no easy way around this. Even when using the non-centered parameterization, McElreath’s m13.4 is a bit of an odd model to translate into brms syntax. To my knowledge, it can’t be done with conventional syntax. But we can fit the model with careful use of the non-linear syntax, which might look like this. data_chimp_brms &lt;- data_chimp %&gt;% dplyr::select(pulled_left, actor, block, treatment) %&gt;% mutate(across(actor:treatment, factor)) brms_c13_model_chimp_multicluster &lt;- brm( data = data_chimp_brms, family = binomial, bf(pulled_left | trials(1) ~ alpha + beta, alpha ~ 1 + (1 | actor) + (1 | block), beta ~ 0 + treatment, nl = TRUE), prior = c(prior(normal(0, 0.5), nlpar = beta), prior(normal(0, 1.5), class = b, coef = Intercept, nlpar = alpha), prior(exponential(1), class = sd, group = actor, nlpar = alpha), prior(exponential(1), class = sd, group = block, nlpar = alpha)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c13_model_chimp_multicluster&quot;) The beta ~ 0 + treatment part of the formula is our expression of what we wrote above as \\(\\beta_{treatment[i]}\\). There’s a lot going on with the alpha ~ 1 + (1 | actor) + (1 | block) part of the formula. The initial 1 outside of the parenthesis is \\(\\bar{\\alpha}\\). The (1 | actor) and (1 | block) parts correspond to \\(z_{actor[i]} \\sigma_{\\alpha}\\) and \\(x_{block[i]} \\sigma_{\\gamma}\\), respectively. chimp_posterior &lt;- as_draws_df(brms_c13_model_chimp_multicluster, add_chain = TRUE) %&gt;% as_tibble() %&gt;% mutate(iter = .iteration, chain = .chain) clr_chains &lt;- function(n = 4, alpha = .7, col_start = clr0dd,col = clr2){scales::colour_ramp(colors = c(col_start, col))(seq(0,1,length.out = n))%&gt;% clr_lighten(.2) %&gt;% clr_alpha(alpha = alpha)} library(bayesplot) chimp_posterior %&gt;% mcmc_trace(pars = vars(-iter, -lp__), facet_args = list(ncol = 4), size = .15) + scale_color_manual(values = clr_chains(col_start = &quot;black&quot;) ) + theme(legend.position = &quot;bottom&quot;) When you use the (1 | &lt;group&gt;) syntax within brm(), the group-specific parameters are not shown with print(). You only get the hierarchical \\(\\sigma_{&lt;group&gt;}\\) summaries, shown here as the two rows for sd(a_Intercept). However, you can get a summary of all the parameters with the posterior_summary() function. posterior_summary(brms_c13_model_chimp_multicluster) %&gt;% round(digits = 2) \\[\\begin{bmatrix} 0.6 &amp;0.73 &amp;-0.82 &amp;2.04 &amp;-0.13 &amp;0.3 &amp;-0.71 &amp;0.48 &amp;0.4 &amp;0.3 &amp;-0.19 &amp;1.01 &amp;-0.47 &amp;0.3 &amp;-1.06 &amp;0.11 &amp;0.28 &amp;0.3 &amp;-0.3 &amp;0.89 &amp;2.02 \\\\0.67 &amp;1.08 &amp;3.59 &amp;0.2 &amp;0.17 &amp;0.01 &amp;0.64 &amp;-0.96 &amp;0.74 &amp;-2.44 &amp;0.52 &amp;4.07 &amp;1.4 &amp;1.93 &amp;7.37 &amp;-1.26 &amp;0.73 &amp;-2.74 &amp;0.19 &amp;-1.25 &amp;0.73 \\\\-2.71 &amp;0.17 &amp;-0.96 &amp;0.73 &amp;-2.42 &amp;0.47 &amp;-0.01 &amp;0.74 &amp;-1.48 &amp;1.42 &amp;1.52 &amp;0.78 &amp;0 &amp;3.09 &amp;-0.17 &amp;0.22 &amp;-0.72 &amp;0.13 &amp;0.04 &amp;0.18 &amp;-0.33 \\\\0.45 &amp;0.05 &amp;0.18 &amp;-0.3 &amp;0.48 &amp;0.01 &amp;0.18 &amp;-0.38 &amp;0.4 &amp;-0.03 &amp;0.18 &amp;-0.43 &amp;0.32 &amp;0.11 &amp;0.19 &amp;-0.2 &amp;0.59 &amp;-286.93 &amp;3.9 &amp;-295.54 &amp;-280.49 \\\\ \\end{bmatrix}\\] color_scheme_set( scales::colour_ramp(colors = c( &quot;white&quot;, str_sub(clr_dark, 1, 7)))(seq(.4,1, length.out = 6)) ) p1 &lt;- mcmc_plot(brms_c13_model_chimp_multicluster, variable = c(&quot;^r_.*&quot;, &quot;^b_.*&quot;, &quot;^sd_.*&quot;), regex = TRUE, outer_size = 0.2, inner_size = 1, point_size = 2) + theme(axis.text.y = element_text(hjust = 0)) alternative version (d.i.y.) p2 &lt;- chimp_posterior %&gt;% pivot_longer(-(lp__:chain)) %&gt;% ggplot(aes(x = value, y = name)) + stat_pointinterval(point_interval = mean_qi, .width = c(.89, .5), shape = 21, point_size = 2, point_fill = clr0, color = clr0dd)+ theme(axis.title = element_blank()) p1 + p2 Comparing the group-level sigma parameters chimp_posterior %&gt;% pivot_longer(starts_with(&quot;sd&quot;)) %&gt;% ggplot(aes(x = value, color = name)) + geom_density(size = .5, aes(fill = after_scale(clr_alpha(color))), adjust = .5) + annotate(geom = &quot;text&quot;, x = 0.67, y = 2, label = &quot;Block&quot;, color = clr_dark) + annotate(geom = &quot;text&quot;, x = 2.725, y = 0.5, label = &quot;Actor&quot;, color = clr0d) + scale_color_manual(values = c(clr0d, clr_dark), guide = &quot;none&quot;) + scale_y_continuous(breaks = NULL) + labs(subtitle = &quot;sigma_group&quot;, x = NULL, y = &quot;density&quot;) + coord_cartesian(xlim = c(0, 4)) Since both the coefficient plots and the density plots indicate there is much more variability among the actor parameters than in the block parameters, we might fit a model that ignores the variation among the levels of block. brms_c13_model_chimp_no_block &lt;- brm( data = data_chimp_brms, family = binomial, bf(pulled_left | trials(1) ~ alpha + beta, alpha ~ 1 + (1 | actor), beta ~ 0 + treatment, nl = TRUE), prior = c(prior(normal(0, 0.5), nlpar = beta), prior(normal(0, 1.5), class = b, coef = Intercept, nlpar = alpha), prior(exponential(1), class = sd, group = actor, nlpar = alpha)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c13_model_chimp_no_block&quot;) brms_c13_model_chimp_multicluster &lt;- add_criterion(brms_c13_model_chimp_multicluster, &quot;waic&quot;) brms_c13_model_chimp_no_block &lt;- add_criterion(brms_c13_model_chimp_no_block, &quot;waic&quot;) loo_compare(brms_c13_model_chimp_multicluster, brms_c13_model_chimp_no_block, criterion = &quot;waic&quot;) %&gt;% print(simplify = FALSE) #&gt; elpd_diff se_diff elpd_waic se_elpd_waic #&gt; brms_c13_model_chimp_no_block 0.0 0.0 -265.6 9.6 #&gt; brms_c13_model_chimp_multicluster -0.4 0.8 -266.0 9.7 #&gt; p_waic se_p_waic waic se_waic #&gt; brms_c13_model_chimp_no_block 8.6 0.4 531.3 19.2 #&gt; brms_c13_model_chimp_multicluster 10.5 0.5 532.1 19.4 model_weights(brms_c13_model_chimp_multicluster, brms_c13_model_chimp_no_block, weights = &quot;waic&quot;) %&gt;% round(digits = 2) #&gt; brms_c13_model_chimp_multicluster brms_c13_model_chimp_no_block #&gt; 0.4 0.6 Even more clusters brms_c13_model_chimp_treatment &lt;- brm( data = data_chimp_brms, family = binomial, pulled_left | trials(1) ~ 1 + (1 | actor) + (1 | block) + (1 | treatment), prior = c(prior(normal(0, 1.5), class = Intercept), prior(exponential(1), class = sd)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c13_model_chimp_treatment&quot;) brms_chimp_posterior &lt;- as_draws_df(brms_c13_model_chimp_treatment) brms_chimp_posterior %&gt;% pivot_longer(starts_with(&quot;sd&quot;)) %&gt;% mutate(group = str_remove(name, &quot;sd_&quot;) %&gt;% str_remove(., &quot;__Intercept&quot;)) %&gt;% mutate(parameter = str_c(&quot;sigma[&quot;, group,&quot;]&quot;)) %&gt;% ggplot(aes(x = value, y = parameter)) + stat_halfeye(.width = .95, size = 1, fill = clr0d, adjust = 0.1) + labs(subtitle = &quot;The variation among treatment levels is small, but the\\nvariation among the levels of block is still the smallest.&quot;) + coord_cartesian(ylim = c(1.5, 3)) + theme(axis.text.y = element_text(hjust = 0), axis.title = element_blank()) brms_c13_model_chimp_treatment &lt;- add_criterion(brms_c13_model_chimp_treatment, &quot;waic&quot;) loo_compare(brms_c13_model_chimp_multicluster, brms_c13_model_chimp_no_block, brms_c13_model_chimp_treatment, criterion = &quot;waic&quot;) %&gt;% print(simplify = FALSE) #&gt; elpd_diff se_diff elpd_waic se_elpd_waic #&gt; brms_c13_model_chimp_no_block 0.0 0.0 -265.6 9.6 #&gt; brms_c13_model_chimp_multicluster -0.4 0.8 -266.0 9.7 #&gt; brms_c13_model_chimp_treatment -0.9 0.8 -266.6 9.6 #&gt; p_waic se_p_waic waic se_waic #&gt; brms_c13_model_chimp_no_block 8.6 0.4 531.3 19.2 #&gt; brms_c13_model_chimp_multicluster 10.5 0.5 532.1 19.4 #&gt; brms_c13_model_chimp_treatment 10.8 0.5 533.1 19.2 model_weights(brms_c13_model_chimp_multicluster, brms_c13_model_chimp_no_block, brms_c13_model_chimp_treatment, weights = &quot;loo&quot;) %&gt;% round(digits = 2) #&gt; brms_c13_model_chimp_multicluster brms_c13_model_chimp_no_block #&gt; 0.32 0.49 #&gt; brms_c13_model_chimp_treatment #&gt; 0.19 14.7.4 Divergent Transitions and Non-Centered Priors Non-centered chimpanzees Because we only fit this model using the non-centered parameterization, we won’t be able to fully reproduce McElreath’s Figure 13.6. But we can still plot our effective sample sizes. Recall that unlike the way rethinking only reports n_eff, brms now reports both bulk_ess and tail_ess (see Vehtari, Gelman, et al., 2019). At the moment, brms does not offer a convenience function that allows users to collect those values in a data frame. However you can do so with help from the posterior package (Bürkner et al., 2020), which has not made its way to CRAN, yet, but can be downloaded directly from GitHub. as_draws_df(brms_c13_model_chimp_multicluster) %&gt;% summarise_draws() %&gt;% ggplot(aes(x = ess_bulk, y = ess_tail)) + geom_abline(linetype = 3, color = clr_dark) + geom_point(color = clr0dd, fill = clr0, size = 2, shape = 21) + xlim(0, 4700) + ylim(0, 4700) + ggtitle(&quot;Effective sample size summaries for b13.4&quot;, subtitle = &quot;ess_bulk is on the x and ess_tail is on the y&quot;) + theme(plot.subtitle = element_text(size = 10), plot.title = element_text(size = 11.5), plot.title.position = &quot;plot&quot;) 14.7.5 Multilevel Posterior Predictions Posterior prediction for same clusters treatment_levels &lt;- c(&quot;R|N&quot;, &quot;L|N&quot;, &quot;R|P&quot;, &quot;L|P&quot;) new_chimp &lt;- data_chimp %&gt;% distinct(treatment) %&gt;% mutate(actor = chimp, block = 1L) chimp_2_fit &lt;- fitted(brms_c13_model_chimp_multicluster, newdata = new_chimp) %&gt;% as_tibble() %&gt;% bind_cols(new_chimp,.) chimp_2_data &lt;- data_chimp %&gt;% filter(actor == chimp) %&gt;% group_by(treatment) %&gt;% summarise(prob = mean(pulled_left)) %&gt;% ungroup() p1 &lt;- chimp_2_fit %&gt;% ggplot(aes(x = treatment, y = Estimate)) + geom_smooth(stat = &#39;identity&#39;, aes(ymin = Q2.5, ymax = Q97.5), fill = clr0dd, color = clr0dd, size = .5) + geom_point(data = chimp_2_data, aes(y = prob), color = clr0dd , fill = clr0, shape = 21) + ggtitle(&quot;Chimp #2&quot;, subtitle = &quot;The posterior mean and 95%\\nintervals are the blue line\\nand orange band, respectively.\\nThe empirical means are\\nthe charcoal dots.&quot;) + coord_cartesian(ylim = c(.75, 1)) p2 &lt;- (brms_chimp_posterior_multi &lt;- as_draws_df(brms_c13_model_chimp_multicluster) %&gt;% as_tibble()) %&gt;% transmute(actor_5 = `r_actor__alpha[5,Intercept]`) %&gt;% ggplot(aes(x = actor_5)) + geom_density(size = .5, color = clr0dd, fill = fll0) + scale_y_continuous(breaks = NULL) + ggtitle(&quot;Chimp #5&#39;s density&quot;) chimp_5_fit &lt;- brms_chimp_posterior_multi %&gt;% pivot_longer(b_beta_treatment1:b_beta_treatment4) %&gt;% mutate(fitted = inv_logit_scaled(b_alpha_Intercept + value + `r_actor__alpha[1,Intercept]` + `r_block__alpha[1,Intercept]`), treatment = treatment_levels[str_sub(name, -1,-1) %&gt;% as.integer()] %&gt;% factor(levels = treatment_levels)) %&gt;% select(name:treatment) %&gt;% group_by(treatment) %&gt;% tidybayes::mean_qi(fitted) chimp_5_data &lt;- data_chimp %&gt;% filter(actor == 5) %&gt;% group_by(treatment) %&gt;% summarise(prob = mean(pulled_left)) %&gt;% ungroup() %&gt;% mutate(treatment = treatment_levels[treatment] %&gt;% factor(levels = treatment_levels)) p3 &lt;- chimp_5_fit %&gt;% ggplot(aes(x = treatment, y = fitted)) + geom_smooth(stat = &#39;identity&#39;, aes(ymin = .lower, ymax = .upper, group = 1), fill = clr0dd, color = clr0dd, size = .5) + geom_point(data = chimp_5_data, aes(y = prob), color = clr0dd , fill = clr0, shape = 21) + ggtitle(&quot;Chimp #5&quot;, subtitle = &quot;This plot is like the last except\\nwe did more by hand.&quot;) + coord_cartesian(ylim = 0:1) p1 + p2 + p3 Posterior prediction for new clusters chimp_mean_fitted &lt;- brms_chimp_posterior_multi %&gt;% pivot_longer(b_beta_treatment1:b_beta_treatment4) %&gt;% mutate(fitted = inv_logit_scaled(b_alpha_Intercept + value)) %&gt;% mutate(treatment = treatment_levels[str_sub(name, -1,-1) %&gt;% as.integer()] %&gt;% factor(levels = treatment_levels)) %&gt;% select(name:treatment) %&gt;% group_by(treatment) %&gt;% # note we&#39;re using 80% intervals mean_qi(fitted, .width = .89) p1 &lt;- chimp_mean_fitted %&gt;% ggplot(aes(x = treatment, y = fitted, group = 1)) + geom_smooth(stat = &#39;identity&#39;, aes(ymin = .lower, ymax = .upper, group = 1), fill = clr0dd, color = clr0dd, size = .5)+ labs(subtitle = &quot;Average actor&quot;) + coord_cartesian(ylim = 0:1) chimp_marginal_fitted &lt;- brms_chimp_posterior_multi %&gt;% # simulated chimpanzees mutate(a_sim = rnorm(n(), mean = b_alpha_Intercept, sd = sd_actor__alpha_Intercept)) %&gt;% pivot_longer(b_beta_treatment1:b_beta_treatment4) %&gt;% mutate(fitted = inv_logit_scaled(a_sim + value), treatment = treatment_levels[str_sub(name, -1,-1) %&gt;% as.integer()] %&gt;% factor(levels = treatment_levels)) %&gt;% group_by(treatment) %&gt;% # note we&#39;re using 80% intervals mean_qi(fitted, .width = .8) p2 &lt;- chimp_marginal_fitted %&gt;% ggplot(aes(x = treatment, y = fitted, group = 1)) + geom_smooth(stat = &#39;identity&#39;, aes(ymin = .lower, ymax = .upper, group = 1), fill = clr0dd, color = clr0dd, size = .5)+ labs(subtitle = &quot;Marginal of actor&quot;) + coord_cartesian(ylim = 0:1) n_chimps &lt;- 100 set.seed(42) chimp_100_random_fitted &lt;- brms_chimp_posterior_multi %&gt;% mutate(iter = 1:n()) %&gt;% slice_sample(n = n_chimps) %&gt;% # simulated chimpanzees mutate(a_sim = rnorm(n(), mean = b_alpha_Intercept, sd = sd_actor__alpha_Intercept)) %&gt;% pivot_longer(b_beta_treatment1:b_beta_treatment4) %&gt;% mutate(fitted = inv_logit_scaled(a_sim + value), treatment = treatment_levels[str_sub(name, -1,-1) %&gt;% as.integer()] %&gt;% factor(levels = treatment_levels)) %&gt;% select(iter:treatment) p3 &lt;- chimp_100_random_fitted %&gt;% ggplot(aes(x = treatment, y = fitted, group = iter)) + geom_line(alpha = .4, color = clr0dd) + labs(subtitle = &quot;100 simulated actors&quot;) + coord_cartesian(ylim = 0:1) p1 + p2 + p3 Let’s use fitted() this time (alternative approach for new clusters) new_chimp_treatment &lt;- distinct(data_chimp, treatment) chimp_mean_fitted2 &lt;- fitted(brms_c13_model_chimp_multicluster, newdata = new_chimp_treatment, re_formula = NA, probs = c(.1, .9)) %&gt;% as_tibble() %&gt;% bind_cols(new_chimp_treatment, .) %&gt;% mutate(treatment = treatment_levels[treatment] %&gt;% factor(levels = treatment_levels)) p4 &lt;- chimp_mean_fitted2 %&gt;% ggplot(aes(x = treatment, y = Estimate, group = 1)) + geom_smooth(stat = &#39;identity&#39;, aes(ymin = Q10, ymax = Q90), fill = fll_current(), color = clr_current, size = .5) + labs(subtitle = &quot;Average actor&quot;) + coord_cartesian(ylim = 0:1) chimp_marginal_fitted2 &lt;- fitted(brms_c13_model_chimp_multicluster, newdata = new_chimp_treatment, probs = c(.1, .9), allow_new_levels = T, sample_new_levels = &quot;gaussian&quot;) %&gt;% as_tibble() %&gt;% bind_cols(new_chimp_treatment,.) %&gt;% mutate(treatment = treatment_levels[treatment] %&gt;% factor(levels = treatment_levels)) p5 &lt;- chimp_marginal_fitted2 %&gt;% ggplot(aes(x = treatment, y = Estimate, group = 1)) + geom_smooth(stat = &#39;identity&#39;, aes(ymin = Q10, ymax = Q90), fill = fll_current(), color = clr_current, size = .5) + labs(subtitle = &quot;Marginal of actor&quot;) + coord_cartesian(ylim = 0:1) new_chimp_random &lt;- new_chimp %&gt;% # define 100 new actors expand(actor = str_c(&quot;new&quot;, 1:n_chimps), treatment) %&gt;% # this adds a row number, which will come in handy, later mutate(row = 1:n()) set.seed(42) chimp_100_random_fitted2 &lt;- fitted(brms_c13_model_chimp_multicluster, newdata = new_chimp_random, allow_new_levels = TRUE, sample_new_levels = &quot;gaussian&quot;, summary = FALSE, ndraws = n_chimps) p6 &lt;- chimp_100_random_fitted2 %&gt;% as_tibble() %&gt;% # name the columns by the `row` values in `nd` set_names(pull(new_chimp_random, row)) %&gt;% # add an iteration index mutate(iter = 1:n()) %&gt;% # make it long pivot_longer(-iter, names_to = &quot;row&quot;) %&gt;% mutate(row = as.integer(row)) %&gt;% # add the new data left_join(new_chimp_random, by = &quot;row&quot;) %&gt;% # extract the numbers from the names of the new actors mutate(actor_number = str_extract(actor, &quot;\\\\d+&quot;) %&gt;% as.integer()) %&gt;% # only keep the posterior iterations that match the `actor_number` values filter(actor_number == iter) %&gt;% # add the `treatment` labels mutate(treatment = treatment_levels[treatment] %&gt;% factor(levels = treatment_levels)) %&gt;% ggplot(aes(x = treatment, y = value, group = actor)) + geom_line(alpha = .4, color = clr_current) + labs(subtitle = &quot;100 simulated actors&quot;) p4 + p5 + p6 Post-stratification If you have estimates \\(p_{i}\\) for each relevant demographic category \\(i\\), the post-stratified prediction for the whole population just re-weights these estimates using the number of individuals \\(N_{i}\\) in each category with the formula \\[\\frac{\\sum_{i}N_{i}p_{i}}{\\sum_{i}N_{i}}\\] Within the multilevel context, this is called multilevel regression and post-stratification (MRP, pronounced “Mister P”). 14.7.6 Bonus: Post-stratification in an example Meet the data load(&quot;data/mrp_data_ch13.rds&quot;) data_names &lt;- d data_cell_counts &lt;- cell_counts rm(d, cell_counts) data_cell_counts %&gt;% ggplot(aes(x = n)) + geom_histogram(binwidth = 2e3, fill = fll0, color = clr0d) + scale_x_continuous(breaks = 0:3 * 1e5, labels = function(x){if_else(x == 0, as.character(x), str_c(1e-3 * x, &quot;K&quot;))}) Settle the MR part of MRP set.seed(42) tibble(n = rnorm(1e6, -1, 1)) %&gt;% mutate(p = inv_logit_scaled(n)) %&gt;% ggplot(aes(x = p)) + geom_density(color = clr0dd, fill = fll0) + scale_y_continuous(breaks = NULL) brms_c13_model_names &lt;- brm( data = data_names, family = binomial, kept_name | trials(1) ~ 1 + (1 | age_group) + (1 | decade_married) + (1 | educ_group) + (1 | state_name), prior = c(prior(normal(-1, 1), class = Intercept), prior(exponential(1), class = sd)), iter = 2000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = .98), seed = 42, file = &quot;brms/brms_c13_model_names&quot;) fixef(brms_c13_model_names) %&gt;% data.frame() %&gt;% knit_precis() param Estimate Est.Error Q2.5 Q97.5 Intercept -0.74 0.63 -2 0.46 as_draws_df(brms_c13_model_names) %&gt;% select(starts_with(&quot;sd_&quot;)) %&gt;% set_names(str_c(&quot;sigma[&quot;, c(&quot;age&quot;, &quot;decade~married&quot;, &quot;education&quot;, &quot;state&quot;), &quot;]&quot;)) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% median_qi(.width = seq(from = .5, to = .9, by = .1)) %&gt;% ggplot(aes(x = value, xmin = .lower, xmax = .upper, y = reorder(name, value))) + geom_interval(aes(alpha = .width), color = clr_dark) + scale_alpha_continuous(&quot;CI width&quot;, range = c(.7, .15)) + scale_y_discrete() + xlim(0, NA) + labs(y = NULL, x = NULL) Post-stratify to put the P in MRP For simplicity, we will only focus on the results for age_group and state. However, we will examine the results for each using three estimation methods: the empirical proportions, the naïve results from the multilevel model, and the MRP estimates. Estimates by age group name_levels &lt;- c(&quot;raw data&quot;, &quot;multilevel&quot;, &quot;MRP&quot;) p1 &lt;- data_names %&gt;% # compute the proportions from the data group_by(age_group, kept_name) %&gt;% summarise(n = n()) %&gt;% group_by(age_group) %&gt;% mutate(prop = n/sum(n), type = factor(&quot;raw data&quot;, levels = name_levels)) %&gt;% filter(kept_name == 1, age_group &lt; 80, age_group &gt; 20) %&gt;% ggplot(aes(x = prop, y = age_group)) + geom_point(color = clr0dd, fill = clr0, shape = 21, size = 1.5) + scale_x_continuous(breaks = c(0, .5, 1), limits = 0:1) + facet_wrap(~ type) new_names &lt;- distinct(data_names, age_group) %&gt;% arrange(age_group) p2 &lt;- fitted(brms_c13_model_names, re_formula = ~ (1 | age_group), newdata = new_names) %&gt;% as_tibble() %&gt;% bind_cols(new_names, .) %&gt;% mutate(prop = Estimate, type = factor(&quot;multilevel&quot;, levels = name_levels)) %&gt;% ggplot(aes(x = prop, xmin = Q2.5, xmax = Q97.5, y = age_group)) + geom_linerange(color = clr0dd, size = .5) + geom_point(color = clr0dd, size = 1.5, fill = clr0, shape = 21) + scale_x_continuous(breaks = c(0, .5, 1), limits = c(0, 1)) + # scale_y_discrete(labels = NULL) + facet_wrap(~ type) + labs(y = NULL) age_prop &lt;- data_cell_counts %&gt;% group_by(age_group) %&gt;% mutate(prop = n / sum(n)) %&gt;% ungroup() names_predicted &lt;- add_predicted_draws(brms_c13_model_names, newdata = age_prop %&gt;% filter(age_group &gt; 20, age_group &lt; 80, decade_married &gt; 1969), allow_new_levels = TRUE) Next comes the MRP magic. If we group the results by age_group and .draw, we can sum the product of the posterior predictions and the weights, which will leave us with 4,000 stratified posterior draws for each of the 11 levels of age_group. This is the essence of the post-stratification equation McElreath presented in Section 13.5.3, \\[\\frac{\\sum_{i}N_{i}p_{i}}{\\sum_{i}N_{i}}\\] We will follow Alexander and call these summary values kept_name_predict. We then complete the project by grouping by age_group and summarizing each stratified posterior predictive distribution by its mean and 95% interval. names_predicted &lt;- names_predicted %&gt;% group_by(age_group, .draw) %&gt;% summarise(kept_name_predict = sum(.prediction * prop)) %&gt;% group_by(age_group) %&gt;% mean_qi(kept_name_predict) p3 &lt;- names_predicted %&gt;% mutate(type = factor(&quot;MRP&quot;, levels = name_levels)) %&gt;% ggplot(aes(x = kept_name_predict, xmin = .lower, xmax = .upper, y = age_group)) + geom_linerange(color = clr_current, size = .5) + geom_point(color = clr_current, fill = clr_lighten(clr_current), size = 1.5, shape = 21) + scale_x_continuous(breaks = c(0, .5, 1), limits = 0:1) + facet_wrap(~ type) + labs(y = NULL) p1 + p2 + p3 + plot_annotation(title = &quot;Proportion of women keeping name after marriage, by age&quot;, subtitle = &quot;Proportions are on the x-axis and age groups are on the y-axis.&quot;) Both multilevel and MRP estimates tended to be a little lower than the raw proportions, particularly for women in the younger age groups. Alexander mused this was “likely due to the fact that the survey has an over-sample of highly educated women, who are more likely to keep their name.” The MRP estimates were more precise than the multilevel predictions, which averaged across the grouping variables other than age. All three estimates show something of an inverted U-shape curve across age, which Alexander noted “is consistent with past observations that there was a peak in name retention in the 80s and 90s.” Estimates by US state Now we turn out attention to variation across states. The workflow, here, will only deviate slightly from what we just did. This time, of course, we will be grouping the estimates by state_name instead of by age_group. library(sf) us_sf &lt;- read_sf(&quot;~/work/geo_store/USA/usa_states_albers_revised.gpkg&quot;) %&gt;% mutate(statename = str_to_title(name)) p1 &lt;- data_names %&gt;% group_by(state_name, kept_name) %&gt;% summarise(n = n()) %&gt;% group_by(state_name) %&gt;% mutate(prop = n/sum(n)) %&gt;% filter(kept_name == 1, state_name != &quot;puerto rico&quot;) %&gt;% mutate(type = factor(&quot;raw data&quot;, levels = name_levels), statename = str_to_title(state_name)) %&gt;% left_join(us_sf,.) %&gt;% ggplot(aes(color = prop)) # + # geom_sf(aes(fill = after_scale(clr_alpha(color,.8)))) + # scale_color_viridis_c(&quot;proportion\\nkeeping\\nname&quot;, option = &quot;B&quot;, limits = c(0, 0.8)) + # theme(legend.position = &quot;none&quot;) + # facet_wrap(~ type) new_names_state &lt;- distinct(data_names, state_name) p2 &lt;- fitted(brms_c13_model_names, re_formula = ~ (1 | state_name), newdata = new_names_state) %&gt;% as_tibble() %&gt;% bind_cols(new_names_state,.) %&gt;% filter(state_name != &quot;puerto rico&quot;) %&gt;% mutate(prop = Estimate, type = factor(&quot;multilevel&quot;, levels = name_levels), statename = str_to_title(state_name)) %&gt;% left_join(us_sf,.) %&gt;% ggplot(aes(color = prop)) state_prop &lt;- data_cell_counts %&gt;% group_by(state_name) %&gt;% mutate(prop = n/sum(n)) %&gt;% ungroup() p3 &lt;- add_predicted_draws(brms_c13_model_names, newdata = state_prop %&gt;% filter(age_group &gt; 20, age_group &lt; 80, decade_married &gt; 1969), allow_new_levels = TRUE) %&gt;% group_by(state_name, .draw) %&gt;% summarise(kept_name_predict = sum(.prediction * prop)) %&gt;% group_by(state_name) %&gt;% mean_qi(kept_name_predict) %&gt;% mutate(prop = kept_name_predict, type = factor(&quot;MRP&quot;, levels = name_levels), statename = str_to_title(state_name)) %&gt;% left_join(us_sf,.) %&gt;% ggplot(aes(color = prop)) p1 + p2 + p3 + plot_annotation(title = &quot;Proportion off Women Keeping Name after Marriage, by State&quot;, theme = theme(plot.margin = margin(0.2, 0, 0.01, 0, &quot;cm&quot;))) + plot_layout(guides = &quot;collect&quot;, nrow = 1) &amp; geom_sf(aes(fill = after_scale(clr_alpha(color,.8)))) &amp; # scale_color_viridis_c(&quot;proportion keeping name&quot;, option = &quot;B&quot;, limits = c(0, 0.8)) &amp; scale_color_gradientn(&quot;proportion keeping name&quot;, colours = c(clr_dark, clr0, clr_saturate(clr_current, .3)), limits = c(0, 0.8)) &amp; guides(color = guide_colorbar(title.position = &quot;top&quot;, barheight = unit(5, &quot;pt&quot;), barwidth = unit(.9, &quot;npc&quot;))) &amp; theme(legend.position = &quot;bottom&quot;) &amp; facet_wrap(~ type) Remember how small the posterior for \\(\\sigma_{state}\\) was relative to the other \\(\\sigma_{&lt;group&gt;}\\) posteriors? We said that would imply more aggressive regularization across states. You can really see that regularization in the panels showing the multilevel and MRP estimates. They are much more uniform than the proportions from the raw data, which are all over the place. This is why you use multilevel models and/or stratify. When you divide the responses up at the state level, the proportions get jerked all around due to small and unrepresentative samples. Even with the regularization from the multilevel partial pooling, you can still see some interesting differences in the multilevel and MRP panels. Both suggest women keep their maiden names in relatively low proportions in Utah and relatively high proportions in New York. For those acquainted with American culture, this shouldn’t be a great surprise. 14.8 pymc3 section × "],["rethinking-chapter-14.html", "15 Rethinking: Chapter 14 15.1 Varying Slopes by Construction 15.2 Advanced Varying Slopes 15.3 Instruments and Causal Design 15.4 Social Relations and Correlated Varying Effects 15.5 Continous Categories and the Gaussian Process 15.6 Homework 15.7 {brms} section 15.8 pymc3 section", " 15 Rethinking: Chapter 14 Adventures in Covariance by Richard McElreath, building on the Summaries by Solomon Kurz. This is the essence of the general exchangeable effects strategy: Any batch of parameters with exchangeable index values can and probably should be pooled. There is a fact that will allow us to squeeze even more information out of the data: Cafés covary in their intercepts and slopes. Eg. Cafés with generally short waiting times (intercept) can not improve as much throughout the day (slope) as those that exhibit longer average waiting times. 15.1 Varying Slopes by Construction 15.1.1 Simulate the Population Setting averages, standard deviations and correlation of intercept and slope parameters. alpha &lt;- 3.5 # average morning waiting time beta &lt;- -1 # average difference between morning and afternoon waiting time sigma_alpha &lt;- 1 # std dev in intercepts sigma_beta &lt;- 0.5 # std dev in slopes rho &lt;- -.7 # correlation between intercepts and slopes Building the 2-dimensional multivariate gaussian distribution from the defined parameters - for this we need the 2-by-2 matrix of variances and covariances (aka. the covariance matrix \\(\\Sigma\\)): \\[ \\Sigma = \\left[ { \\begin{array}{cc} \\textrm{var. of intercepts} &amp; \\textrm{covar. of int. and slp.} \\\\ \\textrm{covar. of int. and slp.} &amp; \\textrm{var. of slopes}\\\\ \\end{array} } \\right] \\] or more concise \\[ \\Sigma = \\left[ { \\begin{array}{cc} \\sigma_{\\alpha}^2 &amp; \\sigma_{\\alpha} \\sigma_{\\beta} \\rho \\\\ \\sigma_{\\alpha} \\sigma_{\\beta} \\rho &amp; \\sigma_{\\beta}^2 \\\\ \\end{array} } \\right] \\] constructing the covariance matrix Mu &lt;- c(alpha, beta) cov_ab &lt;- sigma_alpha * sigma_beta * rho (Sigma &lt;- matrix( c(sigma_alpha, cov_ab, cov_ab, sigma_beta), nrow = 2 )) \\[\\begin{bmatrix} 1 &amp;-0.35 \\\\-0.35 &amp;0.5 \\\\ \\end{bmatrix}\\] Alternative covariance matrix construction: sigmas &lt;- c(sigma_alpha, sigma_beta) # standard deviations Rho &lt;- matrix( c(1, rho, rho, 1), nrow = 2 ) # correlation matrix Sigma &lt;- diag(sigmas) %*% Rho %*% diag(sigmas) # matrix multiplication to get covariance matrix Simulating cafes library(MASS) set.seed(42) n_cafes &lt;- 20 vary_effects &lt;- mvrnorm(n = n_cafes, mu = Mu, Sigma = Sigma) %&gt;% data.frame() %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;alpha_cafe&quot;, &quot;beta_cafe&quot;)) p1 &lt;- vary_effects %&gt;% ggpairs(lower = list(continuous = wrap(&quot;dot_no_facet&quot;, fill = fll0, color = clr_dark, fill = clr_alpha(clr_dark), shape = 21, size = 1.5)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll0, color = clr_dark, fill = clr_alpha(clr_dark), adjust = .7)), upper = list(continuous = wrap(my_upper , size = 4, color = &quot;black&quot;, family = fnt_sel)) ) library(ellipse) p2 &lt;- tibble(level = c(.1, .3, .5, .8, .99)) %&gt;% mutate(ellipse = purrr::map(level, function(level){ ellipse(Sigma, centre = Mu, level = level) %&gt;% data.frame() %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;alpha_cafe&quot;, &quot;beta_cafe&quot;)) })) %&gt;% unnest(ellipse) %&gt;% ggplot(aes(x = alpha_cafe, y = beta_cafe)) + geom_path(aes(group = factor(level), alpha = -level), color = clr_dark, size = .25) + geom_point(data = vary_effects, color = clr0dd, fill = clr0, shape = 21, size = 1.5) + theme(legend.position = &quot;none&quot;) cowplot::plot_grid(p2, ggmatrix_gtable(p1)) 15.1.2 Simulate Observations n_visits &lt;- 10 sigma &lt;- 0.5 # std dev within cafes set.seed(42) # used to replicate example data_cafe &lt;- vary_effects %&gt;% mutate(cafe = 1:n_cafes) %&gt;% expand(nesting(cafe, alpha_cafe, beta_cafe), visit = 1:n_visits) %&gt;% mutate(afternoon = rep(0:1, times = n() / 2)) %&gt;% mutate(mu = alpha_cafe + beta_cafe * afternoon) %&gt;% mutate(waiting_time = rnorm(n = n(), mean = mu, sd = sigma)) data_cafe %&gt;% mutate(afternoon = ifelse(afternoon == 0, &quot;M&quot;, &quot;A&quot;), day = rep(rep(1:5, each = 2), times = n_cafes)) %&gt;% filter(cafe %in% c(12, 19)) %&gt;% mutate(cafe = str_c(&quot;café #&quot;, cafe)) %&gt;% ggplot(aes(x = visit, y = waiting_time, group = day)) + geom_line(color = clr0dd) + geom_point(aes(color = afternoon), size = 2) + scale_color_manual(values = c(clr0dd, clr_current), guide = &quot;none&quot;) + scale_x_continuous(NULL, breaks = 1:10, labels = rep(c(&quot;M&quot;, &quot;A&quot;), times = 5)) + scale_y_continuous(&quot;wait time in minutes&quot;, limits = c(0, NA)) + facet_wrap(~ cafe, nrow = 1) This data is well suited for a varying slopes model: there are multiple clusters (the individual cafes) each cluster is observed under a different conditions \\(\\rightarrow\\) it is thus possible to estimate both individual intercepts as well as individual slopes 🤓 Furthermore, this example is balanced - but this is not a requirement (in fact in un-balanced designs varying effects model can effectively help with the inference for the lesser sampled clusters due to partial pooling). 15.1.3 The Varying Slopes Model Here, the joint population of slopes and intercepts appears: \\[ \\begin{array}{rclr} W_{i} &amp; \\sim &amp; \\textrm{Normal}(\\mu_{i}, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_{i} &amp; = &amp; \\alpha_{CAFÉ[i]} + \\beta_{CAFÉ[i]} A_{i} &amp; \\textrm{[linear model]}\\\\ \\left[\\begin{array}{cc}\\alpha_{CAFÉ} \\\\ \\beta_{CAFÉ}\\end{array} \\right] &amp; \\sim &amp;\\textrm{MVNormal} \\left( \\left[ \\begin{array}{c} \\alpha\\\\ \\beta\\end{array} \\right], S \\right) &amp; \\textrm{[population of varying effects]}\\\\ S &amp; = &amp; \\left( \\begin{array}{cc}\\sigma_{\\alpha} &amp; 0\\\\ 0 &amp; \\sigma_{\\beta}\\end{array}\\right) R \\left( \\begin{array}{cc}\\sigma_{\\alpha} &amp; 0\\\\ 0 &amp; \\sigma_{\\beta}\\end{array}\\right) &amp; \\textrm{[construct covariance matrix]}\\\\ \\alpha &amp; \\sim &amp; \\textrm{Normal}(5, 2) &amp; \\textrm{[prior for average intercept]}\\\\ \\beta &amp; \\sim &amp; \\textrm{Normal}(-1, 0.5) &amp; \\textrm{[prior for average slope]}\\\\ \\sigma &amp; \\sim &amp; \\textrm{Exponential}(1) &amp; \\textrm{[prior for stddev within cafés]}\\\\ \\sigma_{\\alpha} &amp; \\sim &amp; \\textrm{Exponential}(1) &amp; \\textrm{[prior for stddev among intercepts]}\\\\ \\sigma_{\\beta} &amp; \\sim &amp; \\textrm{Exponential}(1) &amp; \\textrm{[prior for stddev among slopes]}\\\\ R &amp; \\sim &amp; \\textrm{LKJcorr}(2) &amp; \\textrm{[prior for correlation matrix]}\\\\ \\end{array} \\] \\(R\\) is a prior for the correlation matrix, thus a distribution of matrices. In this case it is a \\(2\\times2\\) matrix looking like this (needing just one parameter - \\(\\rho\\) - for it’s definition): \\[ R = \\left(\\begin{array}{cc}1 &amp; \\rho \\\\ \\rho &amp; 1\\end{array}\\right) \\] In general, the \\(LKJcorr\\) distribution can be thought of as weakly informative prior on \\(\\rho\\), that is skeptical of extreme correlations (-1 or 1). It can be used as regularizing prior for correlation matrices. It’s single parameter \\(\\eta\\) controls how skeptical the prior is (1 would be completely flat). library(rethinking) n_matrices &lt;- 1e4 eta_rlk &lt;- function(eta){ rlkjcorr(n = n_matrices, K = 2, eta = eta) %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;tl&quot;, &quot;bl&quot;, &quot;tr&quot;,&quot;br&quot;)) } p1 &lt;- tibble(eta = c(.5, 1, 2, 4)) %&gt;% mutate(R = purrr::map(eta, eta_rlk)) %&gt;% unnest(R) %&gt;% ggplot(aes(x = bl, group = eta, color = factor(eta))) + geom_density(adjust = .65, aes(fill = after_scale(clr_alpha(color, .3)))) + scale_color_manual(&quot;eta&quot;, values = c(clr2, clr0d, clr_dark, clr_current), guide = guide_legend(nrow = 2, keyheight = unit(7, &quot;pt&quot;))) + labs(x = NULL, subtitle = &quot;LKJcorr distribution for varying eta&quot;) + theme(legend.position = c(1, 1), legend.justification = c(1,1), legend.direction = &quot;horizontal&quot;) Fitting the model data_cafe_list &lt;- data_cafe %&gt;% dplyr::select(cafe, visit, afternoon, waiting_time) %&gt;% as.list() set.seed(42) model_cafe &lt;- ulam( flist = alist( waiting_time ~ normal( mu, sigma ), mu &lt;- alpha_cafe[cafe] + beta_cafe[cafe] * afternoon, c(alpha_cafe, beta_cafe)[cafe] ~ multi_normal( c(alpha, beta ), Rho, sigma_cafe ), alpha ~ normal(5, 2), beta ~ normal(-1, 0.5), sigma_cafe ~ exponential(1), sigma ~ exponential(1), Rho ~ lkj_corr(2) ), data = data_cafe_list, cores = 4, chains = 4, log_lik = TRUE ) p2 &lt;- extract.samples(model_cafe) %&gt;% data.frame() %&gt;% as_tibble() %&gt;% ggplot() + geom_vline(xintercept = 0, color =clr_dark, linetype = 3) + geom_density(aes(x = Rho.2, color = &quot;posterior&quot;, fill = after_scale(clr_alpha(color))), adjust = .6) + geom_density(data = eta_rlk(eta = 2), aes(x = bl, color = &quot;prior&quot;, fill = after_scale(clr_alpha(color))), adjust = .6) + scale_color_manual(&quot;&quot;, values = c(prior = clr0d, posterior = clr_current), guide = guide_legend(nrow = 2, keyheight = unit(7, &quot;pt&quot;))) + labs(x = NULL, subtitle = &quot;correlation between intercept and slope&quot;) + theme(legend.position = c(1, 1), legend.justification = c(1,1), legend.direction = &quot;horizontal&quot;) p1 + p2 Shrinkage to the inferred prior learned from the variance and covariance of the intercepts and slopes: un_pooled_params &lt;- data_cafe %&gt;% group_by(cafe, afternoon) %&gt;% summarise(mean = mean(waiting_time)) %&gt;% ungroup() %&gt;% mutate(afternoon = ifelse(afternoon == 0, &quot;Intercept&quot;, &quot;Slope&quot;)) %&gt;% pivot_wider(names_from = afternoon, values_from = mean) %&gt;% mutate(Slope = Slope - Intercept) partially_pooled_params &lt;- extract.samples(model_cafe) %&gt;% data.frame() %&gt;% as_tibble() %&gt;% dplyr::select(contains(&quot;alpha&quot;), contains(&quot;beta&quot;)) %&gt;% dplyr::select(-c(alpha, beta)) %&gt;% summarise(across(everything(), mean)) %&gt;% pivot_longer(everything(), names_to = c(&quot;param&quot;, &quot;cafe&quot;), names_sep = &quot;\\\\.&quot;) %&gt;% mutate(param = str_c(param, &quot;_post&quot;), cafe = as.integer(cafe)) %&gt;% pivot_wider(names_from = param, values_from = value) posterior_mean_bivariate_gauss &lt;- extract.samples(model_cafe) %&gt;% as_tibble() %&gt;% dplyr::select(beta, alpha, Rho, sigma_cafe) %&gt;% summarise(mu_est = list(c(mean(alpha), mean(beta))), rho_est = mean(Rho[,1,2]), salpha_est = mean(sigma_cafe[,1]), sbeta_est = mean(sigma_cafe[,2])) %&gt;% mutate(Sigma_est = list(matrix( c(salpha_est ^ 2, cov_ab, cov_ab, sbeta_est ^ 2), ncol = 2))) %&gt;% as.list() posterior_mean_ellipse &lt;- tibble(level = c(.1, .3, .5, .8, .99)) %&gt;% mutate(ellipse = purrr::map(level, function(level){ ellipse(posterior_mean_bivariate_gauss$Sigma_est[[1]], centre = posterior_mean_bivariate_gauss$mu_est[[1]], level = level) %&gt;% data.frame() %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;alpha_cafe&quot;, &quot;beta_cafe&quot;)) })) %&gt;% unnest(ellipse) %&gt;% mutate(rn = row_number()) %&gt;% arrange(rn) p1 &lt;- un_pooled_params %&gt;% left_join(partially_pooled_params) %&gt;% ggplot() + geom_polygon(data = posterior_mean_ellipse, aes(x = alpha_cafe, y = beta_cafe, group = level), color = clr_dark, fill = clr_alpha(clr_current, .1), size = .2) + geom_point(aes(x = Intercept, y = Slope), shape = 1, size = 2, color = clr0dd) + geom_point(aes(x = alpha_cafe_post, y = beta_cafe_post), shape = 21, size = 2, color = clr_current, fill = clr_alpha(clr_current)) + geom_segment(aes(x = Intercept, y = Slope, xend = alpha_cafe_post, yend = beta_cafe_post), arrow = arrow(type = &quot;closed&quot;, length = unit(4, &quot;pt&quot;)), size = .25) + coord_cartesian(xlim = c(0, 7), ylim = c(-2.2, .5), expand = 0) Peripheral points experience stronger shrinkage towards the center, but this happens not in a direction that is straight towards the center (shrinking the slope also results in shrinking the intercept because of the covariance). sigma_distrib_sim &lt;- mvrnorm(n = 1e4, mu = posterior_mean_bivariate_gauss$mu_est[[1]], Sigma = posterior_mean_bivariate_gauss$Sigma_est[[1]]) %&gt;% data.frame() %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;alpha&quot;, &quot;beta&quot;)) %&gt;% mutate(wait_morning = alpha, wait_afternon = alpha + beta) %&gt;% dplyr::select(starts_with(&quot;wait&quot;)) %&gt;% cov(.) mu_sim &lt;- c(posterior_mean_bivariate_gauss$mu_est[[1]][1], sum(posterior_mean_bivariate_gauss$mu_est[[1]])) sim_mean_ellipse &lt;- tibble(level = c(.1, .3, .5, .8, .99)) %&gt;% mutate(ellipse = purrr::map(level, function(level){ ellipse(sigma_distrib_sim, centre = mu_sim, level = level) %&gt;% data.frame() %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;wait_morning&quot;, &quot;wait_afternoon&quot;)) })) %&gt;% unnest(ellipse) %&gt;% mutate(rn = row_number()) %&gt;% arrange(rn) p2 &lt;- un_pooled_params %&gt;% left_join(partially_pooled_params) %&gt;% mutate(wait_morning_unpooled = Intercept, wait_afternnoon_unpooled = Intercept + Slope, wait_morning_partpooled = alpha_cafe_post, wait_afternnoon_partpooled = alpha_cafe_post + beta_cafe_post) %&gt;% ggplot() + geom_polygon(data = sim_mean_ellipse, aes(x = wait_morning, y = wait_afternoon, group = level), color = clr_dark, fill = clr_alpha(clr_current, .1), size = .2) + geom_abline(slope = 1, intercept = 0, linetype = 3, color = clr_dark) + geom_point(aes(x = wait_morning_unpooled, y = wait_afternnoon_unpooled), shape = 1, size = 2, color = clr0dd) + geom_point(aes(x = wait_morning_partpooled, y = wait_afternnoon_partpooled), shape = 21, size = 2, color = clr_current, fill = clr_alpha(clr_current)) + geom_segment(aes(x = wait_morning_unpooled, y = wait_afternnoon_unpooled, xend = wait_morning_partpooled, yend = wait_afternnoon_partpooled), arrow = arrow(type = &quot;closed&quot;, length = unit(4, &quot;pt&quot;)), size = .25) + coord_cartesian(xlim = c(0, 6.5), ylim = c(0, 5.5), expand = 0) p1 + p2 + plot_annotation(subtitle = &quot;shrinkage in two dimensions&quot;) 15.2 Advanced Varying Slopes For models with more than two varying effects (intercept plus multiple slopes for several clusters) we re-examine the cross-classified chimp example. To make the model easier to grasp, it is compartmentalized with sub-models for intercept and slopes. Starting with the likelihood: \\[ \\begin{array}{rclr} L_{i} &amp; \\sim &amp; \\textrm{Binomial}(1, p_{i}) &amp; \\textrm{[likelihood]}\\\\ \\textrm{logit}(p_{i}) &amp; = &amp; \\gamma_{\\textrm{TID}[i]} + \\alpha_{\\textrm{ACTOR}[i],\\textrm{TID}[i]} + \\beta_{\\textrm{BLOCK}[i],\\textrm{TID}[i]} &amp; \\textrm{[linear model]} \\end{array} \\] \\(\\gamma_{\\textrm{TID}[i]}\\) average log-odds for each treatment \\(\\alpha_{\\textrm{ACTOR}[i],\\textrm{TID}[i]}\\) effect for each actor in each treatment \\(\\beta_{\\textrm{BLOCK}[i],\\textrm{TID}[i]}\\) effect for each block in each treatment \\(\\rightarrow\\) an interaction model that allows the effect of each treatment to vary by actor and block, with every actor potentially responding differently. Since this results in lots of parameters (\\(4 + 7 \\times 4 + 6 \\times 4 = 56\\)), pooling is really needed here. Now for the adaptive priors: \\[ \\begin{array}{rclr} \\left[\\begin{array}{c} \\alpha_{j, 1}\\\\ \\alpha_{j, 2}\\\\ \\alpha_{j, 3}\\\\ \\alpha_{j, 4} \\end{array} \\right] &amp; \\sim &amp; \\textrm{MVNormal} \\left(\\left[ \\begin{array}{c} 0\\\\ 0\\\\ 0\\\\ 0 \\end{array} \\right], S_{\\textrm{ACTOR}} \\right) &amp; \\textrm{[prior for intercept]} \\\\ \\left[\\begin{array}{c} \\beta_{j, 1}\\\\ \\beta_{j, 2}\\\\ \\beta_{j, 3}\\\\ \\beta_{j, 4} \\end{array} \\right] &amp; \\sim &amp; \\textrm{MVNormal} \\left(\\left[ \\begin{array}{c} 0\\\\ 0\\\\ 0\\\\ 0 \\end{array} \\right], S_{\\textrm{BLOCK}} \\right) &amp; \\textrm{[prior for slope]} \\\\ \\end{array} \\] \\(\\rightarrow\\) the priors for \\(\\alpha\\) and \\(\\beta\\) come from two different statistical populations with a specific covariance matrix each that relates the four features for each actor. Finally the fixed priors: \\[ \\begin{array}{rcl} \\gamma_{j} &amp; \\sim &amp; \\textrm{Normal}(0, 1) ~ , \\textrm{for}~j = 1,...,4\\\\ \\rho_{\\textrm{ACTOR}} &amp; \\sim &amp; \\textrm{LKJcorr}(4) \\\\ \\rho_{\\textrm{BLOCK}} &amp; \\sim &amp; \\textrm{LKJcorr}(4) \\\\ \\sigma_{\\textrm{ACTOR}} &amp; \\sim &amp; \\textrm{Exponential}(1)\\\\ \\sigma_{\\textrm{BLOCK}} &amp; \\sim &amp; \\textrm{Exponential}(1) \\end{array} \\] data(chimpanzees) data_chimp &lt;- chimpanzees %&gt;% as_tibble() %&gt;% mutate(treatment = 1 + prosoc_left + 2 * condition, side_idx = prosoc_left + 1, # right 1, left 2 condition_idx = condition + 1) # no partner 1, partner 2 rm(chimpanzees) data_chimp_list &lt;- data_chimp %&gt;% dplyr::select(pulled_left, treatment, actor, block) %&gt;% as.list() model_chimp_centered &lt;- ulam( flist = alist( pulled_left ~ dbinom( 1, p ), logit(p) &lt;- gamma[treatment] + alpha[actor, treatment] + beta[block, treatment], # adaptive priors vector[4]:alpha[actor] ~ multi_normal(0, Rho_actor, sigma_actor), vector[4]:beta[block] ~ multi_normal(0, Rho_block, sigma_block), # fixed priors gamma[treatment] ~ dnorm(0, 1), Rho_actor ~ dlkjcorr(4), # 4 = how skeptical are we of extreme correlations Rho_block ~ dlkjcorr(4), sigma_actor ~ dexp(1), sigma_block ~ dexp(1) ), data = data_chimp_list, chains = 4, cores = 4, log_lik = TRUE ) Example of troubles in the trace rank plots (subset): library(bayesplot) clr_chains &lt;- function(n = 4, alpha = .7, col = clr2){ scales::colour_ramp(colors = c(clr0dd, col))( seq(0,1, length.out = n) ) %&gt;% clr_lighten(.2) %&gt;% clr_alpha(alpha = alpha) } model_chimp_centered@stanfit %&gt;% mcmc_rank_overlay(pars = vars(starts_with(&quot;alpha[1&quot;), starts_with(&quot;alpha[2&quot;)), n_bins = 60, facet_args = list(nrow = 2)) + scale_color_manual(values = clr_chains(col = clr_current) ) + labs(subtitle = &quot;centered model (divergent transitions)&quot;) + theme(legend.position = &quot;bottom&quot;, axis.text.x = element_blank()) Re-parametereize to deal with the divergent transitions model_chimp_non_centered &lt;- ulam( flist = alist( pulled_left ~ dbinom( 1, p ), logit(p) &lt;- gamma[treatment] + alpha[actor, treatment] + beta[block, treatment], # adaptive priors, non-centered transpars&gt; matrix[actor, 4]:alpha &lt;- compose_noncentered(sigma_actor, L_Rho_actor, z_actor), transpars&gt; matrix[block, 4]:beta &lt;- compose_noncentered(sigma_block, L_Rho_block, z_block), matrix[4, actor]:z_actor ~ normal(0, 1), matrix[4, block]:z_block ~ normal(0, 1), # fixed priors gamma[treatment] ~ dnorm(0, 1), cholesky_factor_corr[4]:L_Rho_actor ~ lkj_corr_cholesky( 2 ), cholesky_factor_corr[4]:L_Rho_block ~ lkj_corr_cholesky( 2 ), vector[4]:sigma_actor ~ dexp( 1 ), vector[4]:sigma_block ~ dexp( 1 ), # compute ordinary correlation matrices from Cholesky factors gq&gt; matrix[4,4]:Rho_actor &lt;&lt;- Chol_to_Corr(L_Rho_actor), gq&gt; matrix[4,4]:Rho_block &lt;&lt;- Chol_to_Corr(L_Rho_block) ), data = data_chimp_list, chains = 4, cores = 4, log_lik = TRUE ) model_chimp_non_centered@stanfit %&gt;% mcmc_rank_overlay(pars = vars(starts_with(&quot;alpha[1&quot;), starts_with(&quot;alpha[2&quot;)), n_bins = 60, facet_args = list(nrow = 2)) + scale_color_manual(values = clr_chains(col = clr_current) ) + labs(subtitle = &quot;non-centered model&quot;) + theme(legend.position = &quot;bottom&quot;, axis.text.x = element_blank()) tibble(centered = precis(model_chimp_centered, depth = 3, pars = c(&quot;alpha&quot;, &quot;beta&quot;))$n_eff, non_centered = precis(model_chimp_non_centered, depth = 3, pars = c(&quot;alpha&quot;, &quot;beta&quot;))$n_eff) %&gt;% ggplot(aes(x = centered, y = non_centered)) + geom_abline(slope = 1, intercept = 0, color = clr_dark, linetype = 3) + geom_point(color = clr0dd, fill = clr0, shape =21, size = 2) + labs(subtitle = &quot;n_eff comparison (effectiveness of sampling in MCMC)&quot;) WAIC(model_chimp_non_centered) %&gt;% knit_precis() param WAIC lppd penalty std_err 1 543.36 -245.4 26.28 19.62 Inspecting the standard deviation parameters to check the strength of regularization on the varying effects: precis(model_chimp_non_centered, depth = 2, pars = c(&quot;sigma_actor&quot;, &quot;sigma_block&quot;)) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 sigma_actor[1] 1.39 0.50 0.79 2.27 1188.76 1.00 sigma_actor[2] 0.91 0.39 0.42 1.59 1520.42 1.00 sigma_actor[3] 1.87 0.56 1.12 2.87 1709.36 1.00 sigma_actor[4] 1.59 0.59 0.82 2.64 1478.77 1.00 sigma_block[1] 0.43 0.32 0.04 0.99 943.77 1.00 sigma_block[2] 0.43 0.34 0.04 1.02 812.74 1.01 sigma_block[3] 0.30 0.27 0.02 0.80 1699.22 1.00 sigma_block[4] 0.47 0.38 0.04 1.19 1390.82 1.00 treatment_labels &lt;- c(&quot;R|N&quot;, &quot;L|N&quot;, &quot;R|P&quot;, &quot;L|P&quot;, &quot;R|diff&quot;, &quot;L|diff&quot;) chimp_grid &lt;- distinct(.data = data_chimp, actor, treatment) %&gt;% mutate(block = 5L) chimp_posterior_predictions &lt;- link(model_chimp_non_centered, data = chimp_grid) %&gt;% as.matrix() %&gt;% t() %&gt;% as_tibble() %&gt;% bind_cols(chimp_grid, .) %&gt;% pivot_longer(-c(actor, treatment), values_to = &quot;pulled_left&quot;) %&gt;% dplyr::select(-name) %&gt;% group_by(actor, treatment) %&gt;% summarise(p = list(quantile(pulled_left, probs = c(.055, .25, .5, .75, .955))), breaks = list(c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;))) %&gt;% ungroup() %&gt;% unnest(c(p, breaks)) %&gt;% pivot_wider(names_from = breaks, values_from = p) %&gt;% mutate(type = &quot;post. pred.&quot;, side = c(&quot;L&quot;, &quot;R&quot;)[2 - (treatment == 1 | treatment == 3)], condition = c(&quot;N&quot;, &quot;P&quot;)[1 + (treatment &gt; 2)], lab = treatment_labels[treatment]) data_chimp %&gt;% group_by(actor, treatment) %&gt;% summarise(mean_data = mean(pulled_left), type = &quot;data&quot;) %&gt;% mutate(side = c(&quot;L&quot;, &quot;R&quot;)[2 - (treatment == 1 | treatment == 3)], condition = c(&quot;N&quot;, &quot;P&quot;)[1 + (treatment &gt; 2)], lab = treatment_labels[treatment]) %&gt;% ggplot(aes(x = treatment, y = mean_data)) + geom_hline(yintercept = .5, linetype = 3, color = clr_dark) + # geom_line(aes(group = side), color = clr_current) + geom_text(data = . %&gt;% filter(actor == 1), aes(y = mean_data - .5 * (1.5 - as.numeric(factor(side))), label = lab), family = fnt_sel) + geom_line(data = chimp_posterior_predictions, aes(y = m, group = side), color = clr0dd) + geom_segment(data = chimp_posterior_predictions, inherit.aes = FALSE, aes(x = treatment, xend = treatment, y = ll, yend = hh), color = clr0dd) + geom_point(data = chimp_posterior_predictions, inherit.aes = FALSE, aes(x = treatment, y = m, shape = condition), color = clr0dd, fill = clr0, size = 1.8) + geom_point(aes(shape = condition), color = clr_current, fill = clr_lighten(clr_current), size = 1.8) + scale_shape_manual(values = c(&quot;N&quot; = 21, &quot;P&quot; = 19), guide = &quot;none&quot;) + facet_grid(. ~ actor, labeller = label_both) + scale_x_discrete(expand = c(.2,.2)) + labs(x = NULL, y = &quot;pulled_left&quot;) + lims(y = c(0,1)) + theme(panel.background = element_rect(color = clr0, fill = &quot;transparent&quot;)) stancode(model_chimp_non_centered) #&gt; data{ #&gt; int pulled_left[504]; #&gt; int block[504]; #&gt; int actor[504]; #&gt; int treatment[504]; #&gt; } #&gt; parameters{ #&gt; matrix[4,7] z_actor; #&gt; matrix[4,6] z_block; #&gt; vector[4] gamma; #&gt; cholesky_factor_corr[4] L_Rho_actor; #&gt; cholesky_factor_corr[4] L_Rho_block; #&gt; vector&lt;lower=0&gt;[4] sigma_actor; #&gt; vector&lt;lower=0&gt;[4] sigma_block; #&gt; } #&gt; transformed parameters{ #&gt; matrix[7,4] alpha; #&gt; matrix[6,4] beta; #&gt; beta = (diag_pre_multiply(sigma_block, L_Rho_block) * z_block)&#39;; #&gt; alpha = (diag_pre_multiply(sigma_actor, L_Rho_actor) * z_actor)&#39;; #&gt; } #&gt; model{ #&gt; vector[504] p; #&gt; sigma_block ~ exponential( 1 ); #&gt; sigma_actor ~ exponential( 1 ); #&gt; L_Rho_block ~ lkj_corr_cholesky( 2 ); #&gt; L_Rho_actor ~ lkj_corr_cholesky( 2 ); #&gt; gamma ~ normal( 0 , 1 ); #&gt; to_vector( z_block ) ~ normal( 0 , 1 ); #&gt; to_vector( z_actor ) ~ normal( 0 , 1 ); #&gt; for ( i in 1:504 ) { #&gt; p[i] = gamma[treatment[i]] + alpha[actor[i], treatment[i]] + beta[block[i], treatment[i]]; #&gt; p[i] = inv_logit(p[i]); #&gt; } #&gt; pulled_left ~ binomial( 1 , p ); #&gt; } #&gt; generated quantities{ #&gt; vector[504] log_lik; #&gt; vector[504] p; #&gt; matrix[4,4] Rho_actor; #&gt; matrix[4,4] Rho_block; #&gt; Rho_block = multiply_lower_tri_self_transpose(L_Rho_block); #&gt; Rho_actor = multiply_lower_tri_self_transpose(L_Rho_actor); #&gt; for ( i in 1:504 ) { #&gt; p[i] = gamma[treatment[i]] + alpha[actor[i], treatment[i]] + beta[block[i], treatment[i]]; #&gt; p[i] = inv_logit(p[i]); #&gt; } #&gt; for ( i in 1:504 ) log_lik[i] = binomial_lpmf( pulled_left[i] | 1 , p[i] ); #&gt; } 15.3 Instruments and Causal Design 15.3.1 Instrumental Variables Instrumental variables need to be independent of \\(U\\) (\\(Q \\perp \\!\\!\\! \\perp U\\)) be not independent of \\(E\\) (\\(Q \\not\\!\\perp\\!\\!\\!\\perp E\\)) \\(Q\\) cannot influence \\(W\\), except through \\(E\\) (exclusive restriction) dag1 &lt;- dagify(W ~ U + E, E ~ U, exposure = &quot;E&quot;, outcome = &quot;W&quot;, coords = tibble(name = c(&quot;E&quot;, &quot;U&quot;, &quot;W&quot;), x = c(0, .5, 1), y = c(0, 1, 0))) p1 &lt;- dag1 %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;W&quot;, &quot;response&quot;, if_else(name %in% c(&quot;E&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_x_continuous(limits = c(-.1, 1.1)) + labs(subtitle = &quot;unmeasured confound&quot;) dag2 &lt;- dagify(W ~ U + E, E ~ U + Q, exposure = &quot;E&quot;, outcome = &quot;W&quot;, coords = tibble(name = c(&quot;E&quot;, &quot;U&quot;, &quot;W&quot;, &quot;Q&quot;), x = c(0, .5, 1, -.5), y = c(0, 1, 0, 1))) p2 &lt;- dag2 %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;W&quot;, &quot;response&quot;, if_else(name %in% c(&quot;E&quot;, &quot;Q&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_x_continuous(limits = c(-.6, 1.1)) + labs(subtitle = &quot;instrumental variable&quot;) p1 + p2 &amp; scale_y_continuous(limits = c(-.1, 1.1)) &amp; coord_fixed(ratio = .6) &amp; theme(plot.subtitle = element_text(hjust = .5, family = fnt_sel)) library(dagitty) instrumentalVariables(dag2) #&gt; Q Simulate education data set.seed(42) n &lt;- 500 data_edu_sim &lt;- tibble(u = rnorm(n), q = sample(1:4, size = n, replace = TRUE), e = rnorm(n, u + q), w = rnorm(n, u + 0 * e)) %&gt;% mutate(across(q:w, standardize, .names = &quot;{.col}_std&quot;)) model_edu_confound &lt;- ulam( flist = alist( w_std ~ dnorm( mu, sigma ), mu &lt;- alpha_w + beta_ew * e_std, alpha_w ~ dnorm( 0, 0.2 ), beta_ew ~ dnorm( 0, 0.5 ), sigma ~ dexp(1) ), data = data_edu_sim, cores = 4, chain = 4, log_lik = TRUE ) model_edu_bias_amplified &lt;- ulam( flist = alist( w_std ~ dnorm( mu, sigma ), mu &lt;- alpha_w + beta_ew * e_std + beta_qw * q_std, alpha_w ~ dnorm( 0, 0.2 ), beta_ew ~ dnorm( 0, 0.5 ), beta_qw ~ dnorm( 0, 0.5 ), sigma ~ dexp(1) ), data = data_edu_sim, cores = 4, chain = 4, log_lik = TRUE ) Confounded model: precis(model_edu_confound) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_w 0.00 0.04 -0.07 0.07 1970.24 1 beta_ew 0.35 0.04 0.28 0.41 1818.55 1 sigma 0.94 0.03 0.89 0.99 2090.88 1 Model with bias amplification precis(model_edu_bias_amplified) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_w 0.00 0.04 -0.06 0.06 2231.11 1 beta_ew 0.60 0.05 0.52 0.68 1291.31 1 beta_qw -0.38 0.05 -0.47 -0.30 1258.66 1 sigma 0.89 0.03 0.85 0.94 1672.71 1 Using a generative model, translating the DAG into mathematical notation: \\[ \\begin{array}{rclr} W_{i} &amp; \\sim &amp; \\textrm{Normal}(\\mu_{w, i}, \\sigma_{w}) &amp; \\textrm{[likelihood]} \\\\ \\mu_{w,i} &amp; = &amp; \\alpha_{w} + \\beta_{EW} E_{i} + U_{i} &amp; \\textrm{[wage model]} \\end{array} \\] Sub-model for the instrumental variable \\[ \\begin{array}{rclr} E_{i} &amp; \\sim &amp; \\textrm{Normal}(\\mu_{E,i}, \\sigma_{E}) &amp; \\textrm{[likelihood]}\\\\ \\mu_{E,i} &amp; = &amp; \\alpha_{E} + \\beta_{QE} Q_{i} + U_{i} &amp; \\textrm{[education model]} \\\\ \\end{array} \\] A third sub-model for \\(Q\\) \\[ \\begin{array}{rcl} Q_{i} &amp; \\sim &amp; \\textrm{Categorical}([0.25, 0.25, 0.25, 0.25]) \\end{array} \\] The last part of the generative model is a sub-model for \\(U\\) \\[ \\begin{array}{rcl} U_{i} &amp; \\sim &amp; \\textrm{Normal}(0, 1) \\\\ \\end{array} \\] Now, we can translate the generative model into a statistical model (specifically a multivariate linear model) \\[ \\begin{array}{rclr} \\left(\\begin{array}{cc} W_{i} \\\\ E_{i} \\end{array}\\right) &amp; \\sim &amp; \\textrm{MVNormal}\\left(\\left( \\begin{array}{cc} \\mu_{w, i} \\\\ \\mu_{E, i} \\end{array}\\right), S \\right) &amp; \\textrm{[joint wage and education model]}\\\\ \\mu_{w,i} &amp; = &amp; \\alpha_{w} + \\beta_{EW} E_{i} &amp;\\\\ \\mu_{E,i} &amp; = &amp; \\alpha_{E} + \\beta_{QE} Q_{i} \\end{array} \\] model_edu_multivariate &lt;- ulam( flist = alist( c(w_std, e_std) ~ multi_normal( c(mu_w, mu_e), Rho, Sigma ), mu_w &lt;- alpha_w + beta_ew * e_std, mu_e &lt;- alpha_e + beta_qe * q_std, c( alpha_w, alpha_e ) ~ normal( 0, 0.2 ), c( beta_ew, beta_qe ) ~ normal( 0, 0.5 ), Rho ~ lkj_corr( 2 ), Sigma ~ exponential( 1 ) ), data = data_edu_sim, cores = 4, chain = 4) Marginal posterior for the multivariate model: precis( model_edu_multivariate, depth = 3 ) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_e 0.00 0.03 -0.05 0.05 1437.03 1 alpha_w 0.00 0.04 -0.07 0.07 1614.12 1 beta_qe 0.66 0.03 0.61 0.71 1500.13 1 beta_ew 0.02 0.07 -0.09 0.13 1097.50 1 Rho[1,1] 1.00 0.00 1.00 1.00 NaN NaN Rho[1,2] 0.44 0.06 0.34 0.53 1038.75 1 Rho[2,1] 0.44 0.06 0.34 0.53 1038.75 1 Rho[2,2] 1.00 0.00 1.00 1.00 NaN NaN Sigma[1] 1.00 0.04 0.94 1.06 1235.09 1 Sigma[2] 0.75 0.02 0.71 0.79 1780.82 1 set.seed(42) data_edu_sim_2 &lt;- tibble(u = rnorm(n), q = sample(1:4, size = n, replace = TRUE), e = rnorm(n, u + q), w = rnorm(n, u + 0.2 * e)) %&gt;% mutate(across(q:w, standardize, .names = &quot;{.col}_std&quot;)) model_edu_confound_2 &lt;- ulam(model_edu_confound, data = data_edu_sim_2, cores = 4, chains = 4) model_edu_multivariate_2 &lt;- ulam(model_edu_multivariate, data = data_edu_sim_2, cores = 4, chains = 4) Confounded model (new data): precis( model_edu_confound_2, depth = 3 ) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_w 0.00 0.04 -0.06 0.06 1828.88 1 beta_ew 0.55 0.04 0.48 0.61 1769.32 1 sigma 0.84 0.03 0.80 0.88 1986.21 1 Multivariate model (new data): precis( model_edu_multivariate_2, depth = 3 ) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_e 0.00 0.03 -0.06 0.05 1636.45 1.00 alpha_w 0.00 0.04 -0.06 0.06 1460.56 1.00 beta_qe 0.66 0.03 0.61 0.71 1433.08 1.00 beta_ew 0.25 0.06 0.15 0.35 1111.10 1.00 Rho[1,1] 1.00 0.00 1.00 1.00 NaN NaN Rho[1,2] 0.44 0.06 0.35 0.53 1061.27 1.00 Rho[2,1] 0.44 0.06 0.35 0.53 1061.27 1.00 Rho[2,2] 1.00 0.00 1.00 1.00 NaN NaN Sigma[1] 0.89 0.03 0.84 0.95 1064.18 1.01 Sigma[2] 0.75 0.02 0.71 0.79 1821.55 1.01 \\(\\rightarrow\\) compare the estimates for beta_ew. 15.3.2 Other Designs DAG for the front-door criterion dag3 &lt;- dagify(W ~ U + Z, E ~ U, Z ~ E, exposure = &quot;E&quot;, outcome = &quot;W&quot;, coords = tibble(name = c(&quot;E&quot;, &quot;U&quot;, &quot;W&quot;, &quot;Z&quot;), x = c(0, .5, 1, .5), y = c(0, 1, 0, 0))) dag3 %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;W&quot;, &quot;response&quot;, if_else(name %in% c(&quot;E&quot;, &quot;Z&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_x_continuous(limits = c(-.1, 1.1)) + labs(subtitle = &quot;front-door criterion&quot;) + scale_y_continuous(limits = c(-.1, 1.1)) + coord_fixed(ratio = .5) + theme(plot.subtitle = element_text(hjust = .5, family = fnt_sel)) 15.4 Social Relations and Correlated Varying Effects data(KosterLeckie) data_dyads &lt;- kl_dyads %&gt;% as_tibble() p1 &lt;- data_dyads %&gt;% ggplot(aes(x = giftsAB, y = giftsBA)) + geom_point(color = clr0dd, fill = fll0, shape = 21, size = 2.5) + geom_abline(slope = 1, intercept = 0, linetype = 3, color = clr_dark) library(ggraph) library(tidygraph) p2 &lt;- data_dyads %&gt;% dplyr::select(from = hidA, to = hidB, gifts = giftsAB) %&gt;% mutate(dir = &quot;AB&quot;) %&gt;% bind_rows(data_dyads %&gt;% dplyr::select( from = hidB, to = hidA, gifts = giftsBA) %&gt;% mutate(dir = &quot;BA&quot;) ) %&gt;% as_tbl_graph() %&gt;% ggraph(layout = &#39;linear&#39;, circular = TRUE) + geom_edge_arc(aes(filter = gifts != 0, alpha = log10(gifts), width = gifts, color = from), arrow = arrow(type = &quot;closed&quot;,length = unit(4, &quot;pt&quot;)), start_cap = ggraph::circle(12, &#39;pt&#39;), end_cap = ggraph::circle(12, &#39;pt&#39;) ) + geom_node_point(shape = 21, size = 8, color = clr0dd, fill = clr0) + geom_node_text(aes(label = name), family = fnt_sel, hjust = .5 , vjust = .4) + facet_wrap(dir ~ .) + scale_alpha_continuous(range = c(0, 1), limits = c(10, 10)) + scale_edge_width_continuous(range = c(.1, 2)) + scale_edge_color_gradientn(colours = c(clr_dark, clr0, clr_current)) + coord_equal() + theme(legend.position = &quot;none&quot;) p1 + p2 + plot_layout(widths = c(.5, 1)) The Social Relations Model (SRM): First Part \\[ \\begin{array}{rclr} y_{A \\rightarrow B} &amp; \\sim &amp; \\textrm{Poisson}(\\lambda_{AB}) &amp; \\textrm{[likelyhood]}\\\\ \\textrm{log} \\lambda_{AB} &amp; = &amp; \\alpha + g_{A} + r_{B} + d_{AB} &amp; \\textrm{[linear model]} \\end{array} \\] Second part (reverse) \\[ \\begin{array}{rclr} y_{B \\rightarrow A} &amp; \\sim &amp; \\textrm{Poisson}(\\lambda_{BA}) &amp; \\textrm{[likelyhood]}\\\\ \\textrm{log} \\lambda_{BA} &amp; = &amp; \\alpha + g_{B} + r_{A} + d_{BA} &amp; \\textrm{[linear model]} \\end{array} \\] Multi-Normal prior for the households (giving vs receiving) and the dyads (strength of AB vs BA): \\[ \\begin{array}{rclr} \\left( \\begin{array}{c} g_{i}\\\\ r_{i} \\end{array} \\right) &amp; \\sim &amp; \\textrm{MVNormal}\\left( \\left(\\begin{array}{c} 0\\\\0 \\end{array}\\right), \\left(\\begin{array}{cc} \\sigma_{g}^2 &amp; \\sigma_{g} \\sigma_{r} \\rho_{gr}\\\\ \\sigma_{g} \\sigma_{r} \\rho_{gr} &amp; \\sigma_{r}^2 \\end{array}\\right) \\right) &amp; \\textrm{[household prior]}\\\\ \\left( \\begin{array}{c} d_{ij}\\\\ d_{ji} \\end{array} \\right) &amp; \\sim &amp; \\textrm{MVNormal}\\left( \\left(\\begin{array}{c} 0\\\\0 \\end{array}\\right), \\left(\\begin{array}{cc} \\sigma_{d}^2 &amp; \\sigma_{d}^2 \\rho_{d}\\\\ \\sigma_{d}^2 \\rho_{d} &amp; \\sigma_{d}^2 \\end{array}\\right) \\right) &amp; \\textrm{[dyad prior]} \\end{array} \\] data_dyads_list &lt;- data_dyads %&gt;% dplyr::select(hidA, hidB,did, giftsAB,giftsBA) %&gt;% as.list() %&gt;% c(., n = length(.$hidA), n_households = max(.$hidB)) model_dyads &lt;- ulam( flist = alist( giftsAB ~ poisson( lambda_AB ), giftsBA ~ poisson( lambda_BA ), log( lambda_AB ) &lt;- alpha + gr[hidA, 1] + gr[hidB, 2] + d[did, 1], log( lambda_BA ) &lt;- alpha + gr[hidB, 1] + gr[hidA, 2] + d[did, 2], alpha ~ normal( 0, 1 ), ## gr matrix of varying effects vector[2]:gr[n_households] ~ multi_normal(0, Rho_gr, sigma_gr), Rho_gr ~ lkj_corr(4), sigma_gr ~ exponential(1), ## dyad effects transpars&gt; matrix[n, 2]:d &lt;- compose_noncentered(rep_vector(sigma_d, 2), L_Rho_d, z), matrix[2, n]:z ~ normal( 0, 1 ), cholesky_factor_corr[2]:L_Rho_d ~ lkj_corr_cholesky(8), sigma_d ~ exponential(1), # compute correlation matrix for dyads gq&gt; matrix[2,2]:Rho_d &lt;&lt;- Chol_to_Corr( L_Rho_d ) ), data = data_dyads_list, chains = 4, cores = 4, iter = 2000 ) precis(model_dyads, depth = 3, pars = c(&quot;Rho_gr&quot;, &quot;sigma_gr&quot;)) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 Rho_gr[1,1] 1.00 0.00 1.00 1.00 NaN NaN Rho_gr[1,2] -0.40 0.20 -0.71 -0.06 1381.06 1 Rho_gr[2,1] -0.40 0.20 -0.71 -0.06 1381.06 1 Rho_gr[2,2] 1.00 0.00 1.00 1.00 NaN NaN sigma_gr[1] 0.83 0.14 0.63 1.07 2541.34 1 sigma_gr[2] 0.42 0.09 0.29 0.58 1252.74 1 dyad_posterior &lt;- extract.samples(model_dyads) %&gt;% as_tibble() extract_gr &lt;- function(id, layer, posterior){ tibble(id = id, value = list(posterior$alpha + posterior$gr[,id,layer]) )%&gt;% set_names(nm = c(&quot;id&quot;, c(&quot;give&quot;,&quot;receive&quot;)[layer])) } p1 &lt;- map_dfr(1:25, extract_gr, layer = 1, posterior = dyad_posterior) %&gt;% left_join(map_dfr(1:25, extract_gr, layer = 2, posterior = dyad_posterior)) %&gt;% mutate(e_mu_g = purrr::map_dbl(give, function(x){mean(exp(x))}), e_mu_r = purrr::map_dbl(receive, function(x){mean(exp(x))}), Sigma = map2(give, receive, function(x,y){cov(cbind(x, y))}), mu = map2(give, receive, function(x,y){c(mean(x), mean(y))}), ellipse = map2(Sigma, mu, function(x,y){ ellipse(x, centre = y, level = .5 ) %&gt;% exp() %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;el_e_mu_g&quot;, &quot;el_e_mu_r&quot;))})) %&gt;% ggplot(aes(e_mu_g, e_mu_r)) + geom_abline(slope = 1, intercept = 0, color = clr_dark, linetype = 3) + geom_polygon(data = . %&gt;% unnest(ellipse), aes(x = el_e_mu_g, y = el_e_mu_r, group = id), color = fll0dd, fill = &quot;transparent&quot;) + geom_point(color = clr0dd, fill = clr0, shape = 21, size = 2) + coord_equal(xlim = c(0,8.6), ylim = c(0,8.6)) precis(model_dyads, depth = 3, pars = c(&quot;Rho_d&quot;, &quot;sigma_d&quot;)) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 Rho_d[1,1] 1.00 0.00 1.00 1.00 NaN NaN Rho_d[1,2] 0.88 0.03 0.82 0.93 889.96 1.01 Rho_d[2,1] 0.88 0.03 0.82 0.93 889.96 1.01 Rho_d[2,2] 1.00 0.00 1.00 1.00 NaN NaN sigma_d 1.10 0.06 1.01 1.20 1379.58 1.00 mean_d &lt;- function(layer){ dyad_posterior$d[,,layer] %&gt;% t() %&gt;% as_tibble() %&gt;% bind_cols(data_dyads %&gt;% dplyr::select(did), . ) %&gt;% pivot_longer(-did) %&gt;% group_by(did) %&gt;% summarise(mean = mean(value)) %&gt;% ungroup() %&gt;% set_names(nm = c(&quot;did&quot;, c(&quot;A&quot;, &quot;B&quot;)[layer])) } p2 &lt;- left_join(mean_d(1), mean_d(2)) %&gt;% ggplot(aes(A, B)) + geom_hline(yintercept = 0, color = clr_dark, linetype = 3) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_abline(slope = 1, intercept = 0, color = clr_dark, linetype = 3) + geom_point(color = clr0dd, fill = fll0, shape = 21, size = 2) p1 + p2 15.5 Continous Categories and the Gaussian Process 15.5.1 Spatial autocorrelatioin in Oceanic Tools data(islandsDistMatrix) island_distances &lt;- islandsDistMatrix %&gt;% (function(x){ colnames(x) &lt;- c(&quot;Ml&quot;, &quot;Ti&quot;, &quot;SC&quot;, &quot;Ya&quot;, &quot;Fi&quot;, &quot;Tr&quot;, &quot;Ch&quot;, &quot;Mn&quot;, &quot;To&quot;, &quot;Ha&quot;); x }) %&gt;% round(digits = 1) islandsDistMatrix %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;from&quot;) %&gt;% as_tibble() %&gt;% mutate(from = fct_reorder(from, row_number())) %&gt;% pivot_longer(-from, names_to = &quot;to&quot;, values_to = &quot;dist&quot;) %&gt;% mutate(to = str_replace(string = to,pattern = &#39;\\\\.&#39;, replacement = &quot; &quot;) %&gt;% factor(levels = levels(from))) %&gt;% filter(as.numeric(from) &lt; as.numeric(to)) %&gt;% as_tbl_graph() %&gt;% ggraph(layout = &quot;kk&quot;, weights = sqrt(dist)) + geom_edge_link2(aes(color = node.name), start_cap = ggraph::rectangle(35, 23, &#39;pt&#39;, &#39;pt&#39;), end_cap = ggraph::rectangle(35, 23, &#39;pt&#39;,&#39;pt&#39;) )+ geom_node_label(aes(color = name, label = name), family = fnt_sel) + scale_y_reverse() + scale_color_manual(values = clr_pal(), guide = &#39;none&#39;)+ scale_edge_color_manual(values = clr_pal(), guide = &#39;none&#39;) + coord_equal() Creating a model in which the number of tools is correlated to those of the neighbor islands. Building on the scientific model developed in chapter 11: \\[ \\color{#858585FF}{ \\begin{array}{rclr} T_{i} &amp; \\sim &amp; \\textrm{Poisson}(\\lambda_{i}) &amp; \\textrm{[likelihood]}\\\\ \\lambda_{i} &amp; = &amp; \\alpha P_{i}^{\\beta} / \\gamma &amp; \\textrm{[linear model]} \\end{array}} \\] we update to \\[ \\begin{array}{rclr} T_{i} &amp; \\sim &amp; \\textrm{Poisson}(\\lambda_{i}) &amp; \\textrm{[likelihood]}\\\\ \\lambda_{i} &amp; = &amp; \\textrm{exp}(k_{SOCIETY[i]}) \\alpha P_{i}^{\\beta} / \\gamma &amp; \\textrm{[linear model]} \\end{array} \\] with \\(k_{SOCIENTY}\\) as the varying intercept with the multivariate prior: \\[ \\begin{array}{rclr} \\left( \\begin{array}{c} k_{1}\\\\ k_{2} \\\\ k_{3} \\\\ \\vdots \\\\ k_{10} \\end{array} \\right) &amp; \\sim &amp; \\textrm{MVNormal}\\left(\\left(\\begin{array}{c} 0 \\\\0\\\\0\\\\ \\vdots\\\\0 \\end{array} \\right) , K \\right) &amp; \\textrm{[prior for intercepts]}\\\\ K_{ij} &amp; = &amp; \\eta~\\textrm{exp}(-\\rho^{2}D_{ij}^2) + \\delta_{ij}~\\sigma^{2} &amp; \\textrm{[define covariance matrix]} \\end{array} \\] where \\(D_{ij}\\) is the distance between the \\(i^{th}\\) and \\(j^{th}\\) society (covariance declines exponentially). The parameter \\(\\rho\\) defines the rate of decline, and the fact that we square \\(D_{ij}\\) is simply a modelling choice. ggplot() + stat_function(fun = function(x){exp(-1 * x)}, geom = &quot;line&quot;, xlim = c(0, 4), aes(color = glue(&quot;exp( {mth(&#39;\\U03C1&#39;)}^2 x )&quot;), linetype = after_stat(color))) + stat_function(fun = function(x){exp(-1 * x^2)}, geom = &quot;line&quot;, xlim = c(0, 4), aes(color = glue(&quot;exp( {mth(&#39;\\U03C1&#39;)}^2 x^2 )&quot;), linetype = after_stat(color))) + scale_color_manual(&quot;decline shape&quot;, values = c(clr_dark, clr_current), guide = guide_legend(title.position = &quot;top&quot;)) + scale_linetype_manual(&quot;decline shape&quot;, values = c(3, 1), guide = guide_legend(title.position = &quot;top&quot;)) + labs(x = &quot;distance&quot;, y = &#39;correlation&#39;) + theme(legend.text = element_markdown(), legend.direction = &quot;horizontal&quot;, legend.position = c(1, 1), legend.justification = c(1 ,1)) The parameters \\(\\eta^{2}\\) and \\(\\delta_{ij}~\\sigma^{2}\\) are the maximal covariance and the a means to provide extra covariance when \\(i = j\\) (by toggling \\(\\delta_{ij} = 1\\) then and \\(\\delta_{ij} = 0\\) otherwise). This is important when there is more than one observation per unit as there will be variance within the units (not within this example though). So now we also need priors for those parameters (need to be positive): \\[ \\begin{array}{rclr} \\eta^{2} &amp; \\sim &amp; \\textrm{Exponential}(2) &amp; \\textrm{[prior for max. covariance]}\\\\ \\rho^{2} &amp; \\sim &amp; \\textrm{Exponential}(0.5) &amp; \\textrm{[prior for the decline rate]} \\end{array} \\] Now, the model fit: data(Kline2) data_kline &lt;- Kline2 %&gt;% as_tibble() %&gt;% mutate(pop_log_scl = scale(log(population))[,1], contact_idx = 3L - as.integer(factor(contact)), culture_idx = row_number()) data_kline_list &lt;- data_kline %&gt;% dplyr::select(total_tools, population, culture_idx) %&gt;% as.list() %&gt;% c(., list(d_mat = island_distances)) model_island_distance &lt;- ulam( flist = alist( total_tools ~ dpois(lambda), lambda &lt;- (alpha * population ^ beta / gamma) * exp(k[culture_idx]), vector[10]:k ~ multi_normal(0, SIGMA), matrix[10, 10]:SIGMA &lt;- cov_GPL2( d_mat, eta_sq, rho_sq, 0.01 ), c(alpha, beta, gamma) ~ dexp(1), eta_sq ~ dexp(2), rho_sq ~ dexp(0.5) ), data = data_kline_list, chains = 4, cores = 4, iter = 2000 ) library(ggmcmc) ggs(model_island_distance@stanfit, inc_warmup = TRUE) %&gt;% mutate(chain = factor(Chain)) %&gt;% # filter(Parameter %in% c(&quot;alpha&quot;, &quot;beta_c&quot;, &quot;beta_a&quot;)) %&gt;% ggplot(aes(x = Iteration, y = value)) + annotate(geom = &quot;rect&quot;, xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf, fill = clr0d, alpha = .3, size = 0) + geom_line(aes(color = chain), size = .15) + scale_color_manual(values = clr_chains() ) + facet_wrap(~ Parameter, scales = &quot;free_y&quot;, ncol = 5) + labs(x = NULL, y = NULL) + theme(legend.position = &quot;bottom&quot;) precis(model_island_distance, depth = 3) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 k[1] -0.17 0.31 -0.66 0.31 547.99 1.00 k[2] -0.01 0.30 -0.47 0.46 483.06 1.01 k[3] -0.06 0.29 -0.51 0.38 462.31 1.00 k[4] 0.36 0.27 -0.04 0.80 487.83 1.01 k[5] 0.07 0.26 -0.30 0.49 469.88 1.01 k[6] -0.38 0.27 -0.82 0.01 515.66 1.01 k[7] 0.14 0.26 -0.25 0.55 469.41 1.01 k[8] -0.21 0.27 -0.63 0.20 498.61 1.01 k[9] 0.26 0.26 -0.11 0.66 434.98 1.01 k[10] -0.17 0.36 -0.73 0.35 574.67 1.01 gamma 0.61 0.55 0.08 1.65 1364.72 1.00 beta 0.28 0.08 0.15 0.41 926.52 1.00 alpha 1.43 1.08 0.25 3.41 1801.02 1.00 eta_sq 0.18 0.18 0.03 0.51 589.68 1.00 rho_sq 1.39 1.77 0.08 4.52 2007.75 1.00 island_dist_prior &lt;- extract.prior(model_island_distance, n = 1e4) %&gt;% as_tibble() pm_cov_prior &lt;- tibble(x = seq(0, 10, length.out = 101)) %&gt;% mutate(pm_cov = purrr::map(x, function(x){ tibble(cov = island_dist_prior$eta_sq * exp(- island_dist_prior$rho_sq * x^2), .draw = seq_along(island_dist_prior$eta_sq)) })) %&gt;% unnest(pm_cov) %&gt;% arrange(.draw, x) pm_cov_prior_mu &lt;- pm_cov_prior %&gt;% group_by(x) %&gt;% summarise(pi = list(tibble(prob = c(quantile(cov, probs = c(.055, .25, .5, .75, .945)), mean(cov)), label = c(&quot;ll&quot;,&quot;l&quot;,&quot;m&quot;,&quot;h&quot;,&quot;hh&quot;,&quot;mean&quot;)))) %&gt;% unnest(pi) %&gt;% pivot_wider(names_from = label, values_from = prob) p1 &lt;- pm_cov_prior_mu %&gt;% ggplot(aes(x = x)) + geom_line(data = pm_cov_prior %&gt;% filter(.draw &lt; 51), aes(y = cov, group = .draw), color = clr_dark) + geom_smooth(stat = &#39;identity&#39;, aes(y = mean, ymin = ll, ymax = hh), size = .5, linetype = 2, color = &quot;white&quot;, #clr_current %&gt;% clr_alpha(.8), fill = fll_current()) + labs(x = &quot;distance (10^3 km)&quot;, y = &quot;covariance&quot;, subtitle = &quot;gaussian process prior&quot;) + lims(y = c(0, 2)) + theme(axis.title.x = element_markdown()) island_dist_posterior &lt;- extract.samples(model_island_distance) %&gt;% as_tibble() pm_cov &lt;- tibble(x = seq(0, 10, length.out = 101)) %&gt;% mutate(pm_cov = purrr::map(x, function(x){ tibble(cov = island_dist_posterior$eta_sq * exp(- island_dist_posterior$rho_sq * x^2), .draw = seq_along(island_dist_posterior$eta_sq)) })) %&gt;% unnest(pm_cov) %&gt;% arrange(.draw, x) pm_cov_mu &lt;- pm_cov %&gt;% group_by(x) %&gt;% summarise(pi = list(tibble(prob = c(quantile(cov, probs = c(.055, .25, .5, .75, .945)), mean(cov)), label = c(&quot;ll&quot;,&quot;l&quot;,&quot;m&quot;,&quot;h&quot;,&quot;hh&quot;,&quot;mean&quot;)))) %&gt;% unnest(pi) %&gt;% pivot_wider(names_from = label, values_from = prob) p2 &lt;- pm_cov_mu %&gt;% ggplot(aes(x = x)) + geom_line(data = pm_cov %&gt;% filter(.draw &lt; 51), aes(y = cov, group = .draw), color = clr_dark) + geom_smooth(stat = &#39;identity&#39;, aes(y = mean, ymin = ll, ymax = hh), size = .5, linetype = 2, color = &quot;white&quot;, #clr_current %&gt;% clr_alpha(.8), fill = fll_current()) + labs(x = &quot;distance (10^3 km)&quot;, y = &quot;covariance&quot;, subtitle = &quot;gaussian process posterior&quot;) + lims(y = c(0, 2)) + theme(axis.title.x = element_markdown()) p1 + p2 K &lt;- matrix(0, nrow = 10, ncol = 10) for ( i in 1:10 ) { for ( j in 1:10 ) { K[i, j] &lt;- median(island_dist_posterior$eta_sq) * exp(-median(island_dist_posterior$rho_sq) * islandsDistMatrix[i , j] ^2) } } diag(K) &lt;- median(island_dist_posterior$eta_sq) + .01 Rho &lt;- round(cov2cor(K), digits = 2) %&gt;% (function(x){colnames(x) &lt;- colnames(island_distances); x}) %&gt;% (function(x){rownames(x) &lt;- colnames(island_distances); x}) Rho %&gt;% data.frame() %&gt;% knit_precis(param_name = &quot;culture&quot;) culture Ml Ti SC Ya Fi Tr Ch Mn To Ha Ml 1.00 0.78 0.69 0.00 0.29 0.04 0.00 0.00 0.07 0 Ti 0.78 1.00 0.86 0.00 0.29 0.04 0.00 0.00 0.05 0 SC 0.69 0.86 1.00 0.00 0.15 0.10 0.01 0.01 0.02 0 Ya 0.00 0.00 0.00 1.00 0.00 0.01 0.15 0.13 0.00 0 Fi 0.29 0.29 0.15 0.00 1.00 0.00 0.00 0.00 0.60 0 Tr 0.04 0.04 0.10 0.01 0.00 1.00 0.08 0.54 0.00 0 Ch 0.00 0.00 0.01 0.15 0.00 0.08 1.00 0.31 0.00 0 Mn 0.00 0.00 0.01 0.13 0.00 0.54 0.31 1.00 0.00 0 To 0.07 0.05 0.02 0.00 0.60 0.00 0.00 0.00 1.00 0 Ha 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1 island_cor_graph &lt;- Rho %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;from&quot;) %&gt;% pivot_longer(-from, names_to = &quot;to&quot;, values_to = &quot;cor&quot;) %&gt;% as_tbl_graph() %N&gt;% left_join(data_kline %&gt;% mutate(name = colnames(island_distances), p_size = exp(logpop/max(logpop) * 1.5) - 2)) p1 &lt;- island_cor_graph %&gt;% create_layout(layout = . %N&gt;% as.tibble() %&gt;% dplyr::select(x = lon2, y = lat)) %&gt;% ggraph() + geom_edge_link(aes(alpha = cor ^ 2), color = clr_current, start_cap = ggraph::circle(8,&quot;pt&quot;), end_cap = ggraph::circle(8,&quot;pt&quot;)) + geom_point(aes(size = p_size, x = x, y = y), color = clr_current, fill = fll_current(), shape = 21) + ggrepel::geom_text_repel(aes(label = culture, x = x, y = y), nudge_y = 2, nudge_x = 1, min.segment.length = unit(.5, &quot;npc&quot;), family = fnt_sel) + scale_edge_alpha_identity() + theme_minimal(base_family = fnt_sel) + labs(x = &#39;longitude&#39;, y = &#39;latitude&#39;) + theme(legend.position = &quot;none&quot;) tools_interval &lt;- tibble(logpop = seq(6, 14, length.out = 31)) %&gt;% mutate(lambda = purrr::map(logpop, function(x){ tibble(lambda = island_dist_posterior$alpha * exp(x) ^ island_dist_posterior$beta / island_dist_posterior$gamma ) })) %&gt;% unnest(lambda) %&gt;% group_by(logpop) %&gt;% summarise(lambda_pi = list(tibble(prob = quantile(lambda, probs = c(.1, .5, .9)), labels = c(&quot;l&quot;, &quot;m&quot;, &quot;h&quot;)))) %&gt;% unnest(lambda_pi) %&gt;% ungroup() %&gt;% pivot_wider(names_from = &quot;labels&quot;, values_from = &quot;prob&quot;) p2 &lt;- island_cor_graph %&gt;% create_layout(layout = . %N&gt;% as.tibble() %&gt;% dplyr::select(x = logpop, y = total_tools)) %&gt;% ggraph() + geom_edge_link(aes(alpha = cor ^ 2), color = clr_current, start_cap = ggraph::circle(8, &quot;pt&quot;), end_cap = ggraph::circle(8, &quot;pt&quot;)) + geom_point(aes(size = p_size, x = x, y = y), color = clr_current, fill = fll_current(), shape = 21)+ geom_smooth(data = tools_interval, stat = &quot;identity&quot;, aes(x = logpop, y = m, ymin = l, ymax = h), color = clr_dark, fill = clr0d %&gt;% clr_alpha(.3), linetype = 3, size = .6) + ggrepel::geom_text_repel(data = data_kline, aes(x = logpop, y = total_tools, label = culture), family = fnt_sel) + coord_cartesian(xlim = c(7, 12.5), ylim = c(10, 70)) + scale_edge_alpha_identity() + theme_minimal(base_family = fnt_sel) + labs(x = &#39;log population&#39;, y = &#39;total tools&#39;) + theme(legend.position = &quot;none&quot;) p1 + p2 Non-centered islands model_island_distance_non_centered &lt;- ulam( flist = alist( total_tools ~ dpois(lambda), lambda &lt;- (alpha * population ^ beta / gamma) * exp(k[culture_idx]), # non-centered gaussian prior transpars&gt; vector[10]: k &lt;&lt;- L_SIGMA * z, vector[10]:z ~ normal(0, 1), transpars&gt; matrix[10, 10]: L_SIGMA &lt;&lt;- cholesky_decompose( SIGMA ), transpars&gt; matrix[10, 10]: SIGMA &lt;- cov_GPL2( d_mat, eta_sq, rho_sq, 0.01), c(alpha, beta, gamma) ~ dexp(1), eta_sq ~ dexp(2), rho_sq ~ dexp(0.5) ), data = data_kline_list, chains = 4, cores = 4, iter = 2000, log_lik = TRUE ) precis(model_island_distance_non_centered) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 gamma 0.61 0.62 0.07 1.73 2162.49 1 beta 0.28 0.09 0.14 0.42 1032.31 1 alpha 1.39 1.06 0.23 3.38 3585.91 1 eta_sq 0.19 0.17 0.03 0.53 1237.34 1 rho_sq 1.29 1.62 0.08 4.42 2031.06 1 15.5.2 Phylogenetic Distance dag4 &lt;- dagify(G_2 ~ G_1 + U_1, B_2 ~ B_1 + U_1 + G_1, U_2 ~ U_1, exposure = &quot;G_2&quot;, outcome = &quot;B_2&quot;, coords = tibble(name = c(&quot;G_1&quot;, &quot;G_2&quot;, &quot;B_1&quot;, &quot;B_2&quot;, &quot;U_1&quot;, &quot;U_2&quot;), x = c(0, 1, 0, 1, 0, 1), y = c(1, 1, .5, .5, 0, 0))) p1 &lt;- dag4 %&gt;% fortify() %&gt;% mutate(name = str_replace(name, &quot;([A-Z])_([0-9])&quot; , &quot;\\\\1\\\\[\\\\2\\\\]&quot; ), stage = if_else(name == &quot;B[2]&quot;, &quot;response&quot;, if_else(name %in% c(&quot;G[2]&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr_current) + scale_x_continuous(limits = c(-.1, 1.1)) + labs(subtitle = &quot;causal timeseries&quot;) + scale_y_continuous(limits = c(-.1, 1.1)) + coord_fixed(ratio = .6) + theme(plot.subtitle = element_text(hjust = .5, family = fnt_sel)) dag5 &lt;- dagify(G ~ M + U, B ~ G + U + M, M ~ U, U ~ P, exposure = &quot;G&quot;, outcome = &quot;B&quot;, coords = tibble(name = c(&quot;G&quot;, &quot;B&quot;, &quot;M&quot;, &quot;U&quot;, &quot;P&quot;), x = c(0, 1, .5, .5, 1), y = c(1, 1, .5, 0, 0))) p2 &lt;- dag5 %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;B&quot;, &quot;response&quot;, if_else(name %in% c(&quot;G&quot;, &quot;M&quot;, &quot;P&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr_current) + scale_x_continuous(limits = c(-.1, 1.1)) + labs(subtitle = &quot;model with phylogeny&quot;) + scale_y_continuous(limits = c(-.1, 1.1)) + coord_fixed(ratio = .6) + theme(plot.subtitle = element_text(hjust = .5, family = fnt_sel)) p1 + p2 data(&quot;Primates301&quot;) data(&quot;Primates301_nex&quot;) data_primates &lt;- Primates301 %&gt;% as_tibble() %&gt;% mutate(label = as.character(name), across(c(body, brain, group_size), function(x){standardize(log(x))}, .names = &quot;{.col}_log_std&quot;)) library(ggtree) ggtree(Primates301_nex, layout = &#39;fan&#39;) %&gt;% .$data %&gt;% left_join(data_primates) %&gt;% ggtree(layout = &#39;fan&#39;, size = .2, aes(color = brain_log_std)) %&gt;% ggtree::rotate(node = 302) %&gt;% ggtree::rotate_tree(angle = 10) + geom_tiplab(aes(label = str_replace_all(label,&quot;_&quot;, &quot; &quot;)), size = 1.5, family = fnt_sel, fontface = &#39;italic&#39;) + scale_color_gradientn(colours = clr_pal(c1 = clr_saturate(clr_lighten(clr1),.3), c2 = clr2), na.value = &quot;black&quot;) + guides(color = guide_colorbar(title.position = &quot;top&quot;, barwidth = unit(.8, &quot;npc&quot;), barheight = unit(5, &quot;pt&quot;)))+ theme(legend.position = &quot;bottom&quot;, legend.title = element_text(family = fnt_sel), legend.text = element_text(family = fnt_sel)) Preparation: a ordinary regression without the phylogeny: \\[ \\begin{array}{rclr} B &amp; \\sim &amp; \\textrm{MVNormal}(\\mu, S) &amp; \\textrm{[likelihood]}\\\\ \\mu_{i} &amp; = &amp; \\alpha + \\beta_{G}~ G_{i} + \\beta_{M}~M_{i} &amp; \\textrm{[linear model]} \\end{array} \\] with \\(B\\) being a vector of species brain sizes and \\(S\\) being a covariance matrix with one row &amp; column per species in the form of \\(S = \\sigma^{2}~I\\) (\\(\\sigma\\) is the same standard deviation as always, and \\(I\\) is the identity matrix). data_primates_complete &lt;- data_primates %&gt;% filter(complete.cases(group_size, body, brain)) %&gt;% mutate(label = as.character(name), across(c(body, brain, group_size), function(x){standardize(log(x))}, .names = &quot;{.col}_log_std&quot;)) data_primates_list &lt;- data_primates_complete %&gt;% dplyr::select(body_log_std, brain_log_std, group_size_log_std) %&gt;% c(n_spp = nrow(.), ., list(i_mat = diag(nrow(.)))) model_primates_simple &lt;- ulam( flist = alist( brain_log_std ~ multi_normal(mu, SIGMA), mu &lt;- alpha + beta_m * body_log_std + beta_g * group_size_log_std, matrix[n_spp, n_spp]: SIGMA &lt;- i_mat * sigma_sq, alpha ~ normal( 0, 1 ), c(beta_m, beta_g) ~ normal( 0, 0.5 ), sigma_sq ~ exponential(1) ), data = data_primates_list, chains = 4, cores = 4 ) precis(model_primates_simple) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha 0.00 0.02 -0.03 0.03 1937.70 1 beta_g 0.12 0.02 0.09 0.16 1116.39 1 beta_m 0.89 0.02 0.86 0.93 1281.51 1 sigma_sq 0.05 0.01 0.04 0.06 1479.59 1 The brownian motion phylogeny model library(ape) tree_trimmed &lt;- keep.tip(Primates301_nex, tip = as.character(data_primates_complete$name)) ggtree(tree_trimmed, layout = &#39;fan&#39;) %&gt;% .$data %&gt;% left_join(data_primates_complete) %&gt;% ggtree(layout = &#39;fan&#39;, size = .2, aes(color = brain_log_std)) %&gt;% ggtree::rotate(node = 152) %&gt;% ggtree::rotate_tree(angle = 7) + geom_tiplab(aes(label = str_replace_all(label,&quot;_&quot;, &quot; &quot;)), size = 1.5, family = fnt_sel, fontface = &#39;italic&#39;) + scale_color_gradientn(colours = clr_pal(c1 = clr_saturate(clr_lighten(clr1),.3), c2 = clr2), na.value = &quot;black&quot;) + guides(color = guide_colorbar(title.position = &quot;top&quot;, barwidth = unit(.8, &quot;npc&quot;), barheight = unit(5, &quot;pt&quot;)))+ theme(legend.position = &quot;bottom&quot;, legend.title = element_text(family = fnt_sel), legend.text = element_text(family = fnt_sel)) Rho_brownian &lt;- corBrownian(phy = tree_trimmed) Var_Covar &lt;- vcv(Rho_brownian) dist_mat &lt;- cophenetic(tree_trimmed) p1 &lt;- tibble(phyl_distance = as.vector(dist_mat), covariance = as.vector(Var_Covar)) %&gt;% ggplot(aes(x = phyl_distance, y = covariance)) + geom_point(shape = 21, size = 2, color = clr0dd, fill = fll0) p2 &lt;- dist_mat %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;from&quot;) %&gt;% as_tibble() %&gt;% pivot_longer(-from, names_to = &quot;to&quot;, values_to = &quot;phyl_distance&quot;) %&gt;% mutate(across(c(from, to), function(str){str_replace_all(str, &quot;_&quot;, &quot; &quot;)})) %&gt;% ggplot(aes(x = from, y = to, fill = phyl_distance)) + scale_fill_gradientn(colours = c(clr1, clr0), na.value = &quot;black&quot;) p3 &lt;- Var_Covar %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;from&quot;) %&gt;% as_tibble() %&gt;% pivot_longer(-from, names_to = &quot;to&quot;, values_to = &quot;covariance&quot;) %&gt;% mutate(across(c(from, to), function(str){str_replace_all(str, &quot;_&quot;, &quot; &quot;)})) %&gt;% ggplot(aes(x = from, y = to, fill = covariance)) + scale_fill_gradientn(colours = c(clr0, clr2), na.value = &quot;black&quot;) p1 + (p2 + p3 &amp; geom_tile() &amp; guides(fill = guide_colorbar(title.position = &quot;top&quot;, barheight = unit(4, &quot;pt&quot;), barwidth = unit(150, &quot;pt&quot;))) &amp; theme(axis.text.x = element_text(face = &quot;italic&quot;, size = 3, angle = 90, hjust = 1), axis.text.y = element_text(face = &quot;italic&quot;, size = 3), axis.title = element_blank(), legend.position = &quot;bottom&quot;)) + plot_layout(widths = c(.5, 1), guides = &quot;collect&quot;) &amp; theme(legend.position = &quot;bottom&quot;) var_covar_sorted &lt;- Var_Covar[as.character(data_primates_complete$name), as.character(data_primates_complete$name)] data_primates_list2 &lt;- data_primates_complete %&gt;% dplyr::select(body_log_std, brain_log_std, group_size_log_std) %&gt;% c(n_spp = nrow(.), ., list(var_covar = var_covar_sorted, rho = var_covar_sorted / max(var_covar_sorted))) model_primates_brownian &lt;- ulam( flist = alist( brain_log_std ~ multi_normal(mu, SIGMA), mu &lt;- alpha + beta_m * body_log_std + beta_g * group_size_log_std, matrix[n_spp, n_spp]: SIGMA &lt;- rho * sigma_sq, alpha ~ normal( 0, 1 ), c(beta_m, beta_g) ~ normal( 0, 0.5 ), sigma_sq ~ exponential(1) ), data = data_primates_list2, chains = 4, cores = 4 ) precis(model_primates_brownian) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha -0.19 0.17 -0.46 0.08 2688.42 1 beta_g -0.01 0.02 -0.04 0.02 2386.98 1 beta_m 0.70 0.04 0.64 0.76 2171.25 1 sigma_sq 0.16 0.02 0.14 0.19 1897.77 1 The Ornstein-Uhlenbeck process phylogeny model (dampened random walk, returning to some mean, constraining variation, non-linear): The OU process (aka. L1 norm) defines the covariance between two species \\(i\\) and \\(j\\) with an exponential distance kernel: \\[ K(i,j) = \\eta^{2}~ \\textrm{exp}(-\\rho^{2} ~ D_{ij}) \\] data_primates_list3 &lt;- data_primates_complete %&gt;% dplyr::select(body_log_std, brain_log_std, group_size_log_std) %&gt;% c(n_spp = nrow(.), ., list(dist = dist_mat[as.character(data_primates_complete$name), as.character(data_primates_complete$name)] / max(dist_mat))) model_primates_OU_process &lt;- ulam( flist = alist( brain_log_std ~ multi_normal(mu, SIGMA), mu &lt;- alpha + beta_m * body_log_std + beta_g * group_size_log_std, matrix[n_spp, n_spp]: SIGMA &lt;- cov_GPL1(dist, eta_sq, rho_sq, 0.01), alpha ~ normal( 0, 1 ), c(beta_m, beta_g) ~ normal( 0, 0.5 ), eta_sq ~ half_normal( 1, 0.25 ), rho_sq ~ half_normal( 3, 0.25 ) ), data = data_primates_list3, chains = 4, cores = 4 ) precis(model_primates_OU_process) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha -0.07 0.07 -0.19 0.05 1901.13 1 beta_g 0.05 0.02 0.01 0.09 1991.85 1 beta_m 0.83 0.03 0.79 0.88 2188.88 1 eta_sq 0.03 0.01 0.03 0.05 1991.39 1 rho_sq 2.80 0.25 2.39 3.19 2310.90 1 new_dist &lt;- seq(0, 1, length.out = 31) prior_eta &lt;- abs(rnorm(1e3, 1, .25)) prior_rho &lt;- abs(rnorm(1e3, 3, 0.25)) prior_OU &lt;- tibble(distance = new_dist) %&gt;% mutate(covariance = purrr::map(distance, function(d){ prior_eta * exp(-prior_rho * d) })) %&gt;% unnest(covariance) %&gt;% group_by(distance) %&gt;% summarize(pi = list(tibble(prob = c(quantile(covariance, prob = c(.055,.5,.945)), mean(covariance)), lab = c(&quot;l&quot;,&quot;m&quot;,&quot;h&quot;,&quot;mean&quot;)))) %&gt;% ungroup() %&gt;% unnest(pi) %&gt;% pivot_wider(names_from = lab, values_from = prob) extract.samples(model_primates_OU_process) %&gt;% as_tibble() %&gt;% filter(row_number() &lt; 31) %&gt;% mutate(.draw = row_number(), covariance = map2(eta_sq, rho_sq, function(e, r){ tibble(covariance = e * exp(-r * new_dist), distance = new_dist) })) %&gt;% unnest(covariance) %&gt;% ggplot(aes(x = distance, y = covariance)) + geom_line(aes(group = .draw, color = &quot;posterior&quot;)) + geom_smooth(data = prior_OU, stat = &quot;identity&quot;, aes(color = &quot;prior&quot;, fill = after_scale(clr_alpha(color)), y = mean, ymin = l, ymax = h)) + scale_color_manual(&quot;&quot;, values = c(prior = clr0d, posterior = fll_current())) + theme(legend.direction = &quot;horizontal&quot;, legend.position = c(1, 1), legend.justification = c(1,1)) stancode(model_primates_OU_process) #&gt; functions{ #&gt; #&gt; #&gt; matrix cov_GPL1(matrix x, real sq_alpha, real sq_rho, real delta) { #&gt; int N = dims(x)[1]; #&gt; matrix[N, N] K; #&gt; for (i in 1:(N-1)) { #&gt; K[i, i] = sq_alpha + delta; #&gt; for (j in (i + 1):N) { #&gt; K[i, j] = sq_alpha * exp(-sq_rho * x[i,j] ); #&gt; K[j, i] = K[i, j]; #&gt; } #&gt; } #&gt; K[N, N] = sq_alpha + delta; #&gt; return K; #&gt; } #&gt; } #&gt; data{ #&gt; int n_spp; #&gt; vector[151] brain_log_std; #&gt; vector[151] group_size_log_std; #&gt; vector[151] body_log_std; #&gt; matrix[151,151] dist; #&gt; } #&gt; parameters{ #&gt; real alpha; #&gt; real beta_g; #&gt; real beta_m; #&gt; real&lt;lower=0&gt; eta_sq; #&gt; real&lt;lower=0&gt; rho_sq; #&gt; } #&gt; model{ #&gt; vector[151] mu; #&gt; matrix[n_spp,n_spp] SIGMA; #&gt; rho_sq ~ normal( 3 , 0.25 ); #&gt; eta_sq ~ normal( 1 , 0.25 ); #&gt; beta_m ~ normal( 0 , 0.5 ); #&gt; beta_g ~ normal( 0 , 0.5 ); #&gt; alpha ~ normal( 0 , 1 ); #&gt; SIGMA = cov_GPL1(dist, eta_sq, rho_sq, 0.01); #&gt; for ( i in 1:151 ) { #&gt; mu[i] = alpha + beta_m * body_log_std[i] + beta_g * group_size_log_std[i]; #&gt; } #&gt; brain_log_std ~ multi_normal( mu , SIGMA ); #&gt; } library(rlang) chapter14_models &lt;- env( ) write_rds(chapter14_models, &quot;envs/chapter14_models.rds&quot;) 15.6 Homework E1 original: \\[ \\begin{array}{rcl} y_{i} &amp; \\sim &amp; \\textrm{Normal}(\\mu_{i}, \\sigma)\\\\ \\mu_{i} &amp; = &amp; \\alpha_{\\textrm{GROUP}[i]} + \\beta~x_{i}\\\\ \\alpha_{\\textrm{GROUP}} &amp; \\sim &amp; \\textrm{Normal}(\\alpha, \\sigma_{\\alpha}) \\\\ \\alpha &amp; \\sim &amp; \\textrm{Normal}(0, 10) \\\\ \\beta &amp; \\sim &amp; \\textrm{Normal}(0, 1) \\\\ \\sigma &amp; \\sim &amp; \\textrm{Exponential}(1) \\\\ \\sigma_{\\alpha} &amp; \\sim &amp; \\textrm{Exponential}(1) \\\\ \\end{array} \\] with varying slopes \\[ \\begin{array}{rcl} y_{i} &amp; \\sim &amp; \\textrm{Normal}(\\mu_{i}, \\sigma)\\\\ \\mu_{i} &amp; = &amp; \\alpha_{\\textrm{GROUP}[i]} + \\beta_{\\color{#54436D}{GROUP[i]}}~x_{i}\\\\ \\color{#54436D}{\\left[\\begin{array}{c}\\alpha_{GROUP}\\\\\\beta_{GROUP}\\end{array}\\right]} &amp; \\sim &amp; \\color{#54436D}{\\textrm{MVNormal}\\left(\\left[\\begin{array}{c}\\alpha\\\\\\beta\\end{array}\\right], S\\right)}\\\\ \\color{#54436D}{S} &amp; \\sim &amp; \\color{#54436D}{\\left(\\begin{array}{cc}\\sigma_{\\alpha} &amp; 0 \\\\ 0 &amp;\\sigma_{\\beta}\\end{array}\\right) R \\left(\\begin{array}{cc}\\sigma_{\\alpha} &amp; 0 \\\\ 0 &amp;\\sigma_{\\beta}\\end{array}\\right)}\\\\ \\alpha &amp; \\sim &amp; \\textrm{Normal}(0, 10) \\\\ \\beta &amp; \\sim &amp; \\textrm{Normal}(0, 1) \\\\ \\sigma &amp; \\sim &amp; \\textrm{Exponential}(1) \\\\ \\sigma_{\\alpha} &amp; \\sim &amp; \\textrm{Exponential}(1) \\\\ \\color{#54436D}{\\sigma_{\\beta}} &amp; \\sim &amp; \\color{#54436D}{\\textrm{Exponential}(1)} \\\\ \\color{#54436D}{R} &amp; \\sim &amp; \\color{#54436D}{\\textrm{LKJcorr}(2)} \\end{array} \\] E2 Any system with a positive feedback loop / positive density dependence. For example Allee effects - if survival of offspring increases with colony size and food availability. E3 When the covariance acts in a very regularizing fashion leading to a strong shrinkage. This happens when there is little variation between the clusters. M1 set.seed(42) alpha &lt;- 3.5 # average morning waiting time beta &lt;- -1 # average difference between morning and afternoon waiting time sigma_alpha &lt;- 1 # std dev in intercepts sigma_beta &lt;- 0.5 # std dev in slopes rho &lt;- 0 # correlation between intercepts and slopes Mu &lt;- c(alpha, beta) cov_ab &lt;- sigma_alpha * sigma_beta * rho Sigma &lt;- matrix(c(sigma_alpha^2, cov_ab, cov_ab, sigma_beta^2), ncol = 2) n_cafes &lt;- 20 n_visits &lt;- 10 sigma &lt;- 0.5 # std dev within cafes # simulate observations set.seed(6) vary_effects &lt;- mvrnorm(n_cafes, Mu, Sigma) %&gt;% data.frame() %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;alpha_cafe&quot;, &quot;beta_cafe&quot;)) data_cafe_resim &lt;- vary_effects %&gt;% mutate(cafe = 1:n_cafes) %&gt;% tidyr::expand(nesting(cafe, alpha_cafe, beta_cafe), visit = 1:n_visits) %&gt;% mutate(afternoon = rep(0:1, times = n() / 2)) %&gt;% mutate(mu = alpha_cafe + beta_cafe * afternoon, waiting_time = rnorm(n_visits * n_cafes, mu, sigma)) data_cafe_resim_list &lt;- data_cafe_resim %&gt;% dplyr::select(cafe, afternoon, waiting_time) %&gt;% as.list model_cafe_resim &lt;- ulam( flist = alist( waiting_time ~ normal( mu, sigma ), mu &lt;- alpha_cafe[cafe] + beta_cafe[cafe] * afternoon, c(alpha_cafe, beta_cafe)[cafe] ~ multi_normal( c(alpha, beta ), Rho, sigma_cafe ), alpha ~ normal(5, 2), beta ~ normal(-1, 0.5), sigma_cafe ~ exponential(1), sigma ~ exponential(1), Rho ~ lkj_corr(2) ), data = data_cafe_resim_list, cores = 4, chains = 4, seed = 42, log_lik = TRUE ) set.seed(42) p2 &lt;- extract.samples(model_cafe_resim) %&gt;% data.frame() %&gt;% as_tibble() %&gt;% ggplot() + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_density(aes(x = Rho.2, color = &quot;posterior&quot;, fill = after_scale(clr_alpha(color))), adjust = .8) + geom_density(data = eta_rlk(eta = 2), aes(x = bl, color = &quot;prior&quot;, fill = after_scale(clr_alpha(color))), adjust = .6) + scale_color_manual(&quot;&quot;, values = c(prior = clr0d, posterior = clr_dark), guide = guide_legend(nrow = 2, keyheight = unit(7, &quot;pt&quot;))) + labs(x = NULL, subtitle = &quot;correlation between intercept and slope&quot;) + theme(legend.position = c(1, 1), legend.justification = c(1,1), legend.direction = &quot;horizontal&quot;) p1 &lt;- tibble(level = c(.1, .3, .5, .8, .99)) %&gt;% mutate(ellipse = purrr::map(level, function(level){ ellipse(Sigma, centre = Mu, level = level) %&gt;% data.frame() %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;alpha_cafe&quot;, &quot;beta_cafe&quot;)) })) %&gt;% unnest(ellipse) %&gt;% ggplot(aes(x = alpha_cafe, y = beta_cafe)) + geom_path(aes(group = factor(level), alpha = -level), color = clr_dark, size = .25) + geom_point(data = vary_effects, color = clr0dd, fill = clr0, shape = 21, size = 1.5) + theme(legend.position = &quot;none&quot;) p1 + p2 M2 \\[ \\begin{array}{rcl} W_{i} &amp; \\sim &amp; \\textrm{Normal}(\\mu_{i}, \\sigma)\\\\ \\mu_{i} &amp; = &amp; \\alpha_{\\textrm{CAFE}[i]} + \\beta_{\\textrm{CAFE}[i]}~A_{i}\\\\ \\alpha_{\\textrm{CAFE}} &amp; \\sim &amp; \\textrm{Normal}(\\alpha, \\sigma_{\\alpha}) \\\\ \\beta_{\\textrm{CAFE}} &amp; \\sim &amp; \\textrm{Normal}(\\beta, \\sigma_{\\beta}) \\\\ \\alpha &amp; \\sim &amp; \\textrm{Normal}(0, 10) \\\\ \\beta &amp; \\sim &amp; \\textrm{Normal}(0, 1) \\\\ \\sigma, \\sigma_{\\alpha}, \\sigma_{\\beta} &amp; \\sim &amp; \\textrm{Exponential}(1) \\\\ \\end{array} \\] model_cafe_no_covar &lt;- ulam( flist = alist( waiting_time ~ normal( mu, sigma ), mu &lt;- alpha_cafe[cafe] + beta_cafe[cafe] * afternoon, alpha_cafe[cafe] ~ normal( alpha, sigma_alpha ), beta_cafe[cafe] ~ normal( beta, sigma_beta ), alpha ~ normal(5, 2), beta ~ normal(-1, 0.5), sigma ~ exponential(1), sigma_alpha ~ exponential(1), sigma_beta ~ exponential(1) ), data = data_cafe_list, cores = 4, chains = 4, seed = 42, log_lik = TRUE ) compare(model_cafe, model_cafe_no_covar) %&gt;% knit_precis() param WAIC SE dWAIC dSE pWAIC weight model_cafe 315.92 21.4 0.00 NA 31.70 0.85 model_cafe_no_covar 319.43 21.5 3.52 2.9 32.16 0.15 M3 chapter11_models &lt;- read_rds(&quot;envs/chapter11_models.rds&quot;) data_ucb_list3 &lt;- chapter11_models$data_ucb %&gt;% mutate(male = 2 - gid, dept_idx = as.numeric(dept)) %&gt;% dplyr::select(admit, applications, dept_idx, male) %&gt;% as.list() model_ucb_dept_centered &lt;- ulam( flist = alist( admit ~ dbinom( applications, p ), logit(p) &lt;- alpha_dept[dept_idx] + beta_dept[dept_idx] * male, c(alpha_dept, beta_dept)[dept_idx] ~ multi_normal( c(alpha, beta ), Rho, sigma_dept ), alpha ~ dnorm( 0, 1 ), beta ~ dnorm( 0, 1 ), sigma_dept ~ exponential(1), Rho ~ lkj_corr(2) ), data = data_ucb_list3, iter = 4000, chains = 4, cores = 4, log_lik = TRUE ) model_ucb_dept_non_center &lt;- ulam( flist = alist( admit ~ dbinom( applications, p ), logit(p) &lt;- ( alpha_dept_bar + v[dept_idx, 1]) + (beta_dept_bar + v[dept_idx, 2] ) * male, transpars &gt; matrix[dept_idx, 2]:v &lt;- compose_noncentered(sigma_dept, L_Rho, z), matrix[2, dept_idx]:z ~ dnorm(0, 1), alpha_dept_bar ~ dnorm( 0, 1.5 ), beta_dept_bar ~ dnorm( 0, 1 ), vector[2]:sigma_dept ~ dexp(1), cholesky_factor_corr[2]:L_Rho ~ lkj_corr_cholesky(2) ), data = data_ucb_list3, iter = 4000, chains = 4, cores = 4, log_lik = TRUE ) precis(model_ucb_dept_centered, depth = 3) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 beta_dept[1] -0.77 0.27 -1.21 -0.33 2525.47 1 beta_dept[2] -0.21 0.32 -0.72 0.29 4398.76 1 beta_dept[3] 0.08 0.14 -0.15 0.30 7234.94 1 beta_dept[4] -0.09 0.14 -0.32 0.13 7512.79 1 beta_dept[5] 0.11 0.18 -0.18 0.41 6345.83 1 beta_dept[6] -0.12 0.26 -0.54 0.30 5586.59 1 alpha_dept[1] 1.28 0.26 0.87 1.70 2716.39 1 alpha_dept[2] 0.74 0.32 0.23 1.25 4457.76 1 alpha_dept[3] -0.65 0.09 -0.78 -0.51 7406.41 1 alpha_dept[4] -0.62 0.10 -0.79 -0.45 7560.44 1 alpha_dept[5] -1.13 0.11 -1.31 -0.95 6307.85 1 alpha_dept[6] -2.60 0.20 -2.93 -2.28 6130.43 1 alpha -0.37 0.51 -1.15 0.45 7013.77 1 beta -0.17 0.22 -0.52 0.15 4882.39 1 sigma_dept[1] 1.47 0.47 0.91 2.29 5755.02 1 sigma_dept[2] 0.45 0.23 0.18 0.85 3000.35 1 Rho[1,1] 1.00 0.00 1.00 1.00 NaN NaN Rho[1,2] -0.32 0.33 -0.79 0.27 5674.90 1 Rho[2,1] -0.32 0.33 -0.79 0.27 5674.90 1 Rho[2,2] 1.00 0.00 1.00 1.00 NaN NaN precis(model_ucb_dept_non_center, depth = 3) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 z[1,1] 1.26 0.54 0.45 2.18 2702.38 1 z[1,2] 0.87 0.49 0.11 1.68 2517.45 1 z[1,3] -0.15 0.39 -0.77 0.48 1915.71 1 z[1,4] -0.13 0.39 -0.75 0.50 1982.60 1 z[1,5] -0.51 0.41 -1.17 0.14 2019.43 1 z[1,6] -1.59 0.59 -2.57 -0.72 2376.73 1 z[2,1] -1.17 0.79 -2.45 0.04 5147.44 1 z[2,2] 0.20 0.79 -1.05 1.44 6469.73 1 z[2,3] 0.65 0.60 -0.25 1.64 4955.89 1 z[2,4] 0.15 0.59 -0.77 1.09 5035.67 1 z[2,5] 0.59 0.66 -0.42 1.67 5732.59 1 z[2,6] -0.45 0.82 -1.76 0.84 6312.38 1 alpha_dept_bar -0.44 0.57 -1.34 0.45 1994.81 1 beta_dept_bar -0.17 0.21 -0.50 0.14 3182.53 1 sigma_dept[1] 1.47 0.46 0.91 2.28 3055.78 1 sigma_dept[2] 0.45 0.23 0.18 0.84 2553.50 1 L_Rho[1,1] 1.00 0.00 1.00 1.00 NaN NaN L_Rho[1,2] 0.00 0.00 0.00 0.00 NaN NaN L_Rho[2,1] -0.31 0.34 -0.80 0.31 5547.21 1 L_Rho[2,2] 0.88 0.14 0.61 1.00 4659.42 1 v[1,1] 1.71 0.61 0.76 2.68 2296.12 1 v[1,2] -0.59 0.30 -1.10 -0.14 3745.01 1 v[2,1] 1.18 0.62 0.17 2.18 2429.83 1 v[2,2] -0.04 0.32 -0.54 0.47 6847.96 1 v[3,1] -0.20 0.58 -1.11 0.70 1981.49 1 v[3,2] 0.25 0.23 -0.10 0.62 3293.03 1 v[4,1] -0.18 0.58 -1.08 0.72 2035.30 1 v[4,2] 0.07 0.23 -0.28 0.45 3923.55 1 v[5,1] -0.69 0.58 -1.60 0.21 2041.55 1 v[5,2] 0.28 0.25 -0.09 0.71 4081.96 1 v[6,1] -2.16 0.60 -3.11 -1.23 2116.32 1 v[6,2] 0.05 0.30 -0.42 0.51 5801.23 1 compare(model_ucb_dept_centered, model_ucb_dept_non_center) %&gt;% knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight model_ucb_dept_centered 91.71 4.85 0.00 NA 7.09 0.56 model_ucb_dept_non_center 92.18 5.07 0.47 0.44 7.31 0.44 M4 precis(chapter11_models$model_ocean_scientific, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha[1] 0.90 0.69 -0.28 1.96 629.57 1 alpha[2] 0.91 0.86 -0.51 2.25 871.55 1 beta[1] 0.26 0.03 0.21 0.32 1000.66 1 beta[2] 0.29 0.10 0.13 0.47 748.66 1 gamma 1.16 0.79 0.29 2.50 851.27 1 precis(model_island_distance, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 k[1] -0.17 0.31 -0.66 0.31 547.99 1.00 k[2] -0.01 0.30 -0.47 0.46 483.06 1.01 k[3] -0.06 0.29 -0.51 0.38 462.31 1.00 k[4] 0.36 0.27 -0.04 0.80 487.83 1.01 k[5] 0.07 0.26 -0.30 0.49 469.88 1.01 k[6] -0.38 0.27 -0.82 0.01 515.66 1.01 k[7] 0.14 0.26 -0.25 0.55 469.41 1.01 k[8] -0.21 0.27 -0.63 0.20 498.61 1.01 k[9] 0.26 0.26 -0.11 0.66 434.98 1.01 k[10] -0.17 0.36 -0.73 0.35 574.67 1.01 gamma 0.61 0.55 0.08 1.65 1364.72 1.00 beta 0.28 0.08 0.15 0.41 926.52 1.00 alpha 1.43 1.08 0.25 3.41 1801.02 1.00 eta_sq 0.18 0.18 0.03 0.51 589.68 1.00 rho_sq 1.39 1.77 0.08 4.52 2007.75 1.00 compare(chapter11_models$model_ocean_scientific, model_island_distance_non_centered) #&gt; WAIC SE dWAIC dSE #&gt; model_island_distance_non_centered 67.39234 2.29028 0.0000 NA #&gt; chapter11_models$model_ocean_scientific 80.01854 11.18399 12.6262 11.19445 #&gt; pWAIC weight #&gt; model_island_distance_non_centered 4.022846 0.998190876 #&gt; chapter11_models$model_ocean_scientific 4.906543 0.001809124 M5 model_primates_OU_process_reverse &lt;- ulam( flist = alist( group_size_log_std ~ multi_normal(mu, SIGMA), mu &lt;- alpha + beta_m * body_log_std + beta_b * brain_log_std, matrix[n_spp, n_spp]: SIGMA &lt;- cov_GPL1(dist, eta_sq, rho_sq, 0.01), alpha ~ normal( 0, 1 ), c(beta_m, beta_b) ~ normal( 0, 0.5 ), eta_sq ~ half_normal( 1, 0.25 ), rho_sq ~ half_normal( 3, 0.25 ) ), data = data_primates_list3, chains = 4, cores = 4 ) precis(model_primates_OU_process_reverse) #&gt; mean sd 5.5% 94.5% n_eff Rhat4 #&gt; alpha -0.5053841 0.3456288 -1.0381465 0.05768045 1376.664 1.001963 #&gt; beta_b 0.1966539 0.2548399 -0.2176509 0.59215235 1166.416 1.003252 #&gt; beta_m 0.1782800 0.2290642 -0.1792382 0.55471525 1018.621 1.003373 #&gt; eta_sq 0.9301265 0.1236063 0.7479650 1.13487715 1420.556 1.002007 #&gt; rho_sq 3.0304659 0.2488279 2.6482468 3.41942840 1619.587 1.000517 precis(model_primates_OU_process) #&gt; mean sd 5.5% 94.5% n_eff Rhat4 #&gt; alpha -0.06792699 0.073265723 -0.18708943 0.04727763 1901.133 1.0000575 #&gt; beta_g 0.04932900 0.022629955 0.01361525 0.08654041 1991.855 0.9992546 #&gt; beta_m 0.83344136 0.029704956 0.78608237 0.88026958 2188.879 0.9984471 #&gt; eta_sq 0.03486782 0.006610176 0.02568661 0.04643301 1991.390 1.0002500 #&gt; rho_sq 2.79547319 0.245505138 2.39143175 3.18569015 2310.902 0.9997646 H1 data(&quot;bangladesh&quot;) data_bangaldesh &lt;- bangladesh %&gt;% as_tibble() %&gt;% mutate(district_idx = as.integer(as.factor(district))) %&gt;% rename(contraception = use.contraception) data_bangaldesh_list &lt;- data_bangaldesh %&gt;% dplyr::select(woman, district_idx, contraception, urban) %&gt;% as.list() model_bangladesh_covar &lt;- ulam( flist = alist( contraception ~ dbinom(1, p), logit(p) &lt;- ( alpha_dist_bar + v[district_idx, 1]) + (beta_dist_bar + v[district_idx, 2] ) * urban, transpars &gt; matrix[district_idx, 2]:v &lt;- compose_noncentered(sigma_dist, L_Rho, z), matrix[2, district_idx]:z ~ dnorm(0, 1), alpha_dist_bar ~ dnorm( 0, 1.5 ), beta_dist_bar ~ dnorm( 0, 1 ), vector[2]:sigma_dist ~ dexp(1), cholesky_factor_corr[2]:L_Rho ~ lkj_corr_cholesky(2), gq&gt; matrix[2,2]:Rho &lt;&lt;- Chol_to_Corr(L_Rho)), data = data_bangaldesh_list, cores = 4, chain = 4, iter = 4000, log_lik = TRUE ) model_bangladesh_covar_centered &lt;- ulam( alist( contraception ~ bernoulli(p), logit(p) &lt;- alpha[district_idx] + beta[district_idx] * urban, c(alpha, beta)[district_idx] ~ multi_normal(c(alpha_bar, beta_bar), Rho, Sigma), alpha_bar ~ normal(0, 1), beta_bar ~ normal(0, 0.5), Rho ~ lkj_corr(2), Sigma ~ exponential(1) ), data = data_bangaldesh_list, chains = 4, cores = 4, iter = 4000 ) precis(model_bangladesh_covar, depth = 3, pars = c(&quot;alpha_dist_bar&quot; ,&quot;beta_dist_bar&quot;, &quot;sigma_dist&quot;,&quot;Rho&quot;)) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_dist_bar -0.71 0.10 -0.87 -0.55 4630.22 1 beta_dist_bar 0.70 0.17 0.43 0.97 5150.31 1 sigma_dist[1] 0.58 0.10 0.43 0.74 3147.76 1 sigma_dist[2] 0.79 0.21 0.48 1.14 2553.73 1 Rho[1,1] 1.00 0.00 1.00 1.00 NaN NaN Rho[1,2] -0.66 0.17 -0.87 -0.36 3412.32 1 Rho[2,1] -0.66 0.17 -0.87 -0.36 3412.32 1 Rho[2,2] 1.00 0.00 1.00 1.00 NaN NaN precis(model_bangladesh_covar_centered, depth = 3, pars = c(&quot;alpha_bar&quot; ,&quot;beta_bar&quot;, &quot;Sigma&quot;,&quot;Rho&quot;)) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_bar -0.69 0.10 -0.85 -0.53 5812.85 1.00 beta_bar 0.64 0.16 0.38 0.90 3766.35 1.00 Sigma[1] 0.58 0.10 0.43 0.74 1707.52 1.00 Sigma[2] 0.79 0.20 0.48 1.13 904.70 1.01 Rho[1,1] 1.00 0.00 1.00 1.00 NaN NaN Rho[1,2] -0.65 0.17 -0.86 -0.34 1428.75 1.00 Rho[2,1] -0.65 0.17 -0.86 -0.34 1428.75 1.00 Rho[2,2] 1.00 0.00 1.00 1.00 NaN NaN alpha_beta_means &lt;- extract.samples(model_bangladesh_covar_centered) %&gt;% as_tibble() %&gt;% summarise(beta_mean = colMeans(beta), alpha_mean = colMeans(alpha)) extract.samples(model_bangladesh_covar_centered) %&gt;% as_tibble() %&gt;% dplyr::select(alpha, beta) %&gt;% mutate(across(everything(),as_tibble)) %&gt;% summarise(alpha = alpha %&gt;% pivot_longer(everything(),names_to = &quot;dist&quot;, values_to = &quot;alpha&quot;), beta = beta %&gt;% pivot_longer(everything(),names_to = &quot;dist&quot;, values_to = &quot;beta&quot;)) %&gt;% unnest(alpha, beta) %&gt;% dplyr::select(-dist1) %&gt;% ggplot(aes( alpha, beta)) + ( (c(1:9, 9.9)/10) %&gt;% purrr::map(.f = function(lvl){ stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = lvl,linetype = 3, size = .2, alpha = .05, color = &quot;black&quot;, fill = &quot;black&quot;) }) ) + geom_point(data = alpha_beta_means, aes(x = alpha_mean, y = beta_mean), shape = 21, size = 1.5, color = clr_dark, fill = fll0dd) + geom_hline(yintercept = 0, color = clr_dark, linetype = 3) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + coord_cartesian(xlim = c(-2, .6), ylim = c(-.5, 1.6)) H2 dag &lt;- dagify(C ~ A + N,# + U + D, N ~ A, exposure = &quot;C&quot;, outcome = &quot;A&quot;, coords = tibble(name = c(&quot;A&quot;, &quot;N&quot;, &quot;C&quot;),#, &quot;U&quot;, &quot;D&quot;), x = c(0, .5, 1),#, 1.5, 1.5), y = c(0, 1, 0))#, 0, 1)) ) dag %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;C&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;N&quot;, &quot;U&quot;, &quot;D&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr2) + scale_x_continuous(limits = c(-.1, 1.1)) + labs(subtitle = &quot;instrumental variable&quot;) + scale_y_continuous(limits = c(-.1, 1.1)) + coord_fixed(ratio = .6) + theme(plot.subtitle = element_text(hjust = .5, family = fnt_sel)) data_bangaldesh_list2 &lt;- data_bangaldesh %&gt;% mutate(age_std = standardize(age.centered), children_std = standardize(living.children)) %&gt;% dplyr::select(woman, district_idx, contraception, urban, age_std, children_std) %&gt;% as.list() model_bangladesh_age &lt;- ulam( alist( contraception ~ bernoulli(p), logit(p) &lt;- alpha[district_idx] + beta[district_idx] * urban + beta_age * age_std, c(alpha, beta)[district_idx] ~ multi_normal(c(alpha_bar, beta_bar), Rho, Sigma), alpha_bar ~ normal(0, 1), c(beta_bar, beta_age ) ~ normal(0, 0.5), Rho ~ lkj_corr(2), Sigma ~ exponential(1) ), data = data_bangaldesh_list2, chains = 4, cores = 4, iter = 4000 ) model_bangladesh_age_children &lt;- ulam( alist( contraception ~ bernoulli(p), logit(p) &lt;- alpha[district_idx] + beta[district_idx] * urban + beta_age * age_std + beta_children * children_std, c(alpha, beta)[district_idx] ~ multi_normal(c(alpha_bar, beta_bar), Rho, Sigma), alpha_bar ~ normal(0, 1), c(beta_bar, beta_age, beta_children ) ~ normal(0, 0.5), Rho ~ lkj_corr(2), Sigma ~ exponential(1) ), data = data_bangaldesh_list2, chains = 4, cores = 4, iter = 4000 ) precis(model_bangladesh_age) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_bar -0.69 0.10 -0.85 -0.53 5344.89 1 beta_age 0.08 0.05 0.00 0.16 13446.44 1 beta_bar 0.63 0.16 0.38 0.89 4286.55 1 precis(model_bangladesh_age_children) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_bar -0.72 0.10 -0.88 -0.55 5796.66 1 beta_children 0.51 0.07 0.40 0.63 8339.46 1 beta_age -0.27 0.07 -0.38 -0.15 8690.24 1 beta_bar 0.69 0.16 0.43 0.95 3952.92 1 H3 data_bangaldesh_list3 &lt;- data_bangaldesh %&gt;% mutate(age_std = standardize(age.centered)) %&gt;% dplyr::select(woman, district_idx, contraception, urban, age_std, children = living.children) %&gt;% as.list() %&gt;% c(., list(alpha = rep(2,3))) model_bangladesh_age_ordered &lt;- ulam( alist( contraception ~ bernoulli(p), logit(p) &lt;- alpha_distr[district_idx] + beta_distr[district_idx] * urban + beta_age * age_std + beta_children * sum( delta_shell[1:children] ), c(alpha_distr, beta_distr)[district_idx] ~ multi_normal(c(alpha_bar, beta_bar), Rho, Sigma), alpha_bar ~ normal(0, 1), c(beta_bar, beta_age, beta_children ) ~ normal(0, 0.5), Rho ~ lkj_corr(2), Sigma ~ exponential(1), vector[4]: delta_shell &lt;&lt;- append_row( 0 , delta ), simplex[3]: delta ~ dirichlet( alpha ) ), data = data_bangaldesh_list3, chains = 4, cores = 4, iter = 4000 ) precis(model_bangladesh_age_ordered) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_bar -1.55 0.15 -1.79 -1.31 2156.56 1 beta_children 1.26 0.15 1.02 1.51 2188.79 1 beta_age -0.23 0.06 -0.33 -0.12 5092.98 1 beta_bar 0.69 0.16 0.44 0.94 4257.01 1 precis(model_bangladesh_age_ordered, 3, pars = &quot;delta&quot;) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 delta[1] 0.74 0.08 0.60 0.87 10830.19 1 delta[2] 0.17 0.08 0.05 0.30 10823.63 1 delta[3] 0.09 0.05 0.02 0.19 12210.01 1 H4 data(&quot;Oxboys&quot;) data_ox &lt;- Oxboys %&gt;% as_tibble() %&gt;% mutate(subject_idx = coerce_index(Subject), age_std = standardize(age)) data_ox %&gt;% ggplot(aes(x = age, y = height, group = subject_idx)) + geom_line(color = clr_dark) data_ox_list &lt;- data_ox %&gt;% dplyr::select(height, age_std, subject_idx) %&gt;% as.list() model_ox &lt;- ulam( flist = alist( height ~ dnorm(mu, sigma), mu &lt;- alpha_bar + alpha_age[subject_idx] + (beta_bar + beta_age[subject_idx]) * age_std, alpha_bar ~ dnorm(150, 10), beta_bar ~ dnorm(0, 10), c(alpha_age, beta_age)[subject_idx] ~ multi_normal(0, Rho_idx, Sigma_idx), Sigma_idx ~ dexp(1), Rho_idx ~ dlkjcorr(1), sigma ~ dexp(1) ), data = data_ox_list, cores = 4, chains = 4, log_lik = TRUE, iter = 4000, seed = 42 ) precis(model_ox, depth = 2, pars = c(&quot;alpha_bar&quot;, &quot;beta_bar&quot;, &quot;Sigma_idx&quot;)) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_bar 149.47 1.38 147.13 151.61 317.42 1 beta_bar 4.21 0.21 3.88 4.54 449.01 1 Sigma_idx[1] 7.39 0.90 6.10 8.91 5486.74 1 Sigma_idx[2] 1.07 0.15 0.85 1.34 4938.46 1 H5 precis(model_ox, depth = 3, pars = c(&quot;Rho_idx&quot;)) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 Rho_idx[1,1] 1.00 0.00 1.00 1.00 NaN NaN Rho_idx[1,2] 0.55 0.13 0.32 0.73 5832.03 1 Rho_idx[2,1] 0.55 0.13 0.32 0.73 5832.03 1 Rho_idx[2,2] 1.00 0.00 1.00 1.00 NaN NaN posterior_rho &lt;- extract.samples(model_ox) %&gt;% as_tibble() %&gt;% dplyr::select(Rho_idx) %&gt;% mutate(Rho_idx = as_tibble(Rho_idx)) %&gt;% unnest(cols = c(Rho_idx)) %&gt;% set_names(nm = c(&quot;rho_alpha&quot;, &quot;rho_alpha_beta&quot;, &quot;rho_beta_alpha&quot;, &quot;rho_beta&quot;)) posterior_rho %&gt;% ggplot(aes(x = rho_alpha_beta)) + geom_density(adjust = .7, color = clr0d, fill = fll0)+ labs(x = &quot;rho&quot;) H6 posterior_means &lt;- extract.samples(model_ox) %&gt;% as_tibble() %&gt;% dplyr::select(Sigma_idx) %&gt;% mutate(Sigma_idx = as_tibble(Sigma_idx)) %&gt;% unnest(cols = c(Sigma_idx)) %&gt;% set_names(nm = c(&quot;sigma_alpha&quot;, &quot;sigma_beta&quot;)) %&gt;% bind_cols(posterior_rho) %&gt;% bind_cols( extract.samples(model_ox) %&gt;% as_tibble() %&gt;% dplyr::select(sigma, alpha_bar, beta_bar)) %&gt;% summarise(across(everything(),mean)) S &lt;- matrix( c( posterior_means$sigma_alpha ^ 2 , posterior_means$sigma_alpha * posterior_means$sigma_beta * posterior_means$rho_alpha_beta, posterior_means$sigma_alpha * posterior_means$sigma_beta * posterior_means$rho_alpha_beta, posterior_means$sigma_beta ^ 2 ), nrow = 2 ) round( S , 2 ) \\[\\begin{bmatrix} 54.62 &amp;4.36 \\\\4.36 &amp;1.15 \\\\ \\end{bmatrix}\\] set.seed(42) mvrnorm(n = 11, mu = c(0, 0), Sigma = S) %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;alpha&quot;, &quot;beta&quot;)) %&gt;% mutate(.draw = row_number(), heights = map2(alpha, beta, function(a,b){ tibble(age = seq(-1, 1, length.out = 9), height = rnorm(9, mean = posterior_means$alpha_bar + a + (posterior_means$beta_bar + b) * age, sd = posterior_means$sigma)) })) %&gt;% unnest(heights) %&gt;% ggplot(aes(x = age, y = height, group = .draw)) + geom_line(color = clr_dark) 15.7 {brms} section 15.7.1 Varying Slopes by Construction The varying slopes model As it turns out, the shape of the LKJ is sensitive to both \\(\\eta\\) and the \\(K\\) dimensions of the correlation matrix. Our simulations only considered the shapes for when \\(K=2\\). We can use a combination of the parse_dist() and stat_dist_halfeye() functions from the tidybayes package to derive analytic solutions for different combinations of \\(\\eta\\) and \\(K\\). library(tidybayes) crossing(k = 2:5, eta = 1:4) %&gt;% mutate(prior = str_c(&quot;lkjcorr_marginal(&quot;, k, &quot;, &quot;, eta, &quot;)&quot;), strip = str_c(&quot;K = &quot;, k)) %&gt;% parse_dist(prior) %&gt;% ggplot(aes(y = eta, dist = .dist, args = .args)) + stat_dist_halfeye(.width = c(.5, .95), color = clr_dark, fill = clr0) + scale_x_continuous(expression(rho), limits = c(-1, 1), breaks = c(-1, -.5, 0, .5, 1), labels = c(&quot;-1&quot;, &quot;-.5&quot;, &quot;0&quot;, &quot;.5&quot;, &quot;1&quot;)) + scale_y_continuous(glue(&quot;*{mth(&#39;\\U03B7&#39;)}*&quot;), breaks = 1:4) + labs(subtitle = glue(&quot;Marginal correlation for the LKJ prior relative to K and *{mth(&#39;\\U03B7&#39;)}*&quot;)) + facet_wrap(~ strip, ncol = 4) + theme(axis.title.y = element_markdown(), plot.subtitle = element_markdown(), panel.grid = element_blank(), panel.border = element_rect(fill = &quot;transparent&quot;, color = clr0d)) brms_c14_model_cafe &lt;- brm( data = data_cafe, family = gaussian, waiting_time ~ 1 + afternoon + (1 + afternoon | cafe), prior = c(prior(normal(5, 2), class = Intercept), prior(normal(-1, 0.5), class = b), prior(exponential(1), class = sd), prior(exponential(1), class = sigma), prior(lkj(2), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c14_model_cafe&quot;) brms_cafe_posterior &lt;- as_draws_df(brms_c14_model_cafe) %&gt;% as_tibble() r_2 &lt;- rlkjcorr(1e4, K = 2, eta = 2) %&gt;% as_tibble() brms_cafe_posterior %&gt;% ggplot() + geom_density(data = r_2, aes(x = V2, color = &quot;prior&quot;, fill = after_scale(clr_alpha(color))), adjust = .7) + geom_density(aes(x = cor_cafe__Intercept__afternoon, color = &quot;posterior&quot;, fill = after_scale(clr_alpha(color))), adjust = .7) + scale_y_continuous(NULL, breaks = NULL) + scale_color_manual(&quot;&quot;,values = c(clr_dark, clr0d))+ labs(subtitle = &quot;Correlation between intercepts\\nand slopes, prior and posterior&quot;, x = &quot;correlation&quot;)+ theme(legend.position = c(1,1), legend.justification = c(1,1), legend.direction = &quot;horizontal&quot;) # we select each of the 20 cafe&#39;s posterior mean (i.e., Estimate) # for both `Intercept` and `afternoon` partially_pooled_params &lt;- coef(brms_c14_model_cafe)$cafe[ , 1, 1:2] %&gt;% as_tibble() %&gt;% rename(Slope = afternoon) %&gt;% mutate(cafe = 1:nrow(.)) %&gt;% dplyr::select(cafe, everything()) un_pooled_params &lt;- data_cafe %&gt;% group_by(afternoon, cafe) %&gt;% summarise(mean = mean(waiting_time)) %&gt;% ungroup() %&gt;% mutate(afternoon = ifelse(afternoon == 0, &quot;Intercept&quot;, &quot;Slope&quot;)) %&gt;% pivot_wider(names_from = afternoon, values_from = mean) %&gt;% mutate(Slope = Slope - Intercept) params &lt;- bind_rows(partially_pooled_params, un_pooled_params) %&gt;% mutate(pooled = rep(c(&quot;partially&quot;, &quot;not&quot;), each = nrow(.)/2)) # preview params %&gt;% slice(c(1:5, 36:40)) #&gt; # A tibble: 10 × 4 #&gt; cafe Intercept Slope pooled #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1 2.71 -0.865 partially #&gt; 2 2 4.10 -0.524 partially #&gt; 3 3 3.23 -1.10 partially #&gt; 4 4 2.51 -1.02 partially #&gt; 5 5 2.64 -0.960 partially #&gt; 6 16 3.09 -0.486 not #&gt; 7 17 3.85 -0.699 not #&gt; 8 18 6.03 -1.27 not #&gt; 9 19 6.23 -1.48 not #&gt; 10 20 2.36 -0.650 not p1 &lt;- ggplot(data = params, aes(x = Intercept, y = Slope)) + coord_cartesian(xlim = range(params$Intercept), ylim = range(params$Slope)) partially_pooled_estimates &lt;- coef(brms_c14_model_cafe)$cafe[ , 1, 1:2] %&gt;% as_tibble() %&gt;% rename(morning = Intercept) %&gt;% mutate(afternoon = morning + afternoon, cafe = 1:n()) %&gt;% dplyr::select(cafe, everything()) un_pooled_estimates &lt;- data_cafe %&gt;% group_by(afternoon, cafe) %&gt;% summarise(mean = mean(waiting_time)) %&gt;% ungroup() %&gt;% mutate(afternoon = ifelse(afternoon == 0, &quot;morning&quot;, &quot;afternoon&quot;)) %&gt;% pivot_wider(names_from = afternoon, values_from = mean) estimates &lt;- bind_rows(partially_pooled_estimates, un_pooled_estimates) %&gt;% mutate(pooled = rep(c(&quot;partially&quot;, &quot;not&quot;), each = n() / 2)) p2 &lt;- ggplot(data = estimates, aes(x = morning, y = afternoon)) + coord_cartesian(xlim = range(estimates$morning), ylim = range(estimates$afternoon)) p1 + p2 + plot_annotation(title = &quot;Shrinkage in two dimensions&quot;) + plot_layout(guides = &quot;collect&quot;) &amp; ( (c(1:9, 9.9)/10) %&gt;% purrr::map(.f = function(lvl){ stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = lvl,linetype = 3, size = .2, alpha = .05, color = &quot;black&quot;, fill = &quot;black&quot;) }) ) &amp; geom_line(aes(group = cafe), size = .3) &amp; geom_point(aes(group = cafe, fill = pooled), shape = 21, size = 2) &amp; scale_fill_manual(&quot;Pooled?&quot;, values = c(clr_dark, clr0)) &amp; theme(legend.position = &quot;bottom&quot;) 15.7.2 Advanced varying slopes data_chimp_f &lt;- data_chimp %&gt;% dplyr::select(pulled_left, treatment, actor, block) %&gt;% mutate(across(-pulled_left, factor)) brms_c14_model_chimp_non_centered &lt;- brm( data = data_chimp_f, family = binomial, pulled_left | trials(1) ~ 0 + treatment + (0 + treatment | actor) + (0 + treatment | block), prior = c(prior(normal(0, 1), class = b), prior(exponential(1), class = sd, group = actor), prior(exponential(1), class = sd, group = block), prior(lkj(2), class = cor, group = actor), prior(lkj(2), class = cor, group = block)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c14_model_chimp_non_centered&quot;) library(bayesplot) as_draws_df(brms_c14_model_chimp_non_centered, add_chain = TRUE) %&gt;% as_tibble() %&gt;% dplyr::select(b_treatment1:`cor_block__treatment3__treatment4`, chain = `.chain`) %&gt;% mcmc_rank_overlay(n_bins = 30) + scale_color_manual(values = clr_chains()) + scale_x_continuous(breaks = 0:4 * 1e3, labels = c(0, str_c(1:4, &quot;K&quot;))) + coord_cartesian(ylim = c(15, 50)) + theme(legend.position = &quot;bottom&quot;) + facet_wrap(~ parameter, ncol = 4) library(posterior) as_draws_df(brms_c14_model_chimp_non_centered) %&gt;% summarise_draws() %&gt;% pivot_longer(starts_with(&quot;ess&quot;)) %&gt;% ggplot(aes(x = value)) + geom_histogram(binwidth = 250, color = clr0dd, fill = clr0) + xlim(0, NA) + facet_wrap(~ name) brms_c14_model_chimp_non_centered &lt;- add_criterion(brms_c14_model_chimp_non_centered, &quot;waic&quot;) waic(brms_c14_model_chimp_non_centered) #&gt; #&gt; Computed from 4000 by 504 log-likelihood matrix #&gt; #&gt; Estimate SE #&gt; elpd_waic -272.6 9.9 #&gt; p_waic 27.1 1.4 #&gt; waic 545.1 19.7 #&gt; #&gt; 1 (0.2%) p_waic estimates greater than 0.4. We recommend trying loo instead. new_chimps &lt;- data_chimp_f %&gt;% distinct(actor, treatment) %&gt;% mutate(block = 5L) # compute and wrangle the posterior predictions fitted(brms_c14_model_chimp_non_centered, newdata = new_chimps) %&gt;% as_tibble() %&gt;% bind_cols(new_chimps) %&gt;% # add the empirical proportions left_join( data_chimp_f %&gt;% group_by(actor, treatment) %&gt;% mutate(proportion = mean(pulled_left)) %&gt;% distinct(actor, treatment, proportion), by = c(&quot;actor&quot;, &quot;treatment&quot;) ) %&gt;% mutate(labels = factor(treatment_labels[treatment], levels = treatment_labels[1:4]), prosoc_left = factor(as.numeric(grepl(&quot;L&quot;, as.character(labels)))), condition = factor(as.numeric(grepl(&quot;P&quot;, as.character(labels))))) %&gt;% ggplot(aes(x = labels))+ geom_hline(yintercept = .5, color = clr_dark, size = .4, linetype = 3) + # empirical proportions geom_line(aes(y = proportion, group = prosoc_left), size = 1/4, color = clr0d) + geom_point(aes(y = proportion, shape = condition), color = clr0d, fill = clr0, size = 2) + # posterior predictions geom_line(aes(y = Estimate, group = prosoc_left), size = 3/4, color = clr_dark) + geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, shape = condition), color = clr_dark, fill = &quot;white&quot;, fatten = 5, size = 1/3) + scale_shape_manual(values = c(21, 19)) + scale_x_discrete(NULL, breaks = NULL) + scale_y_continuous(&quot;proportion left lever&quot;, breaks = 0:2 / 2, labels = c(&quot;0&quot;, &quot;.5&quot;, &quot;1&quot;)) + facet_wrap(~ actor, nrow = 1, labeller = label_both) + labs(subtitle = &quot;Posterior predictions, in dark gray, against the raw data, in light gray, for\\nmodel brms_c14_model_chimp_non_centered, the cross-classified varying effects model.&quot;) + theme(legend.position = &quot;none&quot;, panel.border = element_rect(fill = &quot;transparent&quot;, color = clr0d)) 15.7.3 Instruments and Causal Designs brms_c14_model_edu_confound &lt;- brm( data = data_edu_sim, family = gaussian, w_std ~ 1 + e_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c14_model_edu_confound&quot;) mixedup::extract_fixef(brms_c14_model_edu_confound, ci_level = .89) #&gt; # A tibble: 2 × 5 #&gt; term value se lower_5.5 upper_94.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Intercept 0 0.041 -0.066 0.065 #&gt; 2 e_std 0.346 0.042 0.278 0.414 brms_c14_model_edu_bias_amplified &lt;- brm( data = data_edu_sim, family = gaussian, w_std ~ 1 + e_std + q_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c14_model_edu_bias_amplified&quot;) mixedup::extract_fixef(brms_c14_model_edu_bias_amplified, ci_level = .89) #&gt; # A tibble: 3 × 5 #&gt; term value se lower_5.5 upper_94.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Intercept -0.001 0.039 -0.062 0.061 #&gt; 2 e_std 0.597 0.053 0.513 0.68 #&gt; 3 q_std -0.381 0.053 -0.465 -0.298 e_std_model &lt;- bf(e_std ~ 1 + q_std) w_std_model &lt;- bf(w_std ~ 1 + e_std) brms_c14_model_edu_multivariate &lt;- brm( data = data_edu_sim, family = gaussian, e_std_model + w_std_model + set_rescor(TRUE), prior = c(# E model prior(normal(0, 0.2), class = Intercept, resp = estd), prior(normal(0, 0.5), class = b, resp = estd), prior(exponential(1), class = sigma, resp = estd), # W model prior(normal(0, 0.2), class = Intercept, resp = wstd), prior(normal(0, 0.5), class = b, resp = wstd), prior(exponential(1), class = sigma, resp = wstd), # rho prior(lkj(2), class = rescor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c14_model_edu_multivariate&quot;) mixedup::extract_fixef(brms_c14_model_edu_multivariate, ci_level = .89) #&gt; # A tibble: 4 × 5 #&gt; term value se lower_5.5 upper_94.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 estd_Intercept -0.001 0.034 -0.054 0.053 #&gt; 2 wstd_Intercept -0.001 0.043 -0.072 0.07 #&gt; 3 estd_q_std 0.66 0.034 0.606 0.715 #&gt; 4 wstd_e_std 0.018 0.067 -0.087 0.123 brms_c14_model_edu_confound_2 &lt;- update( brms_c14_model_edu_confound, newdata = data_edu_sim_2, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c14_model_edu_confound_2&quot;) brms_c14_model_edu_multivariate_2 &lt;- update( brms_c14_model_edu_multivariate, newdata = data_edu_sim_2, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c14_model_edu_multivariate_2&quot;) bind_rows( posterior_summary(brms_c14_model_edu_confound_2)[1:3, ] %&gt;% as_tibble() %&gt;% mutate(param = c(&quot;alpha[W]&quot;, &quot;beta[EW]&quot;, &quot;sigma[W]&quot;), fit = &quot;confound_2&quot;), posterior_summary(brms_c14_model_edu_multivariate_2)[1:7, ] %&gt;% as_tibble() %&gt;% mutate(param = c(&quot;alpha[E]&quot;, &quot;alpha[W]&quot;, &quot;beta[QE]&quot;, &quot;beta[EW]&quot;, &quot;sigma[E]&quot;, &quot;sigma[W]&quot;, &quot;rho&quot;), fit = &quot;multivariate_2&quot;)) %&gt;% mutate(param = factor(param, levels = c(&quot;rho&quot;, &quot;sigma[W]&quot;, &quot;sigma[E]&quot;, &quot;beta[EW]&quot;, &quot;beta[QE]&quot;, &quot;alpha[W]&quot;, &quot;alpha[E]&quot;))) %&gt;% ggplot(aes(x = param, y = Estimate, color = fit)) + geom_hline(yintercept = 0, linetype = 3, color = clr_dark, size = .4) + geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5), fatten = 2, position = position_dodge(width = 0.5)) + scale_color_manual(NULL, values = c(clr_dark, clr0d)) + scale_x_discrete(NULL) + ylab(&quot;marginal posterior&quot;) + coord_flip() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), legend.position = &quot;bottom&quot;) 15.7.4 Social Relations as Correlated Varying Effects It looks like brms is not set up to fit a model like this, at this time. See the Social relations model (SRM) thread on the Stan Forums and issue #502 on the brms GitHub repo for details. In short, the difficulty is brms is not set up to allow covariances among distinct random effects with the same levels and it looks like this will not change any time soon. data_dyads %&gt;% ggplot(aes(x = hidA, y = hidB, label = did)) + geom_tile(aes(fill = did, color = after_scale(clr_darken(fill))), show.legend = FALSE, size = .1) + geom_text(size = 2.25, family = fnt_sel) + scale_fill_gradient(low = clr0, high = clr_lighten(clr_current,.4), limits = c(1, NA)) + scale_x_continuous(breaks = 1:24, expand = c(0,0)) + scale_y_continuous(breaks = 2:25, expand = c(0,0)) + labs(subtitle = &quot;dyad ids&quot;) + theme(axis.text = element_text(size = 9), axis.ticks = element_blank(), panel.grid.major = element_blank()) tibble(`sigma_g` = dyad_posterior$sigma_gr[, 1], `sigma_r` = dyad_posterior$sigma_gr[, 2], `rho_g_r` = dyad_posterior$Rho_gr[, 2, 1]) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = name)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_halfeye(aes( slab_color = name, slab_fill = after_scale(clr_alpha(slab_color)), point_fill = after_scale(slab_color)), slab_size = .7, .width = .89, color = &quot;black&quot;, height = 1.5, interval_size = .4, shape = 21, point_size = 2) + scale_discrete_manual(aesthetics = &quot;slab_color&quot;, values = c(clr0, clr_dark, clr_current)) + scale_y_discrete(NULL) + xlab(&quot;marginal posterior&quot;) + coord_cartesian(ylim = c(1.5, 3.9)) + theme(legend.position = &quot;none&quot;) tibble(`sigma_d` = dyad_posterior$sigma_d, `rho_d` = dyad_posterior$Rho_d[, 2, 1]) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = name, fill = name)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_halfeye(aes( slab_color = name, slab_fill = after_scale(clr_alpha(slab_color)), point_fill = after_scale(slab_color)), slab_size = .7, .width = .89, color = &quot;black&quot;, interval_size = .4, shape = 21, point_size = 2) + scale_discrete_manual(aesthetics = &quot;slab_color&quot;, values = c(clr0, clr_dark, clr_current)) + scale_y_discrete(NULL) + xlab(&quot;marginal posterior&quot;) + coord_cartesian(ylim = c(1.5, 2)) + theme(legend.position = &quot;none&quot;) 15.7.5 Continuous Categories and the Gaussian Process Spatial autocorrelation in Oceanic tools islandsDistMatrix %&gt;% data.frame() %&gt;% rownames_to_column(&quot;row&quot;) %&gt;% pivot_longer(names_to = &quot;column&quot;, values_to = &quot;distance&quot;, -row) %&gt;% mutate(column = str_replace_all(column, &quot;\\\\.&quot;, &quot; &quot;), column = factor(column, levels = colnames(islandsDistMatrix)), row = factor(row, levels = rownames(islandsDistMatrix)) %&gt;% fct_rev(), label = formatC(distance, format = &#39;f&#39;, digits = 2)) %&gt;% ggplot(aes(x = column, y = row)) + geom_tile(aes(fill = distance)) + geom_text(aes(label = label), size = 3, family = fnt_sel, color = &quot;#100F14&quot;) + scale_fill_gradient(low = clr0, high = clr_lighten(clr_current,.3)) + scale_x_discrete(NULL, position = &quot;top&quot;, expand = c(0, 0)) + scale_y_discrete(NULL, expand = c(0, 0)) + guides(fill = guide_colorbar(barwidth = unit(7, &quot;pt&quot;))) + theme(axis.ticks = element_blank()) 👋 Heads up: The brms package is capable of handling a variety of Gaussian process models using the gp() function. As we will see throughout this section, this method will depart in important ways from how McElreath fits Gaussian process models with rethinking. Due in large part to these differences, this section and its analogue in the first edition of Statistical rethinking (McElreath, 2015) baffled me, at first. Happily, fellow enthusiasts Louis Bliard and Richard Torkar reached out and helped me hammer this section out behind the scenes. The method to follow is due in large part to their efforts. 🤝 The brms::gp() function takes a handful of arguments. The first and most important argument, ..., accepts the names of one or more predictors from the data. When fitting a spatial Gaussian process of this kind, we’ll enter in the latitude and longitude data for each of levels of culture. This will be an important departure from the text. For his m14.8, McElreath directly entered in the Dmat distance matrix data into ulam(). In so doing, he defined \\(D_{ij}\\), the matrix of distances between each of the societies. When using brms, we instead estimate the distance matrix from the latitude and longitude variables. Before we practice fitting a Gaussian process with the brms::gp() function, we’ll first need to think a little bit about our data. McElreath’s Dmat measured the distances in thousands of km. However, the lat and lon2 variables in the data above are in decimal degrees, which means they need to be transformed to keep our model in the same metric as McElreath’s. Turns out that one decimal degree is 111.32km (at the equator). Thus, we can turn both lat and lon2 into 1,000 km units by multiplying each by 0.11132. Here’s the conversion. (data_kline_brms &lt;- data_kline %&gt;% mutate(lat_adj = lat * 0.11132, lon2_adj = lon2 * 0.11132)) %&gt;% dplyr::select(culture, lat, lon2, lat_adj:lon2_adj) #&gt; # A tibble: 10 × 5 #&gt; culture lat lon2 lat_adj lon2_adj #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Malekula -16.3 -12.5 -1.81 -1.39 #&gt; 2 Tikopia -12.3 -11.2 -1.37 -1.25 #&gt; 3 Santa Cruz -10.7 -14 -1.19 -1.56 #&gt; 4 Yap 9.5 -41.9 1.06 -4.66 #&gt; 5 Lau Fiji -17.7 -1.90 -1.97 -0.212 #&gt; 6 Trobriand -8.7 -29.1 -0.968 -3.24 #&gt; 7 Chuuk 7.4 -28.4 0.824 -3.16 #&gt; 8 Manus -2.1 -33.1 -0.234 -3.68 #&gt; 9 Tonga -21.2 4.80 -2.36 0.534 #&gt; 10 Hawaii 19.9 24.4 2.22 2.72 Note that because this conversion is valid at the equator, it is only an approximation for latitude and longitude coordinates for our island societies. Now we’ve scaled our two spatial variables, the basic way to use them in a brms Gaussian process is including gp(lat_adj, lon2_adj) into the formula argument within the brm() function. Note however that one of the default gp() settings is scale = TRUE, which scales predictors so that the maximum distance between two points is 1. We don’t want this for our example, so we will set scale = FALSE instead. Our Gaussian process model is an extension of the non-linear model from Section 11.2.1.1, b11.11. Thus our model here will also use the non-linear syntax. Here’s how we might use brms to fit our amended non-centered version of McElreath’s m14.8. brms_c14_model_island_distance &lt;- brm( data = data_kline_brms, family = poisson(link = &quot;identity&quot;), bf(total_tools ~ exp(alpha) * population ^ beta / gamma, alpha ~ 1 + gp(lat_adj, lon2_adj, scale = FALSE), beta + gamma ~ 1, nl = TRUE), prior = c(prior(normal(0, 1), nlpar = alpha), prior(exponential(1), nlpar = beta, lb = 0), prior(exponential(1), nlpar = gamma, lb = 0), prior(inv_gamma(2.874624, 2.941204), class = lscale, coef = gplat_adjlon2_adj, nlpar = alpha), prior(exponential(1), class = sdgp, coef = gplat_adjlon2_adj, nlpar = alpha)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, sample_prior = TRUE, file = &quot;brms/brms_c14_model_island_distance&quot;) posterior_summary(brms_c14_model_island_distance)[1:15, ] %&gt;% round(digits = 2) \\[\\begin{bmatrix} 0.33 &amp;0.86 &amp;-1.41 &amp;1.96 &amp;0.26 &amp;0.08 &amp;0.1 &amp;0.43 &amp;0.67 &amp;0.64 &amp;0.05 &amp;2.45 &amp;0.47 &amp;0.29 &amp;0.16 \\\\1.22 &amp;1.62 &amp;0.89 &amp;0.5 &amp;3.82 &amp;-0.47 &amp;0.74 &amp;-1.96 &amp;0.91 &amp;0.44 &amp;0.85 &amp;-1.24 &amp;2.12 &amp;-0.6 &amp;0.74 \\\\-1.96 &amp;1.02 &amp;0.99 &amp;0.68 &amp;-0.26 &amp;2.38 &amp;0.26 &amp;0.75 &amp;-1.17 &amp;1.73 &amp;-1.06 &amp;0.77 &amp;-2.63 &amp;0.44 &amp;0.17 \\\\0.71 &amp;-1.32 &amp;1.55 &amp;-0.21 &amp;0.87 &amp;-1.94 &amp;1.57 &amp;0.42 &amp;0.92 &amp;-1.51 &amp;2.12 &amp;-0.4 &amp;0.8 &amp;-1.97 &amp;1.14 \\\\ \\end{bmatrix}\\] fixef(brms_c14_model_island_distance, probs = c(.055, .945))[&quot;alpha_Intercept&quot;, c(1, 3:4)] %&gt;% exp() %&gt;% round(digits = 2) #&gt; Estimate Q5.5 Q94.5 #&gt; 1.40 0.34 5.30 Our Gaussian process parameters are different from McElreath’s. From the gp section of the brms reference manual (Bürkner, 2021i), we learn the brms parameterization follows the form \\[k(x_{i},x_{j})=sdgp^{2}~\\textrm{exp}\\left(−||x_{i}−x_{j}||^{2}/(2~lscale^{2})\\right)\\] where \\(k(x_{i},x_{j})\\) is the same as McElreath’s \\(K_{ij}\\) and \\(||xi−xj||^{2}\\) is the Euclidean distance, the same as McElreath’s \\(D_{ij}^{2}\\). Thus we could also express the brms parameterization as \\[K_{ij} = sdgp^{2}~\\textrm{exp}\\left(−D_{ij}^{2}/(2lscale^{2})\\right)\\] which is much closer to McElreath’s \\[K_{ij} = \\eta^{2}~ \\textrm{exp}(−\\rho^{2}~D_{ij}^{2})+\\delta_{ij}~\\sigma^{2}\\] On page 470, McElreath explained that the final \\(\\delta_{ij}~\\sigma^{2}\\) term is mute with the Oceanic societies data. Thus we won’t consider it further. This reduces McElreath’s equation to \\[K_{ij} = \\eta^{2}~ \\textrm{exp}(−\\rho^{2}~D_{ij}^{2})\\] Importantly, what McElreath called \\(\\eta\\), Bürkner called \\(sdgp\\). While McElreath estimated \\(\\eta^{2}\\), brms simply estimated \\(sdgp\\). So we’ll have to square our sdgp(alpha_gplat_adjlon2_adj) before it’s on the same scale as etasq in the text. Here it is. island_dist_posterior_brms &lt;- as_draws_df(brms_c14_model_island_distance) %&gt;% mutate(etasq = sdgp_alpha_gplat_adjlon2_adj^2) island_dist_posterior_brms %&gt;% mean_hdi(etasq, .width = .89) %&gt;% mutate_if(is.double, round, digits = 3) #&gt; # A tibble: 1 × 6 #&gt; etasq .lower .upper .width .point .interval #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 0.306 0.005 0.61 0.89 mean hdi Though our posterior is a little bit larger than McElreath’s, we’re in the ballpark. You may have noticed that in our model brm() code, above, we just went with the flow and kept the exponential(1) prior on \\(sdgp\\). The brms default would have been student_t(3, 0, 15.6). Now look at the denominator of the inner part of Bürkner’s equation, \\(2lscale^{2}\\). This appears to be the brms equivalent to McElreath’s \\(\\rho^{2}\\). Or at least it’s what we’ve got. Anyway, also note that McElreath estimated \\(\\rho^{2}\\) directly as rhosq. If I’m doing the algebra correctly, we might expect \\[\\begin{array}{rclr}\\rho^{2} &amp; = &amp; 1 / (2 \\times lscale ^2)&amp; \\textrm{and thus}\\\\lscale &amp; = &amp; \\sqrt{1 / (2 \\times \\rho^{2})}\\end{array}\\] To get a sense of this relationship, it might be helpful to plot. p1 &lt;- tibble(`rho^2` = seq(from = 0, to = 11, by = 0.01)) %&gt;% mutate(lscale = sqrt(1 / (2 * `rho^2`))) %&gt;% ggplot(aes(x = `rho^2`, y = lscale)) + geom_hline(yintercept = 0, color = clr_dark, linetype = 3) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_line(color = clr0d) + xlab(&quot;rho&lt;sup&gt;2&lt;/sup&gt;&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) p2 &lt;- tibble(lscale = seq(from = 0, to = 11, by = 0.01)) %&gt;% mutate(`rho^2` = 1 / (2 * lscale^2)) %&gt;% ggplot(aes(x = lscale, y = `rho^2`)) + geom_hline(yintercept = 0, color = clr_dark, linetype = 3) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_line(color = clr_dark) + ylab(&quot;rho&lt;sup&gt;2&lt;/sup&gt;&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) p1 + p2 &amp; theme(axis.title.x = element_markdown(), axis.title.y = element_markdown()) The two aren’t quite inverses of one another, but the overall pattern is when one is large, the other is small. Now we have a sense of how they compare and how to covert one to the other, let’s see how our posterior for \\(lscale\\) looks when we convert it to the scale of McElreath’s \\(\\rho^{2}\\). island_dist_posterior_brms &lt;- island_dist_posterior_brms %&gt;% mutate(rhosq = 1 / (2 * lscale_alpha_gplat_adjlon2_adj^2)) island_dist_posterior_brms %&gt;% mean_hdi(rhosq, .width = .89) %&gt;% mutate_if(is.double, round, digits = 3) #&gt; # A tibble: 1 × 6 #&gt; rhosq .lower .upper .width .point .interval #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 0.435 0.008 0.923 0.89 mean hdi This is about a third of the size of the McElreath’s \\(\\rho^{2} = 1.31,~89\\%~ \\textrm{HDI}~[0.08,4.41]\\). The plot deepens. If you look back, you’ll see we used a very different prior for \\(lscale\\). Here it is: inv_gamma(2.874624, 2.941204). Use get_prior() to discover where that came from. get_prior(data = data_kline_brms, family = poisson(link = &quot;identity&quot;), bf(total_tools ~ exp(alpha) * population ^ beta / gamma, alpha ~ 1 + gp(lat_adj, lon2_adj, scale = FALSE), beta + gamma ~ 1, nl = TRUE)) #&gt; prior class coef group resp dpar nlpar #&gt; (flat) b alpha #&gt; (flat) b Intercept alpha #&gt; (flat) lscale alpha #&gt; inv_gamma(2.874624, 2.941204) lscale gplat_adjlon2_adj alpha #&gt; student_t(3, 0, 15.6) sdgp alpha #&gt; student_t(3, 0, 15.6) sdgp gplat_adjlon2_adj alpha #&gt; (flat) b beta #&gt; (flat) b Intercept beta #&gt; (flat) b gamma #&gt; (flat) b Intercept gamma #&gt; bound source #&gt; default #&gt; (vectorized) #&gt; default #&gt; default #&gt; default #&gt; (vectorized) #&gt; default #&gt; (vectorized) #&gt; default #&gt; (vectorized) That is, we used the brms default prior for \\(lscale\\). In a GitHub exchange, Bürkner pointed out that brms uses special priors for \\(lscale\\) parameters based on Michael Betancourt’s (2017) vignette, Robust Gaussian processes in Stan. We can use the dinvgamma() function from the well-named invgamma package (Kahle &amp; Stamey, 2017) to get a sense of what that prior looks like. tibble(lscale = seq(from = 0.01, to = 9, by = 0.01)) %&gt;% mutate(density = invgamma::dinvgamma(lscale, 2.874624, 2.941204)) %&gt;% ggplot(aes(x = lscale, y = density)) + geom_area(color = clr0dd, fill = fll0) + annotate(geom = &quot;text&quot;, x = 4.75, y = 0.75, label = &quot;inverse gamma(2.87, 2.94)&quot;, family = fnt_sel) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 8)) Anyways, let’s make the subplots for our version of Figure 14.11 to get a sense of what this all means. Start with the left panel, the prior predictive distribution for the covariance. set.seed(42) p1 &lt;- prior_draws(brms_c14_model_island_distance) %&gt;% as_tibble() %&gt;% mutate(iter = 1:n(), etasq = sdgp_alpha_gplat_adjlon2_adj^2, rhosq = 1 / (2 * lscale_alpha_1_gplat_adjlon2_adj^2)) %&gt;% slice_sample(n = 100) %&gt;% tidyr::expand(nesting(iter, etasq, rhosq), x = seq(from = 0, to = 10, by = .05)) %&gt;% mutate(covariance = etasq * exp(-rhosq * x^2)) %&gt;% ggplot(aes(x = x, y = covariance)) + geom_line(aes(group = iter), size = 1/4, color = clr_dark) + scale_x_continuous(&quot;distance (thousand km)&quot;, expand = c(0, 0), breaks = 0:5 * 2) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 2)) + labs(subtitle = &quot;Gaussian process prior&quot;) p2 &lt;- island_dist_posterior_brms %&gt;% transmute(iter = 1:n(), etasq = sdgp_alpha_gplat_adjlon2_adj^2, rhosq = 1 / (2 * lscale_alpha_gplat_adjlon2_adj^2)) %&gt;% slice_sample(n = 50) %&gt;% tidyr::expand(nesting(iter, etasq, rhosq), x = seq(from = 0, to = 10, by = .05)) %&gt;% mutate(covariance = etasq * exp(-rhosq * x^2)) %&gt;% ggplot(aes(x = x, y = covariance)) + geom_line(aes(group = iter), size = 1/4, color = clr_dark) + stat_function(fun = function(x){ mean(island_dist_posterior_brms$sdgp_alpha_gplat_adjlon2_adj) ^ 2 * exp(-(1 / (2 * mean(island_dist_posterior_brms$lscale_alpha_gplat_adjlon2_adj)^2)) * x^2) }, color = clr_dark, size = 1) + scale_x_continuous(&quot;distance (thousand km)&quot;, expand = c(0, 0), breaks = 0:5 * 2) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 2)) + labs(subtitle = &quot;Gaussian process posterior&quot;) p1 + p2 Though the Gaussian process parameters from our brms parameterization looked different from McElreath’s, they resulted in a similar decline in spatial covariance. Let’s finish this up and “push the parameters back through the function for K, the covariance matrix” k &lt;- matrix(0, nrow = 10, ncol = 10) for (i in 1:10) for (j in 1:10) k[i, j] &lt;- median(island_dist_posterior_brms$etasq) * exp(-median(island_dist_posterior_brms$rhosq) * islandsDistMatrix[i, j]^2) diag(k) &lt;- median(island_dist_posterior_brms$etasq) + 0.01 rho &lt;- round(cov2cor(k), 2) # add row/col names for convenience colnames(rho) &lt;- c(&quot;Ml&quot;, &quot;Ti&quot;, &quot;SC&quot;, &quot;Ya&quot;, &quot;Fi&quot;, &quot;Tr&quot;, &quot;Ch&quot;, &quot;Mn&quot;, &quot;To&quot;, &quot;Ha&quot;) rownames(rho) &lt;- colnames(rho) rho %&gt;% round(2) %&gt;% data.frame() %&gt;% knit_precis(param_name = &quot;culture&quot;) culture Ml Ti SC Ya Fi Tr Ch Mn To Ha Ml 1.00 0.89 0.85 0.01 0.65 0.34 0.08 0.14 0.40 0 Ti 0.89 1.00 0.92 0.01 0.64 0.35 0.12 0.16 0.36 0 SC 0.85 0.92 1.00 0.02 0.52 0.46 0.18 0.24 0.26 0 Ya 0.01 0.01 0.02 1.00 0.00 0.21 0.52 0.49 0.00 0 Fi 0.65 0.64 0.52 0.00 1.00 0.07 0.02 0.02 0.81 0 Tr 0.34 0.35 0.46 0.21 0.07 1.00 0.42 0.79 0.02 0 Ch 0.08 0.12 0.18 0.52 0.02 0.42 1.00 0.65 0.00 0 Mn 0.14 0.16 0.24 0.49 0.02 0.79 0.65 1.00 0.00 0 To 0.40 0.36 0.26 0.00 0.81 0.02 0.00 0.00 1.00 0 Ha 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1 rho %&gt;% data.frame() %&gt;% mutate(row = data_kline_brms$culture) %&gt;% pivot_longer(-row, values_to = &quot;distance&quot;) %&gt;% mutate(column = factor(name, levels = colnames(rho)), row = factor(row, levels = rownames(islandsDistMatrix)) %&gt;% fct_rev(), label = formatC(distance, format = &#39;f&#39;, digits = 2) %&gt;% str_replace(., &quot;0.&quot;, &quot;.&quot;)) %&gt;% # omit this line to keep the diagonal of 1&#39;s filter(distance != 1) %&gt;% ggplot(aes(x = column, y = row)) + geom_tile(aes(fill = distance)) + geom_text(aes(label = label), size = 2.75, family = fnt_sel) + scale_fill_gradient(&quot;rho&quot;, low = clr0, high = clr_lighten(clr_current, .3), limits = c(0, 1)) + scale_x_discrete(NULL, position = &quot;top&quot;, expand = c(0, 0)) + scale_y_discrete(NULL, expand = c(0, 0)) + guides(fill = guide_colorbar(barwidth = unit(7, &quot;pt&quot;))) + theme(axis.ticks = element_blank(), panel.grid = element_blank()) As far as I can figure, you still have to get rho into a tidy data frame before feeding it into ggplot2. Here’s my attempt at doing so. tidy_rho &lt;- rho %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% bind_cols(data_kline_brms %&gt;% dplyr::select(culture, logpop, total_tools, lon2, lat)) %&gt;% pivot_longer(Ml:Ha, names_to = &quot;colname&quot;, values_to = &quot;correlation&quot;) %&gt;% mutate(group = str_c(pmin(rowname, colname), pmax(rowname, colname))) %&gt;% dplyr::select(rowname, colname, group, culture, everything()) p1 &lt;- tidy_rho %&gt;% ggplot(aes(x = lon2, y = lat)) + geom_line(aes(group = group, alpha = correlation^2), color = clr0dd) + geom_point(data = data_kline_brms, aes(size = logpop), color = clr0) + ggrepel::geom_text_repel(data = data_kline_brms, aes(label = culture), seed = 14, point.padding = .2, size = 2.75, color = clr0dd, family = fnt_sel) + scale_alpha_continuous(range = c(0, 1)) + labs(subtitle = &quot;Among societies in geographic space\\n&quot;, x = &quot;longitude&quot;, y = &quot;latitude&quot;) + coord_cartesian(xlim = range(data_kline_brms$lon2), ylim = range(data_kline_brms$lat)) + theme(legend.position = &quot;none&quot;) fit_kline_brms &lt;- island_dist_posterior_brms %&gt;% tidyr::expand(logpop = seq(from = 6, to = 14, length.out = 30), nesting(b_alpha_Intercept, b_beta_Intercept, b_gamma_Intercept)) %&gt;% mutate(population = exp(logpop)) %&gt;% mutate(lambda = exp(b_alpha_Intercept) * population^b_beta_Intercept / b_gamma_Intercept) %&gt;% group_by(logpop) %&gt;% median_qi(lambda, .width = .8) p2 &lt;- tidy_rho %&gt;% ggplot(aes(x = logpop)) + geom_smooth(data = fit_kline_brms, aes(y = lambda, ymin = .lower, ymax = .upper), stat = &quot;identity&quot;, fill = fll0, color = clr0d, alpha = .5, size = 1.1) + geom_line(aes(y = total_tools, group = group, alpha = correlation^2), color = clr0dd) + geom_point(data = data_kline_brms, aes(y = total_tools, size = logpop), color = clr0d) + ggrepel::geom_text_repel(data = data_kline_brms, aes(y = total_tools, label = culture), seed = 14, point.padding = .2, size = 2.75, color = clr_dark, family = fnt_sel) + scale_alpha_continuous(range = c(0, 1)) + labs(subtitle = &quot;Shown against the relation between\\ntotal tools and log pop&quot;, x = &quot;log population&quot;, y = &quot;total tools&quot;) + coord_cartesian(xlim = range(data_kline_brms$logpop), ylim = range(data_kline_brms$total_tools)) + theme(legend.position = &quot;none&quot;) p1 + p2 + plot_annotation(title = &quot;Posterior median correlations&quot;) Dispersion by other names McElreath remarked it might be a good idea to fit an alternative of this model using the gamma-Poisson likelihood. Let’s take him up on the challenge. Remember that gamma-Poisson models are also referred to as negative binomial models. When using brms, you specify this using family = negbinomial. brms_c14_model_island_distance_gamma_poisson &lt;- brm( data = data_kline_brms, family = negbinomial(link = &quot;identity&quot;), bf(total_tools ~ exp(alpha) * population ^ beta / gamma, alpha ~ 1 + gp(lat_adj, lon2_adj, scale = FALSE), beta + gamma ~ 1, nl = TRUE), prior = c(prior(normal(0, 1), nlpar = alpha), prior(exponential(1), nlpar = beta, lb = 0), prior(exponential(1), nlpar = gamma, lb = 0), prior(inv_gamma(2.874624, 2.941204), class = lscale, coef = gplat_adjlon2_adj, nlpar = alpha), prior(exponential(1), class = sdgp, coef = gplat_adjlon2_adj, nlpar = alpha), # default prior prior(gamma(0.01, 0.01), class = shape)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 14, control = list(adapt_delta = .9), file = &quot;brms/brms_c14_model_island_distance_gamma_poisson&quot;) brms_c14_model_island_distance &lt;- add_criterion(brms_c14_model_island_distance, &quot;waic&quot;) brms_c14_model_island_distance_gamma_poisson &lt;- add_criterion(brms_c14_model_island_distance_gamma_poisson, &quot;waic&quot;) loo_compare(brms_c14_model_island_distance, brms_c14_model_island_distance_gamma_poisson, criterion = &quot;waic&quot;) %&gt;% print(simplify = FALSE) #&gt; elpd_diff se_diff elpd_waic #&gt; brms_c14_model_island_distance 0.0 0.0 -33.3 #&gt; brms_c14_model_island_distance_gamma_poisson -4.0 0.6 -37.3 #&gt; se_elpd_waic p_waic se_p_waic #&gt; brms_c14_model_island_distance 1.3 3.7 0.6 #&gt; brms_c14_model_island_distance_gamma_poisson 1.8 3.4 0.6 #&gt; waic se_waic #&gt; brms_c14_model_island_distance 66.7 2.6 #&gt; brms_c14_model_island_distance_gamma_poisson 74.6 3.7 The WAIC comparison suggests we gain little by switching to the gamma-Poisson. If anything, we may have overfit. Phylogenetic distance The naïve model: data_primates_brms &lt;- data_primates_complete %&gt;% dplyr::select(body_log_std, brain_log_std, group_size_log_std) brms_c14_model_primates_simple &lt;- brm( data = data_primates_brms, family = gaussian, brain_log_std ~ 1 + body_log_std + group_size_log_std, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c14_model_primates_simple&quot;) as_draws_df(brms_c14_model_primates_simple) %&gt;% mutate(sigma_sq = sigma^2) %&gt;% mean_qi(sigma_sq) %&gt;% mutate_if(is.double, round, digits = 2) #&gt; # A tibble: 1 × 6 #&gt; sigma_sq .lower .upper .width .point .interval #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 0.05 0.04 0.06 0.95 mean qi The brownian motion model: Rho_primates &lt;- var_covar_sorted / max(var_covar_sorted) brms_14_model_primates_brownian &lt;- brm( data = data_primates_brms, data2 = list(Rho = Rho_primates), family = gaussian, brain_log_std ~ 1 + body_log_std + group_size_log_std + fcor(Rho), prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_14_model_primates_brownian&quot;) as_draws_df(brms_14_model_primates_brownian) %&gt;% transmute(sigma_sq = sigma^2) %&gt;% mean_hdi(sigma_sq, .width = .89) %&gt;% mutate_if(is.double, round, 2) #&gt; # A tibble: 1 × 6 #&gt; sigma_sq .lower .upper .width .point .interval #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 0.16 0.13 0.19 0.89 mean hdi Sadly for us, brms only supports the exponentiated-quadratic kernel for Gaussian process models, at this time. However, the Ornstein–Uhlenbeck kernel is one of the alternative kernels Bürkner has on his to-do list (see GitHub issue #234). 15.7.6 Multilevel Growth Models and the MELSM (brms only) Borrow some data data_longitudinal &lt;- read_csv(&quot;data/m_melsm_dat.csv&quot;) %&gt;% mutate(day01 = (day - 2) / max((day - 2))) data_longitudinal %&gt;% count(record_id) %&gt;% ggplot(aes(x = n)) + geom_bar(fill = clr0d, color = clr0dd) + scale_x_continuous(&quot;number of days&quot;, limits = c(0, NA)) set.seed(14) data_longitudinal %&gt;% nest(data = c(X1, P_A.std, day, P_A.lag, N_A.lag, steps.pm, steps.pmd, N_A.std, day01)) %&gt;% slice_sample(n = 16) %&gt;% unnest(data) %&gt;% ggplot(aes(x = day, y = N_A.lag)) + geom_line(color = clr_dark) + geom_point(color = clr0d, size = 1/2) + ylab(&quot;negative affect (standardized)&quot;) + facet_wrap(~ record_id, labeller = label_both) Conventional multilevel growth model \\[ \\begin{align*} \\text{NA}_{ij} &amp; \\sim \\operatorname{Normal}(\\mu_{ij}, \\sigma) \\\\ \\mu_{ij} &amp; = \\beta_0 + \\beta_1 \\text{time}_{ij} + \\color{#54436D}{u_{0i} + u_{1i} \\text{time}_{ij}} \\\\ \\sigma &amp; = \\sigma_\\epsilon \\\\ \\color{#54436D}{\\begin{bmatrix} u_{0i} \\\\ u_{1i} \\end{bmatrix}} &amp; \\sim \\color{#54436D}{\\operatorname{MVNormal}\\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf S \\mathbf R \\mathbf S \\end{pmatrix}} \\\\ \\mathbf S &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix} \\\\ \\mathbf R &amp; = \\begin{bmatrix} 1 &amp; \\rho_{12} \\\\ \\rho_{21} &amp; 1 \\end{bmatrix} \\\\ \\beta_0 &amp; \\sim \\operatorname{Normal}(0, 0.2) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 1) \\\\ \\sigma_0 \\text{ and } \\sigma_1 &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\sigma_\\epsilon &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\mathbf R &amp; \\sim \\operatorname{LKJ}(2), \\end{align*} \\] brms_c14_model_longitudinal &lt;- brm( data = data_longitudinal, family = gaussian, N_A.std ~ 1 + day01 + (1 + day01 | record_id), prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 1), class = b), prior(exponential(1), class = sd), prior(exponential(1), class = sigma), prior(lkj(2), class = cor)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c14_model_longitudinal&quot;) new_days &lt;- data_longitudinal %&gt;% distinct(record_id, day01) fitted(brms_c14_model_longitudinal, newdata = new_days) %&gt;% as_tibble() %&gt;% bind_cols(new_days) %&gt;% ggplot(aes(x = day01, y = Estimate, group = record_id)) + geom_line(alpha = 1/3, size = 1/3, color = clr_dark) + geom_segment(x = 0, xend = 1, y = fixef(brms_c14_model_longitudinal)[1, 1], yend = fixef(brms_c14_model_longitudinal)[1, 1] + fixef(brms_c14_model_longitudinal)[2, 1], size = 1, color = clr2) + scale_x_continuous(breaks = c(0, .5, 1)) + ylab(&quot;negative affect (standardized)&quot;) new_days_2 &lt;- data_longitudinal %&gt;% filter(record_id %in% c(30, 115)) %&gt;% dplyr::select(record_id, N_A.std, day01) bind_cols( fitted(brms_c14_model_longitudinal, newdata = new_days_2) %&gt;% data.frame(), predict(brms_c14_model_longitudinal, newdata = new_days_2) %&gt;% data.frame() %&gt;% dplyr::select(Q2.5:Q97.5) %&gt;% set_names(&quot;p_lower&quot;, &quot;p_upper&quot;)) %&gt;% bind_cols(new_days_2) %&gt;% as_tibble() %&gt;% ggplot(aes(x = day01)) + geom_ribbon(aes(ymin = p_lower, ymax = p_upper), fill = clr0dd, alpha = 1/2) + geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = clr_dark, color = clr_dark, alpha = 1/2, size = 1/2) + geom_point(aes(y = N_A.std), color = clr_dark) + scale_x_continuous(breaks = c(0, .5, 1)) + ylab(&quot;negative affect (standardized)&quot;) + facet_wrap(~ record_id) Learn more about your data with the MELSM (not today…) 15.8 pymc3 section × "],["rethinking-chapter-15.html", "16 Rethinking: Chapter 15 16.1 Measurement Error 16.2 Missing Data 16.3 Categorical Errors and Discrete Absences 16.4 Homework 16.5 {brms} section 16.6 pymc3 section", " 16 Rethinking: Chapter 15 Missing Data and Other Opportunities by Richard McElreath, building on the Summaries by Solomon Kurz. sim_pancake &lt;- function(){ pancake &lt;- sample(1:3, size = 1) sides &lt;- matrix(c(1, 1, 1, 0, 0, 0), 2,3)[, pancake] sample(sides) } replicate(1e4, sim_pancake()) %&gt;% t() %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;up&quot;, &quot;down&quot;)) %&gt;% summarise(burnt_up = sum(up), burnt_both = sum(up &amp; down)) %&gt;% mutate(prob_both_burnt = burnt_both/ burnt_up) #&gt; # A tibble: 1 × 3 #&gt; burnt_up burnt_both prob_both_burnt #&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 5074 3357 0.662 16.1 Measurement Error library(rethinking) data(&quot;WaffleDivorce&quot;) data_waffle &lt;- WaffleDivorce %&gt;% as_tibble() %&gt;% mutate(across(.cols = c(Divorce, Marriage, MedianAgeMarriage), .fns = standardize, .names = &quot;{str_to_lower(.col)}_std&quot;), waffle_pop = WaffleHouses / Population, divorce_sd = Divorce.SE / sd(Divorce), marriage_sd = Marriage.SE / sd(Marriage)) %&gt;% rename(median_age_std = &quot;medianagemarriage_std&quot;) p1 &lt;- data_waffle %&gt;% ggplot(aes(x = MedianAgeMarriage, y = Divorce)) p2 &lt;- data_waffle %&gt;% ggplot(aes(x = log(Population), y = Divorce)) p1 + p2 &amp; geom_pointrange(aes(ymin = Divorce - Divorce.SE, ymax = Divorce + Divorce.SE), fatten = 3, shape = 21, linewidth = .3, fill = clr0, color = clr0dd) 16.1.1 Error on the Outcome dag &lt;- dagify(D_obs ~ D + E_D, D ~ A + M, M ~ A, exposure = &quot;M&quot;, outcome = &quot;D_obs&quot;, coords = tibble(name = c(&quot;A&quot;, &quot;M&quot;, &quot;D&quot;, &quot;D_obs&quot;, &quot;E_D&quot;), x = c(0, .5, .5, 1, 1.5), y = c(.5, 1, 0, 0, 0))) dag %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;D_obs&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M&quot;), &quot;predictor&quot;, &quot;confounds&quot;)), name = str_replace(name, &quot;(.*)_([a-zA-Z]*)&quot;,&quot;\\\\1[\\\\2]&quot;)) %&gt;% plot_dag(clr_in = clr_current) + scale_x_continuous(limits = c(-.1, 1.6)) + scale_y_continuous(limits = c(-.1, 1.1)) + coord_fixed(ratio = .5) + theme(plot.subtitle = element_text(hjust = .5, family = fnt_sel)) \\[ \\begin{array}{rclr} \\color{#B35136}{D_{\\textrm{OBS}, i}} &amp; \\sim &amp; \\color{#B35136}{\\textrm{Normal}(D_{\\textrm{TRUE}}, D_{\\textrm{SE}, i})} &amp; \\textrm{[distribution of observed values]}\\\\ \\color{#B35136}{D_{\\textrm{TRUE},i}} &amp; \\sim &amp; \\textrm{Normal}(\\mu_{i}, \\sigma) &amp; \\textrm{[distribution of true values]}\\\\ \\mu_{i} &amp; = &amp; \\alpha + \\beta_{A} ~ A_{i} + \\beta_{M} ~ M_{i} &amp; \\textrm{[linear model to asses A }\\rightarrow\\textrm{ D]}\\\\ \\alpha &amp; \\sim &amp; \\textrm{Normal}(0, 0.2) &amp;&amp;\\\\ \\beta_{A} &amp; \\sim &amp; \\textrm{Normal}(0, 0.5) &amp;&amp;\\\\ \\beta_{M} &amp; \\sim &amp; \\textrm{Normal}(0, 0.5) &amp;&amp;\\\\ \\sigma &amp; \\sim &amp; \\textrm{Exponential}(1) &amp;&amp;\\\\ \\end{array} \\] data_waffle_list &lt;- data_waffle %&gt;% dplyr::select(divorce_obs = divorce_std, divorce_sd, marriage_std, median_age_std, marriage_sd) %&gt;% c(., list(N = nrow(.))) model_divorce &lt;- ulam( flist = alist( divorce_obs ~ dnorm( divorce_true, divorce_sd ), vector[N]:divorce_true ~ dnorm( mu, sigma ), mu &lt;- alpha + beta_age * median_age_std + beta_marriage * marriage_std, alpha ~ dnorm( 0, 0.2 ), beta_age ~ dnorm( 0, 0.5 ), beta_marriage ~ dnorm( 0, 0.5 ), sigma ~ dexp(1) ), data = data_waffle_list, cores = 4, chains = 4, log_lik = TRUE ) precis(model_divorce, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 divorce_true[1] 1.18 0.37 0.59 1.76 2120.60 1 divorce_true[2] 0.69 0.55 -0.17 1.58 1996.13 1 divorce_true[3] 0.42 0.35 -0.12 0.98 2375.88 1 divorce_true[4] 1.41 0.47 0.66 2.18 2359.62 1 divorce_true[5] -0.90 0.13 -1.11 -0.70 2994.41 1 divorce_true[6] 0.66 0.41 0.02 1.32 2478.57 1 divorce_true[7] -1.36 0.36 -1.94 -0.77 3093.85 1 divorce_true[8] -0.33 0.47 -1.10 0.45 2626.66 1 divorce_true[9] -1.90 0.61 -2.82 -0.95 1823.13 1 divorce_true[10] -0.62 0.17 -0.89 -0.35 2365.09 1 divorce_true[11] 0.77 0.29 0.32 1.23 2276.66 1 divorce_true[12] -0.55 0.48 -1.32 0.21 1886.35 1 divorce_true[13] 0.16 0.48 -0.61 0.94 1645.43 1 divorce_true[14] -0.87 0.23 -1.23 -0.51 2823.17 1 divorce_true[15] 0.56 0.30 0.07 1.06 2575.91 1 divorce_true[16] 0.29 0.40 -0.33 0.93 2920.37 1 divorce_true[17] 0.50 0.41 -0.16 1.16 2632.67 1 divorce_true[18] 1.26 0.36 0.68 1.83 2541.17 1 divorce_true[19] 0.42 0.38 -0.18 1.03 2512.71 1 divorce_true[20] 0.40 0.54 -0.46 1.30 1723.19 1 divorce_true[21] -0.54 0.32 -1.06 -0.02 3017.09 1 divorce_true[22] -1.09 0.26 -1.50 -0.67 2994.50 1 divorce_true[23] -0.27 0.26 -0.70 0.15 2668.85 1 divorce_true[24] -1.00 0.30 -1.50 -0.51 2692.42 1 divorce_true[25] 0.44 0.42 -0.21 1.12 2152.32 1 divorce_true[26] -0.04 0.32 -0.54 0.47 3088.87 1 divorce_true[27] -0.03 0.50 -0.85 0.76 2789.47 1 divorce_true[28] -0.15 0.40 -0.81 0.48 2779.04 1 divorce_true[29] -0.27 0.50 -1.04 0.55 2013.86 1 divorce_true[30] -1.80 0.24 -2.19 -1.41 2759.28 1 divorce_true[31] 0.17 0.42 -0.48 0.85 2360.02 1 divorce_true[32] -1.66 0.16 -1.92 -1.40 2786.49 1 divorce_true[33] 0.11 0.25 -0.28 0.52 4394.88 1 divorce_true[34] -0.05 0.51 -0.91 0.75 2082.49 1 divorce_true[35] -0.12 0.23 -0.48 0.25 3614.20 1 divorce_true[36] 1.27 0.41 0.61 1.92 3162.86 1 divorce_true[37] 0.23 0.36 -0.36 0.81 2302.28 1 divorce_true[38] -1.02 0.22 -1.37 -0.67 2445.56 1 divorce_true[39] -0.91 0.55 -1.74 -0.01 1596.33 1 divorce_true[40] -0.69 0.33 -1.22 -0.18 2408.26 1 divorce_true[41] 0.24 0.53 -0.62 1.06 2567.16 1 divorce_true[42] 0.73 0.34 0.22 1.28 2103.94 1 divorce_true[43] 0.19 0.19 -0.12 0.50 3015.36 1 divorce_true[44] 0.80 0.42 0.13 1.48 2321.58 1 divorce_true[45] -0.43 0.54 -1.25 0.44 3009.24 1 divorce_true[46] -0.39 0.26 -0.80 0.03 2700.26 1 divorce_true[47] 0.13 0.30 -0.35 0.62 2733.99 1 divorce_true[48] 0.56 0.47 -0.17 1.32 3033.50 1 divorce_true[49] -0.64 0.28 -1.08 -0.21 2805.86 1 divorce_true[50] 0.85 0.57 -0.07 1.74 1840.04 1 alpha -0.05 0.09 -0.20 0.10 1585.93 1 beta_age -0.61 0.16 -0.86 -0.36 1169.42 1 beta_marriage 0.05 0.17 -0.21 0.32 1014.84 1 sigma 0.59 0.10 0.43 0.76 769.88 1 d_obs_vs_d_est &lt;- precis(model_divorce, depth = 2) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;param&quot;) %&gt;% as_tibble() %&gt;% filter(grepl(&quot;divorce_true&quot;, param)) %&gt;% bind_cols(data_waffle %&gt;% dplyr::select(Loc, Divorce,divorce_std, divorce_sd, median_age_std ), .) p1 &lt;- d_obs_vs_d_est %&gt;% ggplot(aes(x = divorce_sd, y = divorce_std - mean)) + geom_hline(yintercept = 0, linetype = 3, color = clr_dark) + geom_point(color = clr0dd, fill = clr0, shape = 21, size = 2) + ggrepel::geom_text_repel(aes(label = Loc), max.overlaps = 1, family = fnt_sel) posterior_pred_se &lt;- extract.samples(model_divorce) %&gt;% as_tibble() %&gt;% dplyr::select(alpha, beta_age) %&gt;% expand(nesting(alpha, beta_age), age = seq(from = -3.5, to = 3.5, length.out = 50)) %&gt;% mutate(fitted = alpha + beta_age * age) library(tidybayes) p2 &lt;- d_obs_vs_d_est %&gt;% ggplot(aes(x = median_age_std)) + stat_lineribbon(data = posterior_pred_se, aes(y = fitted, x = age), .width = .89, size = .25, color = clr0d, fill = fll0) + geom_linerange(aes(ymin = divorce_std, ymax = mean), color = clr0d)+ geom_point(aes(y = divorce_std), color = clr0dd, fill = clr0, shape = 21, size = 2) + geom_point(aes(y = mean), color = clr_current, fill = clr_lighten(clr_current), shape = 21, size = 2) + ggrepel::geom_text_repel(data = . %&gt;% filter(abs(divorce_std - mean) &gt; .35), aes(y = divorce_std, label = Loc), max.overlaps = 2, family = fnt_sel) + coord_cartesian(ylim = c(-2, 2.2), expand = 0) p1 + p2 16.1.2 Error on Both Outcome and Predictor dag2 &lt;- dagify(D_obs ~ D + E_D, D ~ A + M, M ~ A, M_obs ~ M + E_M, exposure = &quot;M&quot;, outcome = &quot;D_obs&quot;, coords = tibble(name = c(&quot;A&quot;, &quot;M&quot;, &quot;D&quot;, &quot;D_obs&quot;, &quot;E_D&quot;, &quot;M_obs&quot;,&quot;E_M&quot;), x = c(0, .5, .5, 1, 1.5, 1, 1.5), y = c(.5, 1, 0, 0, 0, 1, 1))) dag2 %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;D_obs&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M_obs&quot;), &quot;predictor&quot;, &quot;confounds&quot;)), name = str_replace(name, &quot;(.*)_([a-zA-Z]*)&quot;,&quot;\\\\1[\\\\2]&quot;)) %&gt;% plot_dag(clr_in = clr_current) + scale_x_continuous(limits = c(-.1, 1.6)) + scale_y_continuous(limits = c(-.1, 1.1)) + coord_fixed(ratio = .5) + theme(plot.subtitle = element_text(hjust = .5, family = fnt_sel)) \\[ \\begin{array}{rclr} D_{\\textrm{OBS}, i} &amp; \\sim &amp; \\textrm{Normal}(D_{\\textrm{TRUE}}, D_{\\textrm{SE}, i}) &amp; \\textrm{[distribution of observed D values]}\\\\ D_{\\textrm{TRUE},i} &amp; \\sim &amp; \\textrm{Normal}(\\mu_{i}, \\sigma) &amp; \\textrm{[distribution of true D values]}\\\\ \\mu_{i} &amp; = &amp; \\alpha + \\beta_{A} ~ A_{i} + \\color{#B35136}{\\beta_{M} ~ M_{\\textrm{TRUE}, i}} &amp; \\textrm{[linear model]}\\\\ \\color{#B35136}{M_{\\textrm{OBS},i}} &amp; \\sim &amp; \\color{#B35136}{\\textrm{Normal}(M_{\\textrm{TRUE},i}, M_{\\textrm{SE},i})} &amp; \\textrm{[distribution of observed M values]}\\\\ \\color{#B35136}{M_{\\textrm{TRUE},i}} &amp; \\sim &amp; \\color{#B35136}{\\textrm{Normal}(0,1)} &amp; \\textrm{[distribution of true M values]} \\\\ \\alpha &amp; \\sim &amp; \\textrm{Normal}(0, 0.2) &amp;&amp;\\\\ \\beta_{A} &amp; \\sim &amp; \\textrm{Normal}(0, 0.5) &amp;&amp;\\\\ \\beta_{M} &amp; \\sim &amp; \\textrm{Normal}(0, 0.5) &amp;&amp;\\\\ \\sigma &amp; \\sim &amp; \\textrm{Exponential}(1) &amp;&amp;\\\\ \\end{array} \\] model_divorce_error_out &lt;- ulam( flist = alist( divorce_obs ~ dnorm( divorce_true, divorce_sd ), vector[N]:divorce_true ~ dnorm( mu, sigma ), mu &lt;- alpha + beta_age * median_age_std + beta_marriage * marriage_true[i], marriage_std ~ dnorm( marriage_true, marriage_sd ), vector[N]:marriage_true ~ dnorm(0, 1), alpha ~ dnorm( 0, 0.2 ), beta_age ~ dnorm( 0, 0.5 ), beta_marriage ~ dnorm( 0, 0.5 ), sigma ~ dexp(1) ), data = data_waffle_list, cores = 4, chains = 4, log_lik = TRUE ) extract.samples(model_divorce_error_out) %&gt;% as_tibble() %&gt;% dplyr::select(divorce_true, marriage_true) %&gt;% summarise(divorce_true = colMeans(as_tibble(divorce_true)), marriage_true = colMeans(as_tibble(marriage_true))) %&gt;% bind_cols(data_waffle %&gt;% dplyr::select(Loc, Divorce,divorce_std, divorce_sd, median_age_std, marriage_std ), .) %&gt;% ggplot() + geom_segment(aes(x = marriage_std, xend = marriage_true, y = divorce_std, yend = divorce_true ), color = clr0d ) + geom_point(aes(x = marriage_std, y = divorce_std), color = clr0dd, fill = clr0, shape = 21, size = 2) + geom_point(aes(x = marriage_true, y = divorce_true), color = clr_current, fill = clr_lighten(clr_current), shape = 21, size = 2) + labs(subtitle = &quot;shrinkage of both marriage rate and divorce rate&quot;) 16.1.3 Measurement Terrors dag3 &lt;- dagify(D_obs ~ D + E_D, D ~ A + M, M ~ A, M_obs ~ M + E_M, E_M ~ P, E_D ~ P, exposure = &quot;M&quot;, outcome = &quot;D_obs&quot;, coords = tibble(name = c(&quot;A&quot;, &quot;M&quot;, &quot;D&quot;, &quot;D_obs&quot;, &quot;E_D&quot;, &quot;M_obs&quot;,&quot;E_M&quot;, &quot;P&quot;), x = c(0, .5, .5, 1, 1.5, 1, 1.5, 2), y = c(.5, 1, 0, 0, 0, 1, 1, .5))) dag3 %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;D_obs&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M_obs&quot;, &quot;P&quot;), &quot;predictor&quot;, &quot;confounds&quot;)), name = str_replace(name, &quot;(.*)_([a-zA-Z]*)&quot;,&quot;\\\\1[\\\\2]&quot;)) %&gt;% plot_dag(clr_in = clr_current) + coord_fixed(ratio = .6, xlim = c(-.1, 2.1), ylim = c(-.1, 1.1)) + theme(plot.subtitle = element_text(hjust = .5, family = fnt_sel)) dag4 &lt;- dagify(D_obs ~ D + E_D, D ~ A + M, M ~ A, M_obs ~ M + E_M, E_D ~ M, exposure = &quot;M&quot;, outcome = &quot;D_obs&quot;, coords = tibble(name = c(&quot;A&quot;, &quot;M&quot;, &quot;D&quot;, &quot;D_obs&quot;, &quot;E_D&quot;, &quot;M_obs&quot;,&quot;E_M&quot;), x = c(0, .5, .5, 1, 1.5, 1, 1.5), y = c(.5, 1, 0, 0, 0, 1, 1))) dag4 %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;D_obs&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M_obs&quot;, &quot;P&quot;), &quot;predictor&quot;, &quot;confounds&quot;)), name = str_replace(name, &quot;(.*)_([a-zA-Z]*)&quot;,&quot;\\\\1[\\\\2]&quot;)) %&gt;% plot_dag(clr_in = clr_current) + coord_fixed(ratio = .6, xlim = c(-.1, 1.6), ylim = c(-.1, 1.1)) + theme(plot.subtitle = element_text(hjust = .5, family = fnt_sel)) dag5 &lt;- dagify( A_obs ~ A + E_A, M ~ A, D ~ A, exposure = &quot;M&quot;, outcome = &quot;A&quot;, coords = tibble(name = c(&quot;E_A&quot;, &quot;A_obs&quot;, &quot;A&quot;, &quot;M&quot;, &quot;D&quot;), x = c( 0, .5, 1, 1.5, 1.5 ), y = c(.5, .5, .5, 1, 0))) dag5 %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;D_obs&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A_obs&quot;, &quot;M&quot;, &quot;D&quot;), &quot;predictor&quot;, &quot;confounds&quot;)), name = str_replace(name, &quot;(.*)_([a-zA-Z]*)&quot;,&quot;\\\\1[\\\\2]&quot;)) %&gt;% plot_dag(clr_in = clr_current) + coord_fixed(ratio = .6, xlim = c(-.1, 1.6), ylim = c(-.1, 1.1)) + theme(plot.subtitle = element_text(hjust = .5, family = fnt_sel)) n &lt;- 500 data_sim &lt;- tibble(age = rnorm(n), marriage = rnorm(n, mean = -age), divorce = rnorm(n, mean = age), age_obs = rnorm(n, mean = age)) 16.2 Missing Data 16.2.1 DAG ate my Homework annotate_dag &lt;- function(dag){ dag %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;Hs&quot;, &quot;response&quot;, if_else(name %in% c(&quot;S&quot;, &quot;D&quot;), &quot;predictor&quot;, &quot;confounds&quot;)), name = str_replace(name, &quot;Hs&quot;,&quot;H^&#39;*&#39;&quot;)) } dag6 &lt;- dagify( Hs ~ H + D, H ~ S, exposure = &quot;S&quot;, outcome = &quot;Hs&quot;, coords = tibble(name = c(&quot;S&quot;, &quot;H&quot;, &quot;D&quot;, &quot;Hs&quot;), x = c( 0, 1, 0, 1 ), y = c( 1, 1, 0, 0 ))) %&gt;% annotate_dag() dag7 &lt;- dagify( Hs ~ H + D, H ~ S, D ~ S, exposure = &quot;S&quot;, outcome = &quot;Hs&quot;, coords = tibble(name = c(&quot;S&quot;, &quot;H&quot;, &quot;D&quot;, &quot;Hs&quot;), x = c( 0, 1, 0, 1 ), y = c( 1, 1, 0, 0 ))) %&gt;% annotate_dag() dag8 &lt;- dagify( Hs ~ H + D, H ~ S + X, D ~ X, exposure = &quot;S&quot;, outcome = &quot;Hs&quot;, coords = tibble(name = c(&quot;S&quot;, &quot;H&quot;, &quot;D&quot;, &quot;Hs&quot;, &quot;X&quot;), x = c( 0, 1, 0, 1, .5 ), y = c( 1, 1, 0, 0, .5 ))) %&gt;% annotate_dag() dag9 &lt;- dagify( Hs ~ H + D, H ~ S, D ~ H, exposure = &quot;S&quot;, outcome = &quot;Hs&quot;, coords = tibble(name = c(&quot;S&quot;, &quot;H&quot;, &quot;D&quot;, &quot;Hs&quot;), x = c( 0, 1, 0, 1 ), y = c( 1, 1, 0, 0 ))) %&gt;% annotate_dag() list(dag6, dag7, dag8, dag9) %&gt;% purrr::map(plot_dag, clr_in = clr_current) %&gt;% wrap_plots(ncol = 2, tag_level = &quot;new&quot;) + plot_annotation(tag_levels = &quot;a&quot;) &amp; coord_fixed(ratio = .6, xlim = c(-.1, 1.1), ylim = c(-.1, 1.1)) &amp; theme(plot.subtitle = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) n &lt;- 100 data_homework_a &lt;- tibble( study = rnorm(n), homework = rbinom(n, size = 10 , inv_logit(study)), dog = rbern( n ), homework_missing = if_else(dog == 1, NA_integer_ , homework)) data_homework_b &lt;- data_homework_a %&gt;% mutate(dog = as.integer(study &gt; 0), homework_missing = if_else(dog == 1, NA_integer_ , homework)) n &lt;- 1e3 set.seed(42) data_homework_c &lt;- tibble( x = rnorm(n), study = rnorm(n), homework = rbinom(n, size = 10 , inv_logit(2 + study - 2 * x )), dog = as.integer(x &gt; 0), homework_missing = if_else(dog == 1, NA_integer_ , homework)) data_homework_c_list &lt;- data_homework_c %&gt;% dplyr::select(study, homework) %&gt;% as.list() model_hw_full &lt;- ulam( flist = alist( homework ~ binomial( 10, p ), logit(p) &lt;- alpha + beta_study * study, alpha ~ normal( 0, 1 ), beta_study ~ normal( 0, 0.5 ) ), data = data_homework_c_list, cores = 4, chains = 4, seed = 42 ) precis(model_hw_full) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha 1.26 0.02 1.22 1.29 1264.47 1 beta_study 0.61 0.03 0.57 0.65 1051.59 1 data_homework_c_list2 &lt;- data_homework_c %&gt;% filter(!is.na(homework_missing)) %&gt;% dplyr::select(study, homework) %&gt;% as.list() model_hw_missing &lt;- ulam( flist = alist( homework ~ binomial( 10, p ), logit(p) &lt;- alpha + beta_study * study, alpha ~ normal( 0, 1 ), beta_study ~ normal( 0, 0.5 ) ), data = data_homework_c_list2, cores = 4, chains = 4, seed = 42 ) precis(model_hw_missing) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha 3.06 0.07 2.94 3.18 767.92 1.00 beta_study 0.89 0.06 0.80 1.00 680.65 1.01 data_homework_c_list3 &lt;- data_homework_c %&gt;% mutate(dog = as.integer(abs(x) &lt; 1 ), homework_missing = if_else(dog == 1, NA_integer_ , homework)) %&gt;% filter(!is.na(homework_missing)) %&gt;% dplyr::select(study, homework) %&gt;% as.list() model_hw_opposite_effect &lt;- ulam( flist = alist( homework ~ binomial( 10, p ), logit(p) &lt;- alpha + beta_study * study, alpha ~ normal( 0, 1 ), beta_study ~ normal( 0, 0.5 ) ), data = data_homework_c_list3, cores = 4, chains = 4, seed = 42 ) precis(model_hw_opposite_effect) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha 0.73 0.04 0.67 0.79 1448.69 1 beta_study 0.35 0.04 0.29 0.41 1329.66 1 data_homework_d &lt;- data_homework_a %&gt;% mutate(dog = as.integer(homework &lt; 5), homework_missing = if_else(dog == 1, NA_integer_ , homework)) data_homework_c_list4 &lt;- data_homework_d %&gt;% filter(!is.na(homework_missing)) %&gt;% dplyr::select(study, homework) %&gt;% as.list() model_hw_unfixable &lt;- ulam( flist = alist( homework ~ binomial( 10, p ), logit(p) &lt;- alpha + beta_study * study, alpha ~ normal( 0, 1 ), beta_study ~ normal( 0, 0.5 ) ), data = data_homework_c_list4, cores = 4, chains = 4, seed = 42 ) precis(model_hw_unfixable) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha 0.44 0.10 0.29 0.60 640.41 1.01 beta_study 0.50 0.11 0.33 0.68 727.25 1.01 16.2.2 Imputing Primates dag10 &lt;- dagify( K ~ M + B, B ~ U, M ~ U, exposure = &quot;M&quot;, outcome = &quot;K&quot;, coords = tibble(name = c(&quot;M&quot;, &quot;U&quot;, &quot;K&quot;, &quot;B&quot;), x = c( 0, .5, .5, 1 ), y = c(1, 1, 0, 1))) dag10 %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;K&quot;, &quot;response&quot;, if_else(name %in% c(&quot;M&quot;, &quot;B&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr_current) + coord_fixed(ratio = .6, xlim = c(-.1, 1.1), ylim = c(-.1, 1.1)) + theme(plot.subtitle = element_text(hjust = .5, family = fnt_sel)) primate_dag &lt;- function(dag){ dag %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;K&quot;, &quot;response&quot;, if_else(name %in% c(&quot;M&quot;, &quot;Bs&quot;, &quot;Rb&quot;), &quot;predictor&quot;, &quot;confounds&quot;)), name = str_replace(name, &quot;Bs&quot;,&quot;B^&#39;*&#39;&quot;) %&gt;% str_replace( &quot;Rb&quot;,&quot;R[B]&quot;) ) } dag11 &lt;- dagify( K ~ M + B, B ~ U, M ~ U, Bs ~ B + Rb, exposure = &quot;M&quot;, outcome = &quot;K&quot;, coords = tibble(name = c(&quot;M&quot;, &quot;U&quot;, &quot;K&quot;, &quot;B&quot;, &quot;Bs&quot;, &quot;Rb&quot;), x = c( 0, .5, .5, 1, 1, .5 ), y = c(.75, .75, 0, .75, 1.5, 1.5))) %&gt;% primate_dag() dag12 &lt;- dagify( K ~ M + B, B ~ U, M ~ U, Bs ~ B + Rb, Rb ~ M, exposure = &quot;M&quot;, outcome = &quot;K&quot;, coords = tibble(name = c(&quot;M&quot;, &quot;U&quot;, &quot;K&quot;, &quot;B&quot;, &quot;Bs&quot;, &quot;Rb&quot;), x = c( 0, .5, .5, 1, 1, .5 ), y = c(.75, .75, 0, .75, 1.5, 1.5))) %&gt;% primate_dag() dag13 &lt;- dagify( K ~ M + B, B ~ U, M ~ U, Bs ~ B + Rb, Rb ~ B, exposure = &quot;M&quot;, outcome = &quot;K&quot;, coords = tibble(name = c(&quot;M&quot;, &quot;U&quot;, &quot;K&quot;, &quot;B&quot;, &quot;Bs&quot;, &quot;Rb&quot;), x = c( 0, .5, .5, 1, 1, .5 ), y = c(.75, .75, 0, .75, 1.5, 1.5))) %&gt;% primate_dag() list(dag11, dag12, dag13) %&gt;% purrr::map(plot_dag, clr_in = clr_current) %&gt;% wrap_plots(nrow = 1, tag_level = &quot;new&quot;) + plot_annotation(tag_levels = &quot;a&quot;) &amp; coord_fixed(ratio = .6, xlim = c(-.1, 1.1), ylim = c(-.1, 1.6)) &amp; theme(plot.subtitle = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) dagify( K ~ M + B, B ~ U + V, M ~ U, Bs ~ B + Rb, Rb ~ V, exposure = &quot;M&quot;, outcome = &quot;K&quot;, coords = tibble(name = c(&quot;M&quot;, &quot;U&quot;, &quot;K&quot;, &quot;B&quot;, &quot;Bs&quot;, &quot;Rb&quot;, &quot;V&quot;), x = c( 0, .5, .5, 1, 1.5, 1.5, 1.25 ), y = c(.75, .75, 0, .75, .75, 0, .375))) %&gt;% primate_dag() %&gt;% plot_dag(clr_in = clr_current) + coord_fixed(ratio = .6, xlim = c(-.1, 1.6), ylim = c(-.1, .87)) + theme(plot.subtitle = element_text(hjust = .5, family = fnt_sel)) \\[ \\begin{array}{rclr} K_{i} &amp; \\sim &amp; \\textrm{Normal}(\\mu_{i}, \\sigma) &amp; \\textrm{[ distribution for outcome }k\\textrm{ ]}\\\\ \\mu_{i} &amp; = &amp; \\alpha + \\beta_{B} \\color{#B35136}{B_{i}} + \\beta_{M}~\\textrm{log} M_{i} &amp; \\textrm{[ linear model ]}\\\\ \\color{#B35136}{B_{i}} &amp; \\sim &amp; \\color{#B35136}{\\textrm{Normal}(\\nu, \\sigma_{B})} &amp; \\textrm{[ distribution of obs/missing }B\\textrm{ ]}\\\\ \\alpha &amp; \\sim &amp; \\textrm{Normal}(0,0.5) &amp; \\\\ \\beta_{B} &amp; \\sim &amp; \\textrm{Normal}(0,0.5) &amp; \\\\ \\beta_{M} &amp; \\sim &amp; \\textrm{Normal}(0,0.5) &amp; \\\\ \\sigma &amp; \\sim &amp; \\textrm{Exponential}(1) &amp; \\\\ \\nu &amp; \\sim &amp; \\textrm{Normal}(0.5, 1) &amp; \\\\ \\sigma_{B} &amp; \\sim &amp; \\textrm{Exponential}(1) &amp; \\\\ \\end{array} \\] data(milk) data_milk_full &lt;- milk %&gt;% as_tibble() %&gt;% mutate(mass.log = log(mass), neocortex.prop = neocortex.perc / 100, across(.cols = c(`kcal.per.g`, `neocortex.prop`, `mass.log`), .fns = standardize, .names = &quot;{str_remove_all(.col, &#39;\\\\\\\\..*&#39;)}_std&quot;)) data_milk_list &lt;- data_milk_full %&gt;% dplyr::select(species, kcal_std, neocortex_std, mass_std) %&gt;% as.list() model_primates_imputed &lt;- ulam( flist = alist( kcal_std ~ dnorm( mu, sigma ), mu &lt;- alpha + beta_brain * neocortex_std + beta_mass * mass_std, neocortex_std ~ dnorm( nu, sigma_brain ), c( alpha, nu ) ~ dnorm( 0, 0.5 ), c(beta_brain, beta_mass ) ~ dnorm( 0, 0.5 ), sigma_brain ~ dexp(1), sigma ~ dexp(1) ), data = data_milk_list, cores = 4, chains = 4, seed = 42 ) precis(model_primates_imputed, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 nu -0.06 0.21 -0.39 0.28 2210.15 1.00 alpha 0.03 0.17 -0.23 0.29 2240.59 1.00 beta_mass -0.55 0.20 -0.85 -0.22 1114.24 1.00 beta_brain 0.50 0.23 0.11 0.87 831.86 1.01 sigma_brain 1.01 0.17 0.77 1.33 1444.15 1.00 sigma 0.84 0.14 0.64 1.08 1255.77 1.00 neocortex_std_impute[1] -0.60 0.90 -1.99 0.84 2237.08 1.00 neocortex_std_impute[2] -0.70 0.94 -2.18 0.82 2266.74 1.00 neocortex_std_impute[3] -0.70 0.93 -2.14 0.80 2007.44 1.00 neocortex_std_impute[4] -0.30 0.90 -1.73 1.12 2470.97 1.00 neocortex_std_impute[5] 0.44 0.90 -0.99 1.89 2917.91 1.00 neocortex_std_impute[6] -0.20 0.91 -1.57 1.25 2303.80 1.00 neocortex_std_impute[7] 0.21 0.90 -1.22 1.61 3195.30 1.00 neocortex_std_impute[8] 0.26 0.89 -1.15 1.66 2089.83 1.00 neocortex_std_impute[9] 0.50 0.87 -0.86 1.91 2308.11 1.00 neocortex_std_impute[10] -0.44 0.88 -1.84 0.92 2664.65 1.00 neocortex_std_impute[11] -0.30 0.90 -1.75 1.19 2810.75 1.00 neocortex_std_impute[12] 0.15 0.89 -1.27 1.53 2816.22 1.00 data_milk_list2 &lt;- data_milk_full %&gt;% filter(!is.na(neocortex_std)) %&gt;% dplyr::select(species, kcal_std, neocortex_std, mass_std) %&gt;% as.list() model_primates_non_missing &lt;- ulam( flist = alist( kcal_std ~ dnorm( mu, sigma ), mu &lt;- alpha + beta_brain * neocortex_std + beta_mass * mass_std, neocortex_std ~ dnorm( nu, sigma_brain ), c( alpha, nu ) ~ dnorm( 0, 0.5 ), c(beta_brain, beta_mass ) ~ dnorm( 0, 0.5 ), sigma_brain ~ dexp(1), sigma ~ dexp(1) ), data = data_milk_list2, cores = 4, chains = 4, seed = 42 ) precis(model_primates_non_missing) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 nu 0.00 0.23 -0.37 0.36 1778.70 1 alpha 0.10 0.19 -0.20 0.40 1423.18 1 beta_mass -0.64 0.26 -1.04 -0.20 923.00 1 beta_brain 0.60 0.29 0.12 1.06 870.94 1 sigma_brain 1.03 0.18 0.79 1.37 1908.36 1 sigma 0.87 0.19 0.63 1.22 871.92 1 missing_mean &lt;- coeftab(model_primates_non_missing, model_primates_imputed)@coefs %&gt;% as.data.frame() %&gt;% rownames_to_column(var = &quot;param&quot;) %&gt;% as_tibble() %&gt;% filter(param %in% c(&quot;beta_brain&quot;, &quot;beta_mass&quot;)) coeftab(model_primates_non_missing, model_primates_imputed)@se %&gt;% as.data.frame() %&gt;% rownames_to_column(var = &quot;param&quot;) %&gt;% as_tibble() %&gt;% filter(param %in% c(&quot;beta_brain&quot;, &quot;beta_mass&quot;)) %&gt;% set_names(nm = c(&quot;param&quot;, names(missing_mean)[2:3]))%&gt;% pivot_longer(-param, names_to = &quot;model&quot;, values_to = &quot;se&quot;) %&gt;% left_join(missing_mean %&gt;% pivot_longer(-param, names_to = &quot;model&quot;, values_to = &quot;mean&quot;)) %&gt;% mutate(model = str_remove(model, &quot;model_primates_&quot;)) %&gt;% ggplot(aes(y = model)) + geom_pointrange(aes(x = mean, xmin = mean - se, xmax = mean+se), color = clr0dd, fill = clr0, shape = 21) + facet_grid(param ~ ., switch = &quot;y&quot;) + theme(strip.placement = &quot;outside&quot;) primates_posterior &lt;- extract.samples(model_primates_imputed) %&gt;% as_tibble() imputed_neocortex &lt;- primates_posterior$neocortex_std_impute %&gt;% as_tibble() %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(pi = list(tibble(value = quantile(value, probs = c(.055, .25, .5, .75, .955)), label = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;)))) %&gt;% mutate(name = str_remove(name, &quot;V&quot;) %&gt;% as.integer()) %&gt;% arrange(name) %&gt;% unnest(pi) %&gt;% pivot_wider(names_from = label, values_from = value) %&gt;% bind_cols(data_milk_full %&gt;% filter(is.na(neocortex.perc)),.) p1 &lt;- data_milk_list %&gt;% as_tibble() %&gt;% filter(!is.na(neocortex_std)) %&gt;% ggplot(aes(x = neocortex_std, y = kcal_std)) + geom_pointrange(data = imputed_neocortex, aes(xmin = ll, x = m ,xmax = hh), color = clr0dd, fill = clr0, shape = 21, fatten = 2.6) + geom_point(color = clr_current, size = 1.6) p2 &lt;- data_milk_list %&gt;% as_tibble() %&gt;% filter(!is.na(neocortex_std)) %&gt;% ggplot(aes(y = neocortex_std, x = mass_std)) + geom_pointrange(data = imputed_neocortex, aes(ymin = ll, y = m ,ymax = hh), color = clr0dd, fill = clr0, shape = 21, fatten = 2.6) + geom_point(color = clr_current, size = 1.6) p1 + p2 Updating the imputation within the model to take the correlation of mass_std and neocortex_std into account: \\[ (M_{i}, B_{i}) \\sim \\textrm{MVNormal}\\left(( \\mu_{M}, \\mu_{B}), S \\right) \\] model_primates_impute_covar &lt;- ulam( flist = alist( # kcal as function of neocortex and mass kcal_std ~ dnorm( mu, sigma ), mu &lt;- alpha + beta_brain * neocortex_merge + beta_mass * mass_std, # mass and neocortex correlation mass_neocortex ~ multi_normal( c( mu_mass, mu_neocortex ), Rho_nm, Sigma_nm ), matrix[29, 2]:mass_neocortex &lt;&lt;- append_col( mass_std, neocortex_merge ), # define neocortex_merge as mix of observed and imputed values vector[29]:neocortex_merge &lt;- merge_missing( neocortex_std, neocortex_std_impute ), # priors c( alpha, mu_mass, mu_neocortex ) ~ dnorm( 0, 0.5 ), c( beta_brain, beta_mass ) ~ dnorm( 0, 0.5 ), sigma ~ dexp(1), Rho_nm ~ lkj_corr(2), Sigma_nm ~ dexp(1) ), data = data_milk_list, cores = 4, chains = 4 ) precis(model_primates_impute_covar, depth = 3, pars = c(&quot;beta_mass&quot;, &quot;beta_brain&quot;, &quot;Rho_nm&quot;)) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 beta_mass -0.65 0.22 -0.98 -0.29 1153.75 1 beta_brain 0.59 0.25 0.17 0.96 938.43 1 Rho_nm[1,1] 1.00 0.00 1.00 1.00 NaN NaN Rho_nm[1,2] 0.61 0.13 0.37 0.79 1300.92 1 Rho_nm[2,1] 0.61 0.13 0.37 0.79 1300.92 1 Rho_nm[2,2] 1.00 0.00 1.00 1.00 NaN NaN primates_posterior_covar &lt;- extract.samples(model_primates_impute_covar) %&gt;% as_tibble() imputed_neocortex_covar &lt;- primates_posterior_covar$neocortex_std_impute %&gt;% as_tibble() %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(pi = list(tibble(value = quantile(value, probs = c(.055, .25, .5, .75, .955)), label = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;)))) %&gt;% mutate(name = str_remove(name, &quot;V&quot;) %&gt;% as.integer()) %&gt;% arrange(name) %&gt;% unnest(pi) %&gt;% pivot_wider(names_from = label, values_from = value) %&gt;% bind_cols(data_milk_full %&gt;% filter(is.na(neocortex.perc)),.) p1 &lt;- data_milk_list %&gt;% as_tibble() %&gt;% filter(!is.na(neocortex_std)) %&gt;% ggplot(aes(x = neocortex_std, y = kcal_std)) + geom_pointrange(data = imputed_neocortex_covar, aes(xmin = ll, x = m ,xmax = hh), color = clr0dd, fill = clr0, shape = 21, fatten = 2.6) + geom_point(color = clr_current, size = 1.6) p2 &lt;- data_milk_list %&gt;% as_tibble() %&gt;% filter(!is.na(neocortex_std)) %&gt;% ggplot(aes(y = neocortex_std, x = mass_std)) + geom_pointrange(data = imputed_neocortex_covar, aes(ymin = ll, y = m ,ymax = hh), color = clr0dd, fill = clr0, shape = 21, fatten = 2.6) + geom_point(color = clr_current, size = 1.6) p1 + p2 stancode(model_primates_imputed) #&gt; functions{ #&gt; #&gt; #&gt; vector merge_missing( int[] miss_indexes , vector x_obs , vector x_miss ) { #&gt; int N = dims(x_obs)[1]; #&gt; int N_miss = dims(x_miss)[1]; #&gt; vector[N] merged; #&gt; merged = x_obs; #&gt; for ( i in 1:N_miss ) #&gt; merged[ miss_indexes[i] ] = x_miss[i]; #&gt; return merged; #&gt; } #&gt; } #&gt; data{ #&gt; int species[29]; #&gt; vector[29] kcal_std; #&gt; vector[29] mass_std; #&gt; vector[29] neocortex_std; #&gt; int neocortex_std_missidx[12]; #&gt; } #&gt; parameters{ #&gt; real nu; #&gt; real alpha; #&gt; real beta_mass; #&gt; real beta_brain; #&gt; real&lt;lower=0&gt; sigma_brain; #&gt; real&lt;lower=0&gt; sigma; #&gt; vector[12] neocortex_std_impute; #&gt; } #&gt; model{ #&gt; vector[29] mu; #&gt; vector[29] neocortex_std_merge; #&gt; sigma ~ exponential( 1 ); #&gt; sigma_brain ~ exponential( 1 ); #&gt; beta_brain ~ normal( 0 , 0.5 ); #&gt; beta_mass ~ normal( 0 , 0.5 ); #&gt; alpha ~ normal( 0 , 0.5 ); #&gt; nu ~ normal( 0 , 0.5 ); #&gt; neocortex_std_merge = merge_missing(neocortex_std_missidx, to_vector(neocortex_std), neocortex_std_impute); #&gt; neocortex_std_merge ~ normal( nu , sigma_brain ); #&gt; for ( i in 1:29 ) { #&gt; mu[i] = alpha + beta_brain * neocortex_std_merge[i] + beta_mass * mass_std[i]; #&gt; } #&gt; kcal_std ~ normal( mu , sigma ); #&gt; } 16.2.3 Where is Your God Now? data(Moralizing_gods) data_gods &lt;- Moralizing_gods %&gt;% as_tibble() data_gods %&gt;% group_by(moralizing_gods) %&gt;% count() #&gt; # A tibble: 3 × 2 #&gt; # Groups: moralizing_gods [3] #&gt; moralizing_gods n #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 0 17 #&gt; 2 1 319 #&gt; 3 NA 528 data_gods %&gt;% group_by(moralizing_gods, writing) %&gt;% count() %&gt;% pivot_wider(names_from = &quot;writing&quot;, values_from = &quot;n&quot;) #&gt; # A tibble: 3 × 3 #&gt; # Groups: moralizing_gods [3] #&gt; moralizing_gods `0` `1` #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 0 16 1 #&gt; 2 1 9 310 #&gt; 3 NA 442 86 data_gods %&gt;% ggplot(aes(x = year, y = population)) + geom_point(aes(color = factor(moralizing_gods), shape = factor(moralizing_gods)), size = 1.5, alpha = .7)+ scale_shape_manual(values = c(`0` = 1, `1` = 19), na.value = 4)+ scale_color_manual(values = c(`0` = clr_current, `1` = clr_current), na.value = clr0dd) + theme(legend.position = &quot;none&quot;) dagify( Gs ~ G + Rg, Rg ~ W, W ~ P, P ~ G, exposure = &quot;G&quot;, outcome = &quot;P&quot;, coords = tibble(name = c(&quot;P&quot;, &quot;G&quot;, &quot;Gs&quot;, &quot;W&quot;, &quot;Rg&quot;), x = c( 0, .5, 1, .25, .75 ), y = c(1, 1, 1, 0, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;P&quot;, &quot;response&quot;, if_else(name %in% c(&quot;Gs&quot;, &quot;W&quot;, &quot;Rg&quot;), &quot;predictor&quot;, &quot;confounds&quot;)), name = str_replace(name, &quot;Gs&quot;,&quot;G^&#39;*&#39;&quot;) %&gt;% str_replace( &quot;Rg&quot;,&quot;R[G]&quot;)) %&gt;% plot_dag(clr_in = clr_current) + coord_fixed(ratio = .35, xlim = c(-.1, 1.1), ylim = c(-.1, 1.1)) + theme(plot.subtitle = element_text(hjust = .5, family = fnt_sel)) data_gods %&gt;% filter(polity == &quot;Big Island Hawaii&quot;) %&gt;% dplyr::select(year, writing,moralizing_gods) %&gt;% t() \\[\\begin{bmatrix} 1000 &amp;1100 &amp;1200 \\\\1300 &amp;1400 &amp;1500 \\\\1600 &amp;1700 &amp;1800 \\\\0 &amp;0 &amp;0 \\\\0 &amp;0 &amp;0 \\\\0 &amp;0 &amp;0 \\\\NA &amp;NA &amp;NA \\\\NA &amp;NA &amp;NA \\\\NA &amp;NA &amp;1 \\\\ \\end{bmatrix}\\] 16.3 Categorical Errors and Discrete Absences 16.3.1 Discrete Cats dagify( Cs ~ C + Rc, N ~ C, exposure = &quot;Cs&quot;, outcome = &quot;N&quot;, coords = tibble(name = c(&quot;Rc&quot;, &quot;Cs&quot;, &quot;C&quot;, &quot;N&quot;), x = c( 0, .5, 1, 1.5 ), y = c(0, 0, 0, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;N&quot;, &quot;response&quot;, if_else(name %in% c(&quot;Cs&quot;, &quot;Rc&quot;), &quot;predictor&quot;, &quot;confounds&quot;)), name = str_replace(name, &quot;Cs&quot;,&quot;C^&#39;*&#39;&quot;) %&gt;% str_replace( &quot;Rc&quot;,&quot;R[C]&quot;)) %&gt;% plot_dag(clr_in = clr_current) + coord_fixed(ratio = 1, xlim = c(-.1, 1.6), ylim = c(-.1, .1)) + theme(plot.subtitle = element_text(hjust = .5, family = fnt_sel)) \\[ \\begin{array}{rclr} N_{i} &amp; \\sim &amp; \\textrm{Poisson}(\\lambda_{i}) &amp; \\textrm{[ probability of notes sung ]}\\\\ \\textrm{log}~\\lambda_{i} &amp; = &amp; \\alpha + \\beta~C_{i} &amp; \\textrm{[ rate of notes as funtion of cat ]}\\\\ C_{i} &amp; \\sim &amp; \\textrm{Bernoulli}(k) &amp; \\textrm{[ probability cat is present ]} \\\\ R_{C, i} &amp; \\sim &amp; \\textrm{Bernoulli}(r) &amp; \\textrm{[ probability of not knowing } C_{i} \\textrm{ ]} \\\\ \\end{array} \\] set.seed(42) n_houses &lt;- 50 alpha &lt;- 5 beta &lt;- -3 k &lt;- .5 r &lt;- .2 data_cats &lt;- tibble( cat = rbern( n_houses, k ), notes = rpois( n_houses, alpha + beta * cat ), r_c = rbern( n_houses, r ), cat_obs = if_else(r_c == 0, cat , -9L) ) The likelihood to observe \\(N_i\\) notes, conditional on \\(C_i\\) is \\[ \\begin{array}{rcl} \\textrm{Pr}(N_{i}) &amp; = &amp; (\\textrm{probability of cat}) (\\textrm{probability of } N_{i} \\textrm{ when there is a cat})~+ \\\\ &amp; &amp; (\\textrm{probability of no cat}) (\\textrm{probability of } N_{i} \\textrm{ whenthere is no cat}) \\\\ \\textrm{Pr}(N_{i}) &amp; = &amp; \\textrm{Pr}(C_{i} = 1)~\\textrm{Pr}(N_{i} | C_{i} = 1) + \\textrm{Pr}(C_{i} = 0)~\\textrm{Pr}(N_{i} | C_{i} = 0) \\end{array} \\] model_cat &lt;- ulam( flist = alist( # singing bird model # cat presence is known notes | r_c == 0 ~ poisson(lambda), log(lambda) &lt;- alpha + beta * cat_obs, # cat NA notes | r_c == 1 ~ custom( log_sum_exp( log(k) + poisson_lpmf( notes | exp(alpha + beta)), log(1 - k) + poisson_lpmf(notes | exp(alpha)) ) ), # priors alpha ~ normal( 0, 1 ), beta ~ normal( 0, 0.5 ), # sneaking cat model cat | r_c == 0 ~ bernoulli(k), k ~ beta( 2, 2 ) ), cores = 4, chain = 4, seed = 42, data = data_cats ) \\[ \\textrm{Pr}(C_{i} = 1 | N_{i} ) = \\frac{\\textrm{Pr}(N_{i} | C_{i} = 1 )~\\textrm{Pr}(C_{i} = 1)}{\\textrm{Pr}(N_{i} | C_{i} = 1)~\\textrm{Pr}(C_{i} = 1) + \\textrm{Pr}(N_{i} | C_{i} = 0)~\\textrm{Pr}(C_{i} = 0)} \\] data_cats_list &lt;- data_cats %&gt;% as.list() %&gt;% c(., list(n_houses = n_houses)) model_cat_gq &lt;- ulam( flist = alist( # singing bird model # cat presence is known notes | r_c == 0 ~ poisson(lambda), log(lambda) &lt;- alpha + beta * cat_obs, # cat NA notes | r_c == 1 ~ custom( log_sum_exp( log(k) + poisson_lpmf( notes | exp(alpha + beta)), log(1 - k) + poisson_lpmf(notes | exp(alpha)) ) ), # priors alpha ~ normal( 0, 1 ), beta ~ normal( 0, 0.5 ), # sneaking cat model cat | r_c == 0 ~ bernoulli(k), k ~ beta( 2, 2 ), # imputed values gq&gt; vector[n_houses]:pr_c1 &lt;- exp(lp_c1) / (exp(lp_c1) + exp(lp_c0)), gq&gt; vector[n_houses]:lp_c1 &lt;- log(k) + poisson_lpmf(notes[i] | exp(alpha + beta)), gq&gt; vector[n_houses]:lp_c0 &lt;- log(1 - k) + poisson_lpmf(notes[i] | exp(alpha)) ), cores = 4, chain = 4, seed = 42, data = data_cats_list ) Example for a non-binary categorical value (multiple options, eg up to two cats): notes | r_c == 1 ~ custom( log_sum_exp( binomial_lpmf(2|2, k) + poisson_lpmf( notes | exp(alpha + beta * 2)), binomial_lpmf(1|2, k) + poisson_lpmf( notes | exp(alpha + beta * 1)), binomial_lpmf(0|2, k) + poisson_lpmf(notes | exp(alpha + beta * 0)) ) The extension for multiple categorical variables (eg. cat and dog) with missing values expands to the following. If both cat and dog are NA: \\[ \\begin{array}{rcl} \\textrm{Pr}(N_{i}) &amp; = &amp; \\textrm{Pr}(C_{i} = 1)~\\textrm{Pr}(D_{i} = 1)~\\textrm{Pr}(N_{i} | C_{i} = 1, D_{i} = 1)~+\\\\ &amp;&amp;\\textrm{Pr}(C_{i} = 1)~\\textrm{Pr}(D_{i} = 0)~\\textrm{Pr}(N_{i} | C_{i} = 1, D_{i} = 0)~+ \\\\ &amp;&amp;\\textrm{Pr}(C_{i} = 0)~\\textrm{Pr}(D_{i} = 1)~\\textrm{Pr}(N_{i} | C_{i} = 0, D_{i} = 1)~+ \\\\ &amp;&amp;\\textrm{Pr}(C_{i} = 0)~\\textrm{Pr}(D_{i} = 0)~\\textrm{Pr}(N_{i} | C_{i} = 0, D_{i} = 0) \\end{array} \\] If cat is NA and dog is known present: \\[ \\begin{array}{rcl} \\textrm{Pr}(N_{i}) &amp; = &amp; \\textrm{Pr}(C_{i} = 1)~\\textrm{Pr}(N_{i} | C_{i} = 1, D_{i} = 1) + \\textrm{Pr}(C_{i} = 0)~\\textrm{Pr}(N_{i} | C_{i} = 0, D_{i} = 1) \\end{array} \\] likewise, if dog is NA and cat is known absent: \\[ \\begin{array}{rcl} \\textrm{Pr}(N_{i}) &amp; = &amp; \\textrm{Pr}(D_{i} = 1)~\\textrm{Pr}(N_{i} | C_{i} = 0, D_{i} = 1) + \\textrm{Pr}(D_{i} = 0)~\\textrm{Pr}(N_{i} | C_{i} = 0, D_{i} = 0) \\end{array} \\] (all combinations need to be accounted for though) library(rlang) chapter15_models &lt;- env( ) write_rds(chapter15_models, &quot;envs/chapter15_models.rds&quot;) 16.4 Homework E1 \\[ \\begin{array}{rcl} T_{i} &amp; \\sim &amp; \\textrm{Poisson}(\\mu_{i})\\\\ \\textrm{log}~\\mu_{i} &amp; = &amp; \\alpha + \\beta ~ \\textrm{log}~P_{\\textrm{TRUE},i}\\\\ P_{\\textrm{TRUE}, i} &amp; \\sim &amp; \\textrm{Normal}(P_{\\textrm{OBS},i}, P_{\\textrm{SE}})\\\\ \\alpha &amp; \\sim &amp; \\textrm{Normal}(0, 1.5)\\\\ \\beta &amp; \\sim &amp; \\textrm{Normal}(0, 1) \\\\ P_{SE} &amp; \\sim &amp; \\textrm{Exponential}(1) \\end{array} \\] E2 \\[ \\begin{array}{rcl} T_{i} &amp; \\sim &amp; \\textrm{Poisson}(\\mu_{i})\\\\ \\textrm{log}~\\mu_{i} &amp; = &amp; \\alpha + \\beta ~ P_{\\textrm{TRUE},i}\\\\ P_{\\textrm{TRUE}, i} &amp; \\sim &amp; \\textrm{Normal}(\\overline{P_{\\textrm{OBS}}}, P_{\\textrm{SE}})\\\\ \\alpha &amp; \\sim &amp; \\textrm{Normal}(0, 1.5)\\\\ \\beta &amp; \\sim &amp; \\textrm{Normal}(0, 1) \\\\ \\overline{P_{\\textrm{OBS}}} &amp; \\sim &amp; \\textrm{Normal}(0, 1) \\\\ P_{SE} &amp; \\sim &amp; \\textrm{Exponential}(1) \\end{array} \\] M1 \\[ \\begin{array}{rclr} K_{i} &amp; \\sim &amp; \\textrm{Normal}(\\mu_{i}, \\sigma) &amp; \\textrm{[ distribution for outcome }k\\textrm{ ]}\\\\ \\mu_{i} &amp; = &amp; \\alpha + \\beta_{B} \\color{#B35136}{B_{i}} + \\beta_{M}~\\textrm{log} M_{i} &amp; \\textrm{[ linear model ]}\\\\ \\color{#B35136}{B_{i}} &amp; \\sim &amp; \\color{#B35136}{\\textrm{Normal}(\\nu, \\sigma_{B})} &amp; \\textrm{[ distribution of obs/missing }B\\textrm{ ]}\\\\ \\alpha &amp; \\sim &amp; \\textrm{Normal}(0,0.5) &amp; \\\\ \\beta_{B} &amp; \\sim &amp; \\textrm{Normal}(0,0.5) &amp; \\\\ \\beta_{M} &amp; \\sim &amp; \\textrm{Normal}(0,0.5) &amp; \\\\ \\sigma &amp; \\sim &amp; \\textrm{Exponential}(1) &amp; \\\\ \\nu &amp; \\sim &amp; \\textrm{Normal}(0.5, 1) &amp; \\\\ \\sigma_{B} &amp; \\sim &amp; \\textrm{Exponential}(1) &amp; \\\\ \\end{array} \\] This imputation model assumes that the data generating process behind brain size in primates is gaussian with the average \\(\\nu\\) and a standard deviation of \\(\\sigma_{B}\\). It furthermore assumes that error in measurement is also gaussian, centered on the actual values (0), but with an variance that is equal to the on in the data generating process. Importantly, the error generating process is assumed to be random. M2 data_milk_list2 &lt;- data_milk_full %&gt;% dplyr::select(species, kcal_std, neocortex_prop = neocortex.prop, mass_std) %&gt;% as.list() model_primates_beta &lt;- ulam( flist = alist( kcal_std ~ dnorm( mu, sigma ), mu &lt;- alpha + beta_brain * ( neocortex_prop - 0.67 ) + beta_mass * mass_std, neocortex_prop ~ dbeta2( nu, theta_brain ), nu ~ dbeta( 2, 2 ), alpha ~ dnorm( 0, 0.5 ), beta_brain ~ dnorm( 0, 10 ), beta_mass ~ dnorm( 0, 0.5 ), theta_brain ~ dexp(1), sigma ~ dexp(1), vector[12]:neocortex_prop_impute ~ uniform( 0, 1) ), data = data_milk_list2, cores = 4, chains = 4, seed = 42 ) primates_posterior &lt;- extract.samples(model_primates_beta) %&gt;% as_tibble() imputed_neocortex &lt;- primates_posterior$neocortex_prop_impute %&gt;% as_tibble() %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(pi = list(tibble(value = quantile(value, probs = c(.055, .25, .5, .75, .955)), label = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;)))) %&gt;% mutate(name = str_remove(name, &quot;V&quot;) %&gt;% as.integer()) %&gt;% arrange(name) %&gt;% unnest(pi) %&gt;% pivot_wider(names_from = label, values_from = value) %&gt;% bind_cols(data_milk_full %&gt;% filter(is.na(neocortex.perc)),.) data_milk_list2 %&gt;% as_tibble() %&gt;% filter(!is.na(neocortex_prop)) %&gt;% ggplot(aes(x = neocortex_prop, y = kcal_std)) + geom_pointrange(data = imputed_neocortex, aes(xmin = ll, x = m ,xmax = hh), color = clr0dd, fill = clr0, shape = 21, fatten = 2.6) + geom_point(color = clr_current, size = 1.6) M3 model_divorce_double_se &lt;- ulam( flist = alist( divorce_obs ~ dnorm( divorce_true, divorce_sd * 2.0 ), vector[N]:divorce_true ~ dnorm( mu, sigma ), mu &lt;- alpha + beta_age * median_age_std + beta_marriage * marriage_std, alpha ~ dnorm( 0, 0.2 ), beta_age ~ dnorm( 0, 0.5 ), beta_marriage ~ dnorm( 0, 0.5 ), sigma ~ dexp(1) ), data = data_waffle_list, cores = 4, chains = 4, log_lik = TRUE ) \\(\\rightarrow\\) lots of divergent transitions model_divorce_double_se_noncentered &lt;- ulam( flist = alist( divorce_obs ~ dnorm( mu + z_true * sigma, divorce_sd * 2.0 ), vector[N]:z_true ~ dnorm( 0, 1 ), mu &lt;- alpha + beta_age * median_age_std + beta_marriage * marriage_std, alpha ~ dnorm( 0, 0.2 ), beta_age ~ dnorm( 0, 0.5 ), beta_marriage ~ dnorm( 0, 0.5 ), sigma ~ dexp(1) ), data = data_waffle_list, cores = 4, chains = 4, log_lik = TRUE ) precis(model_divorce) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha -0.05 0.09 -0.20 0.10 1585.93 1 beta_age -0.61 0.16 -0.86 -0.36 1169.42 1 beta_marriage 0.05 0.17 -0.21 0.32 1014.84 1 sigma 0.59 0.10 0.43 0.76 769.88 1 precis(model_divorce_double_se_noncentered) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha -0.12 0.10 -0.28 0.04 2738.54 1.00 beta_age -0.65 0.17 -0.91 -0.37 2324.01 1.00 beta_marriage 0.19 0.19 -0.12 0.48 2191.71 1.00 sigma 0.15 0.11 0.01 0.35 815.90 1.01 M4 dagify( Z ~ Y, Y ~ X, exposure = &quot;X&quot;, outcome = &quot;Y&quot;, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), x = c( 0, .5, 1 ), y = c(0, 0, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;Y&quot;, &quot;response&quot;, if_else(name %in% c(&quot;Z&quot;, &quot;X&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr_current) + coord_fixed(ratio = 1, xlim = c(-.1, 1.1), ylim = c(-.1, .1)) + theme(plot.subtitle = element_text(hjust = .5, family = fnt_sel)) n &lt;- 500 data_sim_dag &lt;- tibble(x = rnorm(n), y = rnorm(n, x), z = rnorm(n , y)) model_sim_dag &lt;- ulam( flist = alist( y ~ dnorm( mu , sigma ), mu &lt;- alpha + beta_x * x + beta_z * z, c(alpha, beta_x, beta_z) ~ dnorm( 0, 1 ), sigma ~ dexp(1) ), data = data_sim_dag, chains = 4, cores = 4) precis(model_sim_dag) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 beta_z 0.45 0.02 0.41 0.48 1441.44 1 beta_x 0.51 0.04 0.45 0.57 1426.12 1 alpha -0.01 0.03 -0.06 0.04 1870.59 1 sigma 0.72 0.02 0.68 0.75 1857.25 1 M5 birds_posterior &lt;- extract.samples(model_cat_gq) %&gt;% as_tibble() birds_posterior$pr_c1 %&gt;% as_tibble() %&gt;% t() %&gt;% bind_cols(data_cats_list %&gt;% as_tibble() %&gt;% mutate(cat_idx = row_number()), .) %&gt;% pivot_longer(starts_with(&quot;..&quot;)) %&gt;% group_by(cat_idx) %&gt;% summarise(cat = cat[[1]], pi = list(tibble(value = c(quantile(value, probs = c(.055, .25, .5, .75, .955)), mean(value)), labels = c(&#39;ll&#39;, &#39;l&#39;, &#39;m&#39;, &#39;h&#39;, &#39;hh&#39;, &#39;mean&#39;)))) %&gt;% ungroup() %&gt;% unnest(pi) %&gt;% pivot_wider(names_from = &quot;labels&quot;, values_from = &quot;value&quot;) %&gt;% mutate(cat_idx = fct_reorder(as.character(cat_idx), mean)) %&gt;% ggplot(aes(x = cat_idx)) + geom_linerange(aes(ymin = ll, ymax = hh), color = clr_current, size = .2)+ geom_point(aes(y = m, shape = factor(cat)), color = clr_current, fill = clr_lighten(clr_current, .8)) + coord_cartesian(ylim = c(0, 1)) + scale_shape_manual(values = c(`0` = 21, `1` = 19), guide = FALSE) M6 n &lt;- 100 data_sim_dog &lt;- tibble(.idx = 1:n, s = rnorm(n), h = rbinom(n, size = 10, inv_logit(s)), d = rbern(n), h_missing = if_else(d == 1, NA_integer_, h)) data_sim_dog_list_1 &lt;- data_sim_dog %&gt;% dplyr::select(-.idx) %&gt;% filter(!is.na(h_missing)) %&gt;% as.list() model_sim_dog_1 &lt;- ulam( flist = alist( h_missing ~ binomial( 10 , p ), logit(p) &lt;- alpha + beta_s * s, alpha ~ normal( 0 , 1.5 ), beta_s ~ normal( 0 , 0.5 ) ), data = data_sim_dog_list_1, chains = 4, cores = 4) precis( model_sim_dog_1 ) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha -0.13 0.10 -0.29 0.02 1336.04 1 beta_s 0.97 0.12 0.79 1.16 1138.22 1 data_sim_dog_list_2 &lt;- data_sim_dog %&gt;% dplyr::select(-.idx) %&gt;% mutate(h_missing = if_else(s &gt; 0, NA_integer_, h)) %&gt;% filter(!is.na(h_missing)) %&gt;% as.list() model_sim_dog_2 &lt;- ulam( flist = alist( h_missing ~ binomial( 10 , p ), logit(p) &lt;- alpha + beta_s * s, alpha ~ normal( 0 , 1.5 ), beta_s ~ normal( 0 , 0.5 ) ), data = data_sim_dog_list_2, chains = 4, cores = 4) precis( model_sim_dog_2 ) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha -0.26 0.13 -0.47 -0.05 695.29 1.01 beta_s 0.80 0.16 0.55 1.06 656.18 1.01 n &lt;- 100 data_sim_dog_3 &lt;- tibble(.idx = 1:n, s = rnorm(n), h = rbinom(n, size = 10, inv_logit(s)), d = if_else(h &lt; 5, 1, 0), h_missing = if_else(d == 1, NA_integer_, h), h_dummy = if_else(d == 1, -9L, h)) data_sim_dog_list_3 &lt;- data_sim_dog_3 %&gt;% dplyr::select(-.idx) %&gt;% filter(!is.na(h_missing)) %&gt;% as.list() model_sim_dog_3 &lt;- ulam( flist = alist( h_missing ~ binomial( 10 , p ), logit(p) &lt;- alpha + beta_s * s, alpha ~ normal( 0 , 1.5 ), beta_s ~ normal( 0 , 0.5 ) ), data = data_sim_dog_list_3, chains = 4, cores = 4) precis( model_sim_dog_3 ) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha 0.42 0.11 0.24 0.59 929.64 1 beta_s 0.57 0.12 0.37 0.76 902.92 1 data_sim_dog_list_4 &lt;- data_sim_dog_3 %&gt;% dplyr::select(-.idx, - h_missing) %&gt;% as.list() stan_sim_dog_4 &lt;- &quot; data{ int h_dummy[100]; vector[100] s; } parameters{ real alpha; real beta_s; } model{ vector[100] p; beta_s ~ normal( 0 , 0.5 ); alpha ~ normal( 0 , 1.5 ); for ( i in 1:100 ) { p[i] = alpha + beta_s * s[i]; p[i] = inv_logit(p[i]); } for ( i in 1:100 ) { if ( h_dummy[i] &gt; -1 ) h_dummy[i] ~ binomial( 10 , p[i] ); if ( h_dummy[i] &lt; 0 ) { vector[5] pv; for ( j in 0:4 ) pv[j+1] = binomial_lpmf( j | 10 , p[i] ); target += log_sum_exp( pv ); } } } &quot; model_sim_dog_4 &lt;- stan( model_code = stan_sim_dog_4, data = data_sim_dog_list_4, chains = 4, cores = 4) precis( model_sim_dog_4 ) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha -0.11 0.08 -0.24 0.02 1100.75 1 beta_s 0.84 0.10 0.69 1.00 1038.57 1 gq_code &lt;- &quot; generated quantities{ int H_impute[100]; for ( i in 1:100 ) { real p = inv_logit(alpha + beta_s * s[i]); if ( h_dummy[i] &gt; -1 ) H_impute[i] = h_dummy[i]; if ( h_dummy[i] &lt; 0 ) { // compute Pr( h_dummy==j | p , h_dummy &lt; 5 ) vector[5] lbp; real Z; for ( j in 0:4 ) lbp[j+1] = binomial_lpmf( j | 10 , p ); // convert to probabilities by normalizing Z = log_sum_exp( lbp ); for ( j in 1:5 ) lbp[j] = exp( lbp[j] - Z ); // generate random sample from posterior H_impute[i] = categorical_rng( lbp ) - 1; } } } &quot; code_new &lt;- concat( stan_sim_dog_4 , gq_code ) model_sim_dog_5 &lt;- stan( model_code = code_new , data = data_sim_dog_list_4, chains = 4, cores = 4 ) sim_dog_5_posterior &lt;- extract.samples(model_sim_dog_5) %&gt;% as_tibble() sim_dog_5_posterior$H_impute %&gt;% as_tibble() %&gt;% t() %&gt;% bind_cols(data_sim_dog_3,. ) %&gt;% filter(is.na(h_missing)) %&gt;% filter(row_number() &lt; 25) %&gt;% pivot_longer(starts_with(&#39;..&#39;)) %&gt;% ggplot(aes(x = value)) + geom_bar(aes(y = ..count.., color = (value == h), fill = after_scale(clr_alpha(color))), width = .7) + facet_wrap(.idx ~ ., nrow = 4) + scale_color_manual(values = c(`TRUE` = clr_current, `FALSE` = clr0d), guide = FALSE) H1 data(elephants) data_elephant &lt;- elephants %&gt;% as_tibble() %&gt;% rename_with(everything(), .fn = str_to_lower) %&gt;% mutate(age_cent = age - 20) data_elephant %&gt;% ggplot(aes(x = age, y = matings)) + geom_point(size = 2.5, color = fll_current(), shape = 21, fill = clr_alpha(clr_lighten(clr_current))) Thinking about the relationship between age and matings: Entering the as is introduces exponential relationship (caused by the log link): \\[ \\textrm{log}~\\lambda_{i} = \\alpha + \\beta A_{i} \\Rightarrow \\lambda_{i} = \\textrm{exp}(\\alpha + \\beta A_{i}) = \\textrm{exp}(\\alpha) ~ \\textrm{exp}(\\beta A_{i}) \\] Alternatively, assuming a logarithmic relationship with increasing / diminishing returns with age (depending if \\(\\beta \\gt 1\\) or \\(\\beta \\lt 1\\)). \\[ \\textrm{log}~\\lambda_{i} = \\alpha + \\textrm{log} \\beta A_{i} \\Rightarrow \\lambda_{i} = \\textrm{exp}(\\alpha)~A_{i}^{\\beta} \\] model_elephant_logistic &lt;- ulam( flist = alist( matings ~ dpois(lambda), lambda &lt;- exp(alpha) * age_cent ^ beta_age, alpha ~ dnorm(0, 1), beta_age ~ dnorm(0, 1) ), data = data_elephant, cores = 4, chains = 4 ) precis(model_elephant_logistic) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha -1.5 0.52 -2.30 -0.67 306.68 1 beta_age 0.9 0.18 0.61 1.18 306.55 1 new_elephant &lt;- tibble(age_cent = seq(25, 55, length.out = 31)-20) elephant_posterior_predict &lt;- link(model_elephant_logistic, data = new_elephant) %&gt;% as_tibble() %&gt;% t() %&gt;% bind_cols(new_elephant,. ) %&gt;% group_by(age_cent) %&gt;% pivot_longer(-age_cent) %&gt;% summarise(pi = list(tibble(value = quantile(value, prob = c(.055,.5,.955)), label = c(&quot;ll&quot;,&quot;m&quot;,&quot;hh&quot;)))) %&gt;% unnest(pi) %&gt;% pivot_wider(values_from = value, names_from = label) p1 &lt;- elephant_posterior_predict %&gt;% ggplot(aes(x = age)) + geom_smooth(stat = &#39;identity&#39;, aes(x = age_cent + 20, ymin = ll, y = m, ymax = hh), color = clr0dd, fill = fll0, size = .4) + geom_point(data = data_elephant, aes(y = matings), size = 2.5, color = fll_current(), shape = 21, fill = clr_alpha(clr_lighten(clr_current))) Including an error-assumption into the model model_elephant_error &lt;- ulam( flist = alist( matings ~ dpois(lambda), lambda &lt;- exp(alpha) * age_est[i] ^ beta_age, age_cent ~ dnorm(age_est, 5), vector[41]:age_est ~ dunif(0, 50), alpha ~ dnorm(0, 1), beta_age ~ dnorm(0, 1) ), data = data_elephant, cores = 4, chains = 4 ) precis(model_elephant_error) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha -1.31 0.58 -2.25 -0.39 975.78 1 beta_age 0.84 0.20 0.52 1.16 992.57 1 elephant_posterior &lt;- extract.samples(model_elephant_error) %&gt;% as_tibble() elephant_age_estimates &lt;- elephant_posterior$age_est%&gt;% t() %&gt;% as_tibble() %&gt;% mutate(elephant_idx = row_number()) %&gt;% pivot_longer(-elephant_idx) %&gt;% group_by(elephant_idx) %&gt;% summarise(pi = list(tibble(value = quantile(value, prob = c(.055, .5, .955)), label = c(&quot;ll&quot;, &quot;m&quot;, &quot;hh&quot;)))) %&gt;% unnest(pi) %&gt;% pivot_wider(values_from = value, names_from = label) %&gt;% bind_cols(data_elephant,. ) %&gt;% mutate(jitter = runif(n = n(),-.15, .15)) elephant_error_posterior_predict &lt;- link(model_elephant_error, data = new_elephant) %&gt;% as_tibble() %&gt;% t() %&gt;% bind_cols(new_elephant,. ) %&gt;% group_by(age_cent) %&gt;% pivot_longer(-age_cent) %&gt;% summarise(pi = list(tibble(value = quantile(value, prob = c(.055,.5,.955)), label = c(&quot;ll&quot;,&quot;m&quot;,&quot;hh&quot;)))) %&gt;% unnest(pi) %&gt;% pivot_wider(values_from = value, names_from = label) p2 &lt;- elephant_error_posterior_predict %&gt;% ggplot(aes(x = age)) + geom_smooth(stat = &#39;identity&#39;, aes(x = age_cent + 20, ymin = ll, y = m, ymax = hh), color = clr0dd, fill = fll0, size = .4) + geom_smooth(data = elephant_posterior_predict, stat = &#39;identity&#39;, aes(x = age_cent + 20, ymin = ll, y = m, ymax = hh), color = clr_lighten(clr_current,.7), fill = clr_alpha(clr_lighten(clr_current,.7), .2), size = .4) + geom_segment(data = elephant_age_estimates, aes(y = matings + jitter, yend = matings + jitter, xend = m + 20), arrow = arrow(type = &quot;closed&quot;, length = unit(4, &quot;pt&quot;)), color = clr0d) + geom_point(data = elephant_age_estimates, aes(y = matings + jitter), size = 2.5, color = fll_current(), shape = 21, fill = clr_alpha(clr_lighten(clr_current))) + geom_point(data = elephant_age_estimates, aes(x = m + 20, y = matings + jitter)) p1 + p2 H2 model_elephant_error_double &lt;- ulam( flist = alist( matings ~ dpois(lambda), lambda &lt;- exp(alpha) * age_est[i] ^ beta_age, age_cent ~ dnorm(age_est, 10), vector[41]:age_est ~ dunif(0, 50), alpha ~ dnorm(0, 1), beta_age ~ dnorm(0, 1) ), data = data_elephant, cores = 4, chains = 4 ) model_elephant_error_quad &lt;- ulam( flist = alist( matings ~ dpois(lambda), lambda &lt;- exp(alpha) * age_est[i] ^ beta_age, age_cent ~ dnorm(age_est, 20), vector[41]:age_est ~ dunif(0, 50), alpha ~ dnorm(0, 1), beta_age ~ dnorm(0, 1) ), data = data_elephant, cores = 4, chains = 4 ) model_elephant_error_100 &lt;- ulam( flist = alist( matings ~ dpois(lambda), lambda &lt;- exp(alpha) * age_est[i] ^ beta_age, age_cent ~ dnorm(age_est, 40), vector[41]:age_est ~ dunif(0, 100), alpha ~ dnorm(0, 1), beta_age ~ dnorm(0, 1) ), data = data_elephant, cores = 4, chains = 4 ) precis(model_elephant_error_double) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha -1.10 0.56 -2.02 -0.24 657.24 1.01 beta_age 0.75 0.19 0.46 1.03 662.25 1.01 precis(model_elephant_error_quad) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha -0.97 0.56 -1.87 -0.08 677.29 1.01 beta_age 0.66 0.18 0.38 0.94 662.11 1.01 precis(model_elephant_error_100) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha -0.95 0.61 -1.97 0.01 767.41 1 beta_age 0.56 0.17 0.31 0.83 754.71 1 H3 set.seed(100) data_impute &lt;- tibble(x = c(rnorm(10), NA), y = c(rnorm(10, x), 100)) precis( lm(y ~ x, data_impute) ) %&gt;% knit_precis() param mean sd 5.5% 94.5% (Intercept) 0.24 0.28 -0.20 0.68 x 1.42 0.52 0.59 2.26 \\[ \\begin{array}{rcl} y_{i} &amp; \\sim &amp; \\textrm{Normal}(\\mu_{i}, \\sigma)\\\\ \\mu_{i} &amp; = &amp; \\alpha + \\beta x_{i}\\\\ x_{i} &amp; \\sim &amp; \\textrm{Normal}(0, 1)\\\\ \\alpha &amp; \\sim &amp; \\textrm{Normal}(0, 1)\\\\ \\beta &amp; \\sim &amp; \\textrm{Normal}(0, 100)\\\\ \\sigma &amp; \\sim &amp; \\textrm{Exponential}(1)\\\\ \\end{array} \\] model_impute &lt;- ulam( flist = alist( y ~ dnorm(mu, sigma), mu &lt;- alpha + beta * x, x ~ dnorm(0, 1), c( alpha, beta ) ~ dnorm(0, 100), sigma ~ dexp(1) ), data = data_impute, chains = 4, cores = 4, iter = 4000, control = list(adapt_delta = .99) ) precis(model_impute, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 beta 21.74 3.60 16.68 28.12 2820.88 1 alpha 2.20 2.97 -2.40 6.96 3706.15 1 sigma 9.44 1.90 6.81 12.85 2962.53 1 x_impute[1] 3.85 0.70 2.74 5.00 2561.12 1 extract.samples(model_impute) %&gt;% as_tibble() %&gt;% ggpairs( lower = list(continuous = wrap(my_lower, col = clr_dark)), diag = list(continuous = wrap(my_diag, fill = fll0, col = clr_dark, adjust = .7)), upper = list(continuous = wrap(my_upper , size = 3, col = &quot;black&quot;, family = fnt_sel)) ) + theme(panel.border = element_rect(color = clr_dark, fill = &quot;transparent&quot;)) new_impute &lt;- seq(-4, 4, length.out = 31) impute_summary_prep &lt;- extract.samples(model_impute) %&gt;% as_tibble() %&gt;% mutate(case = c(&#39;positive&#39;, &#39;negative&#39;)[2 - (beta &gt; 0)]) impute_summary &lt;-impute_summary_prep %&gt;% mutate(mu = map2(alpha, beta, function(a, b){ tibble(x = new_impute, y = a + b * new_impute)})) %&gt;% unnest(mu) %&gt;% group_by(case, x) %&gt;% summarise(pi = list(tibble(value = quantile(y, prob = c(.055,.5,.955)), label = c(&quot;ll&quot;,&quot;m&quot;,&quot;hh&quot;)))) %&gt;% unnest(pi) %&gt;% pivot_wider(values_from = value, names_from = label) impute_summary %&gt;% ggplot() + geom_smooth(stat = &#39;identity&#39;, aes(x = x, ymin = ll, y = m ,ymax = hh), size = .3, color = clr0dd, fill = fll0)+ geom_point(data = impute_summary_prep %&gt;% group_by(case) %&gt;% summarise( x_impute = mean(x_impute)) %&gt;% mutate(y = data_impute$y[is.na(data_impute$x)]), aes(x = x_impute, y = y), shape = 21, color = clr_current, fill = fll_current(), size = 2) + geom_point(data = data_impute,aes(x = x, y = y ), shape = 21, color = clr0d, fill = fll0, size = 1.5) + facet_wrap(case ~ .) + coord_cartesian(ylim = c(0, 100)) H4 data(Primates301) data_primates &lt;- Primates301 %&gt;% as_tibble() %&gt;% dplyr::select(name, brain, body) %&gt;% filter(complete.cases(brain), complete.cases(body)) %&gt;% mutate(across(.cols = brain:body, .fns = function(x){x / max(x)}, .names = &quot;{.col}_scl&quot;), across(.cols = brain_scl:body_scl, .fns = function(x){x * .1}, .names = &quot;{.col}_se&quot;)) Modeling with the relationship \\[ \\begin{array}{rcl} B_{i} &amp; \\sim &amp; \\textrm{Log-Normal}(\\mu_{i}, \\sigma)\\\\ \\mu_{i} &amp; = \\alpha + \\beta~\\textrm{log} M_{i} \\end{array} \\] which implies \\[ E(B_{i}|M_{i}) = \\textrm{exp}(\\alpha) M_{i}^{\\beta} \\] model_primates_error_prep &lt;- ulam( flist = alist( brain_scl ~ dlnorm(mu, sigma), mu &lt;- alpha + beta * log(body_scl), alpha ~ normal( 0, 1 ), beta ~ normal( 0, 1 ), sigma ~ exponential(1) ), data = data_primates, chains = 4, cores = 4 ) precis(model_primates_error_prep) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha 0.43 0.06 0.33 0.53 534.36 1 beta 0.78 0.01 0.76 0.81 535.80 1 sigma 0.29 0.02 0.27 0.32 1079.64 1 data_primates_list &lt;- data_primates %&gt;% as.list() %&gt;% c(., n_b = length(data_primates$name)) model_primates_error &lt;- ulam( flist = alist( # brain model brain_scl ~ normal( brain_true, brain_scl_se ), vector[n_b]:brain_true ~ dlnorm( mu, sigma ), mu &lt;- alpha + beta * log( body_true[i] ), # body model body_scl ~ normal( body_true, body_scl_se ), vector[n_b]:body_true ~ normal( 0.5, 1 ), # priors alpha ~ normal( 0, 1 ), beta ~ normal( 0, 1 ), sigma ~ exponential(1) ), data = data_primates_list, start=list( body_true = data_primates_list$body_scl, brain_true = data_primates_list$brain_scl ), cores = 4, chains = 4, control = list(max_treedepth = 15) ) precis(model_primates_error) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha 0.41 0.06 0.32 0.51 2564.35 1 beta 0.78 0.01 0.76 0.81 2591.35 1 sigma 0.26 0.02 0.23 0.29 3159.17 1 primates_posterior &lt;- extract.samples(model_primates_error) %&gt;% as_tibble() p1 &lt;- ggplot() + (primates_posterior[1:50, ] %&gt;% dplyr::select(alpha, beta) %&gt;% pmap(.f = function(alpha, beta, ...){ stat_function(fun = function(x){ exp(alpha) * x ^ beta }, xlim = c(0, 1), geom = &#39;line&#39;, color = fll0dd, size = .2) })) + geom_point(data = data_primates, aes(x = body_scl, y = brain_scl), color = clr_current, fill = fll_current(), shape = 21) + coord_cartesian(ylim = c(0, 1)) primate_estimates &lt;- primates_posterior$brain_true %&gt;% t() %&gt;% as_tibble() %&gt;% mutate(.idx = row_number()) %&gt;% pivot_longer(-.idx, values_to = &#39;brain_scl&#39;) %&gt;% left_join(primates_posterior$body_true %&gt;% t() %&gt;% as_tibble() %&gt;% mutate(.idx = row_number()) %&gt;% pivot_longer(-.idx, values_to = &#39;body_scl&#39;) ) %&gt;% group_by(.idx) %&gt;% summarise(pi = list(tibble(brain_val = quantile(brain_scl, prob = c(.05, .5, .955)), body_val = quantile(body_scl, prob = c(.05, .5, .955)), label = c(&quot;ll&quot;,&quot;m&quot;,&quot;hh&quot;)))) %&gt;% unnest(pi) %&gt;% pivot_wider(names_from = label, values_from = c(brain_val, body_val)) p2 &lt;- tibble(body_scl = seq(0, 1, length.out = 101)) %&gt;% mutate(brain_scl = map_dbl(body_scl, function(x){ mean( exp(primates_posterior$alpha) * x ^ primates_posterior$beta ) })) %&gt;% ggplot(aes(x = body_scl, y = brain_scl)) + geom_line(color = clr0dd) + geom_point(data = primate_estimates, aes(x = body_val_m, y = brain_val_m), color = clr_current, fill = fll_current(), shape = 21) + geom_point(data = data_primates, color = clr0dd, fill = fll0, shape = 21, size = 1.75) + coord_cartesian(ylim = c(0, 1.05)) p3 &lt;- primates_posterior$brain_true %&gt;% t() %&gt;% as_tibble() %&gt;% mutate(.idx = row_number()) %&gt;% pivot_longer(-.idx, values_to = &#39;brain_scl&#39;) %&gt;% left_join(primates_posterior$body_true %&gt;% t() %&gt;% as_tibble() %&gt;% mutate(.idx = row_number()) %&gt;% pivot_longer(-.idx, values_to = &#39;body_scl&#39;) ) %&gt;% ggplot(aes(x = body_scl, y = brain_scl)) + stat_ellipse(aes(group = .idx), level = .5, geom = &#39;polygon&#39;, color = clr0d, fill = fll0) + geom_point(data = primate_estimates, aes(x = body_val_m, y = brain_val_m), color = clr_current, fill = fll_current(), shape = 21) p1 + p2 + p3 H5 Primates301 %&gt;% as_tibble() %&gt;% summarise(across(everything(), function(x){sum(is.na(x))})) %&gt;% t() %&gt;% as.data.frame() %&gt;% rownames_to_column(var = &quot;column&quot;) %&gt;% rename(n_na = V1) %&gt;% as_tibble() #&gt; # A tibble: 16 × 2 #&gt; column n_na #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 name 0 #&gt; 2 genus 0 #&gt; 3 species 0 #&gt; 4 subspecies 267 #&gt; 5 spp_id 0 #&gt; 6 genus_id 0 #&gt; 7 social_learning 98 #&gt; 8 research_effort 115 #&gt; 9 brain 117 #&gt; 10 body 63 #&gt; 11 group_size 114 #&gt; 12 gestation 161 #&gt; 13 weaning 185 #&gt; 14 longevity 181 #&gt; 15 sex_maturity 194 #&gt; 16 maternal_investment 197 data_primates_2 &lt;- Primates301 %&gt;% as_tibble() %&gt;% dplyr::select(name, brain, body) %&gt;% filter(complete.cases(body)) %&gt;% mutate(across(.cols = brain:body, .fns = function(x){x / max(x, na.rm = TRUE)}, .names = &quot;{.col}_scl&quot;)) data_primates_2 %&gt;% ggplot(aes(x = body_scl, y = is.na(brain_scl))) + geom_point(color = clr0dd, fill = fll0, shape = 21, size = 2) + coord_cartesian(ylim = c(.9,2.1), xlim = c(-.1, 1.1), expand = 0) dagify( Bs ~ B + Rb, Rb ~ M, B ~ M, exposure = &quot;M&quot;, outcome = &quot;Bs&quot;, coords = tibble(name = c(&quot;M&quot;, &quot;B&quot;, &quot;Rb&quot;, &quot;Bs&quot;), x = c( 0, 1, 0, 1 ), y = c(1, 1, 0, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;Bs&quot;, &quot;response&quot;, if_else(name %in% c(&quot;M&quot;, &quot;Rb&quot;, &quot;Rg&quot;), &quot;predictor&quot;, &quot;confounds&quot;)), name = str_replace(name, &quot;Bs&quot;,&quot;B^&#39;*&#39;&quot;) %&gt;% str_replace( &quot;Rb&quot;,&quot;R[B]&quot;)) %&gt;% plot_dag(clr_in = clr_current) + coord_fixed(ratio = .75, xlim = c(-.1, 1.1), ylim = c(-.1, 1.1)) + theme(plot.subtitle = element_text(hjust = .5, family = fnt_sel)) data_primates_2_list &lt;- data_primates_2 %&gt;% dplyr::select(brain_scl, body_scl) %&gt;% as.list() model_primates_missing_default &lt;- ulam( flist = alist( brain_scl ~ dlnorm( mu, sigma ), mu &lt;- alpha + beta * log( body_scl ), alpha ~ normal( 0, 1 ), beta ~ normal( 0, 1 ), sigma ~ exponential(1) ), data = data_primates_2_list, cores = 4, chains = 4, start = list( brain_scl_impute = rep( .5, 56 )) ) model_primates_missing_explicit &lt;- ulam( flist = alist( brain_scl_merge ~ dlnorm( mu, sigma ), mu &lt;- alpha + beta * log( body_scl ), brain_scl_merge &lt;- merge_missing( brain_scl, brain_scl_impute ), alpha ~ normal( 0, 1 ), beta ~ normal( 0, 1 ), sigma ~ exponential(1) ), data = data_primates_2_list, cores = 4, chains = 4, start = list( brain_scl_impute = rep( .5, 56 )) ) precis(model_primates_missing_default) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha 0.43 0.06 0.34 0.52 825.24 1 beta 0.78 0.01 0.76 0.81 972.83 1 sigma 0.29 0.02 0.27 0.32 1495.86 1 precis(model_primates_missing_explicit) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha 0.43 0.06 0.33 0.52 1148.76 1 beta 0.78 0.01 0.76 0.81 1184.11 1 sigma 0.29 0.02 0.27 0.32 1843.00 1 data_primates_3_list &lt;- data_primates %&gt;% dplyr::select(name, brain_scl, body_scl ) %&gt;% as.list() model_primates_missing_complete_cases &lt;- ulam( flist = alist( brain_scl ~ dlnorm( mu, sigma ), mu &lt;- alpha + beta * log( body_scl ), alpha ~ normal( 0, 1 ), beta ~ normal( 0, 1 ), sigma ~ exponential(1) ), data = data_primates_3_list, cores = 4, chains = 4, start = list( brain_scl_impute = rep( .5, 56 )) ) precis(model_primates_missing_complete_cases) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha 0.43 0.06 0.34 0.52 776.91 1 beta 0.78 0.01 0.76 0.81 751.27 1 sigma 0.29 0.02 0.27 0.32 984.91 1 primates_impute_posterior &lt;- extract.samples(model_primates_missing_default) %&gt;% as_tibble() primates_impute_posterior$brain_scl_impute %&gt;% t() %&gt;% as_tibble() %&gt;% mutate(impute_idx = row_number()) %&gt;% pivot_longer(-impute_idx) %&gt;% group_by(impute_idx) %&gt;% summarise(pi = list(tibble(value = quantile(value, prob = c(.055, .25, .5, .75, .955)), label = c(&quot;ll&quot;, &quot;l&quot;, &quot;m&quot;, &quot;h&quot;, &quot;hh&quot;)))) %&gt;% unnest(pi) %&gt;% pivot_wider(values_from = value, names_from = label) %&gt;% bind_cols(data_primates_2 %&gt;% filter(is.na(brain_scl)) %&gt;% dplyr::select(name, body_scl),.) %&gt;% ggplot(aes(x = body_scl)) + geom_point(data = data_primates_2, aes(y = brain_scl), color = clr0dd, fill = fll0, shape = 21) + geom_linerange(aes(ymin = l, ymax = h), color = clr_current) + geom_point(aes(y = m), color = clr_current, fill = clr_lighten(clr_current), shape = 21) H6 data_waffle model_waffle_init &lt;- ulam( flist = alist( divorce_obs ~ dnorm( divorce_true, divorce_sd ), vector[N]: divorce_true ~ dnorm( mu, sigma ), mu &lt;- alpha + beta_age * median_age_std + beta_marriage * marriage_true[i], marriage_std ~ dnorm( marriage_true, marriage_sd ), vector[N]:marriage_true ~ dnorm(mu_marriage, sigma_marriage), mu_marriage &lt;- alpha_marriage + beta_marriage_age * median_age_std, c( alpha, alpha_marriage ) ~ dnorm( 0 ,.2 ), c( beta_age, beta_marriage, beta_marriage_age) ~ dnorm( 0, .5 ), sigma ~ dexp(1), sigma_marriage ~ dexp(1) ), data = data_waffle_list, cores = 4, chains = 4 ) precis(model_waffle_init) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_marriage -0.11 0.08 -0.23 0.01 2269.42 1.00 alpha -0.03 0.10 -0.19 0.13 1812.57 1.00 beta_marriage_age -0.67 0.09 -0.81 -0.53 1910.76 1.00 beta_marriage 0.30 0.25 -0.10 0.70 975.00 1.00 beta_age -0.46 0.19 -0.76 -0.17 1335.99 1.00 sigma 0.56 0.11 0.40 0.74 678.12 1.00 sigma_marriage 0.44 0.07 0.34 0.56 1015.78 1.01 precis(model_divorce_error_out) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha -0.04 0.10 -0.20 0.12 1038.56 1 beta_age -0.54 0.16 -0.78 -0.28 983.30 1 beta_marriage 0.20 0.21 -0.13 0.53 766.98 1 sigma 0.57 0.11 0.41 0.75 570.34 1 get_inferred_true_val &lt;- function(model){ mod &lt;- rlang::enexpr(model) extract.samples(model) %&gt;% as_tibble() %&gt;% pluck(&quot;marriage_true&quot;) %&gt;% as_tibble() %&gt;% t() %&gt;% as_tibble() %&gt;% mutate(.idx = row_number()) %&gt;% pivot_longer(-.idx) %&gt;% mutate(model = as.character(mod)) } ls_states &lt;- c(&quot;DC&quot;, &quot;DE&quot;, &quot;HI&quot;) p1 &lt;- get_inferred_true_val(model_divorce_error_out) %&gt;% bind_rows(get_inferred_true_val(model_waffle_init) ) %&gt;% group_by(.idx, model) %&gt;% summarise(mean = mean(value)) %&gt;% pivot_wider(values_from = mean, names_from = model) %&gt;% bind_cols(data_waffle %&gt;% dplyr::select(Loc),. ) %&gt;% ggplot() + geom_abline(slope = 1, intercept = 0, linetype = 3, color = clr_dark) + geom_point(aes(x = model_divorce_error_out, y = model_waffle_init), color = clr0dd, fill = fll0, shape = 21) + ggrepel::geom_text_repel( data = . %&gt;% filter(Loc %in% ls_states), aes(x = model_divorce_error_out, y = model_waffle_init, label = Loc)) p2 &lt;- data_waffle_list %&gt;% bind_cols(data_waffle %&gt;% dplyr::select(Loc),. ) %&gt;% as_tibble() %&gt;% ggplot(aes(x = median_age_std, y = marriage_std)) + geom_point(aes(color = Loc %in% ls_states, fill = after_scale(clr_alpha(clr_lighten(color), .3))), shape = 21) + scale_color_manual(values = c(`TRUE` = clr_current, `FALSE` = clr0dd), guide = FALSE) p1 + p2 H7 data_andrew &lt;- tibble(freq = c( 18 , 19 , 22 , NA , NA , 19 , 20 , 22 ), value = seq_along(freq)) %&gt;% dplyr::select(value, freq) sum(data_andrew$freq, na.rm = TRUE) #&gt; [1] 120 Parameterizing the number of spins above 120,using a Poisson distribution with a mean of 40: \\[ S \\sim \\textrm{Poisson}(40) + 120 \\] p &lt;- rdirichlet(1e3, alpha = rep(4, 8)) p1 &lt;- p %&gt;% as_tibble() %&gt;% mutate(.idx = row_number()) %&gt;% pivot_longer(-.idx, names_prefix = &quot;V&quot;, names_transform = as.integer, names_to = &quot;value&quot;, values_to = &quot;probability&quot;) %&gt;% filter(.idx &lt; 9) %&gt;% ggplot(aes(x = value, y = probability)) + geom_line(aes(group = .idx), color = clr0dd, size = .4, linetype = 3) + geom_point(color = clr0dd, fill = clr0, shape = 21) twicer &lt;- function( p ) { o &lt;- order( p ) if ( p[o][8]/p[o][1] &gt; 2 ) return( TRUE ) else return( FALSE ) } sum( apply( p , 1 , twicer ) ) #&gt; [1] 976 p &lt;- rdirichlet( 1e3 , alpha = rep(50, 8) ) sum( apply( p , 1 , twicer ) ) #&gt; [1] 15 p2 &lt;- p %&gt;% as_tibble() %&gt;% mutate(.idx = row_number()) %&gt;% pivot_longer(-.idx, names_prefix = &quot;V&quot;, names_transform = as.integer, names_to = &quot;value&quot;, values_to = &quot;probability&quot;) %&gt;% filter(.idx &lt; 9) %&gt;% ggplot(aes(x = value, y = probability)) + geom_line(aes(group = .idx), color = clr0dd, size = .4, linetype = 3) + geom_point(color = clr0dd, fill = clr0, shape = 21) p1 + p2 &amp; coord_cartesian(ylim = c(0, .35)) code_andrew &lt;- &#39; data{ int N; int y[N]; int y_max; // consider at most this many spins for y4 and y5 int S_mean; } parameters{ simplex[N] p; // probabilities of each outcome } model{ vector[(1+y_max)*(1+y_max)] terms; int k = 1; p ~ dirichlet( rep_vector(50,N) ); // loop over possible values for unknown cells 4 and 5 // this code updates posterior of p for ( y4 in 0:y_max ) { for ( y5 in 0:y_max ) { int Y[N] = y; Y[4] = y4; Y[5] = y5; terms[k] = poisson_lpmf(y4+y5|S_mean-120) + multinomial_lpmf(Y|p); k = k + 1; }//y5 }//y4 target += log_sum_exp(terms); } generated quantities{ matrix[y_max+1,y_max+1] P45; // prob y4,y5 takes joint values // now compute Prob(y4,y5|p) { matrix[(1+y_max),(1+y_max)] terms; int k = 1; real Z; for ( y4 in 0:y_max ) { for ( y5 in 0:y_max ) { int Y[N] = y; Y[4] = y4; Y[5] = y5; terms[y4+1,y5+1] = poisson_lpmf(y4+y5|S_mean-120) + multinomial_lpmf(Y|p); }//y5 }//y4 Z = log_sum_exp( to_vector(terms) ); for ( y4 in 0:y_max ) for ( y5 in 0:y_max ) P45[y4+1,y5+1] = exp( terms[y4+1,y5+1] - Z ); } } &#39; y &lt;- c(18, 19, 22, -1, -1, 19, 20, 22) data_andrew_list &lt;- list( N = length(y), y = y, S_mean = 160, y_max = 40 ) model_andrew &lt;- stan( model_code = code_andrew, data = data_andrew_list, chains = 4, cores = 4 ) andrew_posterior &lt;- extract.samples(model_andrew) %&gt;% as_tibble() y_max &lt;- data_andrew_list$y_max extract_k &lt;- function(y4, y5, posterior){ k &lt;- mean( posterior$P45[,y4+1,y5+1] )/0.01 tibble( x = y4 , y = y5 , k = k ) } andrew_k &lt;- crossing(y4 = 0:y_max, y5 = 0:y_max) %&gt;% pmap_df(extract_k, posterior = andrew_posterior) p1 &lt;- andrew_k %&gt;% filter(between(x,10, 30), between(y,10, 30)) %&gt;% ggplot(aes(x , y, color = k)) + geom_point(shape = 21, size = 3, aes(fill = after_scale(clr_alpha(color,.8))))+ labs(subtitle = &quot; dirichlet( rep_vector(50, N) )&quot;) code_andrew2 &lt;- &#39; data{ int N; int y[N]; int y_max; // consider at most this many spins for y4 and y5 int S_mean; } parameters{ simplex[N] p; // probabilities of each outcome } model{ vector[(1+y_max)*(1+y_max)] terms; int k = 1; p ~ dirichlet( rep_vector(2,N) ); // loop over possible values for unknown cells 4 and 5 // this code updates posterior of p for ( y4 in 0:y_max ) { for ( y5 in 0:y_max ) { int Y[N] = y; Y[4] = y4; Y[5] = y5; terms[k] = poisson_lpmf(y4+y5|S_mean-120) + multinomial_lpmf(Y|p); k = k + 1; }//y5 }//y4 target += log_sum_exp(terms); } generated quantities{ matrix[y_max+1,y_max+1] P45; // prob y4,y5 takes joint values // now compute Prob(y4,y5|p) { matrix[(1+y_max),(1+y_max)] terms; int k = 1; real Z; for ( y4 in 0:y_max ) { for ( y5 in 0:y_max ) { int Y[N] = y; Y[4] = y4; Y[5] = y5; terms[y4+1,y5+1] = poisson_lpmf(y4+y5|S_mean-120) + multinomial_lpmf(Y|p); }//y5 }//y4 Z = log_sum_exp( to_vector(terms) ); for ( y4 in 0:y_max ) for ( y5 in 0:y_max ) P45[y4+1,y5+1] = exp( terms[y4+1,y5+1] - Z ); } } &#39; model_andrew2 &lt;- stan( model_code = code_andrew2, data = data_andrew_list, chains = 4, cores = 4 ) andrew_posterior2 &lt;- extract.samples(model_andrew2) %&gt;% as_tibble() andrew_k2 &lt;- crossing(y4 = 0:y_max, y5 = 0:y_max) %&gt;% pmap_df(extract_k, posterior = andrew_posterior2) p2 &lt;- andrew_k2 %&gt;% filter(between(x,0, 40), between(y,0, 40)) %&gt;% ggplot(aes(x , y, color = k)) + geom_point(shape = 21, size = 1, aes(fill = after_scale(clr_alpha(color,.8)))) + labs(subtitle = &quot; dirichlet( rep_vector(2, N) )&quot;) p1 + p2 &amp; scale_color_gradientn(colours = c(clr0, clr_current), guide = &quot;none&quot;) &amp; labs(x = &quot;number of 4s&quot;, y = &quot;number of 5s&quot;) &amp; coord_equal() 16.5 {brms} section 16.5.1 Measurement Error 16.5.1.1 Error on the Outcome With brms, we accommodate measurement error in the criterion using the mi() syntax, following the general form &lt;response&gt; | mi(&lt;se_response&gt;). This follows a missing data logic, resulting in Bayesian missing data imputation for the criterion values. The mi() syntax is based on the missing data capabilities for brms, which we will cover in greater detail in the second half of this chapter. data_waffle_brms &lt;- data_waffle %&gt;% dplyr::select(Loc, divorce_obs = divorce_std, divorce_sd, marriage_std, median_age_std, marriage_sd) brms_c15_model_divorce &lt;- brm( data = data_waffle_brms, family = gaussian, divorce_obs | mi(divorce_sd) ~ 1 + median_age_std + marriage_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 42, # note this line save_pars = save_pars(latent = TRUE), file = &quot;brms/brms_c15_model_divorce&quot;) print(brms_c15_model_divorce) #&gt; Family: gaussian #&gt; Links: mu = identity; sigma = identity #&gt; Formula: divorce_obs | mi(divorce_sd) ~ 1 + median_age_std + marriage_std #&gt; Data: data_waffle_brms (Number of observations: 50) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; Intercept -0.06 0.09 -0.24 0.13 1.00 4477 3324 #&gt; median_age_std -0.61 0.16 -0.92 -0.29 1.00 3686 3307 #&gt; marriage_std 0.06 0.16 -0.26 0.39 1.00 3389 3273 #&gt; #&gt; Family Specific Parameters: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; sigma 0.58 0.11 0.39 0.81 1.00 1278 1962 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). posterior_summary(brms_c15_model_divorce) %&gt;% round(digits = 2) %&gt;% as.data.frame() %&gt;% rownames_to_column(var = &quot;param&quot;) %&gt;% as_tibble() #&gt; # A tibble: 55 × 5 #&gt; param Estimate Est.Error Q2.5 Q97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 b_Intercept -0.06 0.09 -0.24 0.13 #&gt; 2 b_median_age_std -0.61 0.16 -0.92 -0.29 #&gt; 3 b_marriage_std 0.06 0.16 -0.26 0.39 #&gt; 4 sigma 0.58 0.11 0.39 0.81 #&gt; 5 Yl[1] 1.16 0.37 0.46 1.9 #&gt; 6 Yl[2] 0.69 0.56 -0.41 1.78 #&gt; 7 Yl[3] 0.42 0.35 -0.25 1.13 #&gt; 8 Yl[4] 1.41 0.46 0.52 2.32 #&gt; 9 Yl[5] -0.9 0.13 -1.16 -0.65 #&gt; 10 Yl[6] 0.66 0.41 -0.15 1.48 #&gt; # … with 45 more rows states &lt;- c(&quot;AL&quot;, &quot;AR&quot;, &quot;ME&quot;, &quot;NH&quot;, &quot;RI&quot;, &quot;DC&quot;, &quot;VT&quot;, &quot;AK&quot;, &quot;SD&quot;, &quot;UT&quot;, &quot;ID&quot;, &quot;ND&quot;, &quot;WY&quot;) divorce_est &lt;- posterior_summary(brms_c15_model_divorce) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;term&quot;) %&gt;% as_tibble() %&gt;% mutate(divorce_est = Estimate) %&gt;% dplyr::select(term, divorce_est) %&gt;% filter(str_detect(term, &quot;Yl&quot;)) %&gt;% bind_cols(data_waffle_brms) p1 &lt;- divorce_est %&gt;% ggplot(aes(x = divorce_sd, y = divorce_est - divorce_obs)) + geom_hline(yintercept = 0, linetype = 3, color = clr_dark) + geom_point(color = clr0dd, fill = clr0, shape = 21) + ggrepel::geom_text_repel(data = . %&gt;% filter(Loc %in% states), aes(label = Loc), size = 3, seed = 15, family = fnt_sel) library(tidybayes) states &lt;- c(&quot;AR&quot;, &quot;ME&quot;, &quot;RI&quot;, &quot;ID&quot;, &quot;WY&quot;, &quot;ND&quot;, &quot;MN&quot;) p2 &lt;- as_draws_df(brms_c15_model_divorce) %&gt;% as_tibble() %&gt;% expand(nesting(b_Intercept, b_median_age_std), median_age_std = seq(from = -3.5, to = 3.5, length.out = 50)) %&gt;% mutate(fitted = b_Intercept + b_median_age_std * median_age_std) %&gt;% ggplot(aes(x = median_age_std)) + stat_lineribbon(aes(y = fitted), .width = .89, size = .2, color = clr0dd, fill = fll0) + geom_segment(data = divorce_est, aes(xend = median_age_std, y = divorce_obs, yend = divorce_est), size = .2) + geom_point(data = divorce_est, aes(y = divorce_obs), color = clr0dd, shape = 19) + geom_point(data = divorce_est, aes(y = divorce_est), shape = 21, size = 2, color = clr0dd, fill = &quot;white&quot;, stroke = .4) + ggrepel::geom_text_repel(data = data_waffle_brms %&gt;% filter(Loc %in% states), aes(y = divorce_obs, label = Loc), size = 3, seed = 15, family = fnt_sel) + labs(x = &quot;median age marriage (std)&quot;, y = &quot;divorce rate (std)&quot;) + coord_cartesian(xlim = range(data_waffle_brms$median_age_std), ylim = range(data_waffle_brms$divorce_obs)) p1 + p2 16.5.1.2 Error on Both Outcome and Predictor brms_c15_model_divorce_error_out &lt;- brm( data = data_waffle_brms, family = gaussian, divorce_obs | mi(divorce_sd) ~ 1 + median_age_std + me(marriage_std, marriage_sd), prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(normal(0, 1), class = meanme), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 42, # note this line save_pars = save_pars(latent = TRUE), file = &quot;brms/brms_c15_model_divorce_error_out&quot;) posterior_summary(brms_c15_model_divorce_error_out) %&gt;% round(digits = 2) %&gt;% as.data.frame() %&gt;% rownames_to_column(var = &quot;param&quot;) %&gt;% as_tibble() #&gt; # A tibble: 107 × 5 #&gt; param Estimate Est.Error Q2.5 Q97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 b_Intercept -0.03 0.1 -0.22 0.16 #&gt; 2 b_median_age_std -0.52 0.16 -0.82 -0.21 #&gt; 3 bsp_memarriage_stdmarriage_sd 0.26 0.22 -0.17 0.69 #&gt; 4 sigma 0.55 0.11 0.35 0.79 #&gt; 5 Yl[1] 1.13 0.38 0.39 1.86 #&gt; 6 Yl[2] 0.72 0.54 -0.29 1.81 #&gt; 7 Yl[3] 0.43 0.33 -0.21 1.1 #&gt; 8 Yl[4] 1.45 0.47 0.54 2.4 #&gt; 9 Yl[5] -0.9 0.13 -1.15 -0.65 #&gt; 10 Yl[6] 0.71 0.4 -0.05 1.53 #&gt; # … with 97 more rows pull_estimate &lt;- function(model, term_q){ posterior_summary(model) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;term&quot;) %&gt;% filter(str_detect(term, term_q)) %&gt;% pull(Estimate) } full_join( # D data_waffle_brms %&gt;% dplyr::select(Loc, divorce_obs) %&gt;% mutate(marriage_est = pull_estimate(brms_c15_model_divorce_error_out, &quot;Yl&quot;)) %&gt;% pivot_longer(-Loc, values_to = &quot;divorce&quot;) %&gt;% mutate(name = if_else(name == &quot;divorce_obs&quot;, &quot;observed&quot;, &quot;posterior&quot;)), # M data_waffle_brms %&gt;% dplyr::select(Loc, marriage_std) %&gt;% mutate(marriage_est = pull_estimate(brms_c15_model_divorce_error_out, &quot;Xme_&quot;)) %&gt;% pivot_longer(-Loc, values_to = &quot;marriage&quot;) %&gt;% mutate(name = if_else(name == &quot;marriage_std&quot;, &quot;observed&quot;, &quot;posterior&quot;)), by = c(&quot;Loc&quot;, &quot;name&quot;)) %&gt;% ggplot(aes(x = marriage, y = divorce)) + geom_line(aes(group = Loc), size = .2, color = clr0dd) + geom_point(aes(shape = name), color = clr0dd, fill = clr0) + scale_shape_manual(values = c(observed = 21, posterior = 19), guide = &quot;none&quot;) + labs(subtitle = &quot;Shrinkage of both divorce rate and marriage rate&quot;) 16.5.1.3 Measurement Terrors data_sim #&gt; # A tibble: 500 × 4 #&gt; age marriage divorce age_obs #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 -0.172 1.14 0.536 -0.231 #&gt; 2 -0.564 -1.09 -0.477 -1.24 #&gt; 3 0.252 -0.922 -0.326 -0.746 #&gt; 4 0.0798 -1.20 -0.696 -0.113 #&gt; 5 0.818 -2.20 1.99 0.160 #&gt; 6 -0.164 1.05 0.143 0.698 #&gt; 7 0.797 0.413 1.37 -1.88 #&gt; 8 0.435 -2.03 -1.03 1.51 #&gt; 9 -0.254 -0.484 -0.240 -0.00495 #&gt; 10 -0.0502 -0.287 -0.0359 -0.800 #&gt; # … with 490 more rows To get a sense of the havoc ignoring measurement error can cause, we’ll fit to models. These aren’t in the text, but, you know, let’s live a little. The first model will include age, the true predictor for divorce. The second model will include age_obs instead, the version of age with measurement error added in. # the model with age containing no measurement error brms_c15_model_sim_a &lt;- brm( data = data_sim, family = gaussian, divorce ~ 1 + age + marriage, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 42, # note this line save_pars = save_pars(latent = TRUE), file = &quot;brms/brms_c15_model_sim_a&quot;) # The model where age has measurement error, but we ignore it brms_c15_model_sim_b &lt;- brm( data = data_sim, family = gaussian, divorce ~ 1 + age_obs + marriage, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 42, # note this line save_pars = save_pars(latent = TRUE), file = &quot;brms/brms_c15_model_sim_b&quot;) bind_rows(posterior_summary(brms_c15_model_sim_a) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;term&quot;) %&gt;% mutate(model = &quot;model_a&quot;), posterior_summary(brms_c15_model_sim_b) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;term&quot;) %&gt;% mutate(model = &quot;model_b&quot;)) %&gt;% filter(term != &quot;lp__&quot;) %&gt;% mutate(term = str_remove(term,&quot;_obs&quot;) %&gt;% str_remove(&quot;b_&quot;) %&gt;% factor(levels = c(&quot;Intercept&quot;, &quot;age&quot;, &quot;marriage&quot;, &quot;sigma&quot;))) %&gt;% ggplot(aes(x = Estimate, y = model)) + geom_vline(xintercept = 0, linetype = 3, color = clr_dark) + geom_pointrange(aes(xmin = Q2.5, xmax = Q97.5, color = model)) + scale_color_manual(values = c(model_a = clr0dd, model_b = clr0d), guide = &quot;none&quot;) + labs(x = &quot;marginal posterior&quot;, y = NULL) + facet_wrap(~ term, ncol = 1) + theme(panel.background = element_rect(fill = &quot;transparent&quot;, color = clr0d)) 16.5.2 Missing Data 16.5.2.1 DAG Ate my Homework data_homework_c_brms &lt;- data_homework_c %&gt;% as_tibble() p1 &lt;- data_homework_c_brms %&gt;% ggplot(aes(x = study, y = homework)) + geom_point(color = fll0dd) + scale_y_continuous(breaks = 1:10) + labs(subtitle = &quot;true distribution&quot;) p2 &lt;- data_homework_c_brms %&gt;% ggplot(aes(x = study, y = homework_missing)) + geom_point(color = fll0dd) + scale_y_continuous(breaks = 1:10) + labs(subtitle = &quot;missing conditional on x&quot;) p1 + p2 brms_c15_model_hw_full &lt;- brm( data = data_homework_c_brms, family = binomial, homework | trials(10) ~ 1 + study, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 0.5), class = b)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c15_model_hw_full&quot;) mixedup::extract_fixef(brms_c15_model_hw_full) #&gt; # A tibble: 2 × 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Intercept 1.25 0.025 1.20 1.30 #&gt; 2 study 0.609 0.026 0.558 0.661 brms_c15_model_hw_missing &lt;- brm( data = data_homework_c_brms %&gt;% filter(dog == 0), family = binomial, homework | trials(10) ~ 1 + study, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 0.5), class = b)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c15_model_hw_missing&quot;) mixedup::extract_fixef(brms_c15_model_hw_missing) #&gt; # A tibble: 2 × 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Intercept 3.06 0.076 2.92 3.21 #&gt; 2 study 0.895 0.063 0.775 1.02 data_homework_c_brms2 &lt;- data_homework_c_brms %&gt;% mutate(dog = ifelse(abs(x) &lt; 1, 1, 0)) %&gt;% mutate(homework_missing = ifelse(dog == 1, NA_integer_, homework)) p1 &lt;- data_homework_c_brms2 %&gt;% ggplot(aes(x = study, y = homework)) + geom_point(color = fll0dd) + scale_y_continuous(breaks = 1:10) + labs(subtitle = &quot;true distribution&quot;) p2 &lt;- data_homework_c_brms2 %&gt;% ggplot(aes(x = study, y = homework_missing)) + geom_point(color = fll0dd) + scale_y_continuous(breaks = 1:10) + labs(subtitle = &quot;missing conditional on x&quot;) p1 + p2 brms_c15_model_hw_missing2 &lt;- brm( data = data_homework_c_brms2 %&gt;% filter(dog == 0), family = binomial, homework | trials(10) ~ 1 + study, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 0.5), class = b)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c15_model_hw_missing2&quot;) mixedup::extract_fixef(brms_c15_model_hw_missing2) #&gt; # A tibble: 2 × 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Intercept 0.731 0.038 0.655 0.807 #&gt; 2 study 0.354 0.039 0.28 0.431 16.5.2.2 Imputing primates data_milk_brms &lt;- data_milk_full %&gt;% dplyr::select(species, kcal_std, neocortex_std, mass_std) When writing a multivariate model in brms, I find it easier to save the model code by itself and then insert it into the brm() function. Otherwise, things start to feel cluttered. # here&#39;s the primary `k` model b_model &lt;- bf(kcal_std ~ 1 + mi(neocortex_std) + mass_std) + # here&#39;s the model for the missing `b` data bf(neocortex_std | mi() ~ 1) + # here we set the residual correlations for the two models to zero set_rescor(FALSE) Note themi(neocortex_std) syntax in the kcal_std model. This indicates that the predictor, neocortex_std, has missing values that are themselves being modeled. To get a sense of how to specify the priors for such a model in brms, use the get_prior() function. get_prior(data = data_milk_brms, family = gaussian, b_model) #&gt; prior class coef group resp dpar #&gt; (flat) b #&gt; (flat) Intercept #&gt; (flat) b kcalstd #&gt; (flat) b mass_std kcalstd #&gt; (flat) b mineocortex_std kcalstd #&gt; student_t(3, -0.3, 2.5) Intercept kcalstd #&gt; student_t(3, 0, 2.5) sigma kcalstd #&gt; student_t(3, 0.2, 2.5) Intercept neocortexstd #&gt; student_t(3, 0, 2.5) sigma neocortexstd #&gt; nlpar bound source #&gt; default #&gt; default #&gt; (vectorized) #&gt; (vectorized) #&gt; (vectorized) #&gt; default #&gt; default #&gt; default #&gt; default brms_c15_model_primates_imputed &lt;- brm( data = data_milk_brms, family = gaussian, b_model, # here we insert the model prior = c(prior(normal(0, 0.5), class = Intercept, resp = kcalstd), prior(normal(0, 0.5), class = Intercept, resp = neocortexstd), prior(normal(0, 0.5), class = b, resp = kcalstd), prior(exponential(1), class = sigma, resp = kcalstd), prior(exponential(1), class = sigma, resp = neocortexstd)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c15_model_primates_imputed&quot;) mixedup::extract_fixef(brms_c15_model_primates_imputed) #&gt; # A tibble: 4 × 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 kcalstd_Intercept 0.027 0.162 -0.303 0.345 #&gt; 2 neocortexstd_Intercept -0.045 0.21 -0.46 0.36 #&gt; 3 kcalstd_mass_std -0.526 0.209 -0.93 -0.093 #&gt; 4 kcalstd_mineocortex_std 0.473 0.242 -0.018 0.935 posterior_summary(brms_c15_model_primates_imputed) %&gt;% round(digits = 2) %&gt;% as.data.frame() %&gt;% rownames_to_column(var = &quot;param&quot;) %&gt;% as_tibble() #&gt; # A tibble: 19 × 5 #&gt; param Estimate Est.Error Q2.5 Q97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 b_kcalstd_Intercept 0.03 0.16 -0.3 0.34 #&gt; 2 b_neocortexstd_Intercept -0.05 0.21 -0.46 0.36 #&gt; 3 b_kcalstd_mass_std -0.53 0.21 -0.93 -0.09 #&gt; 4 bsp_kcalstd_mineocortex_std 0.47 0.24 -0.02 0.93 #&gt; 5 sigma_kcalstd 0.86 0.15 0.61 1.18 #&gt; 6 sigma_neocortexstd 1.02 0.18 0.74 1.41 #&gt; 7 Ymi_neocortexstd[2] -0.54 0.96 -2.42 1.34 #&gt; 8 Ymi_neocortexstd[3] -0.66 0.99 -2.6 1.29 #&gt; 9 Ymi_neocortexstd[4] -0.68 0.96 -2.55 1.25 #&gt; 10 Ymi_neocortexstd[5] -0.27 0.92 -2.06 1.58 #&gt; 11 Ymi_neocortexstd[9] 0.46 0.93 -1.35 2.28 #&gt; 12 Ymi_neocortexstd[14] -0.15 0.93 -1.9 1.74 #&gt; 13 Ymi_neocortexstd[15] 0.21 0.9 -1.55 2.02 #&gt; 14 Ymi_neocortexstd[17] 0.26 0.92 -1.57 2.1 #&gt; 15 Ymi_neocortexstd[19] 0.48 0.94 -1.38 2.31 #&gt; 16 Ymi_neocortexstd[21] -0.43 0.92 -2.21 1.4 #&gt; 17 Ymi_neocortexstd[23] -0.28 0.93 -2.1 1.64 #&gt; 18 Ymi_neocortexstd[26] 0.14 0.94 -1.75 1.94 #&gt; 19 lp__ -81.6 4.03 -90.6 -74.9 as_draws_df(brms_c15_model_primates_imputed) %&gt;% dplyr::select(starts_with(&quot;Ymi_neocortex&quot;)) %&gt;% set_names(filter(data_milk_brms, is.na(neocortex_std)) %&gt;% pull(species)) %&gt;% pivot_longer(everything(), names_to = &quot;species&quot;) %&gt;% ggplot(aes(x = value, y = reorder(species, value))) + stat_slab(fill = fll0, height = 1.5, slab_color = clr0dd, slab_size = .2) + labs(x = &quot;imputed values for b&quot;, y = NULL) + theme(axis.text.y = element_text(hjust = 0, face = &#39;italic&#39;), axis.ticks.y = element_blank()) brms_c15_model_primates_drop_na &lt;- brm( data = data_milk_brms, family = gaussian, kcal_std ~ 1 + neocortex_std + mass_std, prior = c(prior(normal(0, 0.5), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c15_model_primates_drop_na&quot;) #&gt; Warning message: #&gt; Rows containing NAs were excluded from the model. bind_rows(fixef(brms_c15_model_primates_imputed) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;term&quot;) %&gt;% mutate(fit =&quot;primates_imputed&quot;), fixef(brms_c15_model_primates_drop_na) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;term&quot;) %&gt;% mutate(fit = &quot;primates_drop_na&quot;)) %&gt;% slice(c(4:3, 6:7)) %&gt;% mutate(term = term %&gt;% str_remove(&quot;kcalstd_&quot;) %&gt;% str_remove(&quot;^mi&quot;) )%&gt;% ggplot(aes(x = Estimate, y = fit)) + geom_vline(xintercept = 0, linetype = 3, color = clr_dark) + geom_pointrange(aes(xmin = Q2.5, xmax = Q97.5), color = clr0dd, fill = clr0, shape = 21, stroke = .6) + facet_wrap(. ~ term, ncol = 1) + labs(x = &quot;marginal posterior&quot;, y = NULL) + theme(axis.ticks.y = element_blank(), strip.background = element_rect(color = &quot;transparent&quot;, fill = &quot;transparent&quot;), panel.background = element_rect(fill = &#39;transparent&#39;, color = clr0d)) data_milk_brms_imputed &lt;- data_milk_brms %&gt;% mutate(row = row_number()) %&gt;% left_join( posterior_summary(brms_c15_model_primates_imputed) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;term&quot;) %&gt;% filter(str_detect(term, &quot;Ymi&quot;)) %&gt;% mutate(row = str_extract(term, &quot;(\\\\d)+&quot;) %&gt;% as.integer()), by = &quot;row&quot;) p1 &lt;- data_milk_brms_imputed %&gt;% ggplot(aes(y = kcal_std)) + geom_pointrange(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5), shape = 21, size = .5, stroke = .75, color = clr_dark, fill = clr0) + geom_point(aes(x = neocortex_std), color = clr0d) + labs(x = &quot;neocortex percent (std)&quot;, y = &quot;kcal milk (std)&quot;) + coord_cartesian(xlim = range(data_milk_brms_imputed$neocortex_std, na.rm = T)) p2 &lt;- data_milk_brms_imputed %&gt;% ggplot(aes(x = mass_std)) + geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), shape = 21, size = .5, stroke = .75, color = clr_dark, fill = clr0) + geom_point(aes(y = neocortex_std), color = clr0d) + labs(x = &quot;log body mass (std)&quot;, y = &quot;neocortex percent (std)&quot;) + coord_cartesian(ylim = range(data_milk_brms_imputed$neocortex_std, na.rm = T)) p1 + p2 b_model &lt;- mvbf( bf(kcal_std ~ 1 + mi(neocortex_std) + mass_std), bf(neocortex_std | mi() ~ 1 + mass_std), rescor = FALSE) brms_c15_model_primates_impute_covar &lt;- brm( data = data_milk_brms, family = gaussian, b_model, prior = c(prior(normal(0, 0.5), class = Intercept, resp = kcalstd), prior(normal(0, 0.5), class = Intercept, resp = neocortexstd), prior(normal(0, 0.5), class = b, resp = kcalstd), prior(normal(0, 0.5), class = b, resp = neocortexstd), prior(exponential(1), class = sigma, resp = kcalstd), prior(exponential(1), class = sigma, resp = neocortexstd)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c15_model_primates_impute_covar&quot;) data_milk_brms_imputed2 &lt;- data_milk_brms %&gt;% mutate(row = row_number()) %&gt;% left_join( posterior_summary(brms_c15_model_primates_impute_covar) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;term&quot;) %&gt;% filter(str_detect(term, &quot;Ymi&quot;)) %&gt;% mutate(row = str_extract(term, &quot;(\\\\d)+&quot;) %&gt;% as.integer()), by = &quot;row&quot; ) p1 &lt;- data_milk_brms_imputed2 %&gt;% ggplot(aes(y = kcal_std)) + geom_pointrange(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5), shape = 21, size = .5, stroke = .75, color = clr_dark, fill = clr0) + geom_point(aes(x = neocortex_std), color = clr0d) + labs(x = &quot;neocortex percent (std)&quot;, y = &quot;kcal milk (std)&quot;) + coord_cartesian(xlim = range(data_milk_brms_imputed$neocortex_std, na.rm = T)) p2 &lt;- data_milk_brms_imputed2 %&gt;% ggplot(aes(x = mass_std)) + geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), shape = 21, size = .5, stroke = .75, color = clr_dark, fill = clr0) + geom_point(aes(y = neocortex_std), color = clr0d) + labs(x = &quot;log body mass (std)&quot;, y = &quot;neocortex percent (std)&quot;) + coord_cartesian(ylim = range(data_milk_brms_imputed$neocortex_std, na.rm = T)) p1 + p2 16.5.3 Categorical Errors and Discrete Absences 16.5.3.1 Discrete Cats 😢 Sadly, this is as far as I’m going in this section. I still haven’t made sense of McElreath’s weighted average approach to missing data and I’m not sure whether it’s even possible with brms. 16.6 pymc3 section × "],["rethinking-chapter-16.html", "17 Rethinking: Chapter 16 17.1 Geometric People 17.2 Hidden Minds and Observed Behavior 17.3 Ordinary Differntial Nut Cracking 17.4 Population Dynamics 17.5 Homework 17.6 {brms} section 17.7 pymc3 section", " 17 Rethinking: Chapter 16 Generalized Linear Madness by Richard McElreath, building on the Summaries by Solomon Kurz. 17.1 Geometric People 17.1.1 The Scientific Model Figure 8.1: The ‘Vitruvian Can’ model of human weight as a function of height. The formula for a the volume of a cylinder, with radius as constant proportion of height: \\[ \\begin{array}{crl} V &amp; = &amp; \\pi~r^{2} h\\\\ &amp; = &amp; \\pi~(ph)^{2}h\\\\ &amp; = &amp; \\pi~p^{2}h^{3} \\end{array} \\] Weight as proportion of volume: \\[ \\begin{array}{crl} W &amp; = &amp; kV \\\\ &amp; = &amp; k~\\pi~p^{2}h^{3} \\end{array} \\] 17.1.2 The Statistical Model \\[ \\begin{array}{rclr} W_{i} &amp; \\sim &amp; \\textrm{Log-Normal}(\\mu_{i}, \\sigma) &amp; \\textrm{[distribution of weight]}\\\\ \\textrm{exp}(\\mu_{i}) &amp; = &amp; k~\\pi~p^{2} h_{i}^{3} &amp; \\textrm{[expected median weight]}\\\\ k &amp; \\sim &amp; some~prior &amp; \\textrm{[prior relation between weight and volume]}\\\\ p &amp; \\sim &amp; some~prior &amp; \\textrm{[prior proportionality of radius to height]}\\\\ \\sigma &amp; \\sim &amp; \\textrm{Exponential}(1) &amp; \\textrm{[our old friend, sigma]} \\end{array} \\] Since \\(p\\) and \\(k\\) are multiplied in the model they are not identifiable by the data. Using a compound parameter \\(\\theta = kp^{2}\\) will prohibit the choice of informed priors, so replacing \\(k\\) and \\(p\\) will not help. Thinking about the typical shape of humans lead to the choice of the prior for \\(p\\) that is greater than 0 and less than 1, with most of the mass below 0.5 (as most people are less than half as wide as they are tall): \\[ p \\sim \\textrm{Beta}(2, 18) \\] The parameter \\(k\\) is simply a conversion of arbitrary measurement scales (height to weight). To assign a prior for \\(k\\) we could look up the typical value for this relationship, or we can get rid of the measurement scales by factoring the scales out (here by dividing by a reference value - here, the means of weight and height). library(rethinking) data(Howell1) data_howell &lt;- Howell1 %&gt;% as_tibble() %&gt;% mutate(across(height:weight, .fns = function(x){x/mean(x)}, .names = &quot;{.col}_norm&quot;)) Thinking about the reference case helps with finding a prior, which suggests that \\(k\\) should be greater than 1 (because \\(p \\lt 0.5\\)): \\[ 1 = k~\\pi~p^{2}1^{3} \\] Thus, we choose \\[ k \\sim \\textrm{Exponential}(0.5) \\] Now, for the model: model_weight &lt;- ulam( flist = alist( weight_norm ~ dlnorm( mu, sigma ), exp( mu ) &lt;- 3.141593 * k * p^2 * height_norm ^ 3, p ~ beta( 2, 18 ), k ~ exponential( 0.5 ), sigma ~ exponential( 1 ) ), data = data_howell, chains = 4, cores = 4 ) p1 &lt;- extract.samples(model_weight) %&gt;% as_tibble() %&gt;% dplyr::select(-sigma) %&gt;% ggpairs( lower = list(continuous = wrap(ggally_points, colour = clr_current, size = 1.5, alpha = .7)), diag = list(continuous = wrap(my_diag, fill = fll0, col = clr1, color = clr0d, adjust = .7)), upper = list(continuous = wrap(my_upper , size = 5, color = &quot;black&quot;, family = fnt_sel)) ) + theme(panel.border = element_rect(color = clr_dark, fill = &quot;transparent&quot;)) new_height &lt;- tibble( height_norm = seq(0, max(data_howell$height_norm), length.out = 31)) p2 &lt;- sim(model_weight, data = new_height) %&gt;% t() %&gt;% as_tibble() %&gt;% mutate(idx = row_number()) %&gt;% pivot_longer(-idx) %&gt;% group_by(idx) %&gt;% summarise(pi = list(tibble(value = quantile(value, prob = c(.055, .5, .955)), label = c(&quot;ll&quot;, &quot;m&quot;, &quot;hh&quot;)))) %&gt;% unnest(pi) %&gt;% pivot_wider(names_from = label, values_from = value) %&gt;% bind_cols(new_height) %&gt;% mutate(across(ll:hh, function(x){x / max(data_howell$weight_norm)}, .names = &quot;{.col}_scl&quot;), height_scl = height_norm / max(data_howell$height_norm)) %&gt;% ggplot(aes(x = height_scl)) + geom_smooth(stat = &quot;identity&quot;, aes(ymin = ll_scl, y = m_scl, ymax = hh_scl), color = clr0dd, fill = fll0, size = .5) + geom_point(data = data_howell %&gt;% mutate(height_scl = height_norm / max(height_norm), weight_scl = weight_norm / max(weight_norm)), aes(y = weight_scl), color = fll_current(), size = 2) + coord_cartesian(ylim = 0:1) + labs(y = &quot;weight_scl&quot;) cowplot::plot_grid(ggmatrix_gtable(p1), p2) 17.1.3 GLM in Disguise Consider what happens when taking the logarithm of the model: \\[ \\textrm{log} w_{i} = \\mu_{i} = \\textrm{log}(k \\pi p^{2} h_{i}^{3}) \\] Since, on the logarithm scale multiplication becomes addition, this is \\[ \\textrm{log} w_{i} = \\underbrace{\\textrm{log}(k) + \\textrm{log}(\\pi) + 2~\\textrm{log}(p)}_{\\textrm{intercept}} + 3 ~\\textrm{log}(h_{i}) \\] \\(\\rightarrow\\) On the log-scale, this is a linear regression. 17.2 Hidden Minds and Observed Behavior data(Boxes) data_box &lt;- Boxes %&gt;% as_tibble() precis(data_box) %&gt;% knit_precis(param_name = &quot;column&quot;) column mean sd 5.5% 94.5% histogram y 2.12 0.73 1 3 ▃▁▁▁▇▁▁▁▁▅ gender 1.51 0.50 1 2 ▇▁▁▁▁▁▁▁▁▇ age 8.03 2.50 5 13 ▇▃▅▃▃▃▂▂▂▁ majority_first 0.48 0.50 0 1 ▇▁▁▁▁▁▁▁▁▇ culture 3.75 1.96 1 8 ▃▂▁▇▁▂▁▂▁▂▁▁▁▁ Figure 17.1: The ‘choice box’ - three boxes of differnt color each with a tube to drop a ball into. data_box %&gt;% group_by(y) %&gt;% summarise(freq = n() / nrow(data_box)) #&gt; # A tibble: 3 × 2 #&gt; y freq #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 0.211 #&gt; 2 2 0.456 #&gt; 3 3 0.332 A GLM of these choices would infer frequencies of behavior. But we want to infer strategy. 17.2.1 The Scientific Model Assuming that half of the children follow the majority, while the other half chooe the box at random. set.seed(42) n &lt;- 30 c(sample(1:3, size = .5 * n, replace = TRUE), rep(2, .5 * n)) %&gt;% sample() %&gt;% (function(x){x == 2}) %&gt;% mean() #&gt; [1] 0.7 We will consider five plausible strategies: follow the majority (\\([0, 1, 0]\\)) follow the minority (\\([0, 0, 1]\\)) maverick (follow the unchosen, \\([1, 0, 0]\\)) random (\\([ 1/3,~1/3,~1/3 ]\\)) follow the first 17.2.2 The Statistical Model Since the probabilities for each strategy need to sum to one, this is represented by a simplex and we can use a (weakly informative) Dirichlet prior for \\(p\\) . \\[ p \\sim \\textrm{Dirichlet}(~[~4,~4,~4,~4,~4~]~) \\] Expressing the probability of the data (the likelihood): For each observed choice \\(y_{i}\\), each strategy implies a probability of seeing \\(y_{i}\\). Call this \\(\\textrm{Pr}(y_{i}|s)\\). the probability of the data, conditional on assuming a specific strategy \\(s\\). To get the unconditional probability of the data (\\(\\textrm{Pr}(y_{i})\\)), we need to use \\(p\\) to average over the unknown strategy \\(s\\): \\[ \\textrm{Pr}(y_{i}) = \\sum_{s = 1}^{5} p_{s}~ \\textrm{Pr}(y_{i}|s) \\] \\(\\rightarrow\\) ‘The probability of \\(y_{i}\\) is the weighted average of the probabilities of \\(y_{i}\\) conditional on each strategy \\(s\\)’ (a mixture, that marginalizes out the unknown strategy) This gives the model \\[ \\begin{array}{rcl} y_{i} &amp; \\sim &amp; \\textrm{Categorical}(\\theta)\\\\ \\theta_{j} &amp; = &amp; \\sum_{s=1}^5 p_{s}~\\textrm{Pr}(j|s)~~~\\textrm{for}~j = 1\\dots3\\\\ p &amp; \\sim &amp; \\textrm{Dirichlet}(~[~4,~4,~4,~4,~4~]~) \\end{array} \\] \\(\\rightarrow\\) the vector \\(\\theta\\) holds the average probability of each behavior, conditional on \\(p\\). 17.2.3 Coding up the statistical model This model needs to written in pure stan: boxes_code &lt;- &quot; data{ int N; int y[N]; int majority_first[N]; } parameters{ simplex[5] p; } model{ vector[5] phi; // prior p ~ dirichlet( rep_vector(4,5) ); // probability of data for ( i in 1:N ) { if ( y[i]==2 ) phi[1]=1; else phi[1]=0; // majority if ( y[i]==3 ) phi[2]=1; else phi[2]=0; // minority if ( y[i]==1 ) phi[3]=1; else phi[3]=0; // maverick phi[4]=1.0/3.0; // random if ( majority_first[i]==1 ) // follow first if ( y[i]==2 ) phi[5]=1; else phi[5]=0; else if ( y[i]==3 ) phi[5]=1; else phi[5]=0; // compute log( p_s * Pr(y_i|s ) for ( j in 1:5 ) phi[j] = log(p[j]) + log(phi[j]); // compute average log-probability of y_i target += log_sum_exp( phi ); } } &quot; data_box_list &lt;- data_box %&gt;% dplyr::select(y, majority_first) %&gt;% as.list() %&gt;% c(., list(N = nrow(data_box))) model_boxes &lt;- stan( model_code = boxes_code, data = data_box_list, chains = 4, cores = 4) This allows to get at the assumed strategies behind the behavior: labs &lt;- c(&#39;1 majority&#39;, &#39;2 minority&#39;, &#39;3 maverick&#39;, &#39;4 random&#39;, &#39;5 follow first&#39;) precis(model_boxes, depth = 2) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% as_tibble() %&gt;% mutate(label = factor(labs, levels = rev(labs)) ) %&gt;% ggplot(aes(y = label)) + geom_pointrange(aes(xmin = `X5.5.`, x = mean, xmax = `X94.5.`), color = clr0dd, fill = clr0, shape = 21, stroke = .6) + theme(axis.title = element_blank(), axis.text.y = element_text(hjust = 0)) 17.3 Ordinary Differntial Nut Cracking data(Panda_nuts) data_nuts &lt;- Panda_nuts %&gt;% as_tibble() %&gt;% mutate(age_scl = age / max(age), opening_rate = nuts_opened / seconds, seconds_norm = normalize(seconds)) Figure 17.2: The panda nut (Panda oleosa). 17.3.1 The Scientific Model First, assuming that only strength matters (which we assume to be a function of mass and thus of age). The rate of change in mass for animals with terminal growth is given by the differential equation \\[ \\frac{\\textrm{d}M}{\\textrm{d}t}= k (M_{\\textrm{max}} - M_{t}) \\] where \\(k\\) measures the skill gain with age. The solution to this differential equation is \\[ M_{t} = M_{\\textrm{max}} \\big(1 - \\textrm{exp}(-kt)\\big) \\] Now, we are assuming that strength is proportional to mass (\\(S_{t} = \\beta M_{t}\\)). Since there are multiple ways in which increased strength can interact to make nut-cracking easier, we assume increasing returns (instead of a simple linear relationship between strength and rate of nut opening \\(\\lambda\\). \\[ \\begin{array}{rcl} \\lambda &amp; = &amp; \\alpha~S_{t}^{\\theta}\\\\ &amp; = &amp; \\alpha~\\big(\\beta M_{\\textrm{max}} (1 - \\textrm{exp}(-kt))\\big)^{\\theta} \\end{array} \\] with \\(\\theta\\) greater than 1 (increasing returns). Rescaling body mass to \\(M_{max} = 1\\) simplifies this to \\[ \\lambda = \\alpha \\beta^{\\theta} \\big( 1 - \\textrm{exp}(-kt)\\big)^{\\theta} \\] Here, we can interpret \\(\\alpha\\beta^{\\theta}\\) as the rate of nuts opened per second (\\(\\phi\\)) \\[ \\lambda = \\phi \\big( 1 - \\textrm{exp}(-kt)\\big)^{\\theta} \\] 17.3.2 Statistical Model Assuming the number of opened nuts is far smaller than the number of available nuts qualifies the Poisson distribution for the likelihood \\[ \\begin{array}{rcl} n_{i} &amp; \\sim &amp; \\textrm{Poisson}(\\lambda_{i})\\\\ \\lambda_{i} &amp; = &amp; d_{i} \\phi\\big(1 - \\textrm{exp}(-kt_i)\\big)^{\\theta} \\end{array} \\] where \\(n_{i}\\) is the number of opened nuts, \\(d_{i}\\) is the duration spent on opening nuts (exposure) and \\(t_{i}\\) is the individual’s age at observation \\(i\\). We’ll use the following priors (positive and continuous) based on reasoning about the system \\[ \\begin{array}{rcl} \\phi &amp; \\sim &amp; \\textrm{Log-Normal}\\big(\\textrm{log}(1), 0.1\\big) \\\\ k &amp; \\sim &amp; \\textrm{Log-Normal}\\big(\\textrm{log}(2), 0.25\\big) \\\\ \\theta&amp; \\sim &amp; \\textrm{Log-Normal}\\big(\\textrm{log}(5), 0.25\\big) \\end{array} \\] Checking the implied growth curve: n &lt;- 20 data_nut_prior &lt;- tibble(phi = rlnorm(n, log(1), .1), k = rlnorm(n, log(2), .25), theta = rlnorm(n, log(5), .25)) p1 &lt;- ggplot() + (pmap(data_nut_prior, function(phi, k , theta){ stat_function(fun = function(x){(1 - exp(-k * x))}, geom = &quot;line&quot;, xlim = c(0, 1.5), color = fll0dd) })) + labs(x = &quot;age&quot;, y = &quot;body_mass&quot;, subtitle = &quot;growth curve&quot;) p2 &lt;- ggplot() + (pmap(data_nut_prior, function(phi, k , theta){ stat_function(fun = function(x){phi * (1 - exp(-k * x))^ theta}, geom = &quot;line&quot;, xlim = c(0, 1.5), color = fll0dd) })) + labs(x = &quot;age&quot;, y = &quot;nuts_per_second&quot;, subtitle = &quot;nut opening rate&quot;) p1 + p2 + plot_annotation(title = &quot;prior predictive simulation&quot;) Now, for the actual model… data_nut_list &lt;- data_nuts %&gt;% dplyr::select(nuts_opened, age_scl, seconds) %&gt;% as.list() model_nuts &lt;- ulam( flist = alist( nuts_opened ~ poisson( lambda ), lambda &lt;- seconds * phi * (1 - exp(-k * age_scl)) ^ theta, phi ~ lognormal( log(1), .1 ), k ~ lognormal( log(2), .25 ), theta ~ lognormal( log(5), .25) ), data = data_nut_list, cores = 4, chains = 4 ) nuts_posterior &lt;- extract.samples(model_nuts) %&gt;% as_tibble() ggplot() + (pmap(nuts_posterior[1:30, ], function(phi, k , theta){ stat_function(fun = function(x){phi * (1 - exp(-k * (x/ max(data_nuts$age))))^ theta}, geom = &quot;line&quot;, color = fll0dd) })) + geom_point(data = data_nuts, aes(x = age, y = opening_rate, size = seconds), shape = 1, color = clr_alpha(clr_current, .7), stroke = .7) + scale_size_continuous(limits = c(0, max(data_nuts$seconds))) + labs(x = &quot;age&quot;, y = &quot;nuts_per_second&quot;, subtitle = &quot;posterior predictive distribution&quot;) + lims(x = c(0, max(data_nuts$age))) + theme(legend.position = c(0, 1), legend.justification = c(0, 1)) 17.4 Population Dynamics Loading the time series of hare and lynx populations. data(Lynx_Hare) data_lynx_hare &lt;- Lynx_Hare %&gt;% as_tibble() data_lynx_hare %&gt;% pivot_longer(Lynx:Hare, names_to = &quot;population&quot;, values_to = &quot;n&quot;) %&gt;% ggplot(aes(x = Year, y = n, color = population)) + geomtextpath::geom_textline(aes(label = population), hjust = .09, family = fnt_sel, linetype = 3) + geom_point(aes(fill = after_scale(clr_lighten(color,.85))), shape = 21, size = 2) + scale_color_manual(values = c(Lynx = clr2, Hare = clr1), guide = &quot;none&quot;) + labs(y = &quot;thousands of pelts&quot;) + lims(y = c(0, 80)) This kind of fluctuation data could be modeled in a geocentric way using an autoregressive model (which consider the previous state using a lag variable as predictor). For the hare this would look like this \\[ \\textrm{E}(H_{t}) = \\alpha + \\beta_{1} H_{t - 1} \\] where \\(H_{t}\\) is the number of hares at time \\(t\\). Adding more epicycles - another predictor (such as the Lynx population and deeper lags) in the geocentric model: \\[ \\textrm{E}(H_{t}) = \\alpha + \\beta_{1} H_{t - 1} + \\beta_{2} L_{t-1} + \\beta_{3} H_{t - 2} \\] Autoregressive models are problematic, even if popular: no lag beyond one period makes causal sense the model is propagating errors in the measurements of \\(H_{t}\\) and \\(L_{t}\\) they often lack scientifically interpretable parameters \\(\\rightarrow\\) they can be ok, if you are only interested in forecasting For these reason, we turn to ordinary differential equations (ODE) to model the lynx-hare system instead. 17.4.1 The Scientific Model \\[ \\begin{array}{rcl} \\frac{\\textrm{d}H}{\\textrm{d}t} &amp; = &amp;H_{t} \\times (\\textrm{birth rate}) - H_{t} \\times (\\textrm{death rate})\\\\ &amp; = &amp; H_{t} b_{H} - H_{t} m_{H} \\\\ &amp; = &amp; H_{t} (b_{H} - m_{H}) \\end{array} \\] Including the lynx into the model (via the mortality term) \\[ \\frac{\\textrm{d}H}{\\textrm{d}t} = H_{t} (b_{H} - L_{t}m_{H}) \\] The same goes for the lynx population \\[ \\frac{\\textrm{d}L}{\\textrm{d}t} = L_{t} (H_{t}b_{L} - m_{L}) \\] This coupled set of ODEs is of course the Lotka-Volterra model. Simulating this system by hand: sim_lynx_hare &lt;- function(n_steps, init, theta, dt = .002){ L &lt;- rep(NA, n_steps) H &lt;- rep(NA, n_steps) L[1] &lt;- init[1] H[1] &lt;- init[2] for(i in 2:n_steps){ H[i] &lt;- H[i-1] + dt * H[i-1] * ( theta[1] - theta[2] * L[i-1] ) L[i] &lt;- L[i-1] + dt * L[i-1] * ( theta[3] * H[i-1] - theta[4] ) } tibble(time = 1:n_steps, Hare = H, Lynx = L) } theta &lt;- c(.5, .05, .025, .5) data_lynx_hare_sim &lt;- sim_lynx_hare(1e4, c(data_lynx_hare$Lynx[1], data_lynx_hare$Hare[1]), theta) data_lynx_hare_sim %&gt;% pivot_longer(-time, names_to = &quot;population&quot;, values_to = &quot;n&quot;) %&gt;% ggplot(aes(x = time, y = n, color = population)) + geomtextpath::geom_textline(aes(label = population), hjust = .05, family = fnt_sel) + scale_color_manual(values = c(Lynx = clr2, Hare = clr1), guide = &quot;none&quot;) 17.4.2 The Statistical Model Unpacking the data transformation of pelts (“rounded to the nearest hundred and divided by one thousand”). Here, we assume a beta-distribution for the trapping success of hares of about 10%. n &lt;- 1e4 H_t &lt;- 1e4 tibble(p = rbeta(n, 2, 18), h = rbinom(n, size = H_t, prob = p), h_trans = round(h / 1e3, digits = 2)) %&gt;% ggplot(aes(x = h_trans)) + geom_density(adjust = .4, color = clr0dd, fill = fll0) + labs(x = &quot; thousands of pelts&quot;) \\(\\rightarrow\\) ‘a wide range of pelt counts are consistent with the same true population size. This makes inference about population size difficult.’ …there is no good way to estimate \\(p\\), not without lots of data at least. So, we’re going to just fix it with a strong prior. If this makes you uncomfortable, notice that the model has forced us to realize that we cannot do any better than relative population estimates, unless we have a good estimate of \\(p\\). Defining the probabilities of the observed variables (the pelts): \\[ \\begin{array}{rclr} h_{t} &amp; \\sim &amp; \\textrm{Log-Normal}\\big(\\textrm{log}(p_{H}H_{t}), \\sigma_{H}\\big) &amp; \\textrm{[ prob. observed hare pelts ]}\\\\ l_{t} &amp; \\sim &amp; \\textrm{Log-Normal}\\big(\\textrm{log}(p_{L}L_{t}), \\sigma_{L}\\big) &amp; \\textrm{[ prob. observed lynx pelts ]} \\end{array} \\] Then, those of the unobserved variables: \\[ \\begin{array}{rclr} H_{1} &amp; \\sim &amp; \\textrm{Log-Normal}(\\textrm{log}~10, 1) &amp; \\textrm{[ prior initial hare population ]}\\\\ L_{1} &amp; \\sim &amp; \\textrm{Log-Normal}(\\textrm{log}~10, 1) &amp; \\textrm{[ prior initial lynx population ]}\\\\ H_{T\\gt1} &amp; = &amp; H_{1} + \\int_{1}^{T} H_{t}(b_{H} - m_{H}L_{t}) \\textrm{d}t &amp; \\textrm{[ model for hare population ]}\\\\ L_{T\\gt1} &amp; = &amp; L_{1} + \\int_{1}^{T} L_{t}(b_{L}H_{t} - m_{L}) \\textrm{d}t &amp; \\textrm{[ model for lynx population ]}\\\\ \\sigma_{H} &amp; \\sim &amp; \\textrm{Exponential}(1) &amp; \\textrm{[ prior for measurement dispersion ]}\\\\ \\sigma_{L} &amp; \\sim &amp; \\textrm{Exponential}(1) &amp; \\textrm{[ prior for measurement dispersion ]}\\\\ p_{H} &amp; \\sim &amp; \\textrm{Beta}(\\alpha_{H}, \\beta_{H}) &amp; \\textrm{[ prior for hare trap probability ]}\\\\ p_{L} &amp; \\sim &amp; \\textrm{Beta}(\\alpha_{L}, \\beta_{L}) &amp; \\textrm{[ prior for lynx trap probability ]}\\\\ b_{H} &amp; \\sim &amp; \\textrm{Half-Normal}(1, 0.5) &amp; \\textrm{[ prior hare birth rate ]}\\\\ b_{L} &amp; \\sim &amp; \\textrm{Half-Normal}(0.05, 0.05) &amp; \\textrm{[ prior lynx birth rate ]}\\\\ m_{H} &amp; \\sim &amp; \\textrm{Half-Normal}(0.05, 0.05) &amp; \\textrm{[ prior hare mortality rate ]}\\\\ m_{L} &amp; \\sim &amp; \\textrm{Half-Normal}(1, 0.5) &amp; \\textrm{[ prior lynx mortality rate ]} \\end{array} \\] lynx_hare_code &lt;- &quot; functions { real[] dpop_dt( real t, // time real[] pop_init, // initial state {lynx, hares} real[] theta, // parameters real[] x_r, int[] x_i) { // unused real L = pop_init[1]; real H = pop_init[2]; real bh = theta[1]; real mh = theta[2]; real ml = theta[3]; real bl = theta[4]; // differential equations real dH_dt = (bh - mh * L) * H; real dL_dt = (bl * H - ml) * L; return { dL_dt , dH_dt }; } } data { int&lt;lower=0&gt; N; // number of measurement times real&lt;lower=0&gt; pelts[N,2]; // measured populations } transformed data{ real times_measured[N-1]; // N-1 because first time is initial state for ( i in 2:N ) times_measured[i-1] = i; } parameters { real&lt;lower=0&gt; theta[4]; // { bh, mh, ml, bl } real&lt;lower=0&gt; pop_init[2]; // initial population state real&lt;lower=0&gt; sigma[2]; // measurement errors real&lt;lower=0,upper=1&gt; p[2]; // trap rate } transformed parameters { real pop[N, 2]; pop[1,1] = pop_init[1]; pop[1,2] = pop_init[2]; pop[2:N,1:2] = integrate_ode_rk45( dpop_dt, pop_init, 0, times_measured, theta, rep_array(0.0, 0), rep_array(0, 0), 1e-5, 1e-3, 5e2); } model { // priors theta[{1,3}] ~ normal( 1 , 0.5 ); // bh,ml theta[{2,4}] ~ normal( 0.05, 0.05 ); // mh,bl sigma ~ exponential( 1 ); pop_init ~ lognormal( log(10) , 1 ); p ~ beta(40,200); // observation model // connect latent population state to observed pelts for ( t in 1:N ) for ( k in 1:2 ) pelts[t,k] ~ lognormal( log(pop[t,k]*p[k]) , sigma[k] ); } generated quantities { real pelts_pred[N,2]; for ( t in 1:N ) for ( k in 1:2 ) pelts_pred[t,k] = lognormal_rng( log(pop[t,k]*p[k]) , sigma[k] ); } &quot; data_lynx_hare_list &lt;- data_lynx_hare %&gt;% dplyr::select(Lynx, Hare) %&gt;% list(pelts = ., N = nrow(data_lynx_hare)) Apparently, the stan ODE interface has changed and the function integrate_ode_rk45() is deprecated. model_lynx_hare &lt;- stan( model_code = lynx_hare_code, data = data_lynx_hare_list, chains = 4, cores = 4, control = list( adapt_delta = .95 ) ) lynx_hare_time &lt;- data_lynx_hare %&gt;% dplyr::select(Year) %&gt;% mutate(time = row_number()) lynx_hare_posterior &lt;- extract.samples(model_lynx_hare) population_posterior &lt;- function(pop_idx, type, spec){ lynx_hare_posterior[[type]][,,pop_idx] %&gt;% as_tibble() %&gt;% # t() %&gt;% as_tibble() %&gt;% mutate(.idx = row_number()) %&gt;% pivot_longer(-.idx, names_prefix = &quot;V&quot;, names_transform = as.integer, names_to = &quot;time&quot;) %&gt;% left_join(lynx_hare_time) %&gt;% mutate(population = spec) } p1 &lt;- bind_rows(population_posterior(2, type = &quot;pelts_pred&quot;, &quot;Hare&quot;), population_posterior(1, type = &quot;pelts_pred&quot;, &quot;Lynx&quot;)) %&gt;% filter(.idx &lt; 22) %&gt;% ggplot(aes(x = Year, y = value, color = population)) + geom_line(aes(group = str_c(population,.idx)), alpha = .3) + geom_point(data = data_lynx_hare %&gt;% pivot_longer(-Year, names_to = &quot;population&quot;), aes(fill = after_scale(clr_lighten(color))), shape = 21, size = 2.5) + scale_color_manual(values = c(Lynx = clr2, Hare = clr1)) + labs(y = &quot;thousands of pelts&quot;) + theme(legend.position = c(1, 1), legend.justification = c(1, 1)) p2 &lt;- bind_rows(population_posterior(2, type = &quot;pop&quot;, &quot;Hare&quot;), population_posterior(1, type = &quot;pop&quot;, &quot;Lynx&quot;)) %&gt;% filter(.idx &lt; 22) %&gt;% ggplot(aes(x = Year, y = value, color = population)) + geom_line(aes(group = str_c(population,.idx)), alpha = .6) + scale_color_manual(values = c(Lynx = clr2, Hare = clr1)) + labs(y = &quot;thousands of animals&quot;) + theme(legend.position = c(1, 1), legend.justification = c(1, 1)) p1 / p2 \\(\\rightarrow\\) the model contains random sampling/observational error for the pelts, which makes the posterior for pelts much more jagged than for the actual populations. This is also an illustration of the dangers of modeling time series as if observed data cause observed data in the next step. library(rlang) chapter16_models &lt;- env( ) write_rds(chapter16_models, &quot;envs/chapter16_models.rds&quot;) 17.5 Homework E1 Often, GLMs are rather geocentric models where the parameters do no really have causal scientific meaning. They are simply devices for measuring associations. E2 For example typical population dynamic models like the Lotka-Volterra model featured in the final section. E3 An example is the Poisson oceanic tools model covered in the book: \\[ \\lambda_{i} = \\alpha~P^{\\beta} / \\gamma \\] This can be converted into a linear model by taking the logarithm on both sides: \\[ \\textrm{log}~\\lambda_{i} = \\textrm{log}(\\alpha) + \\beta~\\textrm{log}(P) - \\textrm{log}(\\gamma) \\] M1 model_weight_free &lt;- ulam( flist = alist( weight_norm ~ dlnorm( mu, sigma ), exp( mu ) &lt;- 3.141593 * k * p^2 * height_norm ^ alpha, p ~ beta( 2, 18 ), k ~ exponential( 0.5 ), sigma ~ exponential( 1 ), alpha ~ exponential( 1 ) ), data = data_howell, chains = 4, cores = 4 ) precis( model_weight_free ) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 p 0.24 0.06 0.16 0.34 697.32 1 k 5.84 2.74 2.51 10.97 702.78 1 sigma 0.13 0.00 0.12 0.13 1040.66 1 alpha 2.32 0.02 2.29 2.36 1250.08 1 sim(model_weight_free, data = new_height) %&gt;% t() %&gt;% as_tibble() %&gt;% mutate(idx = row_number()) %&gt;% pivot_longer(-idx) %&gt;% group_by(idx) %&gt;% summarise(pi = list(tibble(value = quantile(value, prob = c(.055, .5, .955)), label = c(&quot;ll&quot;, &quot;m&quot;, &quot;hh&quot;)))) %&gt;% unnest(pi) %&gt;% pivot_wider(names_from = label, values_from = value) %&gt;% bind_cols(new_height) %&gt;% mutate(across(ll:hh, function(x){x / max(data_howell$weight_norm)}, .names = &quot;{.col}_scl&quot;), height_scl = height_norm / max(data_howell$height_norm)) %&gt;% ggplot(aes(x = height_scl)) + geom_smooth(stat = &quot;identity&quot;, aes(ymin = ll_scl, y = m_scl, ymax = hh_scl), color = clr0dd, fill = fll0, size = .5) + geom_point(data = data_howell %&gt;% mutate(height_scl = height_norm / max(height_norm), weight_scl = weight_norm / max(weight_norm)), aes(y = weight_scl), color = fll_current(), size = 2) + coord_cartesian(ylim = 0:1) + labs(y = &quot;weight_scl&quot;) M2 Prior predictive simulations for the cylinder height model n &lt;- 50 prior1 &lt;- tibble( p = rbeta( n , 2, 18 ), k = rexp( n, 0.5 ), sigma = rexp( n, 1)) check_prior &lt;- function(prior){ ggplot() + (pmap(prior, function(p, k, sigma){ stat_function(fun = function(x){ 3.141593 * k * p^2 * x^3 }, geom = &quot;line&quot;, xlim = c(0:1), color = clr_dark) })) + geom_point(data = data_howell %&gt;% mutate(height_scl = height_norm / max(height_norm), weight_scl = weight_norm / max(weight_norm)), aes(x = height_scl, y = weight_scl), color = fll_current(), size = 2) + coord_cartesian(ylim = 0:1) } prior2 &lt;- tibble( p = rbeta( n , 4, 18 ), k = rexp( n, 0.25 ), sigma = rexp( n, 1)) prior3 &lt;- tibble( p = rbeta( n , 4, 18 ), k = rlnorm( n, log(7), 0.2 ), sigma = rexp( n, 1)) check_prior(prior1) + check_prior(prior2) + check_prior(prior3) M3 Prior predictive simulations for the lynx/hare model. n &lt;- 12 theta &lt;- matrix(NA, nrow = n, ncol = 4) filler &lt;- function(data, idx, mu, sd){ data[,idx] &lt;- rnorm(n, mu, sd) data } theta1 &lt;- theta %&gt;% filler(idx = 1, mu = 1, sd = .5) %&gt;% filler(idx = 3, mu = 1, sd = .5) %&gt;% filler(idx = 2, mu = .05, sd = .05) %&gt;% filler(idx = 4, mu = .05, sd = .05) theta2 &lt;- theta %&gt;% filler(idx = 1, mu = .5, sd = .05) %&gt;% filler(idx = 3, mu = .025, sd = .05) %&gt;% filler(idx = 2, mu = .05, sd = .05) %&gt;% filler(idx = 4, mu = .5, sd = .05) sim_run &lt;- function(idx, theta){ sim_lynx_hare( 1e4 , c(data_lynx_hare$Lynx[1], data_lynx_hare$Hare[1]), theta[idx,] ) %&gt;% mutate(.idx = idx) } 1:12 %&gt;% map_dfr(sim_run, theta = theta1) %&gt;% pivot_longer(Hare:Lynx, names_to = &quot;population&quot;, values_to = &quot;n&quot;) %&gt;% ggplot(aes(x = time, y = n, color = population)) + geom_line() + facet_wrap(.idx ~ ., scales = &quot;free_y&quot;)+ scale_color_manual(values = c(Lynx = clr2, Hare = clr1)) + theme(legend.position = &quot;bottom&quot;) theta3 &lt;- theta %&gt;% filler(idx = 1, mu = .5, sd = .1) %&gt;% filler(idx = 3, mu = .025, sd = .05) %&gt;% filler(idx = 2, mu = .05, sd = .05) %&gt;% filler(idx = 4, mu = .5, sd = .1) 1:12 %&gt;% map_dfr(sim_run, theta = theta3) %&gt;% pivot_longer(Hare:Lynx, names_to = &quot;population&quot;, values_to = &quot;n&quot;) %&gt;% ggplot(aes(x = time, y = n, color = population)) + geom_line() + facet_wrap(.idx ~ ., scales = &quot;free_y&quot;)+ scale_color_manual(values = c(Lynx = clr2, Hare = clr1)) + theme(legend.position = &quot;bottom&quot;) M4 \\[ \\begin{array}{rcl} V &amp; = &amp;(4/3) \\pi r^{3}\\\\ &amp; = &amp; (4/3) \\pi (h/2)^{3} \\\\ &amp; = &amp; (1/6) \\pi ~ h^{3}\\\\ W &amp; = &amp; k (1/6) \\pi ~ h^{3} \\end{array} \\] model_weight_sphere &lt;- ulam( flist = alist( weight_norm ~ dlnorm( mu, sigma ), exp( mu ) &lt;- k * 3.141593/6 * height_norm ^ 3, k ~ exponential( 0.5 ), sigma ~ exponential( 1 ) ), data = data_howell, chains = 4, cores = 4 ) precis(model_weight_sphere) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 k 1.81 0.02 1.79 1.83 1531.55 1 sigma 0.21 0.01 0.20 0.22 1228.88 1 sim(model_weight_sphere, data = new_height) %&gt;% t() %&gt;% as_tibble() %&gt;% mutate(idx = row_number()) %&gt;% pivot_longer(-idx) %&gt;% group_by(idx) %&gt;% summarise(pi = list(tibble(value = quantile(value, prob = c(.055, .5, .955)), label = c(&quot;ll&quot;, &quot;m&quot;, &quot;hh&quot;)))) %&gt;% unnest(pi) %&gt;% pivot_wider(names_from = label, values_from = value) %&gt;% bind_cols(new_height) %&gt;% mutate(across(ll:hh, function(x){x / max(data_howell$weight_norm)}, .names = &quot;{.col}_scl&quot;), height_scl = height_norm / max(data_howell$height_norm)) %&gt;% ggplot(aes(x = height_scl)) + geom_smooth(stat = &quot;identity&quot;, aes(ymin = ll_scl, y = m_scl, ymax = hh_scl), color = clr0dd, fill = fll0, size = .5) + geom_point(data = data_howell %&gt;% mutate(height_scl = height_norm / max(height_norm), weight_scl = weight_norm / max(weight_norm)), aes(y = weight_scl), color = fll_current(), size = 2) + coord_cartesian(ylim = 0:1) + labs(y = &quot;weight_scl&quot;) H1 data_nuts_hw &lt;- data_nuts %&gt;% mutate(male_id = as.integer(sex == &quot;m&quot;)) data_nut_hw_list &lt;- data_nuts_hw %&gt;% dplyr::select(nuts_opened, age_scl, seconds, male_id, id = chimpanzee) %&gt;% as.list() model_nuts_by_sex &lt;- ulam( flist = alist( nuts_opened ~ poisson( lambda ), lambda &lt;- seconds * (1 + p_male * male_id) * phi * (1 - exp(-k * age_scl)) ^ theta, phi ~ lognormal( log(1), .1 ), p_male ~ exponential( 2 ), k ~ lognormal( log(2), .25 ), theta ~ lognormal( log(5), .25) ), data = data_nut_hw_list, cores = 4, chains = 4 ) precis(model_nuts_by_sex) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 phi 0.60 0.05 0.53 0.68 898.65 1 p_male 0.66 0.14 0.46 0.89 778.65 1 k 5.21 0.67 4.13 6.25 929.79 1 theta 7.70 1.82 5.09 10.87 995.69 1 nuts_posterior_hw &lt;- extract.samples(model_nuts_by_sex) %&gt;% as_tibble() ggplot() + (pmap(nuts_posterior_hw[1:15, ], function(phi, k , theta, p_male){ stat_function(fun = function(x){ phi * (1 - exp(-k * (x/ max(data_nuts$age))))^ theta}, geom = &quot;line&quot;, aes(color = &quot;f&quot;), alpha = .4) })) + (pmap(nuts_posterior_hw[1:15, ], function(phi, k , theta, p_male){ stat_function(fun = function(x){ (1 + p_male)* phi * (1 - exp(-k * (x/ max(data_nuts$age))))^ theta}, geom = &quot;line&quot;, aes(color = &quot;m&quot;), alpha = .4) })) + geom_point(data = data_nuts, aes(x = age, y = opening_rate, size = seconds, color = sex), shape = 1, stroke = .7) + scale_size_continuous(limits = c(0, max(data_nuts$seconds))) + scale_color_manual(&quot;sex&quot;, values = c(f = clr2, m = clr1)) + labs(x = &quot;age&quot;, y = &quot;nuts_per_second&quot;, subtitle = &quot;posterior predictive distribution&quot;) + lims(x = c(0, max(data_nuts$age))) + theme(legend.position = c(0, 1), legend.justification = c(0, 1)) H2 model_nuts_by_id &lt;- ulam( flist = alist( nuts_opened ~ poisson( lambda ), lambda &lt;- seconds * (phi * z[id] * tau)* (1 - exp(-k * age_scl)) ^ theta, phi ~ lognormal( log(1), .1 ), z[id] ~ exponential( 1 ), tau ~ exponential( 1 ), k ~ lognormal( log(2), .25 ), theta ~ lognormal( log(5), .25), gq&gt; vector[id]:zz &lt;&lt;- z * tau # rescaled ), data = data_nut_hw_list, cores = 4, chains = 4, control = list(adapt_delta = .99), iter = 4000 ) precis(model_nuts_by_id) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 phi 1.00 0.10 0.85 1.17 10152.97 1 tau 0.65 0.21 0.39 1.02 2008.10 1 k 3.09 0.74 2.00 4.36 4645.93 1 theta 3.18 0.67 2.26 4.36 5856.62 1 p1 &lt;- precis(model_nuts_by_id, depth = 2, pars = &quot;zz&quot;) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;id&quot;) %&gt;% as_tibble() %&gt;% mutate(id = fct_reorder(id, -row_number())) %&gt;% ggplot(aes(y = id)) + geom_vline(xintercept = 1, color = clr_dark, linetype = 3) + geom_pointrange(aes(xmin = `X5.5.`, x = mean, xmax = `X94.5.`), color = clr0dd, shape = 21, fill = clr0, size = .5, stroke = .6) + labs(subtitle = &quot;model 1&quot;) model_nuts_by_id2 &lt;- ulam( flist = alist( nuts_opened ~ poisson( lambda ), lambda &lt;- seconds * exp(phi + z[id] * tau)* (1 - exp(-k * age_scl)) ^ theta, phi ~ normal( 0, .5 ), z[id] ~ normal( 0, 1 ), tau ~ exponential( 1 ), k ~ lognormal( log(2), .25 ), theta ~ lognormal( log(5), .25), gq&gt; vector[id]:zz &lt;&lt;- z * tau # rescaled ), data = data_nut_hw_list, cores = 4, chains = 4, control = list(adapt_delta = .99), iter = 4000 ) precis(model_nuts_by_id2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 phi -0.73 0.33 -1.26 -0.20 2247.07 1 tau 1.42 0.35 0.96 2.03 1483.30 1 k 2.67 0.68 1.71 3.87 4247.93 1 theta 3.08 0.60 2.28 4.14 5424.21 1 p2 &lt;- precis(model_nuts_by_id2, depth = 2, pars = &quot;zz&quot;) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;id&quot;) %&gt;% as_tibble() %&gt;% mutate(id = fct_reorder(id, -row_number())) %&gt;% ggplot(aes(y = id)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_pointrange(aes(xmin = `X5.5.`, x = mean, xmax = `X94.5.`), color = clr0dd, shape = 21, fill = clr0, size = .5, stroke = .6) + labs(subtitle = &quot;model 2&quot;) p1 + p2 H3 Fitting an autoregressive model to the Lynx/Hare data: data_lynx_hare_lag &lt;- data_lynx_hare %&gt;% mutate(across(Lynx:Hare, lag, .names = &quot;{.col}_lag&quot;)) %&gt;% filter(complete.cases(Lynx_lag)) \\[ \\begin{array}{rcl} L_{t} &amp; \\sim &amp; \\textrm{Log-Normal}(~\\textrm{log}~\\mu_{L,t}, \\sigma_{L} )\\\\ \\mu_{L,t} &amp; = &amp; \\alpha_{L} + \\beta_{LL}L_{t-1} + \\beta_{LH} H_{t-1}\\\\ H_{t} &amp; \\sim &amp; \\textrm{Log-Normal}(~\\textrm{log}~\\mu_{H,t}, \\sigma_{H} )\\\\ \\mu_{H,t} &amp; = &amp; \\alpha_{H} + \\beta_{HH}H_{t-1} + \\beta_{HL} L_{t-1}\\\\ \\end{array} \\] model_lynx_hare_autoregressive &lt;- ulam( flist = alist( Hare ~ lognormal( log(mu_h) , sigma_h ), Lynx ~ lognormal( log(mu_l) , sigma_l ), mu_h &lt;- alpha_h + phi_hh * Hare_lag + phi_hl * Lynx_lag, mu_l &lt;- alpha_l + phi_ll * Lynx_lag + phi_lh * Hare_lag, c( alpha_h, alpha_l ) ~ normal( 0, 1 ), phi_hh ~ normal(1, 0.5), phi_hl ~ normal(-1, 0.5), phi_ll ~ normal(1, 0.5), phi_lh ~ normal(1, 0.5), c( sigma_h, sigma_l ) ~ exponential(1) ), data = data_lynx_hare_lag, chains = 4, cores = 4 ) precis(model_lynx_hare_autoregressive) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_l -0.82 0.95 -2.37 0.75 1319.86 1 alpha_h 0.82 0.97 -0.76 2.39 1661.53 1 phi_hh 1.15 0.15 0.94 1.40 1248.92 1 phi_hl -0.19 0.10 -0.34 -0.02 1216.17 1 phi_ll 0.54 0.09 0.39 0.69 1542.58 1 phi_lh 0.25 0.05 0.17 0.33 1406.61 1 sigma_l 0.31 0.06 0.23 0.41 1878.99 1 sigma_h 0.45 0.08 0.34 0.59 1274.33 1 link(model_lynx_hare_autoregressive) %&gt;% map2_dfr(.y = c(&quot;Hare&quot;, &quot;Lynx&quot;), .f = function(d,pop){ as_tibble(d) %&gt;% mutate(population = pop, idx = row_number()) }) %&gt;% filter(idx &lt; 22) %&gt;% pivot_longer(c(-idx, -population), names_to = &quot;time&quot;, names_prefix = &quot;V&quot;, names_transform = as.integer) %&gt;% ggplot(aes(x = time, color = population)) + geom_line(aes(y = value, group = str_c(population, idx)), alpha = .3) + geom_point(data = data_lynx_hare %&gt;% pivot_longer(-Year, names_to = &quot;population&quot;), aes(x = Year - 1900, y = value, fill = after_scale(clr_lighten(color))), shape = 21, size = 2.5) + scale_color_manual(values = c(Lynx = clr2, Hare = clr1)) + labs(subtitle = &quot;autoregressiv (no interaction)&quot;) + theme(legend.position = c(1, 1), legend.justification = c(1, 1)) model_lynx_hare_autoregressive_interact &lt;- ulam( flist = alist( Hare ~ lognormal( log(mu_h) , sigma_h ), Lynx ~ lognormal( log(mu_l) , sigma_l ), mu_h &lt;- alpha_h + phi_hh * Hare_lag + phi_hl * Lynx_lag * Hare_lag, mu_l &lt;- alpha_l + phi_ll * Lynx_lag + phi_lh * Hare_lag * Lynx_lag, c( alpha_h, alpha_l ) ~ normal( 0, 1 ), phi_hh ~ normal(1, 0.5), phi_hl ~ normal(-1, 0.5), phi_ll ~ normal(1, 0.5), phi_lh ~ normal(1, 0.5), c( sigma_h, sigma_l ) ~ exponential(1) ), data = data_lynx_hare_lag, chains = 4, cores = 4 ) link(model_lynx_hare_autoregressive_interact) %&gt;% map2_dfr(.y = c(&quot;Hare&quot;, &quot;Lynx&quot;), .f = function(d,pop){ as_tibble(d) %&gt;% mutate(population = pop, idx = row_number()) }) %&gt;% filter(idx &lt; 22) %&gt;% pivot_longer(c(-idx, -population), names_to = &quot;time&quot;, names_prefix = &quot;V&quot;, names_transform = as.integer) %&gt;% ggplot(aes(x = time, color = population)) + geom_line(aes(y = value, group = str_c(population, idx)), alpha = .3) + geom_point(data = data_lynx_hare %&gt;% pivot_longer(-Year, names_to = &quot;population&quot;), aes(x = Year - 1900, y = value, fill = after_scale(clr_lighten(color))), shape = 21, size = 2.5) + scale_color_manual(values = c(Lynx = clr2, Hare = clr1)) + labs(subtitle = &quot;autoregressiv (with interaction)&quot;)+ theme(legend.position = c(1, 1), legend.justification = c(1, 1)) model_lynx_hare_autoregressive_no_intercept &lt;- ulam( flist = alist( Hare ~ lognormal( log(mu_h) , sigma_h ), Lynx ~ lognormal( log(mu_l) , sigma_l ), mu_h &lt;- phi_hh * Hare_lag + phi_hl * Lynx_lag * Hare_lag, mu_l &lt;- phi_ll * Lynx_lag + phi_lh * Hare_lag * Lynx_lag, phi_hh ~ normal(1, 0.5), phi_hl ~ normal(-1, 0.5), phi_ll ~ normal(1, 0.5), phi_lh ~ normal(1, 0.5), c( sigma_h, sigma_l ) ~ exponential(1) ), data = data_lynx_hare_lag, chains = 4, cores = 4 ) link(model_lynx_hare_autoregressive_no_intercept) %&gt;% map2_dfr(.y = c(&quot;Hare&quot;, &quot;Lynx&quot;), .f = function(d,pop){ as_tibble(d) %&gt;% mutate(population = pop, idx = row_number()) }) %&gt;% filter(idx &lt; 22) %&gt;% pivot_longer(c(-idx, -population), names_to = &quot;time&quot;, names_prefix = &quot;V&quot;, names_transform = as.integer) %&gt;% ggplot(aes(x = time, color = population)) + geom_line(aes(y = value, group = str_c(population, idx)), alpha = .3) + geom_point(data = data_lynx_hare %&gt;% pivot_longer(-Year, names_to = &quot;population&quot;), aes(x = Year - 1900, y = value, fill = after_scale(clr_lighten(color))), shape = 21, size = 2.5) + scale_color_manual(values = c(Lynx = clr2, Hare = clr1)) + labs(subtitle = &quot;autoregressiv (no intercept)&quot;)+ theme(legend.position = c(1, 1), legend.justification = c(1, 1)) H4 data_lynx_hare_lag2 &lt;- data_lynx_hare %&gt;% mutate(across(Lynx:Hare, lag, .names = &quot;{.col}_lag&quot;), across(Lynx_lag:Hare_lag, lag, .names = &quot;{.col}_2&quot;)) %&gt;% filter(complete.cases(Lynx_lag_2)) model_lynx_hare_double_lag &lt;- ulam( flist = alist( Hare ~ lognormal( log(mu_h) , sigma_h ), Lynx ~ lognormal( log(mu_l) , sigma_l ), mu_h &lt;- alpha_h + phi_hh * Hare_lag + phi_hl * Lynx_lag + phi2_hh * Hare_lag_2 + phi2_hl * Lynx_lag_2, mu_l &lt;- alpha_l + phi_ll * Lynx_lag + phi_lh * Hare_lag + phi2_ll * Lynx_lag_2 + phi2_lh * Hare_lag_2, c(alpha_h, alpha_l) ~ normal(0,1), phi_hh ~ normal(1,0.5), phi_hl ~ normal(-1,0.5), phi_ll ~ normal(1,0.5), phi_lh ~ normal(1,0.5), phi2_hh ~ normal(0,0.5), phi2_hl ~ normal(0,0.5), phi2_ll ~ normal(0,0.5), phi2_lh ~ normal(0,0.5), c(sigma_h, sigma_l) ~ exponential(1) ), data = data_lynx_hare_lag2, chains = 4 , cores = 4 ) precis( model_lynx_hare_double_lag ) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 alpha_l -0.48 0.96 -2.01 1.04 1863.78 1.00 alpha_h 0.40 1.00 -1.21 2.02 1827.79 1.00 phi_hh 1.02 0.20 0.71 1.34 963.47 1.00 phi_hl -0.75 0.34 -1.28 -0.21 894.35 1.00 phi_ll 0.93 0.24 0.54 1.31 847.18 1.00 phi_lh 0.39 0.13 0.20 0.62 847.50 1.01 phi2_hh 0.18 0.28 -0.25 0.63 956.31 1.00 phi2_hl 0.40 0.16 0.15 0.66 1030.46 1.00 phi2_ll -0.19 0.11 -0.36 -0.02 1087.97 1.00 phi2_lh -0.24 0.20 -0.56 0.07 772.81 1.01 sigma_l 0.30 0.06 0.22 0.40 951.88 1.00 sigma_h 0.40 0.08 0.29 0.53 1067.90 1.00 link( model_lynx_hare_double_lag ) %&gt;% map2_dfr(.y = c(&quot;Hare&quot;, &quot;Lynx&quot;), .f = function(d,pop){ as_tibble(d) %&gt;% mutate(population = pop, idx = row_number()) }) %&gt;% filter(idx &lt; 22) %&gt;% pivot_longer(c(-idx, -population), names_to = &quot;time&quot;, names_prefix = &quot;V&quot;, names_transform = as.integer) %&gt;% ggplot(aes(x = time, color = population)) + geom_line(aes(y = value, group = str_c(population, idx)), alpha = .3) + geom_point(data = data_lynx_hare %&gt;% pivot_longer(-Year, names_to = &quot;population&quot;), aes(x = Year - 1901, y = value, fill = after_scale(clr_lighten(color))), shape = 21, size = 2.5) + scale_color_manual(values = c(Lynx = clr2, Hare = clr1)) + labs(subtitle = &quot;autoregressiv (double-lag, no intercept)&quot;)+ theme(legend.position = c(1, 1), legend.justification = c(1, 1)) model_lynx_hare_double_lag_interaction &lt;- ulam( flist = alist( Hare ~ lognormal( log(mu_h) , sigma_h ), Lynx ~ lognormal( log(mu_l) , sigma_l ), mu_h &lt;- alpha_h + phi_hh * Hare_lag + phi_hl * Lynx_lag * Hare_lag + phi2_hh * Hare_lag_2 + phi2_hl * Lynx_lag_2 * Hare_lag_2, mu_l &lt;- alpha_l + phi_ll * Lynx_lag + phi_lh * Hare_lag * Lynx_lag + phi2_ll * Lynx_lag_2 + phi2_lh * Hare_lag_2 * Lynx_lag_2, c(alpha_h, alpha_l) ~ normal(0,1), phi_hh ~ normal(1,0.5), phi_hl ~ normal(-1,0.5), phi_ll ~ normal(1,0.5), phi_lh ~ normal(1,0.5), phi2_hh ~ normal(0,0.5), phi2_hl ~ normal(0,0.5), phi2_ll ~ normal(0,0.5), phi2_lh ~ normal(0,0.5), c(sigma_h, sigma_l) ~ exponential(1) ), data = data_lynx_hare_lag2, chains = 4 , cores = 4 ) link( model_lynx_hare_double_lag_interaction ) %&gt;% map2_dfr(.y = c(&quot;Hare&quot;, &quot;Lynx&quot;), .f = function(d,pop){ as_tibble(d) %&gt;% mutate(population = pop, idx = row_number()) }) %&gt;% filter(idx &lt; 22) %&gt;% pivot_longer(c(-idx, -population), names_to = &quot;time&quot;, names_prefix = &quot;V&quot;, names_transform = as.integer) %&gt;% ggplot(aes(x = time, color = population)) + geom_line(aes(y = value, group = str_c(population, idx)), alpha = .3) + geom_point(data = data_lynx_hare %&gt;% pivot_longer(-Year, names_to = &quot;population&quot;), aes(x = Year - 1901, y = value, fill = after_scale(clr_lighten(color))), shape = 21, size = 2.5) + scale_color_manual(values = c(Lynx = clr2, Hare = clr1)) + labs(subtitle = &quot;autoregressiv (double-lag, with intercept)&quot;)+ theme(legend.position = c(1, 1), legend.justification = c(1, 1)) H5 data(Mites) data_mites &lt;- Mites %&gt;% as_tibble() %&gt;% dplyr::select(day, predator, prey) data_mites %&gt;% pivot_longer(-day, names_to = &quot;population&quot;, values_to = &quot;n&quot;) %&gt;% ggplot(aes(x = day, y = n, color = population)) + geom_line(linetype = 3) + geom_point(aes(fill = after_scale(clr_lighten(color))), shape = 21, size = 2.5) + scale_color_manual(values = c(prey = clr0dd, predator = clr3))+ theme(legend.position = c(1, 1), legend.justification = c(1, 1)) sim_mites &lt;- function( n_steps , init , theta , dt = 0.002 ) { predator &lt;- rep(NA, n_steps) prey &lt;- rep(NA, n_steps) predator[1] &lt;- init[1] prey[1] &lt;- init[2] for ( i in 2:n_steps ) { predator[i] &lt;- predator[i-1] + dt*predator[i-1]*( theta[3]*prey[i-1] - theta[4] ) prey[i] &lt;- prey[i-1] + dt*prey[i-1]*( theta[1] - theta[2]*predator[i-1] ) } return( cbind(predator, prey) ) } set.seed(42) n &lt;- 16 theta &lt;- matrix( NA, n, 4 ) theta_mites &lt;- theta %&gt;% filler(idx = 1, mu = 1.5, sd = 1) %&gt;% filler(idx = 2, mu = .005, sd = .1) %&gt;% filler(idx = 3, mu = .0005, sd = .1) %&gt;% filler(idx = 4, mu = .5, sd = 1) 1:n %&gt;% map_dfr(sim_run, theta = theta_mites) %&gt;% rename(predator = Lynx, prey = Hare) %&gt;% pivot_longer(prey:predator, names_to = &quot;population&quot;, values_to = &quot;n&quot;) %&gt;% ggplot(aes(x = time, y = n, color = population)) + geom_line() + facet_wrap(.idx ~ ., scales = &quot;free_y&quot;) + scale_color_manual(values = c(prey = clr0dd, predator = clr3)) + theme(legend.position = &quot;bottom&quot;) data_mites_list &lt;- list( N = nrow(data_mites), mites = as.matrix(data_mites %&gt;% dplyr::select(predator, prey)), days = data_mites$day / 7 ) model_mites &lt;- stan( file = &quot;stan/mites.stan&quot; , data = data_mites_list, chains = 4, cores = 4, iter = 2000 , control = list( adapt_delta = 0.99 ) ) precis(model_mites, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% n_eff Rhat4 theta[1] 1.25 0.42 0.58 1.96 732.34 1.01 theta[2] 0.01 0.00 0.00 0.01 664.81 1.02 theta[3] 0.25 0.14 0.01 0.43 2.39 2.43 theta[4] 0.00 0.00 0.00 0.00 2.95 1.74 pop_init[1] 123.90 26.36 88.82 172.17 5.69 1.24 pop_init[2] 236.12 46.09 157.65 307.79 7.49 1.17 sigma[1] 0.82 0.21 0.57 1.21 2.92 1.74 sigma[2] 1.05 0.15 0.84 1.31 18.25 1.07 mites_posterior &lt;- extract.samples(model_mites) population_posterior_mites &lt;- function(pop_idx, type, spec){ mites_posterior[[type]][,,pop_idx] %&gt;% as_tibble() %&gt;% as_tibble() %&gt;% mutate(.idx = row_number()) %&gt;% pivot_longer(-.idx, names_prefix = &quot;V&quot;, names_transform = as.integer, names_to = &quot;day&quot;) %&gt;% left_join(data_mites %&gt;% dplyr::select(day)) %&gt;% mutate(population = spec) } bind_rows(population_posterior_mites(2, type = &quot;pop&quot;, &quot;prey&quot;), population_posterior_mites(1, type = &quot;pop&quot;, &quot;predator&quot;)) %&gt;% filter(.idx &lt; 22) %&gt;% ggplot(aes(x = day, y = value, color = population)) + geom_line(aes(group = str_c(population,.idx)), alpha = .3) + geom_point(data = data_mites %&gt;% pivot_longer(-day, names_to = &quot;population&quot;), aes(x = day / 7, fill = after_scale(clr_lighten(color))), shape = 21, size = 2.5) + scale_color_manual(values = c(prey = clr0dd, predator = clr3)) + labs(y = &quot;mites (n)&quot;) + theme(legend.position = c(1, 1), legend.justification = c(1, 1)) There are some clear problems here. The cycles don’t look to be of the same width each time, and the model cannot handle that. And the predator cycles are not steep enough. Ecologists know that plain Lotka-Volterra models have trouble with realistic data. 🤷 17.6 {brms} section 17.6.1 Geometric People 17.6.1.1 The Statistical Model c(prior(beta(2, 18), nlpar = p, coef = p), prior(exponential(0.5), nlpar = p, coef = k), prior(exponential(1), class = sigma, coef = sigma)) %&gt;% parse_dist(prior) %&gt;% ggplot(aes(y = 0, dist = .dist, args = .args)) + stat_dist_halfeye(.width = .5, size = 1, p_limits = c(0, 0.9995), n = 2e3, normalize = &quot;xy&quot;, fill = fll0dd, color = clr_dark) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;theta&quot;) + facet_wrap(~ coef, scales = &quot;free_x&quot;) + theme(panel.background = element_rect(fill = &quot;transparent&quot;, color = clr0dd, size = .4)) By setting up his model formula as exp(mu) = ..., McElreath effectively used the log link. It turns out that brms only supports the identity and inverse links for family = lognormal. However, we can sneak in the log link by nesting the right-hand side of the formula within log(). brms_c16_model_weight &lt;- brm( data = data_howell, family = lognormal, bf(weight_norm ~ log(3.141593 * k * p ^ 2 * height_norm ^ 3), k + p ~ 1, nl = TRUE), prior = c(prior(beta(2, 18), nlpar = p, lb = 0, ub = 1), prior(exponential(0.5), nlpar = k, lb = 0), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c16_model_weight&quot;) p1 &lt;- as_draws_df(brms_c16_model_weight) %&gt;% select(k = b_k_Intercept, p = b_p_Intercept) %&gt;% ggpairs( lower = list(continuous = wrap(ggally_points, colour = fll0dd, size = 1.5, alpha = .7)), diag = list(continuous = wrap(my_diag, fill = fll0, col = clr_dark, color = clr0d, adjust = .7)), upper = list(continuous = wrap(my_upper , size = 5, color = &quot;black&quot;, family = fnt_sel)) ) + theme(panel.border = element_rect(color = clr_dark, fill = &quot;transparent&quot;)) p2 &lt;- predict(brms_c16_model_weight, newdata = new_height, probs = c(.055, .955)) %&gt;% as_tibble() %&gt;% bind_cols(new_height,. ) %&gt;% mutate(across(c(Estimate, `Q5.5`, `Q95.5`), function(x){x / max(data_howell$weight_norm)}, .names = &quot;{.col}_scl&quot;), height_scl = height_norm / max(data_howell$height_norm)) %&gt;% ggplot(aes(x = height_scl)) + geom_smooth(stat = &quot;identity&quot;, aes(ymin = `Q5.5_scl`, y = Estimate_scl, ymax = `Q95.5_scl`), color = clr0dd, fill = fll0, size = .5) + geom_point(data = data_howell %&gt;% mutate(height_scl = height_norm / max(height_norm), weight_scl = weight_norm / max(weight_norm)), aes(y = weight_scl), color = fll0dd, size = 2) + coord_cartesian(ylim = 0:1) + labs(y = &quot;weight_scl&quot;) cowplot::plot_grid(ggmatrix_gtable(p1), p2) 17.6.2 Hidden Minds and Observed Behavior 17.6.2.1 The Statistical Model rdirichlet(n = 1e5, alpha = rep(4, times = 5)) %&gt;% as_tibble() %&gt;% set_names(1:5) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = name %&gt;% as.double(), alpha = str_c(&quot;alpha[&quot;, name, &quot;]&quot;)) %&gt;% ggplot(aes(x = value, group = name)) + geom_histogram(fill = fll0dd, binwidth = .02, boundary = 0) + scale_x_continuous(expression(italic(p[s])), limits = c(0, 1), breaks = c(0, .2, .5, 1), labels = c(&quot;0&quot;, &quot;.2&quot;, &quot;.5&quot;, &quot;1&quot;), ) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Dirichlet( 4, 4, 4, 4, 4)&quot;) + facet_wrap(~ alpha, nrow = 1) + theme(panel.background = element_rect(fill = &quot;transparent&quot;, color = clr0d)) I’m not aware that one can fit this model directly with brms. My guess is that if it’s possible, it would require a custom likelihood (see Bürkner, 2021a). 17.6.3 Ordinary Differential Nut Cracking 17.6.3.1 Statistical Model set.seed(42) n &lt;- 1e4 tibble(phi = rlnorm(n, meanlog = log(1), sdlog = 0.1), k = rlnorm(n, meanlog = log(2), sdlog = 0.25), theta = rlnorm(n, meanlog = log(5), sdlog = 0.25)) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value)) + geom_histogram(fill = fll0dd, bins = 40, boundary = 0) + scale_x_continuous(&quot;marginal prior&quot;, limits = c(0, NA)) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ name, scales = &quot;free&quot;) + theme(panel.background = element_rect(fill = &#39;transparent&#39;, color = clr0d)) brms_c16_model_nuts &lt;- brm( data = data_nut_list, family = poisson(link = identity), bf(nuts_opened ~ seconds * phi * (1 - exp(-k * age_scl)) ^ theta, phi + k + theta ~ 1, nl = TRUE), prior = c(prior(lognormal(log(1), 0.1), nlpar = phi, lb = 0), prior(lognormal(log(2), 0.25), nlpar = k, lb = 0), prior(lognormal(log(5), 0.25), nlpar = theta, lb = 0)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c16_model_nuts&quot;) n &lt;- 1e4 at &lt;- 0:6 / 4 n_samples &lt;- 50 set.seed(42) as_draws_df(brms_c16_model_nuts) %&gt;% mutate(iter = 1:n()) %&gt;% slice_sample(n = n_samples) %&gt;% expand(nesting(iter, b_phi_Intercept, b_k_Intercept, b_theta_Intercept), age = seq(from = 0, to = 1, length.out = 1e2)) %&gt;% mutate(ns = b_phi_Intercept * (1 - exp(-b_k_Intercept * age))^b_theta_Intercept) %&gt;% ggplot() + geom_line(aes(x = age, y = ns, group = iter), size = 1/4, alpha = 1/2, color = clr_dark) + geom_jitter(data = data_nuts, aes(x = age_scl, y = nuts_opened / seconds, size = seconds), shape = 1, width = 0.01, color = clr0dd) + scale_size_continuous(breaks = c(1, 50, 100), limits = c(1, NA)) + scale_x_continuous(breaks = at, labels = round(at * max(data_nuts$age))) + labs(title = &quot;Posterior predictive distribution for th nut opening model&quot;, y = &quot;nuts per second&quot;) + theme(legend.background = element_blank(), legend.position = c(0,1), legend.justification = c(0,1)) 17.6.4 Population dynamics 17.6.4.1 The Statistical Model On page 546, McElreath encouraged us to try the simulation with different values of \\(H_{t}\\) and \\(p_{t}\\). Here we’ll do so with a 3×3 grid of \\(H_{t} = \\{~5,000;~10,000;~15,000~\\}\\) and \\(p_{t} \\sim \\{\\textrm{Beta}(2,18),~\\textrm{Beta}(10,10),~\\textrm{Beta}(18,2)\\}\\). tibble(shape1 = c(2, 10, 18), shape2 = c(18, 10, 2)) %&gt;% expand(nesting(shape1, shape2), Ht = c(5e3, 1e4, 15e3)) %&gt;% # simulate mutate(pt = purrr::map2(shape1, shape2, ~rbeta(n, shape1 = .x, shape2 = .y))) %&gt;% mutate(ht = purrr::map2(Ht, pt, ~rbinom(n, size = .x, prob = .y))) %&gt;% unnest(c(pt, ht)) %&gt;% # wrangle mutate(ht = round(ht / 1000, digits = 2), beta = str_c(&quot;p[t] Beta(&quot;, shape1, &quot;, &quot;, shape2, &quot;)&quot;), Htlab = str_c(&quot;H[t] = &quot;, Ht)) %&gt;% mutate(beta = factor(beta, levels = c(&quot;p[t] Beta(2, 18)&quot;, &quot;p[t] Beta(10, 10)&quot;, &quot;p[t] Beta(18, 2)&quot;)), Htlab = factor(Htlab, levels = c(&quot;H[t] = 15000&quot;, &quot;H[t] = 10000&quot;, &quot;H[t] = 5000&quot;))) %&gt;% # plot! ggplot(aes(x = ht)) + geom_density(aes(color = beta == &quot;p[t] Beta(2, 18)&quot; &amp; Htlab == &quot;H[t] = 10000&quot;, fill = after_scale(clr_alpha(color,.6))), size = .3, adjust = .5, boundary = 0) + geom_vline(aes(xintercept = Ht / 1000), size = .4, linetype = 3, color = clr_dark) + scale_color_manual(values = c(clr0d, clr_current), guide = &quot;none&quot;) + scale_y_continuous(NULL, breaks = NULL) + facet_grid(Htlab ~ beta, scales = &quot;free_y&quot;)+ xlab(&quot;thousand of pelts (h[t])&quot;) ⚠️ The content to follow is going to diverge from the text, a bit. As you can see from the equation, above, McElreath’s statistical model is a beast. We can fit this model with brms, but the workflow is more complicated than usual. To make this material more approachable, I am going to divide the remainder of this section into two subsections. In the first subsection, we’ll fit a simplified version of McElreath’s m16.5, which does not contain the measurement-error portion. In the second subsection, we’ll tack on the measurement-error portion and fit the full model. ⚠️ The simple Lotka-Volterra model This approach is based on a blog post by Markus Gesmann. As far as the statistical model goes, we might express the revision of McElreath’s model omitting the measurement-error portion as \\[ \\begin{array}{rclr} h_{t} &amp; \\sim &amp; \\textrm{Log-Normal}\\big(\\textrm{log}(H_{t}), \\sigma_{H}\\big) &amp; \\textrm{[ prob. observed hare pelts ]}\\\\ l_{t} &amp; \\sim &amp; \\textrm{Log-Normal}\\big(\\textrm{log}(L_{t}), \\sigma_{L}\\big) &amp; \\textrm{[ prob. observed lynx pelts ]}\\\\ H_{1} &amp; \\sim &amp; \\textrm{Log-Normal}(\\textrm{log}~10, 1) &amp; \\textrm{[ prior initial hare population ]}\\\\ L_{1} &amp; \\sim &amp; \\textrm{Log-Normal}(\\textrm{log}~10, 1) &amp; \\textrm{[ prior initial lynx population ]}\\\\ H_{T\\gt1} &amp; = &amp; H_{1} + \\int_{1}^{T} H_{t}(b_{H} - m_{H}L_{t}) \\textrm{d}t &amp; \\textrm{[ model for hare population ]}\\\\ L_{T\\gt1} &amp; = &amp; L_{1} + \\int_{1}^{T} L_{t}(b_{L}H_{t} - m_{L}) \\textrm{d}t &amp; \\textrm{[ model for lynx population ]}\\\\ b_{H} &amp; \\sim &amp; \\textrm{Half-Normal}(1, 0.5) &amp; \\textrm{[ prior hare birth rate ]}\\\\ b_{L} &amp; \\sim &amp; \\textrm{Half-Normal}(0.05, 0.05) &amp; \\textrm{[ prior lynx birth rate ]}\\\\ m_{H} &amp; \\sim &amp; \\textrm{Half-Normal}(0.05, 0.05) &amp; \\textrm{[ prior hare mortality rate ]}\\\\ m_{L} &amp; \\sim &amp; \\textrm{Half-Normal}(1, 0.5) &amp; \\textrm{[ prior lynx mortality rate ]}\\\\ \\sigma_{H} &amp; \\sim &amp; \\textrm{Exponential}(1) &amp; \\textrm{[ prior for measurement dispersion ]}\\\\ \\sigma_{L} &amp; \\sim &amp; \\textrm{Exponential}(1) &amp; \\textrm{[ prior for measurement dispersion ]} \\end{array} \\] As for our brms, the first issue we need to address is that, at the time of this writing, brms is only set up to fit a univariate ODE model. As Gesmann pointed out, the way around this is to convert the data_lynx_hare data into the long format where the pelt values from the Lynx and Hare columns are all listed in a pelts columns and the two animal populations are differentiated in a population column. We’ll call this long version of the data data_lynx_hare_long. data_lynx_hare_long &lt;- data_lynx_hare %&gt;% pivot_longer(-Year, names_to = &quot;population&quot;, values_to = &quot;pelts&quot;) %&gt;% mutate(delta = if_else(population == &quot;Lynx&quot;, 1, 0), t = Year - min(Year) + 1) %&gt;% arrange(delta, Year) You’ll note how we converted the information in the population column into a dummy variable, delta, which is coded 0 = hares, 1 = lynxes. It’s that dummy variable that will allow us to adjust our model formula so we express a bivariate model as if it were univariate. You’ll see. Also notice how we added a t index for time. This is because the Stan code to follow will expect us to index time in that way. The next step is to write a script that will tell brms how to tell Stan how to fit a Lotka-Volterra model. In his blog, Gesmann called this LotkaVolterra. Our script to follow is a very minor adjustment of his. LotkaVolterra &lt;- &quot; // Sepcify dynamical system (ODEs) real[] ode_LV(real t, // time real [] y, // the system rate real [] theta, // the parameters (i.e., the birth and mortality rates) real [] x_r, // data constant, not used here int [] x_i) { // data constant, not used here // the outcome real dydt[2]; // differential equations dydt[1] = (theta[1] - theta[2] * y[2]) * y[1]; // Hare process dydt[2] = (theta[3] * y[1] - theta[4]) * y[2]; // Lynx process return dydt; // return a 2-element array } // Integrate ODEs and prepare output real LV(real t, real Hare0, real Lynx0, real brHare, real mrHare, real brLynx, real mrLynx, real delta) { real y0[2]; // Initial values real theta[4]; // Parameters real y[1, 2]; // ODE solution // Set initial values y0[1] = Hare0; y0[2] = Lynx0; // Set parameters theta[1] = brHare; theta[2] = mrHare; theta[3] = brLynx; theta[4] = mrLynx; // Solve ODEs y = integrate_ode_rk45(ode_LV, y0, 0, rep_array(t, 1), theta, rep_array(0.0, 0), rep_array(1, 1), 0.001, 0.001, 100); // tolerances, steps // Return relevant population values based on our dummy-variable coding method return (y[1, 1] * (1 - delta) + y[1, 2] * delta); } &quot; Next we define our formula input. To keep from overwhelming the brm() code, we’ll save it, here, as an independent object called lv_formula. lv_formula &lt;- bf( pelts ~ log(eta), # use our LV() function from above nlf(eta ~ LV(t, H1, L1, bh, mh, bl, ml, delta)), # initial population state H1 ~ 1, L1 ~ 1, # hare parameters bh ~ 1, mh ~ 1, # lynx parameters bl ~ 1, ml ~ 1, # population-based measurement errors sigma ~ 0 + population, nl = TRUE ) Note our use of the LV() function in the nlf() line. That’s a function defined in the LotkaVolterra script, above, which will allow us to connect the variables and parameters in our formula code to the underlying statistical model. Next we define our priors and save them as an independent object called lv_priors. lv_priors &lt;- c( prior(lognormal(log(10), 1), nlpar = H1, lb = 0), prior(lognormal(log(10), 1), nlpar = L1, lb = 0), prior(normal(1, 0.5), nlpar = bh, lb = 0), prior(normal(0.05, 0.05), nlpar = bl, lb = 0), prior(normal(0.05, 0.05), nlpar = mh, lb = 0), prior(normal(1, 0.5), nlpar = ml, lb = 0), prior(exponential(1), dpar = sigma, lb = 0) ) brms_c16_model_lynx_hare_simple &lt;- brm( data = data_lynx_hare_long, family = brmsfamily(&quot;lognormal&quot;, link_sigma = &quot;identity&quot;), formula = lv_formula, prior = lv_priors, iter = 2000, warmup = 1000, chains = 4, cores = 4, inits = 0, stanvars = stanvar(scode = LotkaVolterra, block = &quot;functions&quot;), file = &quot;brms/brms_c16_model_lynx_hare_simple&quot;) McElreath recommend we check the chains. Here we’ll pretty them up with help from bayesplot. library(bayesplot) color_scheme_set(&quot;gray&quot;) col_names &lt;- c(&quot;italic(H)[1]&quot;, &quot;italic(L)[1]&quot;, str_c(&quot;italic(&quot;, c(&quot;b[H]&quot;, &quot;m[H]&quot;, &quot;b[L]&quot;, &quot;m[L]&quot;), &quot;)&quot;), &quot;sigma[italic(H)]&quot;, &quot;sigma[italic(L)]&quot;, &quot;lp__&quot;, &quot;chain&quot;, &quot;iter&quot;) as_draws_df(brms_c16_model_lynx_hare_simple) %&gt;% as_tibble() %&gt;% rename(chain = .chain, iter = .iteration) %&gt;% dplyr::select(-.draw) %&gt;% mcmc_trace(pars = vars(-iter, -lp__), facet_args = list(labeller = label_parsed), size = .15) + scale_x_continuous(breaks = NULL) + theme(legend.key.size = unit(0.15, &#39;in&#39;), legend.position = c(.97, .13)) As Gesmann covered in his blog, we need to use the brms::expose_functions() function to expose Stan functions to R before we use some of our favorite post-processing functions. expose_functions(brms_c16_model_lynx_hare_simple, vectorize = TRUE) predict(brms_c16_model_lynx_hare_simple, summary = FALSE, # how many posterior predictive draws would you like? ndraws = 21) %&gt;% data.frame() %&gt;% set_names(1:42) %&gt;% mutate(iter = 1:n()) %&gt;% pivot_longer(-iter, names_to = &quot;row&quot;) %&gt;% mutate(row = as.double(row)) %&gt;% left_join(data_lynx_hare_long %&gt;% mutate(row = 1:n()), by = &quot;row&quot;) %&gt;% ggplot(aes(x = Year, y = value)) + geom_line(aes(group = interaction(iter, population), color = population), size = 1/3, alpha = 1/2) + geom_point(data = . %&gt;% filter(iter == 1), aes(x = Year, fill = population, color = after_scale(clr_lighten(fill))), size = 2.5, shape = 21, stroke = .4) + scale_color_manual(values = c(clr1, clr2)) + scale_fill_manual(values = c(clr1, clr2)) + scale_y_continuous(&quot;thousands of pelts&quot;, breaks = 0:6 * 20) + coord_cartesian(ylim = c(0, 120)) + theme(legend.position = c(1,1), legend.justification = c(1,1)) Add a measurement-error process to the Lotka-Volterra model Now we have a sense of what the current Lotka-Volterra workflow looks like for brms, we’re ready to complicate our model a bit. Happily, we won’t need to update our LotkaVolterra code. That’s good as it is. But we will need to make a couple minor adjustments to our model formula object, which we now call lv_formula_error. Make special note of the first bf() line and the last line before we set nl = TRUE. That’s where all the measurement-error action is at. lv_formula_error &lt;- bf( # this is new pelts ~ log(eta * p), nlf(eta ~ LV(t, H1, L1, bh, mh, bl, ml, delta)), H1 ~ 1, L1 ~ 1, bh ~ 1, mh ~ 1, bl ~ 1, ml ~ 1, sigma ~ 0 + population, # this is new, too p ~ 0 + population, nl = TRUE ) Update the priors and save them as lv_priors_error. lv_priors_error &lt;- c( prior(lognormal(log(10), 1), nlpar = H1, lb = 0), prior(lognormal(log(10), 1), nlpar = L1, lb = 0), prior(normal(1, 0.5), nlpar = bh, lb = 0), prior(normal(0.05, 0.05), nlpar = bl, lb = 0), prior(normal(0.05, 0.05), nlpar = mh, lb = 0), prior(normal(1, 0.5), nlpar = ml, lb = 0), prior(exponential(1), dpar = sigma, lb = 0), # here&#39;s our new prior setting prior(beta(40, 200), nlpar = p, lb = 0, ub = 1) ) brms_c16_model_lynx_hare &lt;- brm( data = data_lynx_hare_long, family = brmsfamily(&quot;lognormal&quot;, link_sigma = &quot;identity&quot;), formula = lv_formula_error, prior = lv_priors_error, iter = 2000, warmup = 1000, chains = 4, cores = 4, inits = 0, stanvars = stanvar(scode = LotkaVolterra, block = &quot;functions&quot;), file = &quot;brms/brms_c16_model_lynx_hare&quot;) p1 &lt;- predict(brms_c16_model_lynx_hare, summary = FALSE, # how many posterior predictive draws would you like? ndraws = 21) %&gt;% data.frame() %&gt;% set_names(1:42) %&gt;% mutate(iter = 1:n()) %&gt;% pivot_longer(-iter, names_to = &quot;row&quot;) %&gt;% mutate(row = as.double(row)) %&gt;% left_join(data_lynx_hare_long %&gt;% mutate(row = 1:n()), by = &quot;row&quot;) %&gt;% ggplot(aes(x = Year, y = value)) + geom_line(aes(group = interaction(iter, population), color = population), size = 1/3, alpha = 1/2) + geom_point(data = . %&gt;% filter(iter == 1), aes(x = Year, fill = population, color = after_scale(clr_lighten(fill))), size = 2.5, shape = 21, stroke = .4) + scale_color_manual(values = c(clr1, clr2)) + scale_fill_manual(values = c(clr1, clr2)) + scale_y_continuous(&quot;thousands of pelts&quot;, breaks = 0:6 * 20) + coord_cartesian(ylim = c(0, 120)) + theme(legend.position = c(1,1), legend.justification = c(1,1), axis.text.x = element_blank(), axis.title.x = element_blank()) Our workflow for the second panel will differ a bit from above and a lot from McElreath’s rethinking-based workflow. In essence, we won’t get the same kind of output McElreath got when he executed post &lt;- extract.samples(m16.5). Our brms_lynx_hare_posterior &lt;- as_draws_df(brms_c16_model_lynx_hare) call only get’s us part of the way there. So we’ll have to be tricky and supplement those results with a little fitted() magic. brms_lynx_hare_posterior &lt;- as_draws_df(brms_c16_model_lynx_hare) brms_lynx_hare_fitted &lt;- fitted(brms_c16_model_lynx_hare, summary = FALSE) Now we’re ready to make our version of the bottom panel of Figure 16.9. The trick is to divide our fitted() based results by the appropriate posterior draws from our \\(p\\) parameters. This is a way of hand computing the post$pop values McElreath showed off in his R code 16.20 block. p2 &lt;- cbind(brms_lynx_hare_fitted[, 1:21] / brms_lynx_hare_posterior$b_p_populationHare, brms_lynx_hare_fitted[, 22:42] / brms_lynx_hare_posterior$b_p_populationLynx) %&gt;% data.frame() %&gt;% set_names(1:42) %&gt;% mutate(iter = 1:n()) %&gt;% pivot_longer(-iter, names_to = &quot;row&quot;) %&gt;% mutate(row = as.double(row)) %&gt;% left_join(data_lynx_hare_long %&gt;% mutate(row = 1:n()), by = &quot;row&quot;) %&gt;% filter(iter &lt; 22) %&gt;% # plot! ggplot(aes(x = Year, y = value)) + geom_line(aes(group = interaction(iter, population), color = population), size = 1/3, alpha = 1/2) + scale_color_manual(values = c(clr1, clr2), guide = &quot;none&quot;) + scale_fill_grey(start = 0, end = .5, breaks = NULL) + scale_y_continuous(&quot;thousands of animals&quot;, breaks = 0:5 * 100) + coord_cartesian(ylim = c(0, 500)) p1 / p2 17.7 pymc3 section × "],["rethinking-chapter-17.html", "18 Rethinking: Chapter 17", " 18 Rethinking: Chapter 17 Horoscopes by Richard McElreath. Parting words: Thinking generatively-how the data could arise-solves many problems. Many statistical problems cannot be solved with statistics. All variables are measured with error. Conditioning on variables creates as many problems as it solves. There is no inference without assumption, but do not choose your assumptions for the sake of inference. Build complex model one piece at a time. Be critical. Be kind. 📕 × "],["bayesian-statistics-the-fun-way.html", "19 Bayesian Statistics the Fun Way 19.1 Conditioning Probabilities 19.2 Combining Probailities based on logic 19.3 The binomial distribution 19.4 The beta distribution 19.5 Bayes’ Theorem 19.6 Parameter Estimation (I) 19.7 The normal distribution 19.8 Cummulative Density and Quantile Function 19.9 Parameter estimation with prior probabilities 19.10 Monte CarloSimulation 19.11 Posterior Odds 19.12 Parameter Estimation (II)", " 19 Bayesian Statistics the Fun Way by Will Kurt 19.1 Conditioning Probabilities \\[ \\begin{eqnarray} D &amp; = &amp; observed~data\\\\ H_{1} &amp; = &amp; Hypothesis\\\\ X &amp; = &amp;prior~belief\\\\ \\end{eqnarray} \\] Allow us to formulate the probability of the observed data given our hypothesis and our prior belief. \\[ P(D | H_{1}, X) \\] To compare different hypothesis, use the ratio of probabilities (odds): \\[ \\frac{P(D | H_{1}, X)}{P(D | H_{2}, X)} &gt; 1 \\] 19.2 Combining Probailities based on logic Rules for \\(AND\\) (\\(\\land\\)), \\(OR\\) (\\(\\lor\\)) and \\(NOT\\) (\\(\\neg\\)). \\(NOT:\\) \\[ \\begin{eqnarray} P(X) &amp; = &amp; p\\\\ \\neg P(X) &amp; = &amp; 1 - p \\end{eqnarray} \\] \\(AND\\) \\[ \\begin{eqnarray} P(Y) &amp; = &amp; q \\\\ P(X) \\land P(Y) &amp; = &amp; P(X,Y) = p \\times q \\end{eqnarray} \\] \\(OR\\) (mutually exclusive events): \\[ P(X) \\lor P(Y) = P(X),P(Y) = p + q \\] while: \\[ P(X) \\land P(Y) = 0 \\] \\(OR\\) (non-mutually exclusive events): \\[ P(X) \\lor P(Y) = P(X) + P(Y) - P(X, Y) \\] 19.3 The binomial distribution Factorial (factorial(x)): \\[ x! = x \\times x-1 \\times x -2 ... \\] The binomial coefficient (choose(n, k)): \\[ {n \\choose k} = \\frac{n!}{k! \\times (n - k)!} \\] The binomial distribution (a Probability Mass Function, PMF): \\[ B(k;n,p) = {n \\choose k} \\times p^k \\times (1 - p) ^{n-k} \\] wdh &lt;- 5000 n &lt;- 10 p = 0.5 tibble(k = rbinom(wdh, n , p)) %&gt;% group_by(k) %&gt;% count(name = &quot;nn&quot;) %&gt;% ungroup() %&gt;% ggplot(aes(x = k, y = nn/wdh)) + geom_bar(stat = &quot;identity&quot;, color = clr1, fill = fll1) + labs(x = &quot;k&quot;, y = glue(&quot;B(k; {n}, {p})&quot;)) + coord_cartesian(xlim = c(-.5, 10.5)) n &lt;- 10 p = 1/6 tibble(k = rbinom(wdh, n , p)) %&gt;% group_by(k) %&gt;% count(name = &quot;nn&quot;) %&gt;% ungroup() %&gt;% ggplot(aes(x = k, y = nn/wdh)) + geom_bar(stat = &quot;identity&quot;, color = clr1, fill = fll1) + labs(x = &quot;k&quot;, y = glue(&quot;B(k; {n}, {round(p,2)})&quot;)) + coord_cartesian(xlim = c(-.5, 10.5)) Cumulative probability to have at least \\(x\\) successes in \\(n\\) trials (pbinom(x-1, n, p, lower.tail = FALSE)): \\[ \\sum_{k=x}^n B(k;n,p) \\] Similarly, less then \\(x\\) successes in \\(n\\) trials (pbinom(x, n, p)): \\[ \\sum_{k=0}^{x-1} B(k;n,p) \\] 19.4 The beta distribution \\[ Beta(p;\\alpha,\\beta) = \\frac{p^{\\alpha -1} \\times (1 - p)^{\\beta - 1}}{beta(\\alpha, \\beta)} \\] Example for an \\(n = 41\\), with \\(\\alpha = 14\\) (successes) and \\(\\beta = 27\\) (fails). alpha &lt;- 14 beta &lt;- 27 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr1, fill = fll1, n = 500) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) Probability, that chance of sucess is less than 0.5: \\[ \\int_{0}^{0.5} Beta(p; 14, 27) \\] x_cutoff &lt;- 0.5 integrate(function(p){ dbeta(p, 14, 27) }, 0, x_cutoff) #&gt; 0.9807613 with absolute error &lt; 5.9e-06 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(xlim = c(0, x_cutoff), fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr2, fill = fll2, n = 500) + geom_vline(xintercept = x_cutoff, linetype = 3) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) alpha &lt;- 5 beta &lt;- 1195 x_cutoff &lt;- 0.005 integrate(function(p){ dbeta(p, alpha, beta) }, x_cutoff, 1) #&gt; 0.2850559 with absolute error &lt; 1e-04 ggplot(tibble(x = seq(0, .01, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(xlim = c(x_cutoff, .01), fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr2, fill = fll2, n = 500) + geom_vline(xintercept = x_cutoff, linetype = 3) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) Exercises # 1) integrate(function(p){ dbeta(p, 4, 6) }, 0.6, 1) #&gt; 0.09935258 with absolute error &lt; 1.1e-15 # 2) integrate(function(p){ dbeta(p, 9, 11) }, 0.45, 0.55) #&gt; 0.30988 with absolute error &lt; 3.4e-15 # 3) integrate(function(p){ dbeta(p, 109, 111) }, 0.45, 0.55) #&gt; 0.8589371 with absolute error &lt; 9.5e-15 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = 9, shape2 = 11) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(fun = function(x){ -dbeta(x, shape1 = 109, shape2 = 111) }, geom = &quot;area&quot;, color = clr1, fill = fll1, n = 500) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) 19.5 Bayes’ Theorem conditional probability The probability of A given B is \\(P(A | B)\\) Dependence updates the product rule of probabilities: \\[ P(A,B) = P(A) \\times P(B | A) \\] (This also holds for independend probabilities, where \\(P(B) = P(B|A)\\)) Bayes’ Theorem (reversing the condition to calculate the probability of the event we are conditioning on \\(P(A|B) \\rightarrow P(B|A)\\)) \\[ P(A|B) = \\frac{P(A)P(B|A)}{P(B)} \\] The important parts here are the posterior probability\\(P(H|D)\\): how strongly we belief in our hypothesis given the data likelyhood\\(P(D|H)\\): the probability of data if the hypothesis were true prior probability\\(P(H)\\): how likely our hypothesis is in the first place Unnormalized posterior \\[ P(H|D) \\propto P(H) \\times P(D|H) \\] 19.6 Parameter Estimation (I) Expectation / mean \\[ \\mu = \\sum_{1}^{n}p_{i}x_{i} \\] n &lt;- 150 mn &lt;- 3 tibble( y = rnorm(n = n, mean = mn), n = 1:n, cum_y = cumsum(y), mean_y = cum_y / n) %&gt;% ggplot(aes(x = n, y = mean_y)) + geom_hline(yintercept = mn, linetype = 3) + geom_point(aes(y = y), color = clr0, size = .75, alpha = .5) + geom_line(color = clr2, size = .75) Spread, mean absolute deviation \\[ MAD(x) = \\frac{1}{n} \\times \\sum_{1}^{n} | x_{i} - \\mu| \\] Spread, variation \\[ Var(x) = \\frac{1}{n} \\times \\sum_{1}^{n} (x_{i} - \\mu) ^{2} \\] Spread, standard deviation \\[ \\sigma = \\sqrt{\\frac{1}{n} \\times \\sum_{1}^{n} (x_{i} - \\mu) ^{2}} \\] Note that in R, var() and sd() uses \\(\\frac{1}{n-1}\\) as denominator for the normalization: var_k &lt;- function(x){ (1/length(x)) * sum( (x - mean(x)) ^ 2 ) } sd_k &lt;- function(x){ sqrt(var_k(x)) } x &lt;- 1:10 n &lt;- length(x) var(x) * ((n-1)/n) == var_k(x) #&gt; [1] TRUE sd(x) * sqrt((n-1)/n) == sd_k(x) #&gt; [1] TRUE 19.7 The normal distribution The probability density function (PDF) for the normal distribution (dnorm()): \\[ N(\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\times e ^{- \\frac{(x - \\mu) ^ 2}{2\\sigma^2}} \\] mu &lt;- 20.6 sigma &lt;- 1.62 x_cutoff &lt;- 18 ggplot(tibble(x = seq(mu - 4 * sigma, mu + 4 * sigma, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dnorm(x, mu, sigma) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(fun = function(x){ dnorm(x, mu, sigma) }, xlim = c(mu - 4 * sigma, x_cutoff), geom = &quot;area&quot;, color = clr2, fill = fll2, n = 500) + geom_vline(xintercept = x_cutoff, linetype = 3) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) integrate(function(x){ dnorm(x, mu, sigma) }, 0, x_cutoff) #&gt; 0.05425369 with absolute error &lt; 3.5e-05 Known probability mass under a normal distribution in terms of ist standard deviation: distance from \\(\\mu\\) probability \\(\\sigma\\) 68 % \\(2 \\sigma\\) 95 % \\(3 \\sigma\\) 99.7 % Excercises x &lt;- c(100, 99.8, 101, 100.5,99.7) mu &lt;- mean(x) sigma &lt;- sd(x) x_cutoff &lt;- 100.4 ggplot(tibble(x = seq(mu - 4 * sigma, mu + 4 * sigma, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dnorm(x, mu, sigma) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(fun = function(x){ dnorm(x, mu, sigma) }, xlim = c(x_cutoff, mu + 4 * sigma), geom = &quot;area&quot;, color = clr2, fill = fll2, n = 500) + geom_vline(xintercept = x_cutoff, linetype = 3) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) 1 - (integrate(function(x){ dnorm(x, mu, sigma) }, mu - sigma, x_cutoff)[[1]] + (1-.68)/2) #&gt; [1] 0.3550062 19.8 Cummulative Density and Quantile Function Beta distribution example Mean of Beta distribution \\[ \\mu_{Beta} = \\frac{\\alpha}{\\alpha + \\beta} \\] alpha &lt;- 300 beta &lt;- 39700 mu &lt;- alpha / (alpha + beta) med &lt;- qbeta(.5, shape1 = alpha, shape2 = beta) bound_left &lt;- .006 bound_right &lt;- .009 19.9 Parameter estimation with prior probabilities alpha_data &lt;- 2 beta_data &lt;- 3 alpha_prior &lt;- 1 beta_prior &lt;- 41 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_data, shape2 = beta_data) }, geom = &quot;line&quot;, color = clr0,linetype = 1, n = 500) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_prior, shape2 = beta_prior) }, geom = &quot;line&quot;, color = clr0,linetype = 2, n = 500) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_data + alpha_prior, shape2 = beta_data + beta_prior) }, geom = &quot;area&quot;, color = clr1, fill = fll1, size = .2, n = 500) + labs(x = &quot;Probability&quot;, y = &quot;Density&quot;, caption = &quot;**solid:** data; **dashed:** prior; **filled:** posterior&quot;) + coord_cartesian(ylim = c(0, 15)) + theme(plot.caption = ggtext::element_markdown(halign = .5, hjust = .5)) 19.10 Monte CarloSimulation alpha_a &lt;- 36 beta_a &lt;- 114 alpha_b &lt;- 50 beta_b &lt;- 100 alpha_prior &lt;- 3 beta_prior &lt;- 7 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_a + alpha_prior, shape2 = beta_a + beta_prior) }, geom = &quot;area&quot;, aes(color = &quot;a&quot;, fill = after_scale(clr_alpha(color))), size = .5, n = 500) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_b + alpha_prior, shape2 = beta_b + beta_prior) }, geom = &quot;area&quot;, aes(color = &quot;b&quot;, fill = after_scale(clr_alpha(color))), size = .5, n = 500) + labs(x = &quot;Probability&quot;, y = &quot;Density&quot;) + scale_color_manual(&quot;Variant&quot;, values = c(a = clr0, b = clr2)) n_trials &lt;- 10^5 mc_simulation &lt;- tibble(samples_a = rbeta(n_trials, alpha_a + alpha_prior, beta_a + beta_prior), samples_b = rbeta(n_trials, alpha_b + alpha_prior, beta_b + beta_prior), samples_ratio = samples_b / samples_a) p_b_superior &lt;- sum(mc_simulation$samples_b &gt; mc_simulation$samples_a)/n_trials p_b_superior #&gt; [1] 0.95873 p_hist &lt;- mc_simulation %&gt;% ggplot(aes(x = samples_ratio)) + geom_histogram(color = clr2, fill = fll2, size = .2, bins = 20,boundary = 1) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) p_ecdf &lt;- mc_simulation %&gt;% ggplot() + stat_function(fun = function(x){(ecdf(x = mc_simulation$samples_ratio))(x)}, xlim = c(range(mc_simulation$samples_ratio)), geom = &quot;area&quot;,color = clr2, fill = fll2, size = .2, n = 500) + labs(x = &quot;Variant Ratio = Improvement&quot;, y = &quot;Cumulative Probability&quot;) p_hist / p_ecdf &amp; coord_cartesian(xlim = c(0.2,3.3), expand = 0) 19.11 Posterior Odds For compering Hypotheses: ratio of posterior: \\[ posterior odds = \\frac{P(H_1) \\times P(D | H_{1})}{P(H_2) \\times P(D | H_{2})} = O(H_{1}) \\times \\frac{P(D | H_{1})}{P(D | H_{2})} \\] This consists of the Bayes factor: \\[ \\frac{P(D | H_{1})}{P(D | H_{2})} \\] and the ratio of prior probabilities \\[ O(H_{1}) = \\frac{P(H_1)}{P(H_2)} \\] rough guide to evaluate poterior odds: Posterior odds Strength of evidence 1 to 3 Interesting but not conclusive 3 to 20 Looks like we’re onto something 20 to 150 Strong evidence in favor of \\(H_1\\) &gt; 150 Overwhelming evidence 19.12 Parameter Estimation (II) dx &lt;- 0.01 bayes_factor &lt;- function(h_top, h_bottom, n_success = 24, n_total = 100){ (h_top ^ n_success * (1 - h_top) ^ (n_total - n_success)) / (h_bottom ^ n_success * (1 - h_bottom) ^ (n_total - n_success)) } bayes_fs &lt;- tibble(hypothesis = seq(0, 1, by = dx), bayes_factor = bayes_factor(hypothesis, h_bottom = 0.5)) %&gt;% mutate(prior = if_else(between(hypothesis, 0.2, 0.3), .001, 1), posterior = prior * bayes_factor, posterior_normalized = posterior / sum(posterior)) p_bayes_factor &lt;- bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = bayes_factor)) + geom_area(color = clr1, fill = fll1, size = .3) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) p_prior &lt;- bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = prior)) + geom_area(color = clr0, fill = fll0, size = .3) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) p_posterior &lt;- bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = posterior)) + geom_area(color = clr2, fill = fll2, size = .3) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) p_posterior_n &lt;- bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = posterior_normalized)) + geom_area(color = clr2, fill = fll2, size = .3) p_bayes_factor / p_prior / p_posterior / p_posterior_n hypothesis bayes_factor prior posterior posterior_normalized 0.24 1478776 0.001 1478.776 0.0004708 Probability of true chance is smaller “one in two”. bayes_fs %&gt;% filter(hypothesis &lt; 0.5) %&gt;% summarise(p_lower_than_half = sum(posterior_normalized)) #&gt; # A tibble: 1 × 1 #&gt; p_lower_than_half #&gt; &lt;dbl&gt; #&gt; 1 1.00 Expectation of the probability distribution (sum of expectations weighted by their value) sum(bayes_fs$posterior_normalized * bayes_fs$hypothesis) #&gt; [1] 0.2402704 Or (because of gap) choose most likely estimate: bayes_fs %&gt;% filter(posterior_normalized == max(posterior_normalized)) %&gt;% knitr::kable() hypothesis bayes_factor prior posterior posterior_normalized 0.19 688568.9 1 688568.9 0.2192415 bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = bayes_factor / sum(bayes_factor))) + geom_area(color = clr2, fill = fll2, size = .3) + stat_function(fun = function(x){ dbeta(x, shape1 = 24 + 1 , shape2 = 76 + 1) * dx}, geom = &quot;point&quot;, shape = 21, color = clr2, fill = fll2, size = 1.5, n = 500) Exercises bayes_fs_e1 &lt;- tibble(hypothesis = seq(0, 1, by = dx), bayes_factor = bayes_factor(hypothesis, h_bottom = 0.24)) %&gt;% mutate(prior = if_else(between(hypothesis, 0.2, 0.3), .001, 1), posterior = prior * bayes_factor, posterior_normalized = posterior / sum(posterior)) bayes_fs_e1 %&gt;% ggplot(aes(x = hypothesis, y = bayes_factor / sum(bayes_factor))) + geom_area(color = clr2, fill = fll2, size = .3) + stat_function(fun = function(x){ dbeta(x, shape1 = 24 + 1 , shape2 = 76 + 1) * dx}, geom = &quot;point&quot;, shape = 21, color = clr2, fill = fll2, size = 1.5, n = 500) bayes_fs_e2 &lt;- tibble(hypothesis = seq(0, 1, by = dx), bayes_factor = bayes_factor(hypothesis, h_bottom = 0.24)) %&gt;% mutate(prior = 1.05 ^ (seq_along(hypothesis) - 1), posterior = prior * bayes_factor, posterior_normalized = posterior / sum(posterior)) bayes_fs_e2 %&gt;% ggplot(aes(x = hypothesis, y = prior)) + geom_area(color = clr0, fill = fll0, size = .3) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) bayes_fs_e2 %&gt;% ggplot(aes(x = hypothesis, y = posterior_normalized)) + geom_area(color = clr2, fill = fll2, size = .3) × "],["mixed-models-with-r.html", "20 Mixed Models with R 20.1 Standard regregression model 20.2 mixed nodel 20.3 Add random slope 20.4 Cross Classified models 20.5 Hierachical structure 20.6 Residual Structure 20.7 Generalized Linear Mixed Models 20.8 Issues/ Considderations 20.9 Formula summary", " 20 Mixed Models with R by Michael Clark load(&quot;data/gpa.RData&quot;) gpa &lt;- gpa %&gt;% as_tibble() 20.1 Standard regregression model \\[ gpa = b_{intercept} + b_{occ} \\times occasion + \\epsilon \\] Coefficients \\(b\\) for intercept and effect of time. The error \\(\\epsilon\\) is assumed to be normally distributed with \\(\\mu = 0\\) and some standard deviation \\(\\sigma\\). \\[ \\epsilon \\sim \\mathscr{N}(0, \\sigma) \\] alternate notation, with emphasis on the data generating process: \\[ gpa ~ \\sim \\mathscr{N}(\\mu, \\sigma)\\\\ \\mu = b_{intercept} + b_{occ} \\times occasion \\] 20.2 mixed nodel 20.2.1 student specific effect (initial depiction) \\[ gpa = b_{intercept} + b_{occ} \\times occasion + ( \\textit{effect}_{student} + \\epsilon )\\\\ \\textit{effect}_{student} \\sim \\mathscr{N}(0, \\tau) \\] focusing on the coefficients (rather than on sources of error): \\[ gpa = ( b_{intercept} + \\textit{effect}_{student} ) + b_{occ} \\times occasion + \\epsilon \\] or (shorter) \\[ gpa = b_{int\\_student} + b_{occ} \\times occasion + \\epsilon \\] \\(\\rightarrow\\) this means student specific intercepts… \\[ b_{int\\_student} \\sim \\mathscr{N}(b_{intercept}, \\tau) \\] …that are normally distributed with the mean of the overall intercept (random intercepts model) 20.2.2 as multi-level model two-part regression model (one at observation level, one at student level) (this is the same as above, just needs ‘plugging in’) \\[ gpa = b_{int\\_student} + b_{occ} \\times occasion + \\epsilon\\\\ b_{int\\_student} = b_{intercept} + \\textit{effect}_{student} \\] ! There is no student-specific effect for \\(occasion\\) (which is termed fixed effect), and there is no random component gpa_lm &lt;- lm(gpa ~ occasion, data = gpa) gpa %&gt;% ggplot(aes(x = year - 1 + as.numeric(semester)/2, y = gpa, group = student)) + geom_line(alpha = .2) + geom_abline(slope = gpa_lm$coefficients[[2]], intercept = gpa_lm$coefficients[[1]], color = clr2, size = 1) + labs(x = &quot;semester&quot;) + coord_cartesian(ylim = c(1,4), expand = 0) pander::pander(summary(gpa_lm), round = 3)   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.599 0.018 145.7 0 occasion 0.106 0.006 18.04 0 Fitting linear model: gpa ~ occasion Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 1200 0.3487 0.2136 0.2129 Student effect not taken into account. 20.2.3 Mixed Model gpa_mixed &lt;- lmer(gpa ~ occasion + (1 | student), data = gpa) (Test automatic equation creation) library(equatiomatic) # Give the results to extract_eq extract_eq(gpa_mixed,) \\[ \\begin{aligned} \\operatorname{gpa}_{i} &amp;\\sim N \\left(\\alpha_{j[i]} + \\beta_{1}(\\operatorname{occasion}), \\sigma^2 \\right) \\\\ \\alpha_{j} &amp;\\sim N \\left(\\mu_{\\alpha_{j}}, \\sigma^2_{\\alpha_{j}} \\right) \\text{, for student j = 1,} \\dots \\text{,J} \\end{aligned} \\] term value se t p_value lower_2.5 upper_97.5 Intercept 2.599 0.022 119.8 0 2.557 2.642 occasion 0.106 0.004 26.1 0 0.098 0.114 group effect variance sd var_prop student Intercept 0.064 0.252 0.523 Residual 0.058 0.241 0.477 Coefficients (fixed effects) for time and intercept are the same as lm() Getting confidence intervals from a mixed model (since \\(p\\) values are not given (== 0 ?)) confint(gpa_mixed) #&gt; 2.5 % 97.5 % #&gt; .sig01 0.22517423 0.2824604 #&gt; .sigma 0.23071113 0.2518510 #&gt; (Intercept) 2.55665145 2.6417771 #&gt; occasion 0.09832589 0.1143027 mm_cinf &lt;- mixedup::extract_vc(gpa_mixed) mm_cinf %&gt;% pander::pander() Table continues below   group effect variance sd sd_2.5 sd_(Intercept)|student student Intercept 0.064 0.252 0.225 sigma Residual 0.058 0.241 0.231   sd_97.5 var_prop sd_(Intercept)|student 0.282 0.523 sigma 0.252 0.477 student effect \\(\\tau\\) = 0.252 / 0.064 (sd / var) Percentage of student variation as share of the total variation (intraclass correlation): 0.064 / 0.122 = 0.5245902 20.2.4 Estimation of random effects Random effect mixedup::extract_random_effects(gpa_mixed) %&gt;% head(5) %&gt;% knitr::kable() group_var effect group value se lower_2.5 upper_97.5 student Intercept 1 -0.071 0.092 -0.251 0.109 student Intercept 2 -0.216 0.092 -0.395 -0.036 student Intercept 3 0.088 0.092 -0.091 0.268 student Intercept 4 -0.187 0.092 -0.366 -0.007 student Intercept 5 0.030 0.092 -0.149 0.210 Random intercept (intercept + random effect) mm_coefs &lt;- mixedup::extract_coef(gpa_mixed) mm_coefs %&gt;% head(5) %&gt;% knitr::kable() group_var effect group value se lower_2.5 upper_97.5 student Intercept 1 2.528 0.095 2.343 2.713 student Intercept 2 2.383 0.095 2.198 2.568 student Intercept 3 2.687 0.095 2.502 2.872 student Intercept 4 2.412 0.095 2.227 2.597 student Intercept 5 2.629 0.095 2.444 2.814 library(merTools) mm_intervals &lt;- predictInterval(gpa_mixed) %&gt;% as_tibble() mm_mean_sd &lt;- REsim(gpa_mixed) %&gt;% as_tibble() sd_level &lt;- .95 mm_mean_sd %&gt;% mutate(sd_leveled = sd * qnorm(1 - ((1 - sd_level)/2)), sig = (median + sd_leveled) &gt;= 0 &amp; (median - sd_leveled) &lt;= 0) %&gt;% arrange(median) %&gt;% mutate(rank = row_number()) %&gt;% arrange(groupID) %&gt;% ggplot(aes(x = rank)) + geom_segment(aes(xend = rank, y = median - sd_leveled, yend = median + sd_leveled, color = sig)) + geom_point(aes(y = median), size = .3, alpha = .5) + geom_hline(yintercept = 0, size = .4, linetype = 3) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;Student&quot;, y = &quot;Coefficient&quot;, caption = &quot;interval estimates of random effects&quot;) 20.2.5 Prediction gpa_predictions &lt;- tibble(lm = predict(gpa_lm), lmm_no_random_effects = predict(gpa_mixed, re.form = NA), lmm_with_random_effects = predict(gpa_mixed)) %&gt;% bind_cols(gpa, .) gpa_predictions %&gt;% ggplot(aes(x = lm)) + geom_point(aes(y = lmm_with_random_effects, color = &quot;with_re&quot;)) + geom_point(aes(y = lmm_no_random_effects, color = &quot;no_re&quot;)) + scale_color_manual(values = c(no_re = clr2, with_re = clr0d)) student_select &lt;- 1:2 gpa_predictions %&gt;% filter(student %in% student_select) %&gt;% ggplot(aes(x = occasion)) + geom_point(aes(y = gpa, color = student))+ geom_abline(data = tibble(slope = c(gpa_lm$coefficients[[2]], mm_fixed$value[c(2,2)]), intercept = c(gpa_lm$coefficients[[1]], mm_coefs$value[as.numeric(as.character(mm_coefs$group)) %in% student_select]), model = c(&quot;lm&quot;, as.character(mm_coefs$group[as.numeric(as.character(mm_coefs$group)) %in% student_select]))), aes(slope = slope, intercept = intercept, color = model), size = .6) + scale_color_manual(values = c(lm = clr2, `1` = clr1, `2` = clr0d), labels = c( &quot;lm&quot;, &quot;lmm (student1)&quot;, &quot;lmm (student2)&quot;)) 20.2.6 Cluster Level Covariates If a cluster level covariate is added (eg. sex), \\(b_{int\\_student}\\) turns into: \\[ b_{int\\_student} = b_{intercept} + b_{sex} \\times \\textit{sex} + \\textit{effect}_{student} \\] plugging this into the model will result in \\[ gpa = b_{intercept} + b_{occ} \\times \\textit{occasion} + b_{sex} \\times \\textit{sex} + ( \\textit{effect}_{student} + \\epsilon) \\] 20.3 Add random slope gpa_mixed2 &lt;- lmer(gpa ~ occasion + (1 + occasion | student), data = gpa) mixedup::extract_fixed_effects(gpa_mixed2) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 2.599 0.018 141.592 0 2.563 2.635 occasion 0.106 0.006 18.066 0 0.095 0.118 mixedup::extract_vc(gpa_mixed2, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop student Intercept 0.045 0.213 0.491 student occasion 0.005 0.067 0.049 Residual 0.042 0.206 0.460 gpa_mixed2_rc &lt;- mixedup::extract_random_coefs(gpa_mixed2) correlation of the intercepts and slopes (negative, so students with a low starting score tend to increase a little more) VarCorr(gpa_mixed2) %&gt;% as_tibble() %&gt;% knitr::kable() grp var1 var2 vcov sdcor student (Intercept) NA 0.0451934 0.2125875 student occasion NA 0.0045039 0.0671114 student (Intercept) occasion -0.0014016 -0.0982391 Residual NA NA 0.0423879 0.2058832 gpa_lm_separate &lt;- gpa %&gt;% group_by(student) %&gt;% nest() %&gt;% mutate(mod = map(data,function(data){lm(gpa ~ occasion, data = data)})) %&gt;% bind_cols(., summarise_model(.)) p_intercepts &lt;- ggplot() + geom_density(data = gpa_mixed2_rc %&gt;% filter(effect == &quot;Intercept&quot;), aes(x = value, color = &quot;mixed&quot;, fill = after_scale(clr_alpha(color)))) + geom_density(data = gpa_lm_separate, aes(x = intercept, color = &quot;separate&quot;, fill = after_scale(clr_alpha(color)))) + labs(x = &quot;intercept&quot;) + xlim(1.5, 4) p_slopes &lt;- ggplot() + geom_density(data = gpa_mixed2_rc %&gt;% filter(effect == &quot;occasion&quot;), aes(x = value, color = &quot;mixed&quot;, fill = after_scale(clr_alpha(color)))) + geom_density(data = gpa_lm_separate, aes(x = slope, color = &quot;separate&quot;, fill = after_scale(clr_alpha(color)))) + labs(x = &quot;slope&quot;) + xlim(-.2, .4) p_intercepts + p_slopes + plot_layout(guides = &quot;collect&quot;) &amp; scale_color_manual(&quot;model&quot;, values = c(separate = clr0d, mixed = clr2)) &amp; theme(legend.position = &quot;bottom&quot;) \\(\\rightarrow\\) mixed model intercepts and slopes are less extreme In both cases the mixed model shrinks what would have been the by-group estimate, which would otherwise overfit in this scenario. This regularizing effect is yet another bonus when using mixed models. gpa_predictions &lt;- tibble(lmm_with_random_slope = predict(gpa_mixed2)) %&gt;% bind_cols(gpa_predictions, .) gpa_mixed2_rc_wide &lt;- gpa_mixed2_rc %&gt;% dplyr::select(group_var, group, effect, value) %&gt;% pivot_wider(names_from = effect, values_from = value) student_select &lt;- 1:2 p_two_students &lt;- gpa_predictions %&gt;% filter(student %in% student_select) %&gt;% ggplot(aes(x = occasion)) + geom_point(aes(y = gpa, color = student))+ geom_abline(data = tibble(slope = c(gpa_lm$coefficients[[2]], gpa_mixed2_rc_wide$occasion[as.numeric(as.character(gpa_mixed2_rc_wide$group)) %in% student_select]), intercept = c(gpa_lm$coefficients[[1]], gpa_mixed2_rc_wide$Intercept[as.numeric(as.character(gpa_mixed2_rc_wide$group)) %in% student_select]), model = c(&quot;lm&quot;, as.character(gpa_mixed2_rc_wide$group[as.numeric(as.character(gpa_mixed2_rc_wide$group)) %in% student_select]))), aes(slope = slope, intercept = intercept, color = model), size = .6) + scale_color_manual(values = c(lm = clr2, `1` = clr1, `2` = clr0d), labels = c( &quot;lm&quot;, &quot;lmm (student1)&quot;, &quot;lmm (student2)&quot;)) p_all_mod &lt;- ggplot(data = gpa_predictions, aes(x = occasion, y = gpa)) + geom_abline(data = tibble(slope = c(gpa_mixed2_rc_wide$occasion, gpa_lm$coefficients[[2]]), intercept = c(gpa_mixed2_rc_wide$Intercept, gpa_lm$coefficients[[1]]), modeltype = c(rep(&quot;lmm (random slope)&quot;, length(gpa_mixed2_rc_wide$Intercept)), &quot;lm&quot;)), aes(slope = slope, intercept = intercept, color = modeltype), size = .6) + scale_color_manual(values = c(`lmm (random slope)` = clr_alpha(clr0d ,.6), lm = clr2)) p_two_students + p_all_mod + plot_layout(guides = &quot;collect&quot;) &amp; xlim(0,5) &amp; ylim(2.2, 4) &amp; theme(legend.position = &quot;bottom&quot;) 20.4 Cross Classified models Setups where data are grouped by several factors but these are not nested (all participants get to see all images). These are crossed random effects. load(&quot;data/pupils.RData&quot;) pupils %&gt;% head() %&gt;% knitr::kable() PUPIL primary_school_id secondary_school_id achievement sex ses primary_denominational secondary_denominational 1 1 2 6.6 female highest no no 2 1 1 5.7 male lowest no yes 3 1 17 4.5 male 2 no no 4 1 3 4.4 male 2 no no 5 1 4 5.8 male 3 no yes 6 1 4 5.0 female 4 no yes pupils_crossed &lt;- lmer( achievement ~ sex + ses + ( 1 | primary_school_id ) + ( 1 | secondary_school_id ), data = pupils ) mixedup::extract_fixed_effects(pupils_crossed) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 5.924 0.123 48.303 0.000 5.684 6.164 sexfemale 0.261 0.046 5.716 0.000 0.171 0.350 ses2 0.132 0.118 1.122 0.262 -0.098 0.362 ses3 0.098 0.110 0.890 0.373 -0.118 0.314 ses4 0.298 0.105 2.851 0.004 0.093 0.503 ses5 0.354 0.101 3.514 0.000 0.156 0.551 seshighest 0.616 0.110 5.602 0.000 0.401 0.832 mixedup::extract_vc(pupils_crossed, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop primary_school_id Intercept 0.173 0.416 0.243 secondary_school_id Intercept 0.066 0.257 0.093 Residual 0.473 0.688 0.664 pupils_varicance_components_random_effects &lt;- REsim(pupils_crossed) %&gt;% as_tibble() pupils_varicance_components_random_effects %&gt;% mutate(sd_leveled = sd * qnorm(1 - ((1 - sd_level)/2)), sig = (median + sd_leveled) &gt;= 0 &amp; (median - sd_leveled) &lt;= 0) %&gt;% group_by(groupFctr) %&gt;% arrange(groupFctr, median) %&gt;% mutate(rank = row_number()) %&gt;% arrange(groupFctr, groupID) %&gt;% ungroup() %&gt;% ggplot(aes(x = rank)) + geom_segment(aes(xend = rank, y = median - sd_leveled, yend = median + sd_leveled, color = sig)) + geom_point(aes(y = median), size = .3, alpha = .5) + geom_hline(yintercept = 0, size = .4, linetype = 3) + facet_wrap(groupFctr ~ ., scales = &quot;free_x&quot;) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;Group&quot;, y = &quot;Effect Range&quot;, caption = &quot;interval estimates of random effects&quot;) Note that we have the usual extensions here if desired. As an example, we could also do random slopes for student level characteristics. 20.5 Hierachical structure These are setups, where different grouping factors are nested within each other (eg. cities, counties, states). load(&quot;data/nurses.RData&quot;) nurses %&gt;% head() %&gt;% knitr::kable() hospital ward wardid nurse age sex experience stress wardtype hospsize treatment 1 1 11 1 36 Male 11 7 general care large Training 1 1 11 2 45 Male 20 7 general care large Training 1 1 11 3 32 Male 7 7 general care large Training 1 1 11 4 57 Female 25 6 general care large Training 1 1 11 5 46 Female 22 6 general care large Training 1 1 11 6 60 Female 22 6 general care large Training nurses_hierach &lt;- lmer( stress ~ age + sex + experience + treatment + wardtype + hospsize + ( 1 | hospital) + ( 1 | hospital:ward), # together same as ( 1 | hospital / ward) data = nurses ) mixedup::extract_fixed_effects(nurses_hierach) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 5.380 0.185 29.128 0.000 5.018 5.742 age 0.022 0.002 10.053 0.000 0.018 0.026 sexFemale -0.453 0.035 -12.952 0.000 -0.522 -0.385 experience -0.062 0.004 -13.776 0.000 -0.070 -0.053 treatmentTraining -0.700 0.120 -5.843 0.000 -0.935 -0.465 wardtypespecial care 0.051 0.120 0.424 0.671 -0.184 0.286 hospsizemedium 0.489 0.202 2.428 0.015 0.094 0.884 hospsizelarge 0.902 0.275 3.280 0.001 0.363 1.440 mixedup::extract_vc(nurses_hierach, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop hospital:ward Intercept 0.337 0.580 0.500 hospital Intercept 0.119 0.345 0.177 Residual 0.217 0.466 0.323 nurses_varicance_components_random_effects &lt;- REsim(nurses_hierach) %&gt;% as_tibble() nurses_varicance_components_random_effects %&gt;% mutate(sd_leveled = sd * qnorm(1 - ((1 - sd_level)/2)), sig = (median + sd_leveled) &gt;= 0 &amp; (median - sd_leveled) &lt;= 0) %&gt;% group_by(groupFctr) %&gt;% arrange(groupFctr, median) %&gt;% mutate(rank = row_number()) %&gt;% arrange(groupFctr, groupID) %&gt;% ungroup() %&gt;% ggplot(aes(x = rank)) + geom_segment(aes(xend = rank, y = median - sd_leveled, yend = median + sd_leveled, color = sig)) + geom_point(aes(y = median), size = .3, alpha = .5) + geom_hline(yintercept = 0, size = .4, linetype = 3) + facet_wrap(groupFctr ~ ., scales = &quot;free_x&quot;) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + ylim(-2,2) + labs(x = &quot;Group&quot;, y = &quot;Effect Range&quot;, caption = &quot;interval estimates of random effects&quot;) 20.5.1 Crossed vs. nested nurses_hierach2 &lt;- lmer( stress ~ age + sex + experience + treatment + wardtype + hospsize + ( 1 | hospital ) + ( 1 | hospital:wardid ), # needs to be wardid now because ward is duplicated over hospitals (not unique) data = nurses ) nurses_nested &lt;- lmer( stress ~ age + sex + experience + treatment + wardtype + hospsize + ( 1 | hospital ) + ( 1 | wardid ), data = nurses ) Nested: mixedup::extract_fixed_effects(nurses_hierach2) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 5.380 0.185 29.128 0.000 5.018 5.742 age 0.022 0.002 10.053 0.000 0.018 0.026 sexFemale -0.453 0.035 -12.952 0.000 -0.522 -0.385 experience -0.062 0.004 -13.776 0.000 -0.070 -0.053 treatmentTraining -0.700 0.120 -5.843 0.000 -0.935 -0.465 wardtypespecial care 0.051 0.120 0.424 0.671 -0.184 0.286 hospsizemedium 0.489 0.202 2.428 0.015 0.094 0.884 hospsizelarge 0.902 0.275 3.280 0.001 0.363 1.440 mixedup::extract_vc(nurses_hierach2, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop hospital:wardid Intercept 0.337 0.580 0.500 hospital Intercept 0.119 0.345 0.177 Residual 0.217 0.466 0.323 Crossed: mixedup::extract_fixed_effects(nurses_nested) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 5.380 0.185 29.128 0.000 5.018 5.742 age 0.022 0.002 10.053 0.000 0.018 0.026 sexFemale -0.453 0.035 -12.952 0.000 -0.522 -0.385 experience -0.062 0.004 -13.776 0.000 -0.070 -0.053 treatmentTraining -0.700 0.120 -5.843 0.000 -0.935 -0.465 wardtypespecial care 0.051 0.120 0.424 0.671 -0.184 0.286 hospsizemedium 0.489 0.202 2.428 0.015 0.094 0.884 hospsizelarge 0.902 0.275 3.280 0.001 0.363 1.440 mixedup::extract_vc(nurses_nested, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop wardid Intercept 0.337 0.580 0.500 hospital Intercept 0.119 0.345 0.177 Residual 0.217 0.466 0.323 20.6 Residual Structure rescov &lt;- function(model, data) { var.d &lt;- crossprod(getME(model,&quot;Lambdat&quot;)) Zt &lt;- getME(model,&quot;Zt&quot;) vr &lt;- sigma(model)^2 var.b &lt;- vr*(t(Zt) %*% var.d %*% Zt) sI &lt;- vr * Diagonal(nrow(data)) var.y &lt;- var.b + sI var.y %&gt;% as.matrix() %&gt;% as_tibble() %&gt;% mutate(row = row_number()) %&gt;% pivot_longer(cols = -row, names_to = &quot;column&quot;) } rescov(gpa_mixed, gpa) %&gt;% mutate(x = as.numeric(column), y = as.numeric(row)) %&gt;% filter(between(x,0,30), between(y,0,30)) %&gt;% ggplot(aes(x = x, y = y, fill = value)) + geom_tile(aes(color = after_scale(clr_darken(fill))), size = .3, width = .9, height = .9) + scale_fill_gradientn(colours = c(clr0, clr_lighten(clr1), clr_lighten(clr2))) + scale_y_reverse() + coord_equal() covariance matrix for a cluster (compound symmetry): \\[ \\Sigma = \\left[ \\begin{array}{ccc} \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 \\\\ \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 \\\\ \\tau^2 &amp; \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 \\\\ \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2\\\\ \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 \\\\ \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} \\\\ \\end{array}\\right] \\] Types of covariance structures: in a standard linear regression model, we have constant variance and no covariance: \\[ \\Sigma = \\left[ \\begin{array}{ccc} \\sigma^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2 \\\\ \\end{array}\\right] \\] next, relax the assumption of equal variances, and estimate each separately. In this case of heterogeneous variances, we might see more or less variance over time, for example. \\[ \\Sigma = \\left[ \\begin{array}{ccc} \\sigma_1^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_3^2 \\\\ \\end{array}\\right] \\] we actually want to get at the underlying covariance/correlation. I’ll switch to the correlation representation, but you can still think of the variances as constant or separately estimated. So now we have something like this, where \\(\\rho\\) represents the residual correlation among observations. \\[ \\Sigma = \\sigma^2 \\left[ \\begin{array}{ccc} 1 &amp; \\rho_1 &amp; \\rho_2 \\\\ \\rho_1 &amp; 1 &amp; \\rho_3 \\\\ \\rho_2 &amp; \\rho_3 &amp; 1 \\\\ \\end{array}\\right] \\] \\(\\rightarrow\\) unstructured / symmetric correlation structure (compound symmetry) Autocorrelation (lag of order one for residuals): \\[ \\Sigma = \\sigma^2 \\left[ \\begin{array}{cccc} 1 &amp; \\rho &amp; \\rho^2 &amp; \\rho^3 \\\\ \\rho &amp; 1 &amp; \\rho &amp; \\rho^2 \\\\ \\rho^2 &amp; \\rho &amp; 1 &amp; \\rho \\\\ \\rho^3 &amp; \\rho^2 &amp; \\rho &amp; 1 \\\\ \\end{array}\\right] \\] 20.6.1 Heterogeneous variance library(nlme) gpa_hetero_res &lt;- lme( gpa ~ occasion, data = gpa, random = ~ 1 | student, weights = varIdent(form = ~ 1 | occasion) ) mixedup::extract_fixed_effects(gpa_hetero_res) %&gt;% knitr::kable() term value se z p_value lower_2.5 upper_97.5 Intercept 2.599 0.026 99.002 0 2.547 2.650 occasion 0.106 0.004 26.317 0 0.098 0.114 mixedup::extract_vc(gpa_hetero_res, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop student Intercept 0.094 0.306 0.404 Residual 0.138 0.372 0.596 alternative approach to heterogeneous variance models: library(glmmTMB) gpa_hetero_res2 &lt;- glmmTMB( gpa ~ occasion + ( 1 | student ) + diag( 0 + occas | student ), data = gpa ) Comparing results of {nlme} and {glmmTMB} tibble(relative_val = c(1, coef(gpa_hetero_res$modelStruct$varStruct, unconstrained = FALSE))) %&gt;% mutate(absolute_val = (relative_val * gpa_hetero_res$sigma) ^ 2, `hetero_res (nlme)` = mixedup::extract_het_var(gpa_hetero_res, scale = &#39;var&#39;, digits = 5) %&gt;% unname() %&gt;% as.vector() %&gt;% t() %&gt;% .[,1], `hetero_res (glmmTMB)` = mixedup::extract_het_var(gpa_hetero_res2, scale = &#39;var&#39;, digits = 5) %&gt;% dplyr::select(-group) %&gt;% unname() %&gt;% as.vector() %&gt;% t() %&gt;% .[,1]) %&gt;% knitr::kable() relative_val absolute_val hetero_res (nlme) hetero_res (glmmTMB) 1.0000000 0.1381504 0.13815 0.13790 0.8261186 0.0942837 0.09428 0.09415 0.6272415 0.0543528 0.05435 0.05430 0.4311126 0.0256764 0.02568 0.02568 0.3484013 0.0167692 0.01677 0.01677 0.4324628 0.0258374 0.02584 0.02580 20.6.2 Autocorrelation gpa_autocorr &lt;- lme( gpa ~ occasion, data = gpa, random = ~ 1 | student, correlation = corAR1(form = ~ occasion) ) mixedup::extract_fixed_effects(gpa_autocorr) %&gt;% knitr::kable() term value se z p_value lower_2.5 upper_97.5 Intercept 2.597 0.023 113.146 0 2.552 2.642 occasion 0.107 0.005 20.297 0 0.097 0.118 mixedup::extract_vc(gpa_autocorr, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop student Intercept 0.046 0.215 0.381 Residual 0.075 0.273 0.619 gpa_autocorr2 &lt;- glmmTMB( gpa ~ occasion + ar1( 0 + occas | student ) + ( 1 | student ), # occas is cotegorical version of occasion data = gpa ) mixedup::extract_fixed_effects(gpa_autocorr2) %&gt;% knitr::kable() term value se z p_value lower_2.5 upper_97.5 Intercept 2.598 0.023 111.077 0 2.552 2.643 occasion 0.107 0.005 19.458 0 0.096 0.118 mixedup::extract_vc(gpa_autocorr2, ci_level = 0) %&gt;% knitr::kable() group variance sd var_prop student 0.093 0.305 0.159 student.1 0.000 0.000 0.000 Residual 0.028 0.167 0.048 20.7 Generalized Linear Mixed Models load(&quot;data/speed_dating.RData&quot;) sdating &lt;- glmer( decision ~ sex + samerace + attractive_sc + sincere_sc + intelligent_sc + ( 1 | iid), data = speed_dating, family = binomial ) mixedup::extract_fixed_effects(sdating) %&gt;% knitr::kable() term value se z p_value lower_2.5 upper_97.5 Intercept -0.743 0.121 -6.130 0.00 -0.981 -0.506 sexMale 0.156 0.164 0.954 0.34 -0.165 0.478 sameraceYes 0.314 0.075 4.192 0.00 0.167 0.460 attractive_sc 0.502 0.015 33.559 0.00 0.472 0.531 sincere_sc 0.089 0.016 5.747 0.00 0.059 0.120 intelligent_sc 0.143 0.017 8.232 0.00 0.109 0.177 mixedup::extract_vc(sdating, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop iid Intercept 2.708 1.645 1 20.8 Issues/ Considderations small number of clusters: problematic - similar to number of samples to compute variance / mean or similar (to get to the variance component we need enough groups). This also touches whether something should be a fixed or a random effect (random is always possible when the number of clusters is large enough) small number of observations within clusters: no problem, but might prevent random slopes (for n == 1) balanced design / missing data: not really a requirement, so as long as it is not extreme likely not an issue 20.8.1 Model comparison Using AIC can help, but should not used to make the decision—reasoning about the implications of the used models should. gpa_1 &lt;- lmer(gpa ~ occasion + (1 + occasion | student), data = gpa) gpa_2 &lt;- lmer(gpa ~ occasion + sex + (1 + occasion | student), data = gpa) gpa_3 &lt;- lmer(gpa ~ occasion + (1 | student), data = gpa) list(gpa_1 = gpa_1, gpa_2 = gpa_2, gpa_3 = gpa_3) %&gt;% map_df(function(mod) data.frame(AIC = AIC(mod)), .id = &#39;model&#39;) %&gt;% arrange(AIC) %&gt;% mutate(`Δ AIC` = AIC - min(AIC)) %&gt;% knitr::kable() model AIC Δ AIC gpa_2 269.7853 0.000000 gpa_1 272.9566 3.171217 gpa_3 416.8929 147.107536 20.9 Formula summary formula meaning (1|group) random group intercept (x|group) = (1+x|group) random slope of x within group with correlated intercept (0+x|group) = (-1+x|group) random slope of x within group: no variation in intercept (1|group) + (0+x|group) uncorrelated random intercept and random slope within group (1|site/block) = (1|site)+(1|site:block) intercept varying among sites and among blocks within sites (nested random effects) site+(1|site:block) fixed effect of sites plus random variation in intercept among blocks within sites (x|site/block) = (x|site)+(x|site:block) = (1 + x|site)+(1+x|site:block) slope and intercept varying among sites and among blocks within sites (x1|site)+(x2|block) two different effects, varying at different levels x*site+(x|site:block) fixed effect variation of slope and intercept varying among sites and random variation of slope and intercept among blocks within sites (1|group1)+(1|group2) intercept varying among crossed random effects (e.g. site, year) equation formula \\(β_0 + β_{1}X_{i} + e_{si}\\) n/a (Not a mixed-effects model) \\((β_0 + b_{S,0s}) + β_{1}X_i + e_{si}\\) ∼ X + (1∣Subject) \\((β_0 + b_{S,0s}) + (β_{1} + b_{S,1s}) X_i + e_{si}\\) ~ X + (1 + X∣Subject) \\((β_0 + b_{S,0s} + b_{I,0i}) + (β_{1} + b_{S,1s}) X_i + e_{si}\\) ∼ X + (1 + X∣Subject) + (1∣Item) As above, but \\(S_{0s}\\), \\(S_{1s}\\) independent ∼ X + (1∣Subject) + (0 + X∣ Subject) + (1∣Item) \\((β_0 + b_{S,0s} + b_{I,0i}) + β_{1}X_i + e_{si}\\) ∼ X + (1∣Subject) + (1∣Item) \\((β_0 + b_{I,0i}) + (β_{1} + b_{S,1s})X_i + e_{si}\\) ∼ X + (0 + X∣Subject) + (1∣Item) × "],["references-and-session-info.html", "21 References and Session Info 21.1 Session Info 21.2 References", " 21 References and Session Info 21.1 Session Info 21.1.1 R settings sessionInfo() #&gt; R version 4.0.3 (2020-10-10) #&gt; Platform: x86_64-pc-linux-gnu (64-bit) #&gt; Running under: Ubuntu 20.04.4 LTS #&gt; #&gt; Matrix products: default #&gt; BLAS: /usr/local/lib/R/lib/libRblas.so #&gt; LAPACK: /usr/local/lib/R/lib/libRlapack.so #&gt; #&gt; locale: #&gt; [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C #&gt; [3] LC_TIME=de_DE.UTF-8 LC_COLLATE=en_US.UTF-8 #&gt; [5] LC_MONETARY=de_DE.UTF-8 LC_MESSAGES=en_US.UTF-8 #&gt; [7] LC_PAPER=de_DE.UTF-8 LC_NAME=C #&gt; [9] LC_ADDRESS=C LC_TELEPHONE=C #&gt; [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C #&gt; #&gt; attached base packages: #&gt; [1] parallel stats graphics grDevices utils datasets methods #&gt; [8] base #&gt; #&gt; other attached packages: #&gt; [1] reticulate_1.22 bayesplot_1.8.1 mixedup_0.3.9 #&gt; [4] tidybayes_3.0.1 rethinking_2.21 cmdstanr_0.4.0.9000 #&gt; [7] rstan_2.21.2 StanHeaders_2.21.0-7 ggdag_0.2.3.9000 #&gt; [10] GGally_2.1.2 ggtext_0.1.1 brms_2.16.1 #&gt; [13] Rcpp_1.0.8 EnvStats_2.4.0 ggraph_2.0.5.9000 #&gt; [16] tidygraph_1.2.0 glue_1.6.2 patchwork_1.1.0.9000 #&gt; [19] prismatic_1.0.0.9000 forcats_0.5.1 stringr_1.4.0 #&gt; [22] dplyr_1.0.8 purrr_0.3.4 readr_1.4.0 #&gt; [25] tidyr_1.2.0 tibble_3.1.6 ggplot2_3.3.5 #&gt; [28] tidyverse_1.3.0.9000 #&gt; #&gt; loaded via a namespace (and not attached): #&gt; [1] readxl_1.3.1 backports_1.2.1 plyr_1.8.6 #&gt; [4] igraph_1.2.11 svUnit_1.0.6 splines_4.0.3 #&gt; [7] crosstalk_1.1.1 rstantools_2.1.1 inline_0.3.17 #&gt; [10] digest_0.6.29 htmltools_0.5.2 viridis_0.5.1 #&gt; [13] rsconnect_0.8.24 fansi_1.0.2 magrittr_2.0.2 #&gt; [16] checkmate_2.0.0 graphlayouts_0.7.1 modelr_0.1.8 #&gt; [19] RcppParallel_5.0.2 matrixStats_0.61.0 xts_0.12.1 #&gt; [22] prettyunits_1.1.1 colorspace_2.0-3 rvest_1.0.0 #&gt; [25] ggrepel_0.9.1 ggdist_3.0.1 haven_2.3.1 #&gt; [28] xfun_0.29 callr_3.6.0 crayon_1.5.0 #&gt; [31] jsonlite_1.8.0 lme4_1.1-26 zoo_1.8-8 #&gt; [34] polyclip_1.10-0 gtable_0.3.0 V8_3.4.0 #&gt; [37] distributional_0.2.2 pkgbuild_1.2.0 shape_1.4.5 #&gt; [40] abind_1.4-5 scales_1.1.1 mvtnorm_1.1-2 #&gt; [43] DBI_1.1.2 miniUI_0.1.1.1 gridtext_0.1.4 #&gt; [46] viridisLite_0.4.0 xtable_1.8-4 stats4_4.0.3 #&gt; [49] DT_0.17 htmlwidgets_1.5.3 httr_1.4.2 #&gt; [52] threejs_0.3.3 arrayhelpers_1.1-0 RColorBrewer_1.1-2 #&gt; [55] posterior_1.1.0 ellipsis_0.3.2 reshape_0.8.8 #&gt; [58] pkgconfig_2.0.3 loo_2.4.1 farver_2.1.0 #&gt; [61] sass_0.4.0.9000 dbplyr_2.1.1 utf8_1.2.2 #&gt; [64] tidyselect_1.1.2 rlang_1.0.1 reshape2_1.4.4 #&gt; [67] later_1.3.0 munsell_0.5.0 cellranger_1.1.0 #&gt; [70] tools_4.0.3 cli_3.2.0 generics_0.1.2 #&gt; [73] broom_0.7.6 ggridges_0.5.2 evaluate_0.14 #&gt; [76] fastmap_1.1.0 yaml_2.2.3 processx_3.5.1 #&gt; [79] knitr_1.37 fs_1.5.2 nlme_3.1-149 #&gt; [82] mime_0.12 projpred_2.0.2 xml2_1.3.3 #&gt; [85] compiler_4.0.3 shinythemes_1.2.0 rstudioapi_0.13 #&gt; [88] png_0.1-7 curl_4.3.2 gamm4_0.2-6 #&gt; [91] reprex_2.0.0 statmod_1.4.35 tweenr_1.0.2 #&gt; [94] bslib_0.3.1 stringi_1.7.6 ps_1.6.0 #&gt; [97] Brobdingnag_1.2-6 lattice_0.20-41 Matrix_1.2-18 #&gt; [100] nloptr_1.2.2.2 markdown_1.1 shinyjs_2.0.0 #&gt; [103] tensorA_0.36.1 vctrs_0.3.8 pillar_1.7.0 #&gt; [106] lifecycle_1.0.1 jquerylib_0.1.4 bridgesampling_1.1-2 #&gt; [109] httpuv_1.6.5 R6_2.5.1 bookdown_0.24 #&gt; [112] promises_1.2.0.1 gridExtra_2.3 codetools_0.2-16 #&gt; [115] boot_1.3-25 colourpicker_1.1.1 MASS_7.3-53 #&gt; [118] gtools_3.8.2 assertthat_0.2.1 withr_2.4.3 #&gt; [121] shinystan_2.5.0 mgcv_1.8-33 hms_1.1.1 #&gt; [124] grid_4.0.3 coda_0.19-4 minqa_1.2.4 #&gt; [127] rmarkdown_2.11.15 ggforce_0.3.2.9000 shiny_1.6.0 #&gt; [130] lubridate_1.7.10 base64enc_0.1-3 dygraphs_1.1.1.6 21.1.2 Python settings import arviz as az import matplotlib.pyplot as plt import numpy as np import pymc3 as pm import scipy.stats as stats import seaborn as sns import matplotlib from matplotlib.colors import ListedColormap import matplotlib.font_manager import session_info session_info.show(html = False) #&gt; ----- #&gt; arviz 0.11.4 #&gt; matplotlib 3.5.1 #&gt; numpy 1.21.4 #&gt; pymc3 3.11.4 #&gt; scipy 1.7.3 #&gt; seaborn 0.11.2 #&gt; session_info 1.0.0 #&gt; ----- #&gt; IPython 7.29.0 #&gt; jupyter_client 7.1.2 #&gt; jupyter_core 4.9.1 #&gt; jupyterlab 3.2.9 #&gt; notebook 6.4.8 #&gt; ----- #&gt; Python 3.8.8 (default, Apr 13 2021, 20:00:26) [GCC 7.3.0] #&gt; Linux-5.13.0-35-generic-x86_64-with-glibc2.10 #&gt; ----- #&gt; Session information updated at 2022-03-21 20:03 21.2 References "]]
