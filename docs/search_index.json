[
["index.html", "Statistical Exploration a small notebook to keep track 1 Intro", " Statistical Exploration a small notebook to keep track Kosmas Hench 2022-01-05 1 Intro So far, the journey includes Statistical Rethinking (McElreath 2020) and Bayesian Statistics the Fun Way (Kurt 2019). × "],
["rethinking-chapter-1.html", "2 Rethinking: Chapter 1", " 2 Rethinking: Chapter 1 The Golem of Prague by Richard McElreath, building on the Summary by Solomon Kurz × "],
["rethinking-chapter-2.html", "3 Rethinking: Chapter 2 3.1 Counting possibilities 3.2 Building a Model 3.3 Making the model go / Bayes’ Theorem 3.4 Motors: Grid Approximation 3.5 Quadratic Approximation 3.6 Marcov Chain Monte Carlo (MCMC) 3.7 Homework 3.8 {brms} section 3.9 pymc3 section", " 3 Rethinking: Chapter 2 Small Worlds and Large Worlds by Richard McElreath, building on the Summary by Solomon Kurz 3.1 Counting possibilities d &lt;- tibble(p1 = 0, p2 = rep(1:0, times = c(1, 3)), p3 = rep(1:0, times = c(2, 2)), p4 = rep(1:0, times = c(3, 1)), p5 = 1) p1 p2 p3 p4 p5 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 d %&gt;% mutate(turn = 1:4)%&gt;% pivot_longer(p1:p5, names_to = &quot;prob&quot;, values_to = &quot;realization&quot;) %&gt;% arrange(prob, turn) %&gt;% mutate(prob = factor(prob, levels = c(&quot;p5&quot;, &quot;p4&quot;, &quot;p3&quot;, &quot;p2&quot;, &quot;p1&quot;)), marble = c(&quot;white&quot;, &quot;dark&quot;)[realization + 1]) %&gt;% ggplot( aes( x = turn, y = prob ) ) + geom_point(shape = 21, size = 4, aes( fill = marble, color = after_scale(clr_darken(fill)))) + geom_text(data = tibble(x = rep(c(.65, 4.45), each = 5), y = rep(str_c(&quot;p&quot;,1:5), 2), label = rep(c(&quot;[&quot;, &quot;]&quot;), each = 5), vjust = .7), aes( x = x, y = y, label = label), family = fnt_sel, size = 6)+ scale_fill_manual(values = c(white = clr0, dark = clrd)) + theme(legend.position = &quot;bottom&quot;) tibble(draws = 1:3, marbles = 4) %&gt;% mutate(possibilities = marbles ^ draws) %&gt;% flextable::flextable() .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-bc8201d6{}.cl-bc7cdc74{font-family:'DejaVu Sans';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-bc7cf024{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-bc7d1cca{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bc7d1ce8{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-bc7d1cf2{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} drawsmarblespossibilities14424163464 layout_round &lt;- function(round = 1, n = 4, angle = 360, start_angle = 0, p = .5, round_prefix = &quot;&quot;){ n_round &lt;- n ^ round tibble(idx_round = 1:n_round, idx_round_sacaled = scales::rescale(idx_round, from = c(.5, n_round+.5), to = c(0, 1) * angle/360 + start_angle/360), idx_draw = rep(1:n, n_round/n), idx_parent = ((idx_round - 1 ) %/% n) + 1, name_parent = str_c(round_prefix, round - 1, &quot;_&quot;, idx_parent), name = str_c(round_prefix, round, &quot;_&quot;, idx_round), x = sin(idx_round_sacaled * 2 * pi) * round, y = cos(idx_round_sacaled * 2 * pi) * round) %&gt;% mutate(marble = c(&quot;white&quot;, &quot;dark&quot;)[1 + ((idx_draw/n) &lt;= p)], round_prefix = round_prefix, round = round) } links_round &lt;- function(round = 1, n = 4, round_prefix = &quot;&quot;){ n_round &lt;- n ^ round n_prev &lt;- n ^ (round - 1) tibble(idx_round = 1:n_round, idx_parent = ((idx_round - 1 ) %/% n) + 1, from = str_c(round_prefix, round - 1, &quot;_&quot;, idx_parent), to = str_c(round_prefix,round, &quot;_&quot;, idx_round), round_prefix = round_prefix) } round_origin &lt;- origin_round &lt;- function(round_prefix = &quot;&quot;){ tibble(idx_round = 0, idx_round_sacaled = 0, idx_draw = 0, name = str_c(round_prefix, &quot;0_1&quot;), x = 0, y = 0, marble = NA) } marble_graph &lt;- function(n_rounds = 3, n_draws = 4, angle = 360,start_angle = 0, p = .5, round_prefix = &quot;&quot;){ tbl_graph(nodes = 1:n_rounds %&gt;% map_dfr(layout_round, n = n_draws, angle = angle, start_angle = start_angle, p = p, round_prefix = round_prefix) %&gt;% bind_rows(round_origin(round_prefix = round_prefix), .), edges = 1:n_rounds %&gt;% map_dfr(links_round, round_prefix = round_prefix)) %E&gt;% mutate(marble = .N()$marble[to], to_name = .N()$name[to], from_name = .N()$name[from]) } marble_graph(p = .25, n_rounds = 3, angle = 180, start_angle = -90) %&gt;% ggraph( layout = &quot;manual&quot;, x = . %N&gt;% as_tibble() %&gt;% .$x, y = . %N&gt;% as_tibble() %&gt;% .$y) + geom_node_point(aes(fill = marble, color = marble, size = -round), shape = 21) + geom_edge_link(aes(color = marble), start_cap = circle(2, &#39;mm&#39;), end_cap = circle(2, &#39;mm&#39;)) + coord_equal() + scale_color_manual(values = clr_darken(c(white = clr0, dark = clrd)), na.value = &quot;transparent&quot;, guide = &quot;none&quot;) + scale_fill_manual(values = c(white = clr0, dark = clrd), na.value = &quot;transparent&quot;) + scale_edge_color_manual(values = c(white = clr_darken(clr0, .2), dark = clrd), guide = &quot;none&quot;) + scale_size_continuous(range = c(1.5,3), guide = &quot;none&quot;) + guides(fill = guide_legend(override.aes = list(size = 4)), edge_alpha = &quot;none&quot;) + labs(title = &quot;p = 0.25&quot;) + theme(legend.position = &quot;bottom&quot;) n_deviders &lt;- 3 n_rounds &lt;- 3 dividers &lt;- tibble(x = rep(0,n_deviders), y = x, tau = seq(from = 0, to = 2*pi, length.out = n_deviders + 1)[2:(n_deviders+1)], xend = sin(tau) * (n_rounds + .1), yend = cos(tau) * (n_rounds + .1)) p_trials &lt;- c(.25, .5, .75) all_conjectures &lt;- tibble(start_angle = c(0, 120, 240), round_prefix = c(&quot;r1_&quot; ,&quot;r2_&quot;, &quot;r3_&quot;), p = p_trials) %&gt;% pmap(marble_graph, angle = 120) %&gt;% reduce(bind_graphs) na_to_false &lt;- function(x){x[is.na(x)] &lt;- FALSE; x} na_to_true &lt;- function(x){x[is.na(x)] &lt;- TRUE; x} tester &lt;- function(x){x$name[x$r1_right]} trial_sequence &lt;- c(&quot;white&quot;, &quot;dark&quot;)[c(2,1,2)] selectors &lt;- all_conjectures %N&gt;% mutate(r1_right = (round == 1 &amp; marble == trial_sequence[1]) %&gt;% na_to_true(), r2_still_in = name_parent %in% name[r1_right], r2_right = r2_still_in &amp; (round == 2 &amp; marble == trial_sequence[2]), r3_still_in = name_parent %in% name[r2_right], r3_right = r3_still_in &amp; (round == 3 &amp; marble == trial_sequence[3]), on_path = r1_right | r2_right |r3_right) %&gt;% as_tibble() %&gt;% filter(on_path) selector_results &lt;- selectors %&gt;% filter(round == n_rounds) %&gt;% group_by(round_prefix) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(tau = seq(from = 0, to = 2*pi, length.out = n_rounds + 1)[2:(n_rounds+1)] - (2*pi)/(n_rounds * 2), x = sin(tau) * (n_rounds + .5), y = cos(tau) * (n_rounds + .5)) all_conjectures %&gt;% ggraph( layout = &quot;manual&quot;, x = . %N&gt;% as_tibble() %&gt;% .$x, y = . %N&gt;% as_tibble() %&gt;% .$y) + geom_node_point(aes(fill = marble, color = marble, size = -round, alpha = name %in% selectors$name), shape = 21) + geom_edge_link(aes(color = marble, alpha = to_name %in% selectors$name), start_cap = circle(2, &#39;mm&#39;), end_cap = circle(2, &#39;mm&#39;)) + geom_segment(data = dividers, aes(x = x, y = y, xend = xend, yend = yend), color = clr_darken(&quot;white&quot;,.10)) + geom_text(data = selector_results, aes( x = x, y = y, label = n), family = fnt_sel, size = 6) + coord_equal() + scale_color_manual(values = clr_darken(c(white = clr0, dark = clrd)), na.value = &quot;transparent&quot;, guide = &quot;none&quot;) + scale_fill_manual(values = c(white = clr0, dark = clrd), na.value = &quot;transparent&quot;) + scale_edge_color_manual(values = c(white = clr_darken(clr0, .2), dark = clrd), guide = &quot;none&quot;) + scale_size_continuous(range = c(1.5,3), guide = &quot;none&quot;) + scale_alpha_manual(values = c(`TRUE` = 1, `FALSE` = .2), guide = &quot;none&quot;) + guides(fill = guide_legend(override.aes = list(size = 4)), edge_alpha = &quot;none&quot;) + labs(caption = str_c(trial_sequence,collapse = &quot;-&quot;)) + theme(legend.position = &quot;bottom&quot;) html_marbles &lt;- c( glue(&quot;&lt;span style=&#39;color:{clr0};filter:drop-shadow(0px 0px 1px black)&#39;&gt;\\U2B24&lt;/span&gt;&quot;), glue(&quot;&lt;span style=&#39;color:{clr1l};filter:drop-shadow(0px 0px 1px black)&#39;&gt;\\U2B24&lt;/span&gt;&quot;)) html_conjecture &lt;- function(x){ str_c(&quot;[ &quot;,str_c(html_marbles[c(x)+1], collapse = &quot; &quot;),&quot; ]&quot;) } tibble(conjectures = list(rep(0,4), rep(1:0, c(1,3)), rep(1:0, c(2,2)), rep(1:0, c(3,1)), rep(1,4)), conjecture = map_chr(conjectures, html_conjecture), ways = map_dbl(conjectures, sum), p = c(0, p_trials, 1), `ways data/prior counts` = c(0, selector_results$n, 0), `new count` = map2_chr( `ways data/prior counts`, ways, .f = function(x,y){glue(&quot;{x} $\\\\times$ {y} = {x * y}&quot;)}), plausibility = (`ways data/prior counts` / sum(`ways data/prior counts`)) %&gt;% round(digits = 2) ) %&gt;% rename(`ways to produce &lt;span style=&#39;color:#85769EFF;filter:drop-shadow(0px 0px 1px black)&#39;&gt;⬤&lt;/span&gt;` = &quot;ways&quot;) %&gt;% dplyr::select(-conjectures) %&gt;% knitr::kable() conjecture ways to produce ⬤ p ways data/prior counts new count plausibility [ ⬤ ⬤ ⬤ ⬤ ] 0 0.00 0 0 \\(\\times\\) 0 = 0 0.00 [ ⬤ ⬤ ⬤ ⬤ ] 1 0.25 3 3 \\(\\times\\) 1 = 3 0.15 [ ⬤ ⬤ ⬤ ⬤ ] 2 0.50 8 8 \\(\\times\\) 2 = 16 0.40 [ ⬤ ⬤ ⬤ ⬤ ] 3 0.75 9 9 \\(\\times\\) 3 = 27 0.45 [ ⬤ ⬤ ⬤ ⬤ ] 4 1.00 0 0 \\(\\times\\) 4 = 0 0.00 3.2 Building a Model d &lt;- tibble(toss = c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;), n_trials = 1:9, sequence = n_trials %&gt;% map_chr(.f = function(x, chr){str_c(chr[1:x], collapse = &quot;&quot;)}, chr = toss), n_success = cumsum(toss == &quot;w&quot;), lag_n_trials = lag(n_trials, default = 0), lag_n_success = lag(n_success, default = 0)) sequence_length &lt;- 50 d %&gt;% expand(nesting(n_trials, toss, n_success), p_water = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% group_by(p_water) %&gt;% # you can learn more about lagging here: https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/lag or here: https://dplyr.tidyverse.org/reference/lead-lag.html mutate(lagged_n_trials = lag(n_trials, k = 1), lagged_n_success = lag(n_success, k = 1)) #&gt; # A tibble: 450 x 6 #&gt; # Groups: p_water [50] #&gt; n_trials toss n_success p_water lagged_n_trials lagged_n_success #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 w 1 0 NA NA #&gt; 2 1 w 1 0.0204 NA NA #&gt; 3 1 w 1 0.0408 NA NA #&gt; 4 1 w 1 0.0612 NA NA #&gt; 5 1 w 1 0.0816 NA NA #&gt; 6 1 w 1 0.102 NA NA #&gt; 7 1 w 1 0.122 NA NA #&gt; 8 1 w 1 0.143 NA NA #&gt; 9 1 w 1 0.163 NA NA #&gt; 10 1 w 1 0.184 NA NA #&gt; # … with 440 more rows stat_binom &lt;- function(n_trials, n_success, lag_n_trials, lag_n_success, sequence, ...){ if(n_trials == 1) { g_lag &lt;- stat_function(data = tibble(n_trials = n_trials, sequence = sequence), fun = function(x){1}, xlim = c(0,1), linetype = 3) } else { g_lag &lt;- stat_function(data = tibble(n_trials = n_trials, sequence = sequence), fun = function(x){ f &lt;- function(x){ dbinom(x = lag_n_success, size = lag_n_trials, prob = x)} f(x)/ integrate(f = f,lower = 0, upper = 1)[[1]]}, xlim = c(0,1), n = 500, linetype = 3) } g_current &lt;- stat_function(data = tibble(n_trials = n_trials, sequence = sequence), fun = function(x){ f &lt;- function(x){ dbinom(x = n_success, size = n_trials, prob = x)} f(x)/ integrate(f = f,lower = 0,upper = 1)[[1]]}, xlim = c(0,1),n = 500) list( g_lag, g_current) } ggplot() + (d %&gt;% pmap(stat_binom) %&gt;% unlist()) + facet_wrap(str_c(n_trials,&quot;: &quot;, sequence) ~ .) 3.3 Making the model go / Bayes’ Theorem \\[ Pr(\\textit{p} | W, L) = \\frac{Pr(W, L | \\textit{p}) ~ Pr(\\textit{p})}{Pr(W,L)}\\\\ Posterior = \\frac{Probability~of~the~Data \\times Prior}{ Average~probability~of~the~Data} \\] f_posterior_unscaled &lt;- function(f_porior, f_like){ function(x){ f_porior(x) * f_like(x)} } f_parts &lt;- c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;) gg_posterior &lt;- function(f_porior, f_like, comp = 1){ list( stat_function(data = tibble(part = factor(&quot;prior&quot;, levels = f_parts), comp = comp), fun = f_porior, xlim = c(0,1), n = 500, geom = &quot;area&quot;, color = clr2, fill = fll2), stat_function(data = tibble(part = factor(&quot;likelihood&quot;, levels = f_parts), comp = comp), fun = f_like, xlim = c(0,1), n = 500, geom = &quot;area&quot;, color = clr2, fill = fll2), stat_function(data = tibble(part = factor(&quot;posterior&quot;, levels = f_parts), comp = comp), fun = f_posterior_unscaled(f_porior = f_porior, f_like = f_like), xlim = c(0,1), n = 500, geom = &quot;area&quot;, color = clr2, fill = fll2) ) } scale_fun &lt;- function(f){ # marginal likelihood function(x){f(x) / integrate(f = f, lower = 0, upper = 1)[[1]]} } f_like_in &lt;- function(x){dbeta(x = x, shape1 = 8, shape2 = 5)} f_uni &lt;- function(x){1} f_step &lt;- function(x){if_else(x &lt; .5, 0, 1)} f_peak &lt;- function(x){if_else(x &lt; .5, (x * 2)^3, ((1 - x) * 2)^3)} ggplot() + gg_posterior(f_porior = f_uni, f_like = f_like_in) + gg_posterior(f_porior = f_step, f_like = f_like_in, comp = 2) + gg_posterior(f_porior = f_peak, f_like = f_like_in, comp = 3) + facet_wrap(comp ~ part, scales = &quot;free_y&quot;) 3.4 Motors: Grid Approximation grid_approx &lt;- function(n_grid = 20, W = 6, L = 3, prior = function(x){rep(1, length(x))}){ tibble(p_grid = seq(0, 1, length.out = n_grid), prior = prior(p_grid), likelihood = dbinom(W, size = W + L, prob = p_grid), posterior_unstand = likelihood * prior, posterior = posterior_unstand / sum(posterior_unstand)) } plot_grid_approx &lt;- function(data){ data %&gt;% ggplot(aes(x = p_grid, y = posterior, color = posterior)) + ggforce::geom_link2() + geom_point(shape = 21, fill = &quot;white&quot;, size = 2.5) + scale_color_gradientn(colours = c(clr_darken(clr0), clr2), guide = &quot;none&quot;) + labs(title = glue(&quot;{length(data$p_grid)} grid points&quot;)) } c(4, 7, 15, 60) %&gt;% map(grid_approx) %&gt;% map(plot_grid_approx) %&gt;% wrap_plots(nrow = 1) &amp; scale_x_continuous(breaks = c(0, .5, 1)) Note how the y scale depends on the number of grid points: the peak reaches ~0.75 for 4 points, but only ~ 0.043 for 60 points. 3.5 Quadratic Approximation library(rethinking) map &lt;- purrr::map compare_qa &lt;- function(w_in, l_in){ globe_qa &lt;- quap( alist( W ~ dbinom( W + L, p ), # binomial likelihood p ~ dunif( 0, 1 ) # uniform prior ), data = list( W = w_in, L = l_in ) ) qa_results &lt;- precis(globe_qa) %&gt;% as_tibble() %&gt;% mutate(qa = glue(&quot;W: {w_in}, L: {l_in}&quot;)) qa_results %&gt;% knitr::kable() %&gt;% print() ggplot() + stat_function(fun = function(x){ dbeta( shape1 = w_in + 1, shape2 = l_in + 1, x = x ) / integrate(f = function(x){ dbeta( shape1 = w_in + 1, shape2 = l_in + 1, x = x )}, lower = 0, upper = 1)[[1]] }, xlim = c(0,1), n = 500, geom = &quot;area&quot;, color = clr0d, fill = fll0)+ stat_function(fun = function(x){ dnorm(x = x, mean = qa_results$mean, sd = qa_results$sd )}, xlim = c(0,1), n = 500, geom = &quot;line&quot;, color = clr2, linetype = 3) + labs(title = glue(&quot;W: {w_in}, L: {l_in}, n = {w_in + l_in}&quot;), y = &quot;density&quot;, x = &quot;proportion water&quot;) } compare_qa(w_in = 6, l_in = 3) + compare_qa(w_in = 12, l_in = 6) + compare_qa(w_in = 24, l_in = 12) #&gt; #&gt; #&gt; | mean| sd| 5.5%| 94.5%|qa | #&gt; |---------:|---------:|---------:|---------:|:----------| #&gt; | 0.6666664| 0.1571338| 0.4155362| 0.9177966|W: 6, L: 3 | #&gt; #&gt; #&gt; | mean| sd| 5.5%| 94.5%|qa | #&gt; |---------:|---------:|---------:|---------:|:-----------| #&gt; | 0.6666663| 0.1111104| 0.4890903| 0.8442422|W: 12, L: 6 | #&gt; #&gt; #&gt; | mean| sd| 5.5%| 94.5%|qa | #&gt; |---------:|---------:|---------:|---------:|:------------| #&gt; | 0.6666666| 0.0785669| 0.5411015| 0.7922317|W: 24, L: 12 | 3.6 Marcov Chain Monte Carlo (MCMC) n_samples &lt;- 1e4 p_init &lt;- rep( NA, n_samples ) p_init[1] &lt;- .5 manual_mcmc &lt;- function(p, W = 6, L = 3){ for ( i in 2:n_samples ) { p_new &lt;- rnorm( n = 1, mean = p[ i - 1], sd = 0.1) if ( p_new &lt; 0 ){ p_new &lt;- abs( p_new ) } if ( p_new &gt; 1 ){ p_new &lt;- 2 - p_new } q0 &lt;- dbinom( W, W + L, p[ i - 1 ] ) q1 &lt;- dbinom( W, W + L, p_new ) p[i] &lt;- if_else( runif(1) &lt; q1 / q0, p_new, p[i - 1] ) } p } p &lt;- manual_mcmc(p_init) p_36 &lt;- manual_mcmc(p_init, W = 24, L = 12) p_p &lt;- tibble(x = p) %&gt;% ggplot() + stat_function(fun = function(x){ dbeta( shape1 = 6 + 1, shape2 = 3 + 1, x = x ) / integrate(f = function(x){ dbeta( shape1 = 6 + 1, shape2 = 3 + 1, x = x )}, lower = 0, upper = 1)[[1]] }, xlim = c(0,1), n = 500, geom = &quot;area&quot;, aes(color = &quot;posterior&quot;, fill = after_scale(clr_alpha(color))))+ geom_density(aes(x = x, color = &quot;MCMC&quot;)) + scale_color_manual(&quot;approach&quot;, values = c(posterior = clr0, MCMC = clr2)) + labs(y = &quot;density&quot;, x = &quot;proportion water&quot;, title = &quot;n = 12&quot;) + theme(legend.position = &quot;bottom&quot;) p_p36 &lt;- tibble(x = p_36) %&gt;% ggplot() + stat_function(fun = function(x){ dbeta( shape1 = 24 + 1, shape2 = 12 + 1, x = x ) / integrate(f = function(x){ dbeta( shape1 = 24 + 1, shape2 = 12 + 1, x = x )}, lower = 0, upper = 1)[[1]] }, xlim = c(0,1), n = 500, geom = &quot;area&quot;, aes(color = &quot;posterior&quot;, fill = after_scale(clr_alpha(color))))+ geom_density(aes(x = x, color = &quot;MCMC&quot;)) + scale_color_manual(&quot;approach&quot;, values = c(posterior = clr0, MCMC = clr2)) + labs(y = &quot;density&quot;, x = &quot;proportion water&quot;, title = &quot;n = 36&quot;) + theme(legend.position = &quot;bottom&quot;) p_p + p_p36 library(rlang) chapter2_models &lt;- env( compare_qa = compare_qa ) write_rds(chapter2_models, &quot;envs/chapter2_models.rds&quot;) 3.7 Homework M1 plot_grid_approx &lt;- function(data){ data %&gt;% ggplot(aes(x = p_grid, y = posterior, color = posterior)) + ggforce::geom_link2() + geom_point(shape = 21, fill = &quot;white&quot;, size = 2.5) + scale_color_gradientn(colours = c(clr_darken(clr0), clr2), guide = &quot;none&quot;) + labs(title = glue(&quot;{length(data$p_grid)} grid points&quot;)) } tibble(n_grid = rep(c(4,7,20), 3), L = rep(c(0,1,2), each = 3), W = rep(c(3,3,5), each = 3)) %&gt;% pmap(grid_approx) %&gt;% map(plot_grid_approx) %&gt;% wrap_plots(nrow = 3) &amp; scale_x_continuous(breaks = c(0, .5, 1)) M2 tibble(n_grid = rep(c(4,7,20), 3), L = rep(c(0,1,2), each = 3), W = rep(c(3,3,5), each = 3)) %&gt;% pmap(grid_approx, prior = f_step ) %&gt;% map(plot_grid_approx) %&gt;% wrap_plots(nrow = 3) &amp; scale_x_continuous(breaks = c(0, .5, 1)) M3 \\[ Pr(Earth | Land) = \\frac{Pr(Land | Earth) \\times Pr(Earth)}{Pr(Land)} \\] p_l_on_earth &lt;- .3 p_l_on_mars &lt;- 1 p_earth &lt;- .5 average_p_l &lt;- .5 * (p_l_on_earth + p_l_on_mars) (p_earth_on_l &lt;- p_l_on_earth * p_earth / average_p_l) #&gt; [1] 0.2307692 M4 &amp; M5 cards &lt;- c(&quot;C1b1|C1b2&quot;, &quot;C2b1|C2w2&quot;, &quot;C3w1|C3w2&quot; ) conjectures &lt;- c(&quot;C1b1|C1b2&quot;, &quot;C1b2|C1b1&quot;, &quot;C2b1|C2w2&quot;, &quot;C2w2|C2b1&quot;, &quot;C3w1|C3w2&quot;, &quot;C3w2|C3w1&quot;) tibble(cards = c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;), ways = c(&quot;C1b1|C1b2 &amp; C1b2|C1b1&quot;, &quot;C2b1|C2w2&quot;, &quot;&quot;), ways_tor_produce_data = c(2, 1, 0), plausibility = (ways_tor_produce_data / sum(ways_tor_produce_data)) %&gt;% round(digits = 3), other_side = c(&quot;b&quot;, &quot;w&quot;, &quot;&quot;), prior = c(2,1,1), posterior = ((plausibility * prior) / sum(plausibility * prior)) %&gt;% round(digits = 3))%&gt;% knitr::kable() cards ways ways_tor_produce_data plausibility other_side prior posterior C1 C1b1|C1b2 &amp; C1b2|C1b1 2 0.667 b 2 0.8 C2 C2b1|C2w2 1 0.333 w 1 0.2 C3 0 0.000 1 0.0 M6 tibble(cards = c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;), ways = c(&quot;C1b1|C1b2 &amp; C1b2|C1b1&quot;, &quot;C2b1|C2w2&quot;, &quot;&quot;), ways_tor_produce_data = c(2, 1, 0), plausibility = (ways_tor_produce_data / sum(ways_tor_produce_data)) %&gt;% round(digits = 3), other_side = c(&quot;b&quot;, &quot;w&quot;, &quot;&quot;), prior = c(1,2,3), posterior = ((plausibility * prior) / sum(plausibility * prior)) %&gt;% round(digits = 3))%&gt;% knitr::kable() cards ways ways_tor_produce_data plausibility other_side prior posterior C1 C1b1|C1b2 &amp; C1b2|C1b1 2 0.667 b 1 0.5 C2 C2b1|C2w2 1 0.333 w 2 0.5 C3 0 0.000 3 0.0 M7 tibble(cards = c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;), ways = c(&quot;C1b1&gt;C2w2 &amp; C1b1&gt;C3w1 &amp; C1b1&gt;C3w2 &amp; C2b1&gt;C2w2 &amp; C2b1&gt;C3w1 &amp; C2b1&gt;C3w2&quot;, &quot;C2b1&gt;C3w1 &amp; C2b1&gt;C3w2&quot;, &quot;&quot;), ways_tor_produce_data = c(6, 2, 0), plausibility = (ways_tor_produce_data / sum(ways_tor_produce_data)) %&gt;% round(digits = 3))%&gt;% knitr::kable() cards ways ways_tor_produce_data plausibility C1 C1b1&gt;C2w2 &amp; C1b1&gt;C3w1 &amp; C1b1&gt;C3w2 &amp; C2b1&gt;C2w2 &amp; C2b1&gt;C3w1 &amp; C2b1&gt;C3w2 6 0.75 C2 C2b1&gt;C3w1 &amp; C2b1&gt;C3w2 2 0.25 C3 0 0.00 H1 \\[ Pr(twin | spec_a) = 0.2 \\\\ Pr(twin | spec_b) = 0.1 \\\\ Pr(twin) = 0.15 \\] \\[ Pr(spec_a | twin) = \\frac{Pr(spec_a) \\times Pr(twin | spec_a)}{Pr(twin)} \\\\ Pr(spec_b | twin) = \\frac{Pr(spec_b) \\times Pr(twin | spec_b)}{Pr(twin)} \\] pr_twn_on_a &lt;- .1 pr_twn_on_b &lt;- .2 pr_twn &lt;- (pr_twn_on_a + pr_twn_on_b) /2 prior_a &lt;- .5 pr_a_on_twn &lt;- (prior_a * pr_twn_on_a) / pr_twn pr_b_on_twn &lt;- ((1 - prior_a) * pr_twn_on_b) / pr_twn (p_next_twn &lt;- pr_a_on_twn * pr_twn_on_a + pr_b_on_twn * pr_twn_on_b) %&gt;% round(digits = 3) #&gt; [1] 0.167 H2 \\[ Pr(spec_a | twin) = \\frac{1}{3} \\] pr_a_on_twn #&gt; [1] 0.3333333 H3 \\[ Pr(single | spec_a) = Pr(\\neg twin | spec_a) = 1 - Pr(twin | spec_a) \\] \\[ Pr( spec_a | single) = \\frac{Pr(single|spec_a)Pr(spec_a)}{Pr(single)} \\] pr_sgl_on_a &lt;- 1 - pr_twn_on_a pr_sgl_on_b &lt;- 1 - pr_twn_on_b pr_sgl &lt;- weighted.mean(x = c(pr_sgl_on_a, pr_sgl_on_b), w = c(pr_a_on_twn, 1- pr_a_on_twn)) prior_a &lt;- pr_a_on_twn pr_a_on_sgl &lt;- (prior_a * pr_sgl_on_a) / pr_sgl pr_b_on_sgl &lt;- ((1 - prior_a) * pr_sgl_on_b) / pr_sgl tibble(pr_a_on_sgl = pr_a_on_sgl, pr_b_on_sgl = pr_b_on_sgl, control = pr_a_on_sgl + pr_b_on_sgl) %&gt;% round(digits = 4) %&gt;% knitr::kable() pr_a_on_sgl pr_b_on_sgl control 0.36 0.64 1 H4 \\[ Pr(spec_a | test ) = 0.8 \\\\ Pr(spec_b | test ) = 0.65 \\\\ Pr(spec_a | test ) = \\frac{Pr( test | spec_a ) \\times Pr(spec_a)}{Pr(test_positive)} \\] pr_testa_on_a &lt;- .8 pr_testb_on_b &lt;- .65 pr_testa_on_b &lt;- 1 - pr_testb_on_b prior_a &lt;- .5 pr_testa &lt;- (prior_a * pr_testa_on_a) + ((1- prior_a) * pr_testa_on_b) tibble(pr_on_testa = c(&quot;A&quot;,&quot;B&quot;), genetic_test = c((prior_a * pr_testa_on_a) / pr_testa, ((1 - prior_a) * pr_testa_on_b) / pr_testa)) #&gt; # A tibble: 2 x 2 #&gt; pr_on_testa genetic_test #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 A 0.696 #&gt; 2 B 0.304 prior_a_updated &lt;- pr_a_on_sgl pr_testa_updated &lt;- (prior_a_updated * pr_testa_on_a) + ((1- prior_a_updated) * pr_testa_on_b) tibble(pr_on_testa = c(&quot;A&quot;,&quot;B&quot;), genetic_test = c((prior_a_updated * pr_testa_on_a) / pr_testa_updated, ((1 - prior_a_updated) * pr_testa_on_b) / pr_testa_updated)) #&gt; # A tibble: 2 x 2 #&gt; pr_on_testa genetic_test #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 A 0.562 #&gt; 2 B 0.438 3.8 {brms} section brms_c2_36_tosses &lt;- brm( data = list(w = 24), family = binomial(link = &quot;identity&quot;), w | trials(36) ~ 0 + Intercept, prior(beta(1, 1), class = b, lb = 0, ub = 1), seed = 42, file = &quot;brms/brms_c2_36_tosses&quot; ) brms_c2_36_tosses %&gt;% summary() #&gt; Family: binomial #&gt; Links: mu = identity #&gt; Formula: w | trials(36) ~ 0 + Intercept #&gt; Data: list(w = 24) (Number of observations: 1) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; Intercept 0.66 0.08 0.49 0.80 1.00 1123 1813 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). posterior_summary(brms_c2_36_tosses) %&gt;% round(digits = 3) %&gt;% knitr::kable() Estimate Est.Error Q2.5 Q97.5 b_Intercept 0.657 0.078 0.492 0.798 lp__ -3.994 0.744 -6.055 -3.465 as_draws_df(brms_c2_36_tosses) %&gt;% as_tibble() %&gt;% ggplot(aes(x = b_Intercept)) + geom_density(color = clr1, fill = fll1) + scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) + labs(y = &quot;density&quot;, x = &quot;proportion water&quot;, title = &quot;{brms} posterior probability (n = 36)&quot;) 3.9 pymc3 section Taken from from pymc-devs… import arviz as az import matplotlib.pyplot as plt import numpy as np import pymc3 as pm import scipy.stats as stats import seaborn as sns import matplotlib from matplotlib.colors import ListedColormap import matplotlib.font_manager #matplotlib.font_manager.rebuild() ways = np.array([0, 3, 8, 9, 0]) ways / ways.sum() #&gt; array([0. , 0.15, 0.4 , 0.45, 0. ]) \\[ Pr(w | n, p) = \\frac{n!}{w! (n - w)!}p^{w}(1 - p)^{n-w} \\] stats.binom.pmf(6, n = 9, p = 0.5) #&gt; 0.16406250000000003 Computing the posterior using a grid approximation inside a python function. def posterior_grid_approx(grid_points = 5, success = 6, tosses = 9): &quot;&quot;&quot;&quot;&quot;&quot; # define grid p_grid = np.linspace(0, 1, grid_points) # define prior prior = np.repeat(5, grid_points) # uniform # prior = (p_grid &gt;= 0.5).astype(int) # truncated # prior = np.exp(- 5 * abs(p_grid - 0.5)) # double exp # compute likelihood at each point in the grid likelihood = stats.binom.pmf(success, tosses, p_grid) # compute product of likelihood and prior unstd_posterior = likelihood * prior # standardize the posterior, so it sums to 1 posterior = unstd_posterior / unstd_posterior.sum() return p_grid, posterior w, n = 6, 9 fig, ax = plt.subplots(1, 3, figsize = (12, 4)) points = (5, 10, 20) for idx, ps in enumerate(points): p_grid, posterior = posterior_grid_approx(ps, w, n) ax[idx].plot(p_grid, posterior, &quot;o-&quot;, color = r.clr3, label = f&quot;successes = {w}\\ntosses = {n}&quot;) ax[idx].set_xlabel(&quot;probability of water&quot;) ax[idx].set_ylabel(&quot;posterior probability&quot;) ax[idx].set_title(f&quot;{ps} points&quot;) ax[idx].legend(loc = 0) ax[idx].spines[&#39;left&#39;].set_visible(False) ax[idx].spines[&#39;right&#39;].set_visible(False) ax[idx].spines[&#39;top&#39;].set_visible(False) ax[idx].spines[&#39;bottom&#39;].set_visible(False) ax[idx].grid(linestyle = &#39;dotted&#39;) np.repeat((0, 1), (3, 6)) #&gt; array([0, 0, 0, 1, 1, 1, 1, 1, 1]) data = np.repeat((0, 1), (3, 6)) with pm.Model() as normal_approximation: p = pm.Uniform(&quot;p&quot;, 0, 1) # uniform priors w = pm.Binomial(&quot;w&quot;, n=len(data), p=p, observed=data.sum()) # binomial likelihood mean_q = pm.find_MAP() std_q = ((1 / pm.find_hessian(mean_q, vars=[p])) ** 0.5)[0] # display summary of quadratic approximation #&gt; █ print(&quot;Mean\\tStandard deviation\\np {:.2}\\t{:.2}&quot;.format(mean_q[&quot;p&quot;], std_q[0])) #&gt; Mean Standard deviation #&gt; p 0.67 0.16 # Compute the 89% percentile interval norm = stats.norm(mean_q, std_q) prob = 0.89 z = stats.norm.ppf([(1 - prob) / 2, (1 + prob) / 2]) pi = mean_q[&quot;p&quot;] + std_q * z print(&quot;5.5%, 94.5% \\n{:.2}, {:.2}&quot;.format(pi[0], pi[1])) #&gt; 5.5%, 94.5% #&gt; 0.42, 0.92 # analytical calculation w, n = 6, 9 x = np.linspace(0, 1, 100) plt.plot(x, stats.beta.pdf(x, w + 1, n - w + 1), label=&quot;True posterior&quot;, color = r.clr0d) # quadratic approximation plt.plot(x, stats.norm.pdf(x, mean_q[&quot;p&quot;], std_q), label=&quot;Quadratic approximation&quot;, color = r.clr3) plt.legend(loc = 0) plt.title(f&quot;n = {n}&quot;) plt.xlabel(&quot;Proportion water&quot;); plt.gca().spines[&#39;left&#39;].set_visible(False) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(False) plt.grid(linestyle = &#39;dotted&#39;) plt.show() x = np.linspace(0, 1, 100) w, n = [6, 12, 24], [9, 18, 36] fig, ax = plt.subplots(1, 3, figsize=(12, 4)) for idx, ps in enumerate(zip(w, n)): data = np.repeat((0, 1), (ps[1] - ps[0], ps[0])) with pm.Model() as normal_approximation: p = pm.Uniform(&quot;p&quot;, 0, 1) # uniform priors w = pm.Binomial(&quot;w&quot;, n=len(data), p=p, observed=data.sum()) # binomial likelihood mean_q = pm.find_MAP() std_q = ((1 / pm.find_hessian(mean_q, vars=[p])) ** 0.5)[0] ax[idx].plot(x, stats.beta.pdf(x, ps[0] + 1, ps[1] - ps[0] + 1), label = &quot;True posterior&quot;, color = r.clr0d) ax[idx].plot(x, stats.norm.pdf(x, mean_q[&quot;p&quot;], std_q), label = &quot;Quadratic approximation&quot;, color = r.clr3) ax[idx].set_xlabel(&quot;probability of water&quot;) ax[idx].set_ylabel(&quot;density&quot;) ax[idx].set_title(r&quot;$n={}$&quot;.format(ps[1])) ax[idx].legend(loc=&quot;upper left&quot;) ax[idx].spines[&#39;left&#39;].set_visible(False) ax[idx].spines[&#39;right&#39;].set_visible(False) ax[idx].spines[&#39;top&#39;].set_visible(False) ax[idx].spines[&#39;bottom&#39;].set_visible(False) ax[idx].grid(linestyle = &#39;dotted&#39;) n_samples = 1000 p = np.zeros(n_samples) p[0] = 0.5 W = 6 L = 3 for i in range(1, n_samples): p_new = stats.norm(p[i - 1], 0.1).rvs(1) if p_new &lt; 0: p_new = -p_new if p_new &gt; 1: p_new = 2 - p_new q0 = stats.binom.pmf(W, n = W + L, p = p[i - 1]) q1 = stats.binom.pmf(W, n = W + L, p = p_new) if stats.uniform.rvs(0, 1) &lt; q1 / q0: p[i] = p_new else: p[i] = p[i - 1] az.plot_kde(p, label = &quot;Metropolis approximation&quot;, plot_kwargs = {&quot;color&quot;: r.clr3}) x = np.linspace(0, 1, 100) plt.plot(x, stats.beta.pdf(x, W + 1, L + 1), &quot;C1&quot;, label = &quot;True posterior&quot;, color = r.clr0d) plt.legend() plt.gca().spines[&#39;left&#39;].set_visible(False) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(False) plt.grid(linestyle = &#39;dotted&#39;) plt.show() × "],
["rethinking-chapter-3.html", "4 Rethinking: Chapter 3 4.1 Sampling from a grid approximate posterior 4.2 Sampling to Summarize 4.3 Point estimates 4.4 sample to simulate prediction 4.5 Homework 4.6 {brms} section 4.7 pymc3 section", " 4 Rethinking: Chapter 3 Sampling the Imaginary by Richard McElreath, building on the Summary by Solomon Kurz \\[ Pr(vampire|positive) = \\frac{Pr(positive|vampire) \\times Pr(vampire)}{Pr(positive)} \\] pr_positive_on_vamp &lt;- .95 pr_positive_on_mort &lt;- .01 pr_vamp &lt;- .001 pr_positive &lt;- pr_positive_on_vamp * pr_vamp + pr_positive_on_mort * (1 - pr_vamp) (pr_vamp_on_positive &lt;- pr_positive_on_vamp * pr_vamp /pr_positive) #&gt; [1] 0.08683729 4.1 Sampling from a grid approximate posterior posterior here means simply ‘the probability of p conditional on the data’: grid_approx &lt;- function(n_grid = 20, L = 6, W = 3, prior = function(x){rep(1, length(x))}){ tibble(p_grid = seq(0, 1, length.out = n_grid), prior = prior(p_grid), likelihood = dbinom(L, size = W + L, prob = p_grid), posterior_unstand = likelihood * prior, posterior = posterior_unstand / sum(posterior_unstand)) } grid_data &lt;- grid_approx(n_grid = 10^4) samples &lt;- tibble(sample = 1:(10^4), proportion_water = sample(x = grid_data$p_grid, size = length(sample), prob = grid_data$posterior, replace = TRUE)) p_scatter &lt;- samples %&gt;% ggplot(aes(x = sample, y = proportion_water)) + geom_point(size = .75, shape = 21, color = clr_alpha(clr2,.3), fill = clr_alpha(clr2,.1)) + scale_x_continuous(expand = c(0,0)) p_dens &lt;- samples %&gt;% ggplot(aes(x = proportion_water)) + geom_density( color = clr2, fill = fll2) + scale_x_continuous(limits = c(0,1), expand = c(0, 0)) p_scatter + p_dens 4.2 Sampling to Summarize Once the posterior distribution is created, the model is done. Typical targets / questions: intervals of defined boundaries intervals of defined probability mass point estimates 4.2.1 Intervals of devined boundaries sum(grid_data$posterior[grid_data$p_grid &lt; 0.5]) #&gt; [1] 0.171875 sum(samples$proportion_water &lt; .5) / length(samples$proportion_water) #&gt; [1] 0.1734 sum(samples$proportion_water &gt; .5 &amp; samples$proportion_water &lt; .75) / length(samples$proportion_water) #&gt; [1] 0.6056 # f_post &lt;- function(x){dbeta(x = x, shape1 = 9 +1 , shape2 = 6 +1)} f_post &lt;- function(x){dbinom(x = 6, size = 9, prob = x)} f_post_norm &lt;- function(x){ f_post(x) / integrate(f = f_post,lower = 0, upper = 1)[[1]]} plot_intervals &lt;- function(x_bounds = c(0, 1), x_line = as.numeric(NA), f_posterior = f_post_norm, data = samples, ylim = c(0, 3)){ p_d &lt;- ggplot() + stat_function(fun = f_posterior, xlim = c(0,1), geom = &quot;area&quot;, color = clr0, fill = fll0) + stat_function(fun = f_posterior, xlim = x_bounds, geom = &quot;area&quot;, color = clr2, fill = fll2) + labs(y = &quot;density&quot;, x = &quot;proportion_water&quot;) p_d_emp &lt;- data %&gt;% ggplot(aes(x = proportion_water)) + geom_density( color = clr0, fill = fll0) + stat_function(fun = function(x){demp(obs = data$proportion_water, x = x)}, xlim = x_bounds, geom = &quot;area&quot;, color = clr2, fill = fll2) + labs(y = &quot;empirical density&quot;) p_d + p_d_emp &amp; geom_vline(data = tibble(x = x_line), aes(xintercept = x), linetype = 3) &amp; scale_y_continuous(limits = ylim)&amp; scale_x_continuous(limits = c(0, 1)) } plot_intervals(x_bounds = c(0, .5), x_line = .5) / plot_intervals(x_bounds = c(.5, .75), x_line = c(.5, .75)) 4.2.2 Intervals of defined mass aka.: compatibility interval credible interval percentile interval special form: highest posterior density interval (HPDI) qnt_80 &lt;- quantile(samples$proportion_water, probs = .8) qnt_80_inner &lt;- quantile(samples$proportion_water, probs = c(.1, .9)) plot_intervals(x_bounds = c(0, qnt_80), x_line = qnt_80)/ plot_intervals(x_bounds = qnt_80_inner, x_line = qnt_80_inner) library(rethinking) map &lt;- purrr::map grid_data_skew &lt;- grid_approx(L = 3, W = 0, n_grid = 10^4) samples_skew &lt;- tibble(sample = 1:(10^4), proportion_water = sample(x = grid_data_skew$p_grid, size = length(sample), prob = grid_data_skew$posterior, replace = TRUE)) f_post_skew &lt;- function(x){dbinom(x = 3, size = 3, prob = x)} f_post_norm_skew &lt;- function(x){ f_post_skew(x) / integrate(f = f_post_skew, lower = 0, upper = 1)[[1]]} qnt_50_inner &lt;- PI(samples_skew$proportion_water, prob = .5) qnt_50_high_dens &lt;- HPDI(samples_skew$proportion_water, prob = .5) plot_intervals(x_bounds = qnt_50_inner, x_line = qnt_50_inner, f_posterior = f_post_norm_skew, data = samples_skew, ylim = c(0, 4)) / plot_intervals(x_bounds = qnt_50_high_dens, x_line = qnt_50_high_dens, f_posterior = f_post_norm_skew, data = samples_skew, ylim = c(0, 4)) 4.3 Point estimates point_estimates &lt;- tibble(proportion_water= list(mean, median, chainmode) %&gt;% map_dbl(.f = function(f, vals){ f(vals) }, vals = samples_skew$proportion_water), statistic = c(&quot;mean&quot;, &quot;median&quot;, &quot;mode&quot;)) p_point_estimates &lt;- ggplot() + stat_function(fun = f_post_norm_skew, xlim = c(0,1), geom = &quot;area&quot;, color = clr0, fill = fll0) + geom_vline(data = point_estimates, aes(xintercept = proportion_water, linetype = statistic), color = clr1) + labs(y = &quot;density&quot;) f_loss &lt;- function(x){ map_dbl(x, function(x){ sum( grid_data_skew$posterior * abs( x - grid_data_skew$p_grid)) })} f_loss_quad &lt;- function(x){ map_dbl(x, function(x){ sum( grid_data_skew$posterior * ( x - grid_data_skew$p_grid) ^ 2) })} p_loss &lt;- ggplot() + stat_function(fun = f_loss, xlim = c(0,1), geom = &quot;area&quot;, color = clr0, fill = fll0) + geom_point(data = tibble(x = point_estimates$proportion_water[2], y = f_loss(point_estimates$proportion_water[2])), aes(x = x, y = y), shape = 1, size = 3, color = clr1) + labs(x = &quot;poportion_water&quot;, y = &quot;expected proportional loss&quot;) p_loss_quad &lt;- ggplot() + stat_function(fun = f_loss_quad, xlim = c(0,1), geom = &quot;area&quot;, color = clr0, fill = fll0) + geom_point(data = tibble(x = point_estimates$proportion_water[2], y = f_loss_quad(point_estimates$proportion_water[2])), aes(x = x, y = y), shape = 1, size = 3, color = clr1) + labs(x = &quot;poportion_water&quot;, y = &quot;expected proportional loss&quot;) p_point_estimates + p_loss + p_loss_quad + plot_layout(guide = &quot;collect&quot;) &amp; theme(legend.position = &quot;bottom&quot;) 4.4 sample to simulate prediction binomial likelihood \\[ Pr(W | N ,p) = \\frac{N!}{W! (N -W)!} p^{W}(1 - p)^{N-W} \\] dbinom( 0:2, size = 2, prob = .7) #&gt; [1] 0.09 0.42 0.49 rbinom( 10, size = 2, prob = .7) #&gt; [1] 1 1 1 1 1 1 2 0 2 1 create_dummy_w &lt;- function(size, prob){ tibble(x = rbinom(10^5, size = size, prob = prob), size = size, prob = prob) } dummy_w &lt;- create_dummy_w(size = 9, prob = .7) dummy_w %&gt;% group_by(x) %&gt;% count() %&gt;% ungroup() %&gt;% ggplot(aes(x = factor(x), y = n)) + geom_bar(stat = &quot;identity&quot;, color = clr1, fill = fll1, width = .6) + labs(y = &quot;count&quot;, x = &quot;dummy water count&quot;) tibble(size = rep(c(3,6,9), each = 3), prob = rep(c(.3,.6,.9), 3)) %&gt;% pmap_dfr(create_dummy_w) %&gt;% group_by(x, size , prob) %&gt;% count() %&gt;% ungroup() %&gt;% ggplot(aes(x = factor(x), y = n)) + geom_bar(stat = &quot;identity&quot;, color = clr1, fill = fll1, width = .6) + facet_grid(prob ~ size, scales = &quot;free&quot;, space = &quot;free_x&quot;, labeller = label_both) + labs(y = &quot;count&quot;, x = &quot;dummy water count&quot;) + theme(panel.background = element_rect(color = clr0d, fill = clr_alpha(clr0d,.2))) grid_data &lt;- grid_approx(n_grid = 10^4 + 1, L = 6, W = 3, prior = function(x){rep(1, length(x))}) %&gt;% mutate(idx = 1:(10^4 + 1)) grid_data %&gt;% ggplot(aes(x = p_grid))+ geom_line(aes(y = posterior, color = &quot;posterior&quot;)) + scale_color_manual(values = c(posterior = clr1), guide = &quot;none&quot;) + theme(legend.position = &quot;bottom&quot;) samples &lt;- grid_data %&gt;% slice_sample(n = 10^5 , weight_by = posterior, replace = TRUE) %&gt;% mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 9), seq = map(w, .f = function(x){sample(x = rep(c(&quot;W&quot;,&quot;L&quot;), c(x, 9-x)), size = 9, replace = FALSE)}), max_run_length = map_dbl(seq,.f = function(x){rle(x)$lengths %&gt;% max()}), n_switches = map_dbl(seq,.f = function(x){(rle(x)$lengths %&gt;% length()) -1})) p_posterior &lt;- grid_data %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_area(color = clr0d, fill = fll0) + geom_segment(data = grid_data %&gt;% filter(idx %in% (1+ (1:9)*1000)), aes(xend = p_grid, yend = 0, size = posterior), color = fll1) + geom_point(data = grid_data %&gt;% filter(idx %in% (1+ (1:9)*1000)), color = clr1) + scale_size_continuous(range = c(.1, 1), guide = &quot;none&quot;) + labs(x = &quot;probability of water&quot;, y = &quot;density&quot;, title = &quot;Posterior Probability&quot;) simulate_binom &lt;- function(probability, n_draws = 10^5, size = 9) { rbinom(n_draws, size = size, prob = probability) } d_small &lt;- tibble(probability = seq(from = .1, to = .9, by = .1)) %&gt;% mutate(draws = purrr::map(probability, simulate_binom)) %&gt;% unnest(draws) p_small &lt;- d_small %&gt;% ggplot(aes(x = draws)) + geom_bar(stat = &quot;count&quot;, color = clr1, fill = fll1, width = .6) + facet_wrap(probability ~ ., nrow = 1)+ labs(title = &quot;Sampling Distributions&quot;) p_posterior_predictive &lt;- samples %&gt;% ggplot(aes(x = factor(w))) + geom_bar(stat = &quot;count&quot;, aes(color = w == 6 , fill = after_scale(clr_alpha(color, .3))), width = .6) + scale_color_manual(values = c(`TRUE` = clr1, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of water samples&quot;, title = &quot;Posterior Predictive Distribution&quot;) p_posterior / p_small / p_posterior_predictive sum( samples$w == 6 ) / length( samples$w ) #&gt; [1] 0.19961 globe_data &lt;- c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;) globe_run_length &lt;- rle(globe_data)$lengths %&gt;% max() globe_n_switches &lt;- (rle(globe_data)$lengths %&gt;% length()) -1 p_run_length &lt;- samples %&gt;% ggplot(aes(x = factor(max_run_length))) + geom_bar(stat = &quot;count&quot;, aes(color = max_run_length == globe_run_length, fill = after_scale(clr_alpha(color))), width = .6) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;longest run length&quot;) p_switches &lt;- samples %&gt;% ggplot(aes(x = factor(n_switches))) + geom_bar(stat = &quot;count&quot;, aes(color = n_switches == globe_n_switches, fill = after_scale(clr_alpha(color))), width = .6) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of switches&quot;) p_run_length + p_switches library(rlang) chapter3_models &lt;- env( grid_data = grid_data ) write_rds(chapter3_models, &quot;envs/chapter3_models.rds&quot;) 4.5 Homework n &lt;- 1e4 easy_data &lt;- tibble(p_grid = seq( from = 0, to = 1, length.out = n ), prior = rep(1 , n), likelihood = dbinom( 6, size = 9, prob = p_grid), posterior_unscaled = likelihood * prior, posterior = posterior_unscaled / sum(posterior_unscaled), cummulative_posterior = cumsum(posterior)) easy_data %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = prior / sum(prior), color = &quot;prior&quot;)) + geom_line(aes(y = likelihood / sum(likelihood), color = &quot;likelihood&quot;)) + geom_line(aes(y = posterior, color = &quot;posterior&quot;), linetype = 3) + geom_line(aes(y = cummulative_posterior / sum(cummulative_posterior), color = &quot;cummulative_posterior&quot;), linetype = 3) + scale_color_manual(values = c(prior = clr1, likelihood = clr0d, posterior = clr2, cummulative_posterior = &quot;black&quot;)) + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), legend.position = &quot;bottom&quot;) set.seed( 100 ) easy_samples &lt;- easy_data %&gt;% slice_sample(n = n, weight_by = posterior, replace = TRUE) easy_samples %&gt;% ggplot(aes(x = p_grid)) + geom_density(color = clr0d, fill = fll0) + scale_x_continuous(limits = c(0,1)) + geom_vline(data = tibble(x = c(.2, .8)), aes(xintercept = x), linetype = 3) E1 mean(easy_samples$p_grid &lt;= .2) #&gt; [1] 5e-04 E2 mean(easy_samples$p_grid &gt; .8) #&gt; [1] 0.1219 E3 mean( easy_samples$p_grid &gt; .2 &amp; easy_samples$p_grid &lt; .8) #&gt; [1] 0.8776 E4 quantile(easy_samples$p_grid , probs = .2) #&gt; 20% #&gt; 0.5145315 E5 quantile(easy_samples$p_grid , probs = .8) #&gt; 80% #&gt; 0.7618962 E6 HPDI(easy_samples$p_grid, prob = .66) #&gt; |0.66 0.66| #&gt; 0.5138514 0.7886789 p_e6 &lt;- easy_samples %&gt;% ggplot(aes(x = p_grid)) + geom_density(color = clr0d, fill = fll0) + stat_function(fun = function(x){demp(x = x, obs = easy_samples$p_grid)}, geom = &quot;area&quot;, xlim = HPDI(easy_samples$p_grid, prob = .66), color = clr2, fill = fll2) + scale_color_manual(values = c(posterior = clr2), guide = &quot;none&quot;) + theme(legend.position = &quot;bottom&quot;) E7 PI(easy_samples$p_grid, prob = .66) #&gt; 17% 83% #&gt; 0.4972327 0.7745775 p_e7 &lt;- easy_samples %&gt;% ggplot(aes(x = p_grid)) + geom_density(color = clr0d, fill = fll0) + stat_function(fun = function(x){demp(x = x, obs = easy_samples$p_grid)}, geom = &quot;area&quot;, xlim = PI(easy_samples$p_grid, prob = .66), color = clr2, fill = fll2) + scale_color_manual(values = c(posterior = clr2), guide = &quot;none&quot;) + theme(legend.position = &quot;bottom&quot;) p_e6 + p_e7 M1 grid_data &lt;- grid_approx(n_grid = 10^4 + 1, L = 8, W = 7, prior = function(x){rep(1, length(x))}) %&gt;% mutate(idx = 1:(10^4 + 1)) grid_data %&gt;% ggplot(aes(x = p_grid))+ geom_line(aes(y = posterior, color = &quot;posterior&quot;)) + scale_color_manual(values = c(posterior = clr2), guide = &quot;none&quot;) + theme(legend.position = &quot;bottom&quot;) M2 samples &lt;- grid_data %&gt;% slice_sample(n = 10^5 , weight_by = posterior, replace = TRUE) %&gt;% mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 15)) HPDI(samples$p_grid, prob = .9) #&gt; |0.9 0.9| #&gt; 0.3329 0.7218 M3 p_posterior &lt;- grid_data %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_area(color = clr0d, fill = fll0) + geom_segment(data = grid_data %&gt;% filter(idx %in% (1+ (1:9)*1000)), aes(xend = p_grid, yend = 0, size = posterior), color = fll2) + geom_point(data = grid_data %&gt;% filter(idx %in% (1+ (1:9)*1000)), color = clr2) + scale_size_continuous(range = c(.1, 1), guide = &quot;none&quot;) + labs(x = &quot;probability of water&quot;, y = &quot;density&quot;, title = &quot;Posterior Probability&quot;) d_small &lt;- tibble(probability = seq(from = .1, to = .9, by = .1)) %&gt;% mutate(draws = purrr::map(probability, simulate_binom, size = 15)) %&gt;% unnest(draws) p_small &lt;- d_small %&gt;% ggplot(aes(x = draws)) + geom_bar(stat = &quot;count&quot;, color = clr2, fill = fll2, width = .6) + facet_wrap(probability ~ ., nrow = 1)+ labs(title = &quot;Sampling Distributions&quot;) p_posterior_predictive &lt;- samples %&gt;% ggplot(aes(x = factor(w))) + geom_bar(stat = &quot;count&quot;, aes(color = w == 8 , fill = after_scale(clr_alpha(color, .3))), width = .6) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of water samples&quot;, title = &quot;Posterior Predictive Distribution&quot;) p_posterior / p_small / p_posterior_predictive sum( samples$w == 8 ) / length( samples$w ) #&gt; [1] 0.14738 M4 samples &lt;- grid_data %&gt;% slice_sample(n = 10^5 , weight_by = posterior, replace = TRUE) %&gt;% mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 9)) samples %&gt;% ggplot(aes(x = factor(w))) + geom_bar(stat = &quot;count&quot;, aes(color = w == 6 , fill = after_scale(clr_alpha(color, .3))), width = .6) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of water samples&quot;, title = &quot;Posterior Predictive Distribution&quot;) sum( samples$w == 6 ) / length( samples$w ) #&gt; [1] 0.17634 M5 grid_data &lt;- grid_approx(n_grid = 10^4 + 1, L = 8, W = 7, prior = function(x){if_else(x &lt; .5, 0, 1)}) %&gt;% mutate(idx = 1:(10^4 + 1)) samples &lt;- grid_data %&gt;% slice_sample(n = 10^5 , weight_by = posterior, replace = TRUE) %&gt;% mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 15)) HPDI(samples$p_grid, prob = .9) #&gt; |0.9 0.9| #&gt; 0.5000 0.7117 p_posterior &lt;- grid_data %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_area(color = clr0d, fill = fll0) + geom_segment(data = grid_data %&gt;% filter(idx %in% (1+ (1:9)*1000)), aes(xend = p_grid, yend = 0, size = posterior), color = fll2) + geom_point(data = grid_data %&gt;% filter(idx %in% (1+ (1:9)*1000)), color = clr2) + scale_size_continuous(range = c(.1, 1), guide = &quot;none&quot;) + labs(x = &quot;probability of water&quot;, y = &quot;density&quot;, title = &quot;Posterior Probability&quot;) d_small &lt;- tibble(probability = seq(from = .1, to = .9, by = .1)) %&gt;% mutate(draws = purrr::map(probability, simulate_binom, size = 15)) %&gt;% unnest(draws) p_small &lt;- d_small %&gt;% ggplot(aes(x = draws)) + geom_bar(stat = &quot;count&quot;, color = clr2, fill = fll2, width = .6) + facet_wrap(probability ~ ., nrow = 1)+ labs(title = &quot;Sampling Distributions&quot;) p_posterior_predictive &lt;- samples %&gt;% ggplot(aes(x = factor(w))) + geom_bar(stat = &quot;count&quot;, aes(color = w == 8 , fill = after_scale(clr_alpha(color, .3))), width = .6) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of water samples&quot;, title = &quot;Posterior Predictive Distribution&quot;) p_posterior / p_small / p_posterior_predictive sum( samples$w == 8 ) / length( samples$w ) #&gt; [1] 0.15846 M6 random_tosses &lt;- function(n, n_grid = 1e4, n_posterior_sample = 1e4, prior = function(x){rep(1, length(x))}){ grid_data &lt;- tibble(p_grid = seq(0, 1, length.out = n_grid), prior = prior(p_grid), likelihood = dbinom(rbinom(1, size = n, prob = .7), size = n, prob = p_grid), posterior_unstand = likelihood * prior, posterior = posterior_unstand / sum(posterior_unstand)) samples &lt;- grid_data %&gt;% slice_sample(n = n_posterior_sample, weight_by = posterior, replace = TRUE) tibble(n = n, grid_data = list(grid_data), samples = list(samples), hpdi = list(HPDI(samples[[1]]$p_grid, prob = .99)), hpdi_width = diff(hpdi[[1]])) } c(c(1:10),((1:100) *30)) %&gt;% map_dfr(random_tosses) %&gt;% ggplot(aes(x = n, y = hpdi_width)) + geom_point(aes(color = hpdi_width &lt; .05)) + geom_hline(yintercept = .05, color = &quot;black&quot;, linetype = 3) + scale_color_manual(values = c(`FALSE` = clr0d, `TRUE` = clr2), guide = &quot;none&quot;) H1 library(rethinking) data(homeworkch3) n_grid &lt;- 1e4 + 1 grid_data &lt;- tibble(p_grid = seq(0, 1, length.out = n_grid), prior = (function(x){rep(1, length(x))})(p_grid), likelihood = dbinom(sum(birth1 + birth2), size = length(c(birth1, birth2)), prob = p_grid), posterior_unstand = likelihood * prior, posterior = posterior_unstand / sum(posterior_unstand)) samples &lt;- grid_data %&gt;% slice_sample(n = 1e5, weight_by = posterior, replace = TRUE) (mode_posterior &lt;- chainmode(samples$p_grid)) #&gt; [1] 0.5547754 grid_data %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_area(color = clr0d, fill = fll0, size = .5) + geom_vline(xintercept = mode_posterior, color = clr2, linetype = 3) H2 (percentile_intervals &lt;- tibble(boundary = c(&quot;lower&quot;, &quot;upper&quot;), p50 = HPDI(samples$p_grid, prob = .5), p89 = HPDI(samples$p_grid, prob = .89), p97 = HPDI(samples$p_grid, prob = .97))) %&gt;% knitr::kable() boundary p50 p89 p97 lower 0.5306 0.4977 0.4775 upper 0.5778 0.6090 0.6274 grid_data %&gt;% ggplot(aes(x = p_grid, y = posterior)) + stat_function(fun = function(x){demp(x = x, obs = samples$p_grid)}, geom = &quot;line&quot;, color = clr0d, xlim = c(0,1), n = 501) + stat_function(fun = function(x){demp(x = x, obs = samples$p_grid)}, geom = &quot;area&quot;, color = clr2, fill = clr_alpha(fll2,.2), xlim = percentile_intervals$p50, n = 501) + stat_function(fun = function(x){demp(x = x, obs = samples$p_grid)}, geom = &quot;area&quot;, color = clr2, fill = clr_alpha(fll2,.2), xlim = percentile_intervals$p89, n = 501) + stat_function(fun = function(x){demp(x = x, obs = samples$p_grid)}, geom = &quot;area&quot;, color = clr2, fill = clr_alpha(fll2,.2), xlim = percentile_intervals$p97, n = 501) H3 random_births &lt;- grid_data %&gt;% slice_sample(n = 1e4, weight_by = posterior, replace = TRUE) %&gt;% mutate(births = map(p_grid, .f = function(x){rbinom(n = 200, size = 1, prob = x)}), n_boys = map_dbl(births, sum), n_girls = 200 - n_boys, n_boys_firstborn = map_dbl(births, function(x){ sum(x[1:100]) })) sum(random_births$n_boys &lt; sum(birth1 + birth2)) / sum(birth1 + birth2) #&gt; [1] 43.45946 p_all &lt;- random_births %&gt;% ggplot(aes(x = n_boys)) + geom_density(color = clr0d, fill = fll0) + geom_vline(xintercept = sum(birth1 + birth2), color = clr2, linetype = 3) + scale_x_continuous(limits = c(0, 200)) p_first &lt;- random_births %&gt;% ggplot(aes(x = n_boys_firstborn)) + geom_density(color = clr0d, fill = fll0) + geom_vline(xintercept = sum(birth1), color = clr2, linetype = 3) + scale_x_continuous(limits = c(0, 100)) p_all + p_first H5 births_first_girl &lt;- tibble(birth1 = birth1, birth2 = birth2) %&gt;% filter(birth1 == 0) n_first_girl &lt;- length(births_first_girl$birth2) random_births &lt;- grid_data %&gt;% slice_sample(n = 1e4, weight_by = posterior, replace = TRUE) %&gt;% mutate(births = map(p_grid, .f = function(x){rbinom(n = n_first_girl, size = 1, prob = x)}), n_boys = map_dbl(births, sum), n_girls = n_first_girl - n_boys) sum(random_births$n_boys &lt; sum(births_first_girl$birth2)) / 1e4 #&gt; [1] 0.9991 random_births %&gt;% ggplot(aes(x = n_boys)) + geom_density(color = clr0d, fill = fll0) + geom_vline(xintercept = sum(births_first_girl$birth2), color = clr2, linetype = 3) + scale_x_continuous(limits = c(0, n_first_girl), expand = c(0, 0)) 4.6 {brms} section brms_c3_6in9 &lt;- brm(data = list(w = 6), family = binomial(link = &quot;identity&quot;), w | trials(9) ~ 0 + Intercept, # this is a flat prior prior(beta(1, 1), class = b, lb = 0, ub = 1), iter = 5000, warmup = 1000, seed = 42, file = &quot;brms/brms_c3_6in9&quot;) posterior_summary(brms_c3_6in9) %&gt;% round(digits = 3) %&gt;% knitr::kable() Estimate Est.Error Q2.5 Q97.5 b_Intercept 0.637 0.140 0.346 0.879 lp__ -3.310 0.748 -5.392 -2.780 n_trials &lt;- 9 samples_brms &lt;- fitted(brms_c3_6in9, summary = FALSE, scale = &quot;linear&quot;) %&gt;% as_tibble() %&gt;% set_names(nm = &quot;p&quot;) %&gt;% mutate(w = rbinom(n(), size = n_trials, prob = p)) p_brms_posterior &lt;- samples_brms %&gt;% ggplot(aes(x = p)) + geom_density(color = clr1, fill = fll1) + scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) + labs(y = &quot;density&quot;, x = &quot;proportion water&quot;, title = &quot;{brms} posterior probability (6 in 9)&quot;) p_brms_posterior_predictive &lt;- samples_brms %&gt;% ggplot(aes(x = factor(w))) + geom_bar(stat = &quot;count&quot;, aes(color = w == 6 , fill = after_scale(clr_alpha(color, .3))), width = .6) + scale_color_manual(values = c(`TRUE` = clr1, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(y = &quot;count&quot;, x = &quot;number of water samples&quot;, title = &quot;posterior predictive distribution&quot;) p_brms_posterior + p_brms_posterior_predictive 4.7 pymc3 section Taken from pymc-devs… import arviz as az import matplotlib.pyplot as plt import numpy as np import pymc3 as pm import scipy.stats as stats import seaborn as sns import matplotlib from matplotlib.colors import ListedColormap import matplotlib.font_manager #matplotlib.font_manager._rebuild() def posterior_grid_approx(grid_points = 5, success = 6, tosses = 9): &quot;&quot;&quot;&quot;&quot;&quot; # define grid p_grid = np.linspace(0, 1, grid_points) # define prior prior = np.repeat(5, grid_points) # uniform # prior = (p_grid &gt;= 0.5).astype(int) # truncated # prior = np.exp(- 5 * abs(p_grid - 0.5)) # double exp # compute likelihood at each point in the grid likelihood = stats.binom.pmf(success, tosses, p_grid) # compute product of likelihood and prior unstd_posterior = likelihood * prior # standardize the posterior, so it sums to 1 posterior = unstd_posterior / unstd_posterior.sum() return p_grid, posterior PrPV = 0.95 PrPM = 0.01 PrV = 0.001 PrP = PrPV * PrV + PrPM * (1 - PrV) PrVP = PrPV * PrV / PrP PrVP #&gt; 0.08683729433272395 p_grid, posterior = posterior_grid_approx(grid_points = 100, success = 6, tosses = 9) samples = np.random.choice(p_grid, p = posterior, size = int(1e4), replace = True) fig, (ax0, ax1) = plt.subplots(1, 2, figsize = (12, 4)) ax0.plot(samples, &quot;o&quot;, alpha = 0.2, color = r.clr3) ax0.set_xlabel(&quot;sample number&quot;) ax0.set_ylabel(&quot;proportion water (p)&quot;) ax0.spines[&#39;left&#39;].set_visible(False) ax0.spines[&#39;right&#39;].set_visible(False) ax0.spines[&#39;top&#39;].set_visible(False) ax0.spines[&#39;bottom&#39;].set_visible(False) ax0.grid(linestyle = &#39;dotted&#39;) az.plot_kde(samples, ax = ax1, plot_kwargs = {&quot;color&quot;: r.clr3}) ax1.set_xlabel(&quot;proportion water (p)&quot;) ax1.set_ylabel(&quot;density&quot;) ax1.spines[&#39;left&#39;].set_visible(False) ax1.spines[&#39;right&#39;].set_visible(False) ax1.spines[&#39;top&#39;].set_visible(False) ax1.spines[&#39;bottom&#39;].set_visible(False) ax1.grid(linestyle = &#39;dotted&#39;) plt.show() sum(posterior[p_grid &lt; 0.5]) #&gt; 0.17183313110747475 sum(samples &lt; 0.5) / 1e4 #&gt; 0.1767 sum((samples &gt; 0.5) &amp; (samples &lt; 0.75)) / 1e4 #&gt; 0.6113 np.percentile(samples, 80) #&gt; 0.7575757575757577 np.percentile(samples, [10, 90]) #&gt; array([0.44444444, 0.80808081]) p_grid, posterior = posterior_grid_approx(grid_points = 100, success = 3, tosses = 3) plt.plot(p_grid, posterior, color = r.clr3) plt.xlabel(&quot;proportion water (p)&quot;) plt.ylabel(&quot;Density&quot;) plt.gca().spines[&#39;left&#39;].set_visible(False) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(False) plt.grid(linestyle = &#39;dotted&#39;) plt.show() samples = np.random.choice(p_grid, p = posterior, size = int(1e4), replace = True) np.percentile(samples, [25, 75]) #&gt; array([0.71717172, 0.93939394]) az.hdi(samples, hdi_prob = 0.5) #&gt; array([0.84848485, 1. ]) p_grid[posterior == max(posterior)] #&gt; array([1.]) stats.mode(samples)[0] #&gt; array([1.]) np.mean(samples), np.median(samples) #&gt; (0.8061979797979799, 0.8484848484848485) sum(posterior * abs(0.5 - p_grid)) #&gt; 0.31626874808692995 loss = [sum(posterior * abs(p - p_grid)) for p in p_grid] p_grid[loss == min(loss)] #&gt; array([0.84848485]) stats.binom.pmf(range(3), n = 2, p = 0.7) #&gt; array([0.09, 0.42, 0.49]) stats.binom.rvs(n = 2, p = 0.7, size = 1) #&gt; array([1]) stats.binom.rvs(n = 2, p = 0.7, size = 10) #&gt; array([2, 2, 1, 2, 1, 0, 2, 1, 2, 2]) dummy_w = stats.binom.rvs(n = 2, p = 0.7, size = int(1e5)) [(dummy_w == i).mean() for i in range(3)] #&gt; [0.08832, 0.42272, 0.48896] dummy_w = stats.binom.rvs(n = 9, p = 0.7, size = int(1e5)) # dummy_w = stats.binom.rvs(n=9, p=0.6, size=int(1e4)) # dummy_w = stats.binom.rvs(n=9, p=samples) bar_width = 0.7 plt.hist(dummy_w, bins = np.arange(0, 11) - bar_width / 2, width = bar_width, color = r.clr3) #&gt; (array([2.0000e+00, 4.2000e+01, 3.9200e+02, 2.1170e+03, 7.4230e+03, #&gt; 1.7153e+04, 2.6752e+04, 2.6557e+04, 1.5640e+04, 3.9220e+03]), array([-0.35, 0.65, 1.65, 2.65, 3.65, 4.65, 5.65, 6.65, 7.65, #&gt; 8.65, 9.65]), &lt;BarContainer object of 10 artists&gt;) plt.xlim(0, 9.5) #&gt; (0.0, 9.5) plt.xlabel(&quot;dummy water count&quot;) plt.ylabel(&quot;Frequency&quot;) plt.gca().spines[&#39;left&#39;].set_visible(False) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(False) plt.grid(linestyle = &#39;dotted&#39;) plt.show() p_grid, posterior = posterior_grid_approx(grid_points = 100, success = 6, tosses = 9) np.random.seed(100) samples = np.random.choice(p_grid, p = posterior, size = int(1e4), replace = True) birth1 = np.array([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1]) birth2 = np.array([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0]) sum(birth1) + sum(birth2) #&gt; 111 × "],
["rethinking-chapter-4.html", "5 Rethinking: Chapter 4 5.1 Why normal distributions are normal 5.2 Normal by multiplication and by log-multiplication 5.3 A language for describing models 5.4 Linear Prediction 5.5 Curves from lines 5.6 Splines 5.7 Homework 5.8 {brms} section 5.9 pymc3 section", " 5 Rethinking: Chapter 4 Geocentric Models by Richard McElreath, building on the Summaries by Solomon Kurz and Jake Thompson. 5.1 Why normal distributions are normal 5.1.1 Normal by addition n_people &lt;- 1e3 position &lt;- crossing(person = 1:n_people, step = 0:16) %&gt;% mutate(deviation = map_dbl(step, ~if_else(. == 0, 0, runif(1, -1, 1)))) %&gt;% group_by(person) %&gt;% mutate(position = cumsum(deviation)) %&gt;% ungroup() p_all_steps &lt;- position %&gt;% ggplot(aes(x = step, y = position, group = person)) + geom_line(aes(color = person == n_people)) + geom_point(data = position %&gt;% filter(person == n_people), aes(color = &quot;TRUE&quot;), size = 1) + geom_vline(data = tibble(step = c(4, 8, 16)), aes(xintercept = step), linetype = 3, color = rgb(0,0,0,.5)) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr_alpha(clr0d, .05)), guide = &quot;none&quot;) + scale_x_continuous(breaks = c(0,4,8,16)) + theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) plot_steps &lt;- function(step_nr, data = position, add_ideal = FALSE){ data_step &lt;- data %&gt;% filter(step == step_nr) p &lt;- data_step %&gt;% ggplot(aes(x = position)) + geom_density(adjust = .2, color = clr0d, fill = fll0) + scale_x_continuous(limits = c(-6, 6)) + labs(title = glue(&quot;{step_nr} steps&quot;)) if(add_ideal){p &lt;- p + stat_function(fun = function(x){dnorm(x, mean = 0, sd = sd(data_step$position))}, n = 501, color = clr2, linetype = 3)} p } p_all_steps / (plot_steps(step_nr = 4) + plot_steps(step_nr = 8) + plot_steps(step_nr = 16, add_ideal = TRUE)) 5.2 Normal by multiplication and by log-multiplication normal_by_multiplication &lt;- function(effect_size = 0.1, x_scale = ggplot2::scale_x_continuous(), x_lab = &quot;normal&quot;){ tibble(person = 1:n_people, growth = replicate(length(person), prod(1 + runif(12, 0, effect_size)))) %&gt;% ggplot(aes(x = growth)) + geom_density(color = clr0d, fill = fll0) + labs(title = glue(&quot;effect size: {effect_size}&quot;), x = glue(&quot;growth ({x_lab})&quot;)) + x_scale } normal_by_multiplication(effect_size = .01) + normal_by_multiplication(effect_size = .1) + normal_by_multiplication(effect_size = .5)+ normal_by_multiplication(effect_size = .5, x_scale = scale_x_log10(), x_lab = &quot;log10&quot;) + plot_layout(nrow = 1) 5.2.1 using the Gaussian distribution part of the exponential family probability density function \\(\\mu\\): mean \\(\\sigma\\): standard deviation \\(\\tau\\): precision \\[ p( y | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp \\left( \\frac{(y-\\mu)^2}{2\\sigma^2} \\right)\\\\ \\tau = 1 / \\sigma^2 \\\\ p( y | \\mu, \\tau) = \\sqrt{\\frac{\\tau}{2\\pi}}exp(-\\tfrac{1}{2}\\tau(y - \\mu)^2) \\] 5.3 A language for describing models The first line defines the likelihood used in Bayes’ theorem, the other lines describe the priors used. The tilde means that the relationships are stochastic. re-describing the globe-toss model: The count \\(W\\) is distributed binomially with a sample size \\(N\\) and the probabiliy \\(p\\). The prior for \\(p\\) is assumed to be uniform between zero and one \\[ W \\sim Binomial(N, p)\\\\ p \\sim Uniform(0, 1) \\] Substituting in Bayes’ theorem: \\[ Pr(p | w, n) = \\frac{Binomial(w|n,p)~Uniform(p|0,1)}{\\int Binomial(w|n,p)~Uniform(p|0,1) dp} \\] w &lt;- 6 n &lt;- 9 grid_data &lt;- tibble(p_grid = seq(0,1, length.out = 101), likelihood = dbinom(w, n, p_grid), prior = dunif(p_grid, 0, 1), posterior_unstand = likelihood * prior, posterior = posterior_unstand / sum(posterior_unstand)) grid_data %&gt;% pivot_longer(cols = c(prior, likelihood, posterior), names_to = &quot;bayes_part&quot;, values_to = &quot;p&quot;) %&gt;% mutate(bayes_part = factor(bayes_part, levels = names(clr_bayes))) %&gt;% ggplot(aes(x = p_grid)) + geom_area(aes(y = p, color = bayes_part, fill = after_scale(clr_alpha(color)))) + scale_color_manual(values = clr_bayes, guide = &quot;none&quot;) + facet_wrap(bayes_part ~ ., scales = &quot;free_y&quot;) 5.3.1 Gaussian model of height 5.3.1.1 The data library(rethinking) data(Howell1) (data &lt;- as_tibble(Howell1)) %&gt;% precis() %&gt;% as_tibble(rownames = NA) %&gt;% knitr::kable() mean sd 5.5% 94.5% histogram height 138.2635963 27.6024476 81.108550 165.73500 ▁▁▁▁▁▁▁▂▁▇▇▅▁ weight 35.6106176 14.7191782 9.360721 54.50289 ▁▂▃▂▂▂▂▅▇▇▃▂▁ age 29.3443934 20.7468882 1.000000 66.13500 ▇▅▅▃▅▂▂▁▁ male 0.4724265 0.4996986 0.000000 1.00000 ▇▁▁▁▁▁▁▁▁▇ (data_adults &lt;- data %&gt;% filter(age &gt;= 18)) %&gt;% precis() %&gt;% as_tibble(rownames = NA) %&gt;% knitr::kable() mean sd 5.5% 94.5% histogram height 154.59709 7.7423321 142.8750 167.00500 ▁▃▇▇▅▇▂▁▁ weight 44.99049 6.4567081 35.1375 55.76588 ▁▅▇▇▃▂▁ age 41.13849 15.9678551 20.0000 70.00000 ▂▅▇▅▃▇▃▃▂▂▂▁▁▁▁ male 0.46875 0.4997328 0.0000 1.00000 ▇▁▁▁▁▁▁▁▁▇ data_adults %&gt;% ggplot(aes(x = height)) + geom_density(adjust = .5, color = clr0d, fill = fll0) + scale_x_continuous(limits = c(130,185)) 5.3.1.2 The model \\[ \\begin{array}{cccr} h_i &amp; \\stackrel{iid}{\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu &amp; \\sim &amp; Normal(178, 20) &amp; \\textrm{[$\\mu$ prior]}\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] where, \\(iid\\) means “independent and identically distributed”. Prior predictive simulation (‘what does the model think before seeing the data?’) n_samples &lt;- 1e4 prior_simulation &lt;- tibble( sample_mu = rnorm(n_samples, 178, 20), sample_sigma = runif(n_samples, 0, 50), prior_h = rnorm(n_samples, sample_mu, sample_sigma), bad_mu = rnorm(n_samples, 178, 100), bad_prior = rnorm(n_samples, bad_mu, sample_sigma) ) p_mu &lt;- ggplot() + stat_function(fun = function(x){dnorm(x = x, mean = 178, sd = 20)}, xlim = c(100,250), color = clr0d, fill = fll0, geom = &quot;area&quot;) + labs(title = glue(&quot;*\\U03BC* {mth(&#39;\\U007E&#39;)} dnorm( 178, 20 )&quot;), y = &quot;density&quot;, x = &quot;*\\U03BC*&quot;) p_sigma &lt;- ggplot() + stat_function(fun = function(x){dunif(x = x, min = 0, max = 50)}, xlim = c(-5, 55), color = clr1, fill = fll1, geom = &quot;area&quot;) + labs(title = glue(&quot;*{mth(&#39;\\U03C3&#39;)}* {mth(&#39;\\U007E&#39;)} dunif( 0, 50 )&quot;), y = &quot;density&quot;, x = glue(&quot;*{mth(&#39;\\U03C3&#39;)}*&quot;)) p_prior_sim &lt;- prior_simulation %&gt;% ggplot(aes(x = prior_h)) + geom_density(color = clr2, fill = fll2, adjust = .4) + scale_x_continuous(limits = c(0,356), breaks = c(0,73,178,283)) + labs(title = glue(&quot;*h&lt;sub&gt;i&lt;/sub&gt;* {mth(&#39;\\U007E&#39;)} dnorm( *\\U03BC*, {mth(&#39;\\U03C3&#39;)} )&quot;), x = &quot;height&quot;) p_bad_prior &lt;- prior_simulation %&gt;% ggplot(aes(x = bad_prior)) + geom_density(color = clr2, fill = fll2, adjust = .4) + scale_x_continuous(limits = c(-222,578), breaks = c(-128,0,178,484), expand = c(0,0)) + geom_vline(data = tibble(h = c(0,272)), aes(xintercept = h), linetype = 3)+ labs(title = glue(&quot;*h&lt;sub&gt;i&lt;/sub&gt;* {mth(&#39;\\U007E&#39;)} dnorm( *\\U03BC*, {mth(&#39;\\U03C3&#39;)} )&lt;br&gt;*\\U03BC* {mth(&#39;\\U007E&#39;)} dnorm( 178, 100 )&quot;), x = &quot;height&quot;) p_mu + p_sigma + p_prior_sim + p_bad_prior &amp; theme(plot.title = element_markdown(), axis.title.x = element_markdown()) 5.3.1.3 grid approximation of the posterior distribution n_grid &lt;- 101 grid_data &lt;- cross_df(list(mu = seq(from = 152, to = 157, length.out = n_grid), sigma = seq(from = 6.5, to = 9, length.out = n_grid))) %&gt;% mutate(log_likelihood = map2_dbl(.x = mu, .y = sigma, .f = function(x, y){ dnorm(x = data_adults$height, mean = x, sd = y, log = TRUE) %&gt;% sum() }), prior_mu = dnorm(mu, mean = 178, sd = 20, log = TRUE), prior_sigma = dunif(sigma, min = 0, max = 50, log = TRUE), product = log_likelihood + prior_mu + prior_sigma, probability = exp(product - max(product))) grid_data %&gt;% ggplot(aes(x = mu, y = sigma, z = probability)) + geom_raster(aes(fill = probability)) + geom_contour(color = rgb(1,1,1,.1)) + coord_cartesian(xlim = range(grid_data$mu), ylim = range(grid_data$sigma)) + scale_fill_gradientn(colours = clr_grd5 %&gt;% clr_alpha(alpha = .8), limits = c(0,1)) + coord_cartesian(xlim = range(grid_data$mu), ylim = range(grid_data$sigma), expand = 0) + guides(fill = guide_colorbar(title.position = &quot;top&quot;, barwidth = unit(.9,&quot;npc&quot;), barheight = unit(5, &quot;pt&quot;))) + labs(x = &quot; *\\U03BC*&quot;, y = glue(&quot;*{mth(&#39;\\U03C3&#39;)}*&quot;))+ theme(legend.position = &quot;bottom&quot;, axis.title.x = element_markdown(), axis.title.y = element_markdown()) Sampling from the posterior distribution n_posterior_sample &lt;- 1e4 samples &lt;- grid_data %&gt;% slice_sample(n = n_posterior_sample, weight_by = probability, replace = TRUE) p_samples &lt;- samples %&gt;% group_by(mu, sigma) %&gt;% count() %&gt;% ungroup() %&gt;% ggplot(aes(x = mu, y = sigma, color = n)) + geom_point(size = .4) + scale_color_gradientn(colours = clr_grd5 %&gt;% clr_alpha(alpha = .8)) + coord_cartesian(xlim = buffer_range(grid_data$mu), ylim = buffer_range(grid_data$sigma), expand = 0) + guides(color = guide_colorbar(title.position = &quot;top&quot;, barwidth = unit(.2,&quot;npc&quot;), barheight = unit(5, &quot;pt&quot;))) + labs(x = &quot; *\\U03BC*&quot;, y = glue(&quot;*{mth(&#39;\\U03C3&#39;)}*&quot;))+ theme(legend.position = &quot;bottom&quot;, axis.title.x = element_markdown(), axis.title.y = element_markdown()) p_mu_dens &lt;- samples %&gt;% ggplot(aes(x = mu)) + geom_density(color = clr0d, fill = fll0) + scale_x_continuous(limits = buffer_range(grid_data$mu), expand = c(0, 0)) + labs(y = &quot;marginal&lt;br&gt;density&quot;) + theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.title.y = element_markdown()) p_sigma_dens &lt;- samples %&gt;% ggplot(aes(x = sigma)) + geom_density(color = clr0d, fill = fll0) + scale_x_continuous(limits = buffer_range(grid_data$sigma), expand = c(0, 0)) + labs(y = &quot;marginal density&quot;) + coord_flip() + theme(axis.title.y = element_blank(), axis.text.y = element_blank()) p_mu_dens + patchwork::guide_area() + p_samples + p_sigma_dens + plot_layout(guides = &quot;collect&quot;, widths = c(1,.3), heights = c(.3,1)) Exploration of long tail for \\(\\sigma\\) when original sample size is small: heights_subset &lt;- sample(data_adults$height, size = 20) grid_data_subset &lt;- cross_df(list(mu = seq(from = 145, to = 165, length.out = n_grid), sigma = seq(from = 4.5, to = 16, length.out = n_grid))) %&gt;% mutate(log_likelihood = map2_dbl(.x = mu, .y = sigma, .f = function(x, y){ dnorm(x = heights_subset, mean = x, sd = y, log = TRUE) %&gt;% sum() }), prior_mu = dnorm(mu, mean = 178, sd = 20, log = TRUE), prior_sigma = dunif(sigma, min = 0, max = 50, log = TRUE), product = log_likelihood + prior_mu + prior_sigma, probability = exp(product - max(product))) samples_subset &lt;- grid_data_subset %&gt;% slice_sample(n = n_posterior_sample, weight_by = probability, replace = TRUE) p_samples &lt;- samples_subset %&gt;% group_by(mu, sigma) %&gt;% count() %&gt;% ungroup() %&gt;% ggplot(aes(x = mu, y = sigma, color = n)) + geom_point(size = .4) + scale_color_gradientn(colours = clr_grd4 %&gt;% clr_alpha(alpha = .8)) + coord_cartesian(xlim = buffer_range(grid_data_subset$mu), ylim = buffer_range(grid_data_subset$sigma), expand = 0) + guides(color = guide_colorbar(title.position = &quot;top&quot;, barwidth = unit(.2,&quot;npc&quot;), barheight = unit(5, &quot;pt&quot;))) + labs(x = &quot; *\\U03BC*&quot;, y = glue(&quot;*{mth(&#39;\\U03C3&#39;)}*&quot;))+ theme(legend.position = &quot;bottom&quot;, axis.title.x = element_markdown(), axis.title.y = element_markdown()) p_mu_dens &lt;- samples_subset %&gt;% ggplot(aes(x = mu)) + geom_density(color = clr0d, fill = fll0) + scale_x_continuous(limits = buffer_range(grid_data_subset$mu), expand = c(0, 0)) + labs(y = &quot;marginal&lt;br&gt;density&quot;) + theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.title.y = element_markdown()) p_sigma_dens &lt;- samples_subset %&gt;% ggplot(aes(x = sigma)) + geom_density(color = clr0d, fill = fll0) + scale_x_continuous(limits = buffer_range(grid_data_subset$sigma), expand = c(0, 0)) + labs(y = &quot;marginal density&quot;) + coord_flip() + theme(axis.title.y = element_blank(), axis.text.y = element_blank()) p_mu_dens + patchwork::guide_area() + p_samples + p_sigma_dens + plot_layout(guides = &quot;collect&quot;, widths = c(1,.3), heights = c(.3,1)) 5.3.1.4 Quadratic approximation of the posterior distribution \\[ \\begin{array}{cccr} h_i &amp; \\stackrel{iid}{\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\verb|height ~ dnorm(mu, sigma)|\\\\ \\mu &amp; \\sim &amp; Normal(178, 20) &amp; \\verb|mu ~ dnorm(178, 20)|\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; \\verb|sigma ~ dunif(0, 50)| \\end{array} \\] model_spec &lt;- alist( height ~ dnorm(mu, sigma), mu ~ dnorm(178, 20), sigma ~ dunif(0, 50) ) # &quot;maximum a priori estimate&quot; map_starting_points &lt;- list( mu = mean(data_adults$height), sigma = sd(data_adults$height) ) model_heights_quap_weak_prior &lt;- quap(flist = model_spec, data = data_adults, start = map_starting_points) precis(model_heights_quap_weak_prior) %&gt;% as_tibble(rownames = NA) %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% 154.61 0.41 153.95 155.27 7.73 0.29 7.27 8.20 Comparing how a stronger prior for \\(\\mu\\) (narrower distribution) forces a larger estimate of \\(\\sigma\\) to compensate for this. quap( flist = alist( height ~ dnorm( mu , sigma ), mu ~ dnorm( 178, 0.1 ), sigma ~ dunif( 0, 50 ) ), data = data_adults, start = map_starting_points) %&gt;% precis() %&gt;% as_tibble(rownames = NA) %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% 177.86 0.10 177.70 178.02 24.52 0.93 23.03 26.00 The variance-covariance matrix of the quadratic aprroximation for sampling the multi-dimensional gaussian distribution: vcov_mod_heights &lt;- vcov(model_heights_quap_weak_prior) vcov_mod_heights %&gt;% round(digits = 6) %&gt;% knitr::kable() mu sigma mu 0.169740 0.000218 sigma 0.000218 0.084906 diag(vcov_mod_heights) #&gt; mu sigma #&gt; 0.16973961 0.08490582 round(cov2cor(vcov_mod_heights), digits = 5) \\[\\begin{bmatrix} 1 &amp;0.00182 \\\\0.00182 &amp;1 \\\\ \\end{bmatrix}\\] sampling from the multi-dimensional posterior distribution posterior_sample &lt;- extract.samples(model_heights_quap_weak_prior, n = 1e4) %&gt;% as_tibble() precis(posterior_sample) %&gt;% as_tibble() %&gt;% knitr::kable() mean sd 5.5% 94.5% histogram 154.611364 0.4110215 153.962560 155.263924 ▁▁▁▅▇▂▁▁ 7.730652 0.2933477 7.261701 8.205258 ▁▁▁▁▂▅▇▇▃▁▁▁▁ 5.4 Linear Prediction ggplot(data_adults, aes(height, weight)) + geom_point(shape = 21, size = 1.5, color = clr1, fill = fll1) \\[ \\begin{array}{cccr} h_i &amp; {\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta (x_i - \\bar{x}) &amp; \\textrm{[linear model]}\\\\ % alternatively \\overline{x} \\alpha &amp; \\sim &amp; Normal(178, 20) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] The current prior for \\(\\beta\\) is a bad choice, because it allows negative as well as unreasonably high and low dependencies of \\(h\\) (height) on \\(x\\) (weight): set.seed(2971) N &lt;- 100 linear_priors &lt;- tibble(n = 1:N, alpha = rnorm( n = N, mean = 178, sd = 20 ), beta_1 = rnorm( n = N, mean = 0, sd = 10), beta_2 = rlnorm( n = N, mean = 0, sd = 1)) %&gt;% expand(nesting(n, alpha, beta_1, beta_2), weight = range(data_adults$weight)) %&gt;% mutate(height_1 = alpha + beta_1 * (weight - mean(data_adults$weight)), height_2 = alpha + beta_2 * (weight - mean(data_adults$weight))) p_lin_pr &lt;- ggplot(linear_priors, aes(x = weight, y = height_1, group = n)) + labs(title = glue(&quot;{mth(&#39;*\\U03B2* ~&#39;)} Normal(0, 10)&quot;), y = &quot;height&quot;) p_log_pr &lt;- ggplot(linear_priors, aes(x = weight, y = height_2, group = n)) + labs(title = glue(&quot;{mth(&#39;*\\U03B2* ~&#39;)} Log-Normal(0, 1)&quot;), y = &quot;height&quot;) p_lnorm &lt;- ggplot() + stat_function(fun = function(x){dlnorm(x = x, meanlog = 0, sdlog = 1)}, xlim = c(0,5), geom = &quot;area&quot;, color = clr2, fill = fll2, n = 501) + labs(title = &quot;Log-Norm(0, 0.1)&quot;, y = &quot;density&quot;) (p_lin_pr + p_log_pr &amp; geom_hline(data = tibble(height = c(0, 272), type = 1:2), aes(yintercept = height, linetype = factor(type)), size = .4) &amp; geom_line(color = clr2, alpha = .25) &amp; scale_linetype_manual(values = c(`1` = 3, `2` = 1), guide = &quot;none&quot;) &amp; coord_cartesian(xlim = range(data_adults$weight), ylim = c(-100, 400)) &amp; theme(plot.title = element_markdown())) + p_lnorm The log-normal prior seems more sensible, so we update the model priors as such: \\[ \\begin{array}{cccr} \\beta &amp; \\sim &amp; Log-Normal(0, 1) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\end{array} \\] 5.4.1 Finding the posterior Distribution xbar &lt;- mean(data_adults$weight) model_hight &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * ( weight - xbar ), alpha ~ dnorm( 178, 20 ), beta ~ dlnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_adults ) Table of marginal distributions of the parameters after training the model on the data centered_remember_hw &lt;- precis(model_hight) %&gt;% round(digits = 3) %&gt;% as_tibble(rownames = NA) centered_remember_hw %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 154.602 0.270 154.170 155.034 beta 0.903 0.042 0.836 0.970 sigma 5.072 0.191 4.766 5.377 We also need thevariance-covariance matrix to fully describe the audratic approximation completely: model_hight %&gt;% vcov() %&gt;% round(digits = 2) %&gt;% as.data.frame(row.names = row.names(.)) %&gt;% knitr::kable() alpha beta sigma alpha 0.07 0 0.00 beta 0.00 0 0.00 sigma 0.00 0 0.04 model_hight_smp &lt;- extract.samples(model_hight) %&gt;% as_tibble() model_hight_smp_mean &lt;- model_hight_smp %&gt;% summarise(across(.cols = everything(), mean)) model_hight_smp %&gt;% ggpairs( lower = list(continuous = wrap(ggally_points, colour = clr1, size = .2, alpha = .1)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll1, color = clr1, adjust = .5)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) Plotting the posterior distribution against the data ggplot(data_adults, aes(x = weight, y = height)) + geom_point(color = clr0d) + stat_function(fun = function(x){model_hight_smp_mean$alpha + model_hight_smp_mean$beta * (x - xbar)}, color = clr2, n = 2) A demonstration of the the effect of sample size on the uncertainty of the linear fit sub_model &lt;- function(N = 10){ data_inner &lt;- data_adults[1:N,] xbar &lt;- mean(data_inner$weight) model_hight_inner &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * ( weight - xbar ), alpha ~ dnorm( 178, 20 ), beta ~ dlnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_inner ) model_hight_smp_inner &lt;- extract.samples(model_hight_inner) %&gt;% as_tibble() %&gt;% sample_n(20) ggplot(data_inner, aes(x = weight, y = height)) + geom_point(color = clr0d) + (purrr::map(1:20, function(i){stat_function( fun = function(x){model_hight_smp_inner$alpha[i] + model_hight_smp_inner$beta[i] * (x - xbar)}, color = clr2, n = 2, alpha = .1)})) + labs(title = glue(&quot;N: {N}&quot;)) } sub_model(10) + sub_model(50) + sub_model(150) + sub_model(352) adding intervals mu_at_50 &lt;- model_hight_smp %&gt;% mutate(mu_at_50 = alpha + beta * (50 - xbar)) p_density &lt;- mu_at_50 %&gt;% ggplot(aes(x = mu_at_50)) + geom_density(adjust = .5, color = clr0d, fill = fll0) + stat_function(fun = function(x){demp(x, obs = mu_at_50$mu_at_50, density.arg.list = list(adjust = .5))}, xlim = mu_at_50$mu_at_50 %&gt;% PI(), geom = &quot;area&quot;, fill = fll2, color = clr2) + geom_vline(data = tibble(weights = mu_at_50$mu_at_50 %&gt;% PI()), aes(xintercept = weights), linetype = 3)+ scale_x_continuous(glue(&quot;{mth(&#39;*\\U03BC*&#39;)} | weight = 50&quot;), limits = c(157.7, 160.8)) + theme(axis.title.x = element_markdown()) mu_at_50$mu_at_50 %&gt;% PI() #&gt; 5% 94% #&gt; 158.5857 159.6717 weight_seq &lt;- seq(from = 25, to = 70, by = 1) model_hight_mu &lt;- link(model_hight, data = data.frame(weight = weight_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) p_dots &lt;- model_hight_mu %&gt;% ggplot(aes(x = weight, y = height)) + geom_point(aes(color = weight == 50), alpha = .1, size = .3) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) model_hight_mu_interval &lt;- model_hight_mu %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() p_interval &lt;- model_hight_mu_interval %&gt;% ggplot(aes(x = weight)) + geom_point(data = data_adults, aes(y = height), color = clr0, size = .3) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr2, alpha = .35) + geom_line(aes(y = mean)) p_density + p_dots + p_interval Prediction intervals model_hight_sd &lt;- sim(model_hight, data = data.frame(weight = weight_seq), n = 1e4) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq) %&gt;% pivot_longer(cols = `25`:`70`, names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) model_hight_sd %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = weight)) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data_adults, aes(y = height), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = model_hight_mu_interval, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_hight_mu_interval, aes(y = mean)) 5.5 Curves from lines The full data (including kids) is clearly curved in shape: ggplot(data = data, aes(x = weight, y = height)) + geom_point(color = clr0d) We will work on standardized \\(x\\) values to prevent “numerical glitches” by transforming \\(x\\) via \\(x_s = (\\frac{x - \\bar{x}}{sd(x)})\\): quadratic polynomial fit \\[ \\begin{array}{cccr} h_i &amp; {\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_1 x_i + \\beta_2 x_i ^ 2&amp; \\textrm{[linear model]}\\\\ % alternatively \\overline{x} \\alpha &amp; \\sim &amp; Normal(178, 20) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_1 &amp; \\sim &amp; Log-Normal(0, 1) &amp; \\textrm{[$\\beta_1$ prior]}\\\\ \\beta_2 &amp; \\sim &amp; Normal(0, 1) &amp; \\textrm{[$\\beta_2$ prior]}\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] cubic polynomial fit \\[ \\begin{array}{cccr} h_i &amp; {\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_1 x_i + \\beta_2 x_i ^ 2 + \\beta_3 x_i ^ 3 &amp; \\textrm{[linear model]}\\\\ % alternatively \\overline{x} \\alpha &amp; \\sim &amp; Normal(178, 20) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_1 &amp; \\sim &amp; Log-Normal(0, 1) &amp; \\textrm{[$\\beta_1$ prior]}\\\\ \\beta_2 &amp; \\sim &amp; Normal(0, 1) &amp; \\textrm{[$\\beta_2$ prior]}\\\\ \\beta_3 &amp; \\sim &amp; Normal(0, 1) &amp; \\textrm{[$\\beta_3$ prior]}\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] plot_model_intervals &lt;- function(mod, data, weight_seq = list(weight_s = seq(from = min(data_model$weight_s), to = max(data_model$weight_s), length.out = 70))){ model_hight_mu_interval &lt;- link(mod, data = weight_seq) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq$weight_s ) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight_s&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight_s = as.numeric(weight_s)) %&gt;% group_by(weight_s) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() model_hight_sd &lt;- sim(mod, data = weight_seq, n = 1e4) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq$weight_s) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight_s&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight_s = as.numeric(weight_s)) model_hight_sd %&gt;% group_by(weight_s) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = weight_s)) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data, aes(y = height), color = rgb(0,0,0,.25), size = .4) + geom_ribbon(data = model_hight_mu_interval, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_hight_mu_interval, aes(y = mean)) } data_model &lt;- data %&gt;% mutate(weight_s = (weight - mean(weight))/sd(weight), weight_s2 = weight_s ^ 2, weight_s3 = weight_s ^ 3) model_hight_s1 &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * weight_s , alpha ~ dnorm( 178, 20 ), beta ~ dlnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_model ) model_hight_s2 &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta1 * weight_s + beta2 * weight_s2, alpha ~ dnorm( 178, 20 ), beta1 ~ dlnorm( 0, 1 ), beta2 ~ dnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_model ) model_hight_s3 &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta1 * weight_s + beta2 * weight_s2 + beta3 * weight_s3, alpha ~ dnorm( 178, 20 ), beta1 ~ dlnorm( 0, 1 ), beta2 ~ dnorm( 0, 1 ), beta3 ~ dnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_model ) plot_model_intervals(model_hight_s1, data_model) + plot_model_intervals(model_hight_s2, data_model, weight_seq = tibble(weight_s = seq(from = min(data_model$weight_s), to = max(data_model$weight_s), length.out = 70), weight_s2 = weight_s ^ 2)) + plot_model_intervals(model_hight_s3, data_model, weight_seq = tibble(weight_s = seq(from = min(data_model$weight_s), to = max(data_model$weight_s), length.out = 70), weight_s2 = weight_s ^ 2, weight_s3 = weight_s ^ 3)) plot_model_intervals(model_hight_s3, data_model, weight_seq = tibble(weight_s = seq(from = min(data_model$weight_s), to = max(data_model$weight_s), length.out = 70), weight_s2 = weight_s ^ 2, weight_s3 = weight_s ^ 3)) + scale_x_continuous(&quot;weight [kg]&quot;, breaks = (seq(5,65, length.out = 5) - mean(data_model$weight)) / sd(data_model$weight), labels = seq(5,65, length.out = 5)) + labs(y = &quot;height [cm]&quot;) 5.6 Splines Loading the Hanami data (花見), containing the historical dates of first annual cherry tree blossom. data(cherry_blossoms) precis(cherry_blossoms) %&gt;% as_tibble() %&gt;% knitr::kable() mean sd 5.5% 94.5% histogram 1408.000000 350.8845964 867.77000 1948.23000 ▇▇▇▇▇▇▇▇▇▇▇▇▁ 104.540508 6.4070362 94.43000 115.00000 ▁▂▅▇▇▃▁▁ 6.141886 0.6636479 5.15000 7.29470 ▁▃▅▇▃▂▁▁ 7.185151 0.9929206 5.89765 8.90235 ▁▂▅▇▇▅▂▂▁▁▁▁▁▁▁ 5.098941 0.8503496 3.78765 6.37000 ▁▁▁▁▁▁▁▃▅▇▃▂▁▁▁ cherry_blossoms %&gt;% ggplot(aes(x = year, y = doy)) + geom_point(color = clr2, alpha = .3) + labs(y = &quot;Day of first blossom&quot;) data_cherry &lt;- cherry_blossoms %&gt;% filter(complete.cases(doy)) %&gt;% as_tibble() n_knots &lt;- 15 knot_list &lt;- quantile(data_cherry$year, probs = seq(0, 1, length.out = n_knots)) library(splines) b_spline_cherry &lt;- bs(data_cherry$year, knots = knot_list[-c(1, n_knots)], degree = 3, intercept = TRUE) b_spline_tib &lt;- b_spline_cherry %&gt;% as_tibble() %&gt;% set_names(nm = str_pad(1:17, width = 2, pad = 0)) %&gt;% bind_cols(select(data_cherry, year)) %&gt;% pivot_longer(cols = -year, names_to = &quot;bias_function&quot;, values_to = &quot;bias&quot;) ggplot() + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = &quot;black&quot;) + geom_line(data = b_spline_tib, aes(x = year, y = bias, color = as.numeric(bias_function) , group = bias_function), size = 1, alpha = .75) + scale_color_gradientn(colours = c(&quot;black&quot;, clr0d, clr2), guide = &quot;none&quot;)+ theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) B-spline model: \\[ \\begin{array}{cccr} D_i &amp; \\sim &amp; Normal( \\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\sum_{k=1}^K w_k B_{k,i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(100, 10) &amp; \\textrm{[$\\alpha$ prior]}\\\\ w_i &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[w prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_cherry &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- a + B %*% w, a ~ dnorm(100, 10), w ~ dnorm(0, 10), sigma ~ dexp(1) ), data = list(D = data_cherry$doy, B = b_spline_cherry), start = list(w = rep(0, ncol(b_spline_cherry))) ) precis(model_cherry, depth = 2) %&gt;% round(digits = 2) %&gt;% as_tibble(rownames = NA) %&gt;% knitr::kable() mean sd 5.5% 94.5% w[1] -3.02 3.86 -9.19 3.15 w[2] -0.83 3.87 -7.01 5.36 w[3] -1.06 3.58 -6.79 4.67 w[4] 4.85 2.88 0.25 9.44 w[5] -0.84 2.87 -5.43 3.76 w[6] 4.32 2.91 -0.33 8.98 w[7] -5.32 2.80 -9.79 -0.84 w[8] 7.85 2.80 3.37 12.33 w[9] -1.00 2.88 -5.61 3.60 w[10] 3.04 2.91 -1.61 7.69 w[11] 4.67 2.89 0.05 9.29 w[12] -0.15 2.87 -4.74 4.43 w[13] 5.56 2.89 0.95 10.18 w[14] 0.72 3.00 -4.08 5.51 w[15] -0.80 3.29 -6.06 4.46 w[16] -6.96 3.38 -12.36 -1.57 w[17] -7.67 3.22 -12.82 -2.52 a 103.35 2.37 99.56 107.13 sigma 5.88 0.14 5.65 6.11 cherry_samples &lt;- extract.samples(model_cherry) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;a&quot;, &quot;sigma&quot;, str_pad(1:17, 2,pad = 0))) cherry_samples_mu &lt;- cherry_samples %&gt;% summarise(across(everything(), mean)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;bias_function&quot;, values_to =&quot;weight&quot;) ggplot() + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = &quot;black&quot;) + geom_line(data = b_spline_tib %&gt;% left_join(cherry_samples_mu), aes(x = year, y = bias * weight, color = as.numeric(bias_function) , group = bias_function), size = 1, alpha = .75) + scale_color_gradientn(colours = c(&quot;black&quot;, clr0d, clr2), guide = &quot;none&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) model_cherry_samples &lt;- link(model_cherry) %&gt;% as_tibble()%&gt;% set_names(nm = data_cherry$year) %&gt;% pivot_longer(cols = everything(), names_to = &quot;year&quot;, values_to = &quot;doy&quot;) %&gt;% mutate(year = as.numeric(year)) %&gt;% arrange(year) model_cherry_stats &lt;- model_cherry_samples %&gt;% group_by(year) %&gt;% nest() %&gt;% mutate(mean = map_dbl(data, function(data){mean(data$doy)}), PI_lower = map_dbl(data, function(data){PI(data$doy)[1]}), PI_upper = map_dbl(data, function(data){PI(data$doy)[2]})) model_cherry_stats %&gt;% ggplot(aes(x = year)) + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = &quot;black&quot;) + geom_point(data = cherry_blossoms, aes(y = doy), color = clr2, alpha = .3) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr2, alpha = .35) + geom_line(aes(y = mean)) + labs(y = &quot;Day of first blossom&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) library(rlang) chapter4_models &lt;- env( data_adults = data_adults, model_heights_quap_weak_prior = model_heights_quap_weak_prior, model_hight = model_hight, data_model = data_model, model_hight_s1 = model_hight_s1, model_hight_s2 = model_hight_s2, model_hight_s3 = model_hight_s3, data_cherry = data_cherry, b_spline_cherry = b_spline_cherry, model_cherry = model_cherry ) write_rds(chapter4_models, &quot;envs/chapter4_models.rds&quot;) 5.7 Homework E1 \\[ \\begin{array}{cccr} y_i &amp; \\sim &amp; Normal( \\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\mu$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] E2 There are two parameters: \\(\\mu\\) \\(\\sigma\\) E3 \\[ \\begin{array}{rcl} Pr( \\mu, \\sigma | y ) &amp; = &amp; \\frac{Normal( y | \\mu, \\sigma ) Pr(y)}{Pr(\\mu, \\sigma)} \\\\ Pr( \\mu, \\sigma | y ) &amp; = &amp; \\frac{\\prod_i Pr( y_i | \\mu, \\sigma) Normal( \\mu | 0, 10) Exponential(\\sigma | 1) }{ \\int\\int\\prod_i Pr( y_i | \\mu, \\sigma) Normal( \\mu | 0, 10) Exponential(\\sigma | 1)d\\mu d\\sigma} \\end{array} \\] E4 \\[ \\begin{array}{cccr} y_i &amp; \\sim &amp; Normal( \\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta x_i &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta &amp; \\sim &amp; Normal(0, 1) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(2) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] E5 There are three parameters \\(\\alpha\\) \\(\\beta\\) \\(\\sigma\\) M1 \\[ \\begin{array}{cccr} y_i &amp; \\sim &amp; Normal( \\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\mu$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] Using grid approximation n &lt;- 5e3 sample_data &lt;- tibble(y = rnorm(n = n, # sample size mean = rnorm(n = n, mean = 0, sd = 10), # mu prior sd = rexp(n = n, rate = 1))) # sigma prior sample_data %&gt;% ggplot(aes(x = y)) + geom_density(color = clr0d, fill = fll0) + stat_function(fun = function(x){dnorm(x = x, mean = 0, sd = 10 )}, geom = &quot;line&quot;, linetype = 3, color = clr1) + scale_x_continuous(limits = c(-50, 50)) + labs(y = &quot;density&quot;) M2 quap_formula &lt;- alist( y ~ dnorm(mu, sigma), # likelihood mu ~ dnorm(0, 10), # mu prior sigma ~ exp(1) # sigma prior ) M3 \\[ \\begin{array}{cccr} y_i &amp; \\sim &amp; Normal( \\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta x_i &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta &amp; \\sim &amp; Uniform(0, 1) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] M4 \\[ \\begin{array}{cccr} h_i &amp; \\sim &amp; Normal( \\mu, \\sigma) &amp; \\textrm{[likelihood]} \\\\ \\mu_i &amp; = &amp; \\alpha + \\beta h_i &amp; \\textrm{[linear model]} \\\\ \\alpha &amp; \\sim &amp; Normal(150, 5) &amp; \\textrm{[$\\alpha$ prior, starting size]} \\\\ \\beta &amp; \\sim &amp; Uniform(0, 10) &amp; \\textrm{[$\\beta$ prior, yearly growth]} \\\\ \\sigma &amp; \\sim &amp; Normal(0, 8) &amp; \\textrm{[$\\sigma$ prior, size variation]} \\end{array} \\] M5 No, the chosen prior for \\(\\beta\\) already covers this information: \\(\\beta \\sim Uniform(0, 10)\\) is always positive, forcing a positive growth per year. M6 Limiting the variance of height to 64cm could be done in different ways: by choosing a uniform prior with fixed boundaries [eg. \\(Uniform(0,64)\\)], or by limiting the variance of an unbound distribution [eg. \\(\\sigma\\) for a normal distribution. 99.7% of the mass is within \\(3 \\sigma\\), so \\(Normal(32, 10)\\) would do as well]. M7 model_uncentered &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * weight, alpha ~ dnorm( 178, 20), beta ~ dlnorm( 0, 1 ), sigma ~ dunif( 0, 50 ) ), data = data_adults ) precis(model_uncentered) %&gt;% round(digits = 3) %&gt;% as_tibble(rownames = NA) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 114.534 1.898 111.501 117.567 beta 0.891 0.042 0.824 0.957 sigma 5.073 0.191 4.767 5.378 model_uncentered %&gt;% vcov() %&gt;% round(digits = 2) %&gt;% as.data.frame(row.names = row.names(.)) %&gt;% knitr::kable() alpha beta sigma alpha 3.60 -0.08 0.01 beta -0.08 0.00 0.00 sigma 0.01 0.00 0.04 compare to the centered version: centered_remember_hw %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 154.602 0.270 154.170 155.034 beta 0.903 0.042 0.836 0.970 sigma 5.072 0.191 4.766 5.377 model_hight %&gt;% vcov() %&gt;% round(digits = 2) %&gt;% as.data.frame(row.names = row.names(.)) %&gt;% knitr::kable() alpha beta sigma alpha 0.07 0 0.00 beta 0.00 0 0.00 sigma 0.00 0 0.04 The un-centered model shows higher covariances between \\(\\alpha\\) and all other parameters. model_uncentered_mu &lt;- link(model_uncentered, data = data.frame(weight = weight_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) model_uncentered_mu_interval &lt;- model_uncentered_mu %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() model_uncentered_sd &lt;- sim(model_uncentered, data = data.frame(weight = weight_seq), n = 5e3) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq) %&gt;% pivot_longer(cols = `25`:`70`, names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) p_1 &lt;- model_uncentered_sd %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = weight)) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data_adults, aes(y = height), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = model_uncentered_mu_interval, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_uncentered_mu_interval, aes(y = mean))+ labs(title = &quot; uncentered&quot;) p_2 &lt;- model_hight_sd %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = weight)) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data_adults, aes(y = height), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = model_hight_mu_interval, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_hight_mu_interval, aes(y = mean))+ labs(title = &quot; centered&quot;) p_1 + p_2 Hmm 🤔`: I can’t see a difference - maybe that is the point? M8 spline_check &lt;- function(n_knots = 15, inner = TRUE){ knot_list &lt;- quantile(data_cherry$year, probs = seq(0, 1, length.out = n_knots)) b_spline_cherry &lt;- bs(data_cherry$year, knots = knot_list[-c(1, n_knots)], degree = 3, intercept = TRUE) b_spline_tib &lt;- b_spline_cherry %&gt;% as_tibble() %&gt;% set_names(nm = str_pad(1:(n_knots+2), width = 2, pad = 0)) %&gt;% bind_cols(select(data_cherry, year)) %&gt;% pivot_longer(cols = -year, names_to = &quot;bias_function&quot;, values_to = &quot;bias&quot;) model_cherry &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- a + B %*% w, a ~ dnorm(100, prior_sd_a), w ~ dnorm(0, prior_sd_w), sigma ~ dexp(1) ), data = list(D = data_cherry$doy, B = b_spline_cherry), start = list(w = rep(0, ncol(b_spline_cherry))) ) cherry_samples &lt;- extract.samples(model_cherry) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;a&quot;, &quot;sigma&quot;, str_pad(1:(n_knots+2), 2,pad = 0))) cherry_samples_mu &lt;- cherry_samples %&gt;% summarise(across(everything(), mean)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;bias_function&quot;, values_to =&quot;weight&quot;) model_cherry_samples &lt;- link(model_cherry) %&gt;% as_tibble()%&gt;% set_names(nm = data_cherry$year) %&gt;% pivot_longer(cols = everything(), names_to = &quot;year&quot;, values_to = &quot;doy&quot;) %&gt;% mutate(year = as.numeric(year)) %&gt;% arrange(year) model_cherry_stats &lt;- model_cherry_samples %&gt;% group_by(year) %&gt;% nest() %&gt;% mutate(mean = map_dbl(data, function(data){mean(data$doy)}), PI_lower = map_dbl(data, function(data){PI(data$doy)[1]}), PI_upper = map_dbl(data, function(data){PI(data$doy)[2]})) p_splines_pure &lt;- ggplot() + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = rgb(0,0,0,.5)) + geom_line(data = b_spline_tib %&gt;% left_join(cherry_samples_mu), aes(x = year, y = bias * weight, color = as.numeric(bias_function) , group = bias_function), size = .3, alpha = .75) + scale_color_gradientn(colours = c(&quot;black&quot;, clr0d, clr1), guide = &quot;none&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.title.x = element_blank(), axis.text.x = element_blank()) + labs(title = glue(&quot;{n_knots} kn, sd a: {prior_sd_a}, sd w: {prior_sd_w}&quot;)) p_splines_fitted &lt;- model_cherry_stats %&gt;% ggplot(aes(x = year)) + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = rgb(0,0,0,.5)) + geom_point(data = cherry_blossoms, aes(y = doy), color = clr1, alpha = .1, size = .2) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .65) + geom_line(aes(y = mean)) + labs(y = &quot;Day of first blossom&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.title.x = element_blank(), axis.text.x = element_blank()) if(inner){ p_splines_pure &lt;- p_splines_pure + theme(axis.title.y = element_blank()) p_splines_fitted &lt;- p_splines_fitted + theme(axis.title.y = element_blank()) } p_splines_pure + p_splines_fitted + plot_layout(ncol = 1, heights = c(.5, 1)) } set.seed(14) prior_sd_a = 10 prior_sd_w &lt;- 10 p1 &lt;- spline_check(n_knots = 3, inner = FALSE) prior_sd_w &lt;- 50 p2 &lt;- spline_check(n_knots = 3, inner = TRUE) set.seed(42) prior_sd_w &lt;- 100 p3 &lt;- spline_check(n_knots = 3, inner = TRUE) p1 | p2 | p3 set.seed(41) prior_sd_w &lt;- 10 p1 &lt;- spline_check(n_knots = 10, inner = FALSE) prior_sd_w &lt;- 50 p2 &lt;- spline_check(n_knots = 10, inner = TRUE) prior_sd_w &lt;- 100 p3 &lt;- spline_check(n_knots = 10, inner = TRUE) p1 | p2 | p3 set.seed(42) prior_sd_w &lt;- 10 p1 &lt;- spline_check(n_knots = 30, inner = FALSE) prior_sd_w &lt;- 50 p2 &lt;- spline_check(n_knots = 30, inner = TRUE) prior_sd_w &lt;- 100 p3 &lt;- spline_check(n_knots = 30, inner = TRUE) p1 | p2 | p3 They control the division of data and the initial scale for the weighting H1 model_hight_smp %&gt;% mutate(mu_at_50 = alpha + beta * (50 - xbar), `46.95` = alpha + beta * (46.95 - xbar), `43.72` = alpha + beta * (43.72 - xbar), `64.78` = alpha + beta * (64.78 - xbar), `32.59` = alpha + beta * (32.59 - xbar), `54.63` = alpha + beta * (54.63 - xbar)) %&gt;% dplyr::select(`46.95`:`54.63` ) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% group_by(weight) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(median_weight = map_dbl(data, function(x){median(x$height)}), mean_weight = map_dbl(data, function(x){mean(x$height)}), lower_89 = map_dbl(data, function(x){PI(x$height)[[1]]}), upper_89 = map_dbl(data, function(x){PI(x$height)[[2]]}), individual = 1:5) %&gt;% dplyr::select(individual, weight,median_weight:upper_89) %&gt;% mutate(across(everything(), .fns = ~ round(as.numeric(.x), digits = 5))) %&gt;% knitr::kable() individual weight median_weight mean_weight lower_89 upper_89 1 46.95 156.3742 156.3739 155.9302 156.8173 2 43.72 153.4580 153.4585 153.0147 153.8970 3 64.78 172.4641 172.4671 171.0703 173.8520 4 32.59 143.4179 143.4127 142.4567 144.3474 5 54.63 163.3039 163.3058 162.5306 164.0879 H2 data_children &lt;- data %&gt;% filter(age &lt; 18) ggplot(data_children, aes(weight, height)) + geom_point(shape = 21, size = 1.5, color = clr1, fill = fll1) xbar_children &lt;- mean(data_children$weight) model_hight_children &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * ( weight - xbar_children ), alpha ~ dnorm( 178, 20 ), beta ~ dlnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_children ) precis(model_hight_children) %&gt;% round(digits = 3) %&gt;% as_tibble(rownames = NA) #&gt; # A tibble: 3 x 4 #&gt; mean sd `5.5%` `94.5%` #&gt; * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 108. 0.609 107. 109. #&gt; 2 2.72 0.068 2.61 2.83 #&gt; 3 8.44 0.431 7.75 9.13 model_hight_children %&gt;% vcov() %&gt;% cov2cor() %&gt;% round(digits = 2) %&gt;% as.data.frame(row.names = row.names(.)) %&gt;% knitr::kable() alpha beta sigma alpha 1.00 0.00 0.01 beta 0.00 1.00 -0.01 sigma 0.01 -0.01 1.00 model_hight_smp_children &lt;- extract.samples(model_hight_children) %&gt;% as_tibble() model_hight_smp_mean_children &lt;- model_hight_smp_children %&gt;% summarise(across(.cols = everything(), mean)) ggplot(data_children, aes(x = weight, y = height)) + geom_point(color = clr0d) + stat_function(fun = function(x){model_hight_smp_mean_children$alpha + model_hight_smp_mean_children$beta * (x - xbar_children)}, color = clr2, n = 2) model_hight_smp_mean_children %&gt;% knitr::kable() alpha beta sigma 108.3763 2.716742 8.44416 A child get 27.1674225128912 cm taller per 10 kg weight. weight_seq_children &lt;- seq(from = 2, to = 45, by = 1) model_hight_mu_children &lt;- link(model_hight_children, data = data.frame(weight = weight_seq_children)) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq_children) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) model_hight_mu_interval_children &lt;- model_hight_mu_children %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() model_hight_sd_children &lt;- sim(model_hight_children, data = data.frame(weight = weight_seq_children), n = 1e4) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq_children) %&gt;% pivot_longer(cols = `2`:`45`, names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) model_hight_sd_children %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = weight)) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data_children, aes(y = height), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = model_hight_mu_interval_children, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_hight_mu_interval_children, aes(y = mean)) The model seems to be systematically overestimate the height for the more extreme weights (very light and rather heavy). The relationship does not appear to be linear in the first place, so a non-lnear fit would be better - ideally one that is biologically motivated. H3 data_log &lt;- data %&gt;% mutate(weight_log = log10(weight)) xbar_log &lt;- mean(data_log$weight_log) model_hight_log &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * ( weight_log - xbar_log ), alpha ~ dnorm( 179, 20 ), beta ~ dlnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_log ) precis(model_hight_log) %&gt;% round(digits = 3) %&gt;% as_tibble(rownames = NA) #&gt; # A tibble: 3 x 4 #&gt; mean sd `5.5%` `94.5%` #&gt; * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 138. 0.22 138. 139. #&gt; 2 108. 0.881 107. 110. #&gt; 3 5.14 0.156 4.89 5.38 model_hight_log %&gt;% vcov() %&gt;% cov2cor() %&gt;% round(digits = 2) %&gt;% as.data.frame(row.names = row.names(.)) %&gt;% knitr::kable() alpha beta sigma alpha 1 0 0 beta 0 1 0 sigma 0 0 1 model_hight_smp_log &lt;- extract.samples(model_hight_log) %&gt;% as_tibble() model_hight_smp_mean_log &lt;- model_hight_smp_log %&gt;% summarise(across(.cols = everything(), mean)) ggplot(data_log, aes(x = weight_log, y = height)) + geom_point(color = clr0d) + stat_function(fun = function(x){model_hight_smp_mean_log$alpha + model_hight_smp_mean_log$beta * (x - xbar_log)}, color = clr2, n = 2) weight_seq_log &lt;- log10(seq(from = 2, to = 70, by = 1)) model_hight_mu_log &lt;- link(model_hight_log, data = data.frame(weight_log = weight_seq_log)) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq_log) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight_log&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight_log = as.numeric(weight_log)) model_hight_mu_interval_log &lt;- model_hight_mu_log %&gt;% group_by(weight_log) %&gt;% summarise(mean = mean(height), PI_lower = PI(height, prob = .97)[1], PI_upper = PI(height, prob = .97)[2]) %&gt;% ungroup() model_hight_sd_log &lt;- sim(model_hight_log, data = data.frame(weight_log = weight_seq_log), n = 1e4) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq_log) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight_log&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight_log = as.numeric(weight_log)) model_hight_sd_log %&gt;% group_by(weight_log) %&gt;% summarise(mean = mean(height), PI_lower = PI(height, prob = .97)[1], PI_upper = PI(height, prob = .97)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = 10^(weight_log))) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data, aes(x = weight, y = height), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = model_hight_mu_interval_log, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_hight_mu_interval_log, aes(y = mean), linetype = 3) H4 N &lt;- 25 cubic_priors &lt;- tibble(n = 1:N, alpha = rnorm( n = N, mean = -128, sd = 20 ), beta_1 = rnorm( n = N, mean = 11, sd = .1), beta_2 = rnorm( n = N, mean = -.1, sd = .01)) %&gt;% expand(nesting(n, alpha, beta_1, beta_2), weight = range(data_adults$weight)) %&gt;% mutate(height = alpha + beta_1 * (weight - mean(data_adults$weight)) + beta_2 * (weight - mean(data_adults$weight))^2) ggplot(cubic_priors, aes(x = weight, y = height, group = n)) + pmap(cubic_priors, function(alpha, beta_1, beta_2, ...){ stat_function(fun = function(x){alpha + beta_1 * x + beta_2 * x^2}, color = fll1, alpha = .1, n = 100, lwd = .2, geom = &quot;line&quot;) }) + geom_hline(data = tibble(height = c(0, 272), type = 1:2), aes(yintercept = height, linetype = factor(type)), size = .4) + scale_linetype_manual(values = c(`1` = 3, `2` = 1), guide = &quot;none&quot;) + coord_cartesian(xlim = range(data_adults$weight), ylim = c(-100, 400)) + theme(plot.title = element_markdown()) H5 cherry_blossoms_tib &lt;- cherry_blossoms %&gt;% as_tibble() %&gt;% filter(!is.na(temp) &amp; !is.na(doy)) %&gt;% mutate(temp_s = (temp - mean(temp, na.rm = TRUE))/sd(temp, na.rm = TRUE)) temp_bar &lt;- mean(cherry_blossoms_tib$temp, na.rm = TRUE) temp_sd &lt;- sd(cherry_blossoms_tib$temp, na.rm = TRUE) cherry_blossoms_tib %&gt;% as_tibble() %&gt;% ggplot(aes(x = temp, y = doy)) + geom_point(size = 1.2, color = fll2) \\[ \\begin{array}{cccr} d_i &amp; \\sim &amp; Normal( \\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta x_i &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(105, 10) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_temp &lt;- quap( flist = alist( doy ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * temp_s, alpha ~ dnorm( 105, 10 ), beta ~ dnorm( 0, 10 ), sigma ~ dexp( 1 ) ), data = cherry_blossoms_tib ) temp_seq &lt;- (seq(from = 4.5, to = 8.4, by = .1) - temp_bar) / temp_sd model_temp_mu &lt;- link(model_temp, data = data.frame(temp_s = temp_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = temp_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;temp_s&quot;, values_to = &quot;doy&quot;) %&gt;% mutate(temp_s = as.numeric(temp_s), temp = temp_s * temp_sd + temp_bar) model_temp_mu_interval &lt;- model_temp_mu %&gt;% group_by(temp) %&gt;% summarise(mean = mean(doy), PI_lower = PI(doy)[1], PI_upper = PI(doy)[2]) %&gt;% ungroup() model_temp_sd &lt;- sim(model_temp, data = data.frame(temp_s = temp_seq), n = 1e4) %&gt;% as_tibble() %&gt;% set_names(nm = temp_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;temp_s&quot;, values_to = &quot;doy&quot;) %&gt;% mutate(temp_s = as.numeric(temp_s), temp = temp_s * temp_sd + temp_bar) model_temp_sd %&gt;% group_by(temp) %&gt;% summarise(mean = mean(doy), PI_lower = PI(doy)[1], PI_upper = PI(doy)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = temp)) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = cherry_blossoms_tib, aes(y = doy), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = model_temp_mu_interval, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_temp_mu_interval, aes(y = mean)) H6 n_knots &lt;- 15 knot_list &lt;- quantile(data_cherry$year, probs = seq(0, 1, length.out = n_knots)) b_spline_cherry &lt;- bs(data_cherry$year, knots = knot_list[-c(1, n_knots)], degree = 3, intercept = TRUE) prior_predictive &lt;- function(n = 100, prior_w = function(knots){rnorm(n = knots + 2, 0, 10)}){ tibble(.draw = 1:n, alpha = rnorm(n, 100, 10), w = purrr::map(seq_len(n), # random weighting of knots function(x, knots){ w &lt;- prior_w(knots) w }, knots = n_knots)) %&gt;% mutate(mu = purrr::map2(alpha, w, .f = function(alpha, w, b){ mu &lt;- alpha + b %*% w mu %&gt;% as_tibble(.name_repair = ~&quot;mu&quot;) %&gt;% mutate(year = data_cherry$year, .before = 1) }, b = b_spline_cherry)) %&gt;% unnest(cols = mu) } p1 &lt;- prior_predictive(n = 50) %&gt;% ggplot(aes(x = year, y = mu)) + labs(title = &quot;weight sd: 10&quot;) p2 &lt;- prior_predictive(n = 50, prior_w = function(knots){rnorm(n = knots + 2, mean = 0, sd = .3)}) %&gt;% ggplot(aes(x = year, y = mu)) + labs(title = &quot;weight sd: 0.3&quot;) p1 + p2 &amp; geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = &quot;black&quot;) &amp; geom_line(aes(group = .draw, color = .draw), alpha = .2) &amp; scale_color_gradientn(colours = c(&quot;black&quot;, clr0d, clr2), guide = &quot;none&quot;) &amp; theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) H7 This one is missing… H8 set.seed(42) model_cherry2 &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- B %*% w, w ~ dnorm(0, 10), sigma ~ dexp(1) ), data = list(D = data_cherry$doy, B = b_spline_cherry), start = list(w = rep(0, ncol(b_spline_cherry))), control = list(maxit = 5000) ) precis(model_cherry2, depth = 2) %&gt;% round(digits = 2) %&gt;% as_tibble(rownames = NA) %&gt;% knitr::kable() mean sd 5.5% 94.5% w[1] 92.84 3.22 87.69 97.98 w[2] 102.76 3.09 97.83 107.69 w[3] 100.43 2.75 96.03 104.83 w[4] 108.34 1.64 105.71 110.96 w[5] 101.56 1.68 98.88 104.24 w[6] 106.97 1.74 104.19 109.75 w[7] 97.56 1.53 95.12 99.99 w[8] 110.64 1.53 108.19 113.08 w[9] 101.68 1.68 99.00 104.36 w[10] 105.75 1.73 102.99 108.52 w[11] 107.37 1.70 104.66 110.08 w[12] 102.67 1.65 100.02 105.31 w[13] 108.15 1.69 105.44 110.86 w[14] 103.77 1.87 100.78 106.75 w[15] 101.31 2.34 97.57 105.05 w[16] 95.98 2.44 92.08 99.87 w[17] 92.12 2.30 88.45 95.80 sigma 5.95 0.15 5.71 6.18 cherry_samples &lt;- extract.samples(model_cherry2) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;sigma&quot;, str_pad(1:17, 2,pad = 0))) cherry_samples_mu &lt;- cherry_samples %&gt;% summarise(across(everything(), mean)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;bias_function&quot;, values_to =&quot;weight&quot;) model_cherry_samples &lt;- link(model_cherry2) %&gt;% as_tibble()%&gt;% set_names(nm = data_cherry$year) %&gt;% pivot_longer(cols = everything(), names_to = &quot;year&quot;, values_to = &quot;doy&quot;) %&gt;% mutate(year = as.numeric(year)) %&gt;% arrange(year) model_cherry_stats &lt;- model_cherry_samples %&gt;% group_by(year) %&gt;% nest() %&gt;% mutate(mean = map_dbl(data, function(data){mean(data$doy)}), PI_lower = map_dbl(data, function(data){PI(data$doy)[1]}), PI_upper = map_dbl(data, function(data){PI(data$doy)[2]})) model_cherry_stats %&gt;% ggplot(aes(x = year)) + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = &quot;black&quot;) + geom_point(data = cherry_blossoms, aes(y = doy), color = clr2, alpha = .3) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr2, alpha = .35) + geom_line(aes(y = mean)) + labs(y = &quot;Day of first blossom&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) 5.8 {brms} section 5.8.1 linear model of adult height finding the posterior with {brms} brms_c4_adult_heights &lt;- brm(data = data_adults, family = gaussian, height ~ 1, prior = c(prior(normal(178, 20), class = Intercept), prior(uniform(0, 50), class = sigma)), iter = 31000, warmup = 30000, chains = 7, cores = 7, seed = 4, file = &quot;brms/brms_c4_adult_heights&quot;) posterior_summary(brms_c4_adult_heights,probs = c(.055, .945)) %&gt;% round(digits = 3) %&gt;% knitr::kable() Estimate Est.Error Q5.5 Q94.5 b_Intercept 154.604 0.418 153.947 155.275 sigma 7.742 0.304 7.276 8.250 lp__ -1226.924 1.058 -1228.772 -1225.927 brms_summary_plot &lt;- function(mod, n_chains = 4){ bayes_data &lt;- bayesplot::mcmc_areas_data(mod, prob = .95) %&gt;% filter(parameter != &quot;lp__&quot;) bayes_chains_data &lt;- bayesplot::mcmc_trace_data(mod) %&gt;% filter(parameter != &quot;lp__&quot;) p_dens &lt;- bayes_data %&gt;% filter(interval == &quot;outer&quot;) %&gt;% ggplot(aes(x = x, y = scaled_density)) + geom_area(color = clr0d, fill = fll0) + geom_area(data = bayes_data %&gt;% filter(interval == &quot;inner&quot;), color = clr2, fill = fll2) + facet_wrap(parameter ~ . , scales = &quot;free&quot;, ncol = 1) p_chains &lt;- bayes_chains_data %&gt;% ggplot(aes(x = iteration, y = value, group = chain)) + geom_line(aes(color = chain), alpha = .6) + facet_wrap(parameter ~ . , scales = &quot;free&quot;, ncol = 1) + scale_color_manual(values = scales::colour_ramp(colors = c(&quot;black&quot;,clr0d,clr2))(seq(0,1,length.out = n_chains))) p_dens + p_chains } brms_summary_plot(brms_c4_adult_heights) sampling from the posterior # equivalent to `rethinking::extract.samples()` brms_post &lt;- as_draws_df(brms_c4_adult_heights) %&gt;% as_tibble() head(brms_post) #&gt; # A tibble: 6 x 6 #&gt; b_Intercept sigma lp__ .chain .iteration .draw #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 154. 8.07 -1227. 1 1 1 #&gt; 2 154. 7.84 -1227. 1 2 2 #&gt; 3 155. 8.21 -1228. 1 3 3 #&gt; 4 154. 7.99 -1227. 1 4 4 #&gt; 5 155. 7.77 -1226. 1 5 5 #&gt; 6 154. 7.93 -1227. 1 6 6 select(brms_post, b_Intercept:sigma) %&gt;% cov() %&gt;% cov2cor() \\[\\begin{bmatrix} 1 &amp;-0.0192241582730429 \\\\-0.0192241582730429 &amp;1 \\\\ \\end{bmatrix}\\] brms_post %&gt;% dplyr::select(-(lp__:.draw)) %&gt;% pivot_longer(cols = everything()) %&gt;% group_by(name) %&gt;% summarise(quantiles = list(tibble(quant = quantile(value, probs = c(.5, .025, .75)), perc = str_c(&quot;q&quot;,names(quant)))) )%&gt;% unnest(quantiles) %&gt;% pivot_wider(names_from = perc, values_from = quant) %&gt;% mutate(across(.cols = -name, ~ round(.x, digits = 2))) %&gt;% knitr::kable() name q50% q2.5% q75% b_Intercept 154.59 153.80 154.89 sigma 7.73 7.18 7.94 posterior_summary(brms_post) \\[\\begin{bmatrix} 154.603681717344 &amp;0.417698657902289 &amp;153.797143321477 &amp;155.411872045528 &amp;7.7417412324536 &amp;0.304409842943225 \\\\7.17548730321122 &amp;8.38470302045445 &amp;-1226.92435306886 &amp;1.05809371424477 &amp;-1229.68670608237 &amp;-1225.89312495532 \\\\2.5 &amp;1.11817376920787 &amp;1 &amp;4 &amp;500.5 &amp;288.711081398222 \\\\25.975 &amp;975.025 &amp;2000.5 &amp;1154.84486692658 &amp;100.975 &amp;3900.025 \\\\ \\end{bmatrix}\\] the height model with a predictor data_adults &lt;- data_adults %&gt;% mutate(weight_centered = weight - mean(weight)) brms_c4_heights_x &lt;- brm(data = data_adults, family = gaussian, height ~ 1 + weight_centered, prior = c(prior(normal(178, 20), class = Intercept), prior(lognormal(0, 1), class = b), prior(uniform(0, 50), class = sigma)), iter = 28000, warmup = 27000, chains = 7, cores = 7, seed = 42, file = &quot;brms/brms_c4_heights_x&quot;) brms_summary_plot(brms_c4_heights_x, n_chains = 12) Logs and exps (m4.3b) brms_c4_heights_x_log &lt;- brm(data = data_adults, family = gaussian, bf(height ~ alpha + exp(logbeta) * weight_centered, alpha ~ 1, logbeta ~ 1, nl = TRUE), prior = c(prior(normal(178, 20), class = b, nlpar = alpha), prior(normal(0, 1), class = b, nlpar = logbeta), prior(uniform(0, 50), class = sigma)), iter = 31000, warmup = 30000, chains = 7, cores = 7, seed = 42, file = &quot;brms/brms_c4_heights_x_log&quot;) posterior_summary(brms_c4_heights_x)[1:3, ] %&gt;% round(digits = 2)%&gt;% as.data.frame() #&gt; Estimate Est.Error Q2.5 Q97.5 #&gt; b_Intercept 154.60 0.27 154.08 155.12 #&gt; b_weight_centered 0.90 0.04 0.82 0.99 #&gt; sigma 5.11 0.20 4.74 5.52 vcov(brms_c4_heights_x) %&gt;% cov2cor() %&gt;% round(3) %&gt;% as.data.frame() #&gt; Intercept weight_centered #&gt; Intercept 1.000 -0.003 #&gt; weight_centered -0.003 1.000 brms_posterior_samples &lt;- as_draws_df(brms_c4_heights_x) %&gt;% as_tibble() %&gt;% select(-(lp__:.draw)) brms_posterior_samples %&gt;% cov() %&gt;% cov2cor() %&gt;% round(digits = 3)%&gt;% as.data.frame() #&gt; b_Intercept b_weight_centered sigma #&gt; b_Intercept 1.000 -0.003 0.006 #&gt; b_weight_centered -0.003 1.000 0.000 #&gt; sigma 0.006 0.000 1.000 ggpairs(brms_posterior_samples, lower = list(continuous = wrap(ggally_points, colour = clr1, size = .3, alpha = .1)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll1, color = clr1, adjust = .5)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) weight_seq &lt;- tibble(weight = 25:70) %&gt;% mutate(weight_centered = weight - mean(data_adults$weight)) brms_model_hight_mu &lt;- fitted(brms_c4_heights_x, summary = FALSE, newdata = weight_seq) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq$weight_centered) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight_centered&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight_centered = as.numeric(weight_centered)) brms_model_hight_mu_interval &lt;- brms_model_hight_mu %&gt;% group_by(weight_centered) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() brms_model_hight_samples &lt;- predict(brms_c4_heights_x, summary = FALSE, newdata = weight_seq) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq$weight_centered) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight_centered&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight_centered = as.numeric(weight_centered)) brms_model_hight_samples_interval &lt;- brms_model_hight_samples %&gt;% group_by(weight_centered) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() data_adults %&gt;% ggplot(aes(x = weight_centered, y = height)) + geom_point(shape = 21, size = 2, color = clr1, fill = fll1) + geom_abline(intercept = fixef(brms_c4_heights_x)[1], slope = fixef(brms_c4_heights_x)[2]) brms_model_hight_mu_interval %&gt;% ggplot(aes(x = weight_centered)) + geom_ribbon(data = brms_model_hight_samples_interval, aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data_adults, aes(y = height), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(aes(y = mean)) brms_c4_curve_x &lt;- brm(data = data_model, family = gaussian, height ~ 1 + weight_s + weight_s2, prior = c(prior(normal(178, 20), class = Intercept), prior(lognormal(0, 1), class = b, coef = &quot;weight_s&quot;), prior(normal(0, 1), class = b, coef = &quot;weight_s2&quot;), prior(uniform(0, 50), class = sigma)), iter = 30000, warmup = 29000, chains = 4, cores = 4, seed = 4, file = &quot;brms/brms_c4_curve_x&quot;) brms_summary_plot(brms_c4_curve_x, n_chains = 4) weight_seq &lt;- tibble(weight_s = seq(from = -2.5, to = 2.5, length.out = 30)) %&gt;% mutate(weight_s2 = weight_s^2) fitd_quad &lt;- fitted(brms_c4_curve_x, newdata = weight_seq, probs = c(.055, .955)) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) %&gt;% as_tibble() pred_quad &lt;- predict(brms_c4_curve_x, newdata = weight_seq, probs = c(.055, .955)) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) %&gt;% as_tibble() ggplot(data = data_model, aes(x = weight_s)) + geom_ribbon(data = pred_quad, aes(ymin = Q5.5, ymax = Q95.5), fill = clr0d, alpha = .35) + geom_smooth(data = fitd_quad, aes(y = Estimate, ymin = Q5.5, ymax = Q95.5), stat = &quot;identity&quot;, color = clr1, fill = fll1, alpha = .35, size = .2) + geom_point(aes(y = height), color = rgb(0,0,0,.5), size = .6) num_knots &lt;- 15 knot_list &lt;- quantile(data_cherry$year, probs = seq(from = 0, to = 1, length.out = num_knots)) B &lt;- bs(data_cherry$year, knots = knot_list[-c(1, num_knots)], degree = 3, intercept = TRUE) data_cherry_B &lt;- data_cherry %&gt;% mutate(B = B) data_cherry_B %&gt;% glimpse() #&gt; Rows: 827 #&gt; Columns: 6 #&gt; $ year &lt;int&gt; 812, 815, 831, 851, 853, 864, 866, 869, 889, 891, 892, 894,… #&gt; $ doy &lt;int&gt; 92, 105, 96, 108, 104, 100, 106, 95, 104, 109, 108, 106, 10… #&gt; $ temp &lt;dbl&gt; NA, NA, NA, 7.38, NA, 6.42, 6.44, NA, 6.83, 6.98, 7.11, 6.9… #&gt; $ temp_upper &lt;dbl&gt; NA, NA, NA, 12.10, NA, 8.69, 8.11, NA, 8.48, 8.96, 9.11, 8.… #&gt; $ temp_lower &lt;dbl&gt; NA, NA, NA, 2.66, NA, 4.14, 4.77, NA, 5.19, 5.00, 5.11, 5.5… #&gt; $ B &lt;bs[,17]&gt; &lt;bs[26 x 17]&gt; brms_c4_cherry_spline &lt;- brm(data = data_cherry_B, family = gaussian, doy ~ 1 + B, prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;brms/brms_c4_cherry_spline&quot;) brms_posterior_samples &lt;- as_draws_df(brms_c4_cherry_spline) years_seq &lt;- tibble(year = seq(from = min(data_cherry$year), to = max(data_cherry$year), by = 10)) fitd_quad &lt;- fitted(brms_c4_cherry_spline, probs = c(.055, .955)) %&gt;% data.frame() %&gt;% bind_cols(data_cherry_B) %&gt;% as_tibble() pred_quad &lt;- predict(brms_c4_cherry_spline, probs = c(.055, .955)) %&gt;% data.frame() %&gt;% bind_cols(data_cherry_B) %&gt;% as_tibble() ggplot(data = data_cherry, aes(x = year)) + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = rgb(0,0,0,.4)) + geom_point(aes(y = doy), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = pred_quad, aes(ymin = Q5.5, ymax = Q95.5), fill = clr0d, alpha = .35) + geom_smooth(data = fitd_quad, aes(y = Estimate, ymin = Q5.5, ymax = Q95.5), stat = &quot;identity&quot;, color = clr1, fill = fll1, alpha = .35, size = .2) + geom_hline(yintercept = fixef(brms_c4_cherry_spline)[1, 1], color = clr1, linetype = 2) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) get_prior(data = data_cherry, family = gaussian, doy ~ 1 + s(year)) #&gt; prior class coef group resp dpar nlpar bound #&gt; (flat) b #&gt; (flat) b syear_1 #&gt; student_t(3, 105, 5.9) Intercept #&gt; student_t(3, 0, 5.9) sds #&gt; student_t(3, 0, 5.9) sds s(year) #&gt; student_t(3, 0, 5.9) sigma #&gt; source #&gt; default #&gt; (vectorized) #&gt; default #&gt; default #&gt; (vectorized) #&gt; default Using a thin plate spline brms_c4_cherry_smooth &lt;- brm(data = data_cherry, family = gaussian, doy ~ 1 + s(year), prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = b), prior(student_t(3, 0, 5.9), class = sds), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, control = list(adapt_delta = .99), file = &quot;brms/brms_c4_cherry_smooth&quot;) fitted(brms_c4_cherry_smooth, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(select(data_cherry, year, doy)) %&gt;% as_tibble() %&gt;% ggplot(aes(x = year, y = doy, ymin = Q5.5, ymax = Q94.5)) + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = rgb(0,0,0,.4)) + geom_point(aes(y = doy), color = rgb(0,0,0,.5), size = .6) + geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5), stat = &quot;identity&quot;, color = clr1, fill = fll1, alpha = .35, size = .2) + geom_hline(yintercept = fixef(brms_c4_cherry_smooth)[1, 1], color = clr1, linetype = 2) + labs(subtitle = &quot;brms smooth using s(year) (thin plate)&quot;, y = &quot;day in year&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) brms_c4_cherry_smooth2 &lt;- brm(data = data_cherry, family = gaussian, doy ~ 1 + s(year, bs = &quot;bs&quot;, k = 19), prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = b), prior(student_t(3, 0, 5.9), class = sds), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, control = list(adapt_delta = .99), file = &quot;brms/brms_c4_cherry_smooth2&quot;) fitted(brms_c4_cherry_smooth2, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(select(data_cherry, year, doy)) %&gt;% as_tibble() %&gt;% ggplot(aes(x = year, y = doy, ymin = Q5.5, ymax = Q94.5)) + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = rgb(0,0,0,.4)) + geom_point(aes(y = doy), color = rgb(0,0,0,.5), size = .6) + geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5), stat = &quot;identity&quot;, color = clr1, fill = fll1, alpha = .35, size = .2) + geom_hline(yintercept = fixef(brms_c4_cherry_smooth2)[1, 1], color = clr1, linetype = 2) + labs(subtitle = &quot;brms smooth using s(year) (B-spline)&quot;, y = &quot;day in year&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) Add-on about matrix-columns (4.7 Second bonus) n &lt;- 100 # how many continuous x predictor variables would you like? k &lt;- 10 # simulate a dichotomous dummy variable for z # simulate an n by k array for X set.seed(4) data_matrix_columns &lt;- tibble(z = sample(0:1, size = n, replace = T), X = array(runif(n * k, min = 0, max = 1), dim = c(n, k))) # set the data-generating parameter values a &lt;- 1 theta &lt;- 5 b &lt;- 1:k sigma &lt;- 2 # simulate the criterion data_matrix_columns &lt;- data_matrix_columns %&gt;% mutate(y = as.vector(a + X %*% b + theta * z + rnorm(n, mean = 0, sd = sigma))) # data_matrix_columns %&gt;% glimpse() # data_matrix_columns$X[1,] brms_c4_matrix_column &lt;- brm(data = data_matrix_columns, family = gaussian, y ~ 1 + z + X, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;brms/brms_c4_matrix_column&quot;) summary(brms_c4_matrix_column) #&gt; Family: gaussian #&gt; Links: mu = identity; sigma = identity #&gt; Formula: y ~ 1 + z + X #&gt; Data: data_matrix_columns (Number of observations: 100) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; Intercept 0.95 1.19 -1.47 3.34 1.00 6928 3291 #&gt; z 4.74 0.42 3.91 5.56 1.00 7389 3032 #&gt; X1 0.57 0.75 -0.86 2.04 1.00 6798 3312 #&gt; X2 0.90 0.69 -0.47 2.26 1.00 6421 2949 #&gt; X3 3.41 0.75 1.96 4.90 1.00 7375 3214 #&gt; X4 2.81 0.73 1.36 4.25 1.00 6712 3469 #&gt; X5 5.74 0.72 4.32 7.12 1.00 6621 3588 #&gt; X6 6.40 0.73 4.97 7.82 1.00 6552 3228 #&gt; X7 8.49 0.73 7.06 9.90 1.00 7098 3335 #&gt; X8 8.40 0.69 7.04 9.76 1.00 8718 3363 #&gt; X9 8.82 0.81 7.27 10.39 1.00 8378 3273 #&gt; X10 9.32 0.73 7.88 10.78 1.00 8260 2894 #&gt; #&gt; Family Specific Parameters: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; sigma 1.99 0.16 1.71 2.32 1.00 5722 3433 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). 5.9 pymc3 section × "],
["rethinking-chapter-5.html", "6 Rethinking: Chapter 5 6.1 Directed Acyclic Graphs 6.2 Multiple Regression notion 6.3 Masked relationship 6.4 Categorical Variables 6.5 Homework 6.6 {brms} section 6.7 pymc3 section", " 6 Rethinking: Chapter 5 Spurious waffles by Richard McElreath, building on the Summaries by Solomon Kurz and Jake Thompson. library(sf) library(rethinking) library(ggfx) data(WaffleDivorce) WaffleDivorce &lt;- WaffleDivorce %&gt;% as_tibble() usa &lt;- read_sf(&quot;~/work/geo_store/USA/usa_states_albers_revised.gpkg&quot;) %&gt;% left_join(WaffleDivorce, by = c(name = &quot;Location&quot; )) p_waffle &lt;- usa %&gt;% ggplot(aes(fill = WaffleHouses / Population)) + scale_fill_gradientn(colours = c(clr0d, clr2) %&gt;% clr_lighten(.3)) p_divorce &lt;- usa %&gt;% ggplot(aes(fill = Divorce))+ scale_fill_gradientn(colours = c(clr0d, clr1) %&gt;% clr_lighten(.3)) p_age &lt;- usa %&gt;% ggplot(aes(fill = MedianAgeMarriage))+ scale_fill_gradientn(colours = c(clr_lighten(clr0d, .3), clr3)) p_waffle + p_divorce + p_age + plot_layout(guides = &quot;collect&quot;) &amp; with_shadow(geom_sf(aes(color = after_scale(clr_darken(fill)))), x_offset = 0, y_offset = 0, sigma = 3) &amp; guides(fill = guide_colorbar(title.position = &quot;top&quot;, barheight = unit(5,&quot;pt&quot;))) &amp; theme(legend.position = &quot;bottom&quot;) Age Model: first model (divorce rate depends on age at marriage) \\[ \\begin{array}{cccr} D_i &amp; {\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{A} A_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.2) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{A} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] data_waffle &lt;- WaffleDivorce %&gt;% mutate(across(.cols = c(Divorce, Marriage, MedianAgeMarriage), .fns = standardize, .names = &quot;{str_to_lower(.col)}_std&quot;), waffle_pop = WaffleHouses / Population) %&gt;% rename(median_age_std = &quot;medianagemarriage_std&quot;) sd(data_waffle$MedianAgeMarriage) #&gt; [1] 1.24363 model_age &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_A * median_age_std , alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle ) set.seed(10) age_priors &lt;- extract.prior(model_age) %&gt;% as_tibble() prior_prediction_range &lt;- c(-2, 2) age_prior_predictions &lt;- link(model_age, post = age_priors, data = list(median_age_std = prior_prediction_range)) %&gt;% as_tibble() %&gt;% set_names(nm = as.character(prior_prediction_range)) %&gt;% mutate(.draw = row_number()) age_prior_predictions %&gt;% filter(.draw &lt; 51) %&gt;% ggplot() + geom_segment(aes(x = -2, xend = 2, y = `-2`, yend = `2`, group = .draw), color = clr2, alpha = .2) + labs(x = &quot;median age of marriage (std)&quot;, y = &quot;divorce rate (std)&quot;) age_seq &lt;- seq(min(data_waffle$median_age_std), max(data_waffle$median_age_std), length.out = 101) model_age_posterior_prediction_samples &lt;- link(model_age, data = data.frame(median_age_std = age_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = age_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;median_age_std&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(median_age_std = as.numeric(median_age_std), MedianAgeMarriage = median_age_std * sd(data_waffle$MedianAgeMarriage) + mean(data_waffle$MedianAgeMarriage), Divorce = divorce_std * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) model_age_posterior_prediction_pi &lt;- model_age_posterior_prediction_samples %&gt;% group_by(median_age_std, MedianAgeMarriage) %&gt;% summarise(mean = mean(Divorce), PI_lower = PI(Divorce)[1], PI_upper = PI(Divorce)[2]) %&gt;% ungroup() model_age_posterior_prediction_simulation &lt;- sim(model_age, data = data.frame(median_age_std = age_seq), n = 5e3) %&gt;% as_tibble() %&gt;% set_names(nm = age_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;median_age_std&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(median_age_std = as.numeric(median_age_std), MedianAgeMarriage = median_age_std * sd(data_waffle$MedianAgeMarriage) + mean(data_waffle$MedianAgeMarriage), Divorce = divorce_std * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) model_age_posterior_prediction_simulation_pi &lt;- model_age_posterior_prediction_simulation %&gt;% group_by(median_age_std, MedianAgeMarriage) %&gt;% summarise(mean = mean(Divorce), PI_lower = PI(Divorce)[1], PI_upper = PI(Divorce)[2]) %&gt;% ungroup() p_age &lt;- ggplot(mapping = aes(x = MedianAgeMarriage)) + geom_ribbon(data = model_age_posterior_prediction_simulation_pi, aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_smooth(data = model_age_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr3, fill = fll3, size = .4) + geom_point(data = data_waffle, aes(y = Divorce), color = rgb(0,0,0,.5), size = .6) + labs(y = &quot; divorce&quot;) Marriage Model: alternative model (divorce rate depends on marriage rate) \\[ \\begin{array}{cccr} D_i &amp; {\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{M} M_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.2) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{M} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_marriage &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std , alpha ~ dnorm( 0, 0.2 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle ) marriage_seq &lt;- seq(min(data_waffle$marriage_std), max(data_waffle$marriage_std), length.out = 101) model_marriage_posterior_prediction_samples &lt;- link(model_marriage, data = data.frame(marriage_std = marriage_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = marriage_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;marriage_std&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(marriage_std = as.numeric(marriage_std), Marriage = marriage_std * sd(data_waffle$Marriage) + mean(data_waffle$Marriage), Divorce = divorce_std * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) model_marriage_posterior_prediction_pi &lt;- model_marriage_posterior_prediction_samples %&gt;% group_by(marriage_std, Marriage) %&gt;% summarise(mean = mean(Divorce), PI_lower = PI(Divorce)[1], PI_upper = PI(Divorce)[2]) %&gt;% ungroup() model_marriage_posterior_prediction_simulation &lt;- sim(model_marriage, data = data.frame(marriage_std = marriage_seq), n = 5e3) %&gt;% as_tibble() %&gt;% set_names(nm = marriage_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;marriage_std&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(marriage_std = as.numeric(marriage_std), Marriage = marriage_std * sd(data_waffle$Marriage) + mean(data_waffle$Marriage), Divorce = divorce_std * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) model_marriage_posterior_prediction_simulation_pi &lt;- model_marriage_posterior_prediction_simulation %&gt;% group_by(marriage_std, Marriage) %&gt;% summarise(mean = mean(Divorce), PI_lower = PI(Divorce)[1], PI_upper = PI(Divorce)[2]) %&gt;% ungroup() p_marriage &lt;- ggplot(mapping = aes(x = Marriage)) + geom_ribbon(data = model_marriage_posterior_prediction_simulation_pi, aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_smooth(data = model_marriage_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr1, fill = fll1, size = .2) + geom_point(data = data_waffle, aes(y = Divorce), color = rgb(0,0,0,.5), size = .6) + labs(y = &quot; divorce&quot;) Waffle Model: model_waffle &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_W * waffle_pop , alpha ~ dnorm( 0, 0.2 ), beta_W ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle ) waffle_seq &lt;- seq(min(data_waffle$waffle_pop), max(data_waffle$waffle_pop), length.out = 101) model_waffle_posterior_prediction_samples &lt;- link(model_waffle, data = data.frame(waffle_pop = waffle_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = waffle_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;waffle_pop&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(waffle_pop = as.numeric(waffle_pop), Divorce = divorce_std * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) model_waffle_posterior_prediction_pi &lt;- model_waffle_posterior_prediction_samples %&gt;% group_by(waffle_pop) %&gt;% summarise(mean = mean(Divorce), PI_lower = PI(Divorce)[1], PI_upper = PI(Divorce)[2]) %&gt;% ungroup() p_waffle &lt;- ggplot(mapping = aes(x = waffle_pop)) + geom_smooth(data = model_waffle_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr2, fill = fll2, size = .2) + geom_point(data = data_waffle, aes(y = Divorce), color = rgb(0,0,0,.5), size = .6) + labs(y = &quot; divorce&quot;) p_waffle + p_marriage + theme(axis.title.y = element_blank(), axis.text.y = element_blank()) + p_age + theme(axis.title.y = element_blank(), axis.text.y = element_blank()) &amp; lims(y = c(4, 15)) 6.1 Directed Acyclic Graphs dag1 &lt;- dagify( D ~ A + M, M ~ A, exposure = &quot;A&quot;, outcome = &quot;M&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,1,.5), y = c(1,1, .4))) %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) dag2 &lt;- dagify( D ~ A, M ~ A, exposure = &quot;A&quot;, outcome = &quot;M&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,.5,1), y = c(1, .4, 1))) %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag1, clr_in = clr3) + plot_dag(dag2, clr_in = clr3) &amp; scale_y_continuous(limits = c(.35, 1.05)) &amp; coord_equal() DAG notation: \\(Y \\perp \\!\\!\\! \\perp X | Z\\): “\\(Y\\) is independent of \\(X\\) conditional on \\(Z\\)” \\(D \\not\\!\\perp\\!\\!\\!\\perp A\\): \"\\(D\\) is associated with \\(A\\)\" Check pair wise correlations with cor(): data_waffle %&gt;% dplyr::select(divorce_std,marriage_std, median_age_std) %&gt;% cor() %&gt;% as.data.frame(row.names = row.names(.)) %&gt;% round(digits = 2) %&gt;% knitr::kable() divorce_std marriage_std median_age_std divorce_std 1.00 0.37 -0.60 marriage_std 0.37 1.00 -0.72 median_age_std -0.60 -0.72 1.00 library(dagitty) dagitty(&#39;dag{ D &lt;- A -&gt; M -&gt; D}&#39;) %&gt;% impliedConditionalIndependencies() dagitty(&#39;dag{ D &lt;- A -&gt; M }&#39;) %&gt;% impliedConditionalIndependencies() #&gt; D _||_ M | A 6.2 Multiple Regression notion \\[ \\begin{array}{cccr} D_i &amp; {\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{M} M_{i} + \\beta_{A} A_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.2) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{M} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_M$ prior]}\\\\ \\beta_{A} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_A$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] or compact notion \\[ \\mu_i = \\alpha + \\sum_{j = 1}^{n} \\beta_jx_{ji} \\] or even matrix notion \\[ m = Xb \\] model_multiple &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle ) precis(model_multiple) %&gt;% round(digits = 2) %&gt;% as.matrix() %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.10 -0.16 0.16 beta_A -0.61 0.15 -0.85 -0.37 beta_M -0.07 0.15 -0.31 0.18 sigma 0.79 0.08 0.66 0.91 ct &lt;- coeftab(model_age, model_marriage, model_multiple,se = TRUE) plot_coeftab(ct) beta_A doesn’t really change, it only grows more uncertain, yet beta_M is only associated with divorce, when marriage rate is missing from the model. “Once we know the median age at marriage for a State, there is little to no additional predictive power in also knowing the rate of marriage at that State.” \\(\\rightarrow\\) \\(D \\perp \\!\\!\\! \\perp M | A\\) simulating the divorcee example n &lt;- 50 data_divorce_sim &lt;- tibble(median_age_std = rnorm(n), marriage_std = rnorm(n, mean = -median_age_std), divorce_std = rnorm(n, mean = median_age_std), divorce_codep = rnorm(n, mean = median_age_std + marriage_std)) p1 &lt;- ggpairs(data_divorce_sim %&gt;% dplyr::select(-divorce_codep), lower = list(continuous = wrap(ggally_points, colour = clr1, size = .9, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll1, color = clr1, adjust = 1)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) p2 &lt;- ggpairs(data_divorce_sim %&gt;% dplyr::select(-divorce_std), lower = list(continuous = wrap(ggally_points, colour = clr2, size = .9, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll2, color = clr2, adjust = 1)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) cowplot::plot_grid(ggmatrix_gtable(p1), ggmatrix_gtable(p2)) simulating the right DAG (\\(D \\perp \\!\\!\\! \\perp M | A\\)) model_multiple_sim &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_divorce_sim ) model_age_sim &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_divorce_sim ) model_marriage_sim &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std, alpha ~ dnorm( 0, 0.2 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_divorce_sim ) ct_sim &lt;- coeftab(model_age_sim, model_marriage_sim, model_multiple_sim, se = TRUE) plot_coeftab(ct_sim) simulating the left DAG (\\(D \\not\\!\\perp\\!\\!\\!\\perp M | A\\)) model_multiple_sim_codep &lt;- quap( flist = alist( divorce_codep ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_divorce_sim ) model_age_sim_codep &lt;- quap( flist = alist( divorce_codep ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_divorce_sim ) model_marriage_sim_codep &lt;- quap( flist = alist( divorce_codep ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std, alpha ~ dnorm( 0, 0.2 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_divorce_sim ) ct_sim_codep &lt;- coeftab(model_age_sim_codep, model_marriage_sim_codep, model_multiple_sim_codep, se = TRUE) plot_coeftab(ct_sim_codep) 6.2.1 Visualizations for multivariate regressions Predictor residual plots. useful for understanding the model, but not much else Posterior prediction plots. checking fit and assessing predictions Counterfactual plots. implied predictions for imaginary experiments 6.2.1.1 Predictor residual plots predictor residual plot for marriage rate pred_res_marriage &lt;- quap( flist = alist( marriage_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_AM * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_AM ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle ) residuals_marriage &lt;- link(pred_res_marriage) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(data_waffle$median_age_std)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;fit_marriage&quot;) %&gt;% group_by(row_idx) %&gt;% summarise(mean_marriage = mean(fit_marriage), lower_pi = PI(fit_marriage)[1], upper_pi = PI(fit_marriage)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx)) %&gt;% left_join(data_waffle %&gt;% mutate(row_idx = row_number()),. ) %&gt;% mutate(residual_marriage = marriage_std - mean_marriage) p_11 &lt;- residuals_marriage %&gt;% ggplot(aes(x = median_age_std)) + geom_segment(aes(xend = median_age_std, y = mean_marriage, yend = marriage_std), color = rgb(0,0,0,.6), linetype = 3) + geom_line(aes(y = mean_marriage), color = clr1) + geom_point(aes(y = marriage_std), color = clr1, fill = clr_lighten(clr1, .35), shape = 21) + geom_text(data = residuals_marriage %&gt;% filter(Loc %in% c(&quot;DC&quot;, &quot;HI&quot;, &quot;ND&quot;, &quot;ME&quot;, &quot;WY&quot;)), aes(x = median_age_std - .1, y = marriage_std, label = Loc), hjust = 1) pred_res_marriage_mu &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta * residual_marriage, alpha ~ dnorm( 0, 0.2 ), beta ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = residuals_marriage ) seq_res &lt;- seq(min(residuals_marriage$residual_marriage), max(residuals_marriage$residual_marriage), length.out = 101) residual_lm_posterior &lt;- link(pred_res_marriage_mu, data = data.frame(residual_marriage = seq_res)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_res) %&gt;% pivot_longer(cols = everything(), names_to = &quot;residual_marriage&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(residual_marriage = as.numeric(residual_marriage)) %&gt;% group_by(residual_marriage) %&gt;% summarise(mean = mean(divorce_std), PI_lower = PI(divorce_std)[1], PI_upper = PI(divorce_std)[2]) %&gt;% ungroup() p_12 &lt;- ggplot(mapping = aes(x = residual_marriage)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_smooth(data = residual_lm_posterior, aes(y = mean, ymin = PI_lower, ymax = PI_upper), stat = &quot;identity&quot;, color = clr1, fill = fll1, size = .4) + geom_point(data = residuals_marriage, aes(y = divorce_std), color = clr1, fill = clr_lighten(clr1,.35), shape = 21) + geom_text(data = residuals_marriage %&gt;% filter(Loc %in% c(&quot;DC&quot;, &quot;HI&quot;, &quot;ND&quot;, &quot;ME&quot;, &quot;WY&quot;)), aes(y = divorce_std - .4, label = Loc)) + labs(y = &quot;divorce_rate (std)&quot;) predictor residual plot for age at marriage pred_res_age &lt;- quap( flist = alist( median_age_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_MA * marriage_std, alpha ~ dnorm( 0, 0.2 ), beta_MA ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle ) residuals_age &lt;- link(pred_res_age) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(data_waffle$marriage_std)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;fit_age&quot;) %&gt;% group_by(row_idx) %&gt;% summarise(mean_age = mean(fit_age), lower_pi = PI(fit_age)[1], upper_pi = PI(fit_age)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx)) %&gt;% left_join(data_waffle %&gt;% mutate(row_idx = row_number()),. ) %&gt;% mutate(residual_age = median_age_std - mean_age) p_21 &lt;- residuals_age %&gt;% ggplot(aes(x = marriage_std)) + geom_segment(aes(xend = marriage_std, y = mean_age, yend = median_age_std), color = rgb(0,0,0,.6), linetype = 3) + geom_line(aes(y = mean_age), color = clr2) + geom_point(aes(y = median_age_std), color = clr2, fill = clr_lighten(clr2, .35), shape = 21) + geom_text(data = residuals_marriage %&gt;% filter(Loc %in% c(&quot;DC&quot;, &quot;HI&quot;, &quot;ID&quot;)), aes(x = marriage_std - .1, y = median_age_std, label = Loc), hjust = 1) pred_res_age_mu &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta * residual_age, alpha ~ dnorm( 0, 0.2 ), beta ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = residuals_age ) seq_res_age &lt;- seq(min(residuals_age$residual_age), max(residuals_age$residual_age), length.out = 101) residual_lm_posterior_age &lt;- link(pred_res_age_mu, data = data.frame(residual_age = seq_res_age)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_res_age) %&gt;% pivot_longer(cols = everything(), names_to = &quot;residual_age&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(residual_age = as.numeric(residual_age)) %&gt;% group_by(residual_age) %&gt;% summarise(mean = mean(divorce_std), PI_lower = PI(divorce_std)[1], PI_upper = PI(divorce_std)[2]) %&gt;% ungroup() p_22 &lt;- ggplot(mapping = aes(x = residual_age)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_smooth(data = residual_lm_posterior_age, aes(y = mean, ymin = PI_lower, ymax = PI_upper), stat = &quot;identity&quot;, color = clr2, fill = fll2, size = .4) + geom_point(data = residuals_age, aes(y = divorce_std), color = clr2, fill = clr_lighten(clr2,.35), shape = 21) + geom_text(data = residuals_age %&gt;% filter(Loc %in% c(&quot;DC&quot;, &quot;HI&quot;, &quot;ID&quot;)), aes(y = divorce_std - .4, label = Loc)) + labs(y = &quot;divorce_rate (std)&quot;) p_11 + p_21 + p_12 + p_22 6.2.1.2 Posterior Preediction Plots posterior_prediction &lt;- link(model_multiple) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(data_waffle$divorce_std)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;divorce_predicted&quot;) %&gt;% group_by(row_idx) %&gt;% summarise(divorce_predicted_mean = mean(divorce_predicted), lower_pi = PI(divorce_predicted)[1], upper_pi = PI(divorce_predicted)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx)) %&gt;% left_join(data_waffle %&gt;% mutate(row_idx = row_number()), . ) posterior_simmulation &lt;- sim(model_multiple) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(data_waffle$divorce_std)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;divorce_predicted&quot;) %&gt;% group_by(row_idx) %&gt;% summarise(lower_pi = PI(divorce_predicted)[1], upper_pi = PI(divorce_predicted)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx)) %&gt;% left_join(data_waffle %&gt;% mutate(row_idx = row_number()), . ) ggplot(mapping = aes(x = divorce_std)) + geom_abline(slope = 1, size = .7, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(data = posterior_prediction, aes(ymin = lower_pi, ymax = upper_pi, color = Loc %in% c(&quot;ID&quot;, &quot;UT&quot;)))+ geom_point(data = posterior_prediction, aes(y = divorce_predicted_mean, color = Loc %in% c(&quot;ID&quot;, &quot;UT&quot;), fill = after_scale(clr_lighten(color ,.5))), shape = 21, size = 1.5)+ geom_text(data = posterior_prediction %&gt;% filter(Loc %in% c(&quot;ID&quot;, &quot;ME&quot;, &quot;RI&quot;, &quot;UT&quot;)), aes(x = divorce_std - .15, y = divorce_predicted_mean, label = Loc)) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) Regressions tend to under-estimate variable in the high end of the range and over-estimate in the low end of the range. This is normal, they “pull towards the mean”. The labeled States however (ID, ME, RI, UT), are not well predicted by the Model (eg. due to additional social factors). Simulating spurious association N &lt;- 100 data_spurious &lt;- tibble(x_real = rnorm(N), x_spur = rnorm(N, x_real), y = rnorm(N, x_real)) ggpairs(data_spurious, lower = list(continuous = wrap(ggally_points, colour = clr3, size = .9, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll3, color = clr3, adjust = 1)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) model_spurious &lt;- quap( flist = alist( y ~ dnorm(mu, sigma), mu &lt;- alpha + beta_r * x_real + beta_s * x_spur, alpha ~ dnorm(0, .2), beta_r ~ dnorm(0, .5), beta_s ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_spurious ) precis(model_spurious) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.09 0.09 -0.06 0.24 beta_r 0.87 0.14 0.64 1.10 beta_s 0.09 0.11 -0.09 0.27 sigma 1.06 0.07 0.94 1.17 Note, how the estimated mean for beta_s is close to 0 (0.09) – despite the correlation shown above 🤔`. 6.2.1.3 Counterfactual Plots model_counterfactual &lt;- quap( flist = alist( # A -&gt; D &lt;- M divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ), # A -&gt; M marriage_std ~ dnorm( mu_M, sigma_M ), mu_M &lt;- alpha_M + beta_AM * median_age_std, alpha_M ~ dnorm( 0, 0.2 ), beta_AM ~ dnorm( 0, 0.5 ), sigma_M ~ dexp(1) ), data = data_waffle ) precis(model_counterfactual) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.10 -0.16 0.16 beta_A -0.61 0.15 -0.85 -0.37 beta_M -0.07 0.15 -0.31 0.18 sigma 0.79 0.08 0.66 0.91 alpha_M 0.00 0.09 -0.14 0.14 beta_AM -0.69 0.10 -0.85 -0.54 sigma_M 0.68 0.07 0.57 0.79 Note, that marriage_std and median_age_std are strongly negatively correlated (-0.69) A_seq &lt;- seq(-2, 2, length.out = 30) unpack_sim &lt;- function(x, seq = A_seq){ nms &lt;- names(x) purrr::map(.x = nms, .f = function(y, x, seq_in = seq){ x[[y]] %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(seq_in)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;value&quot;) %&gt;% mutate(parameter = y) }, x = x) %&gt;% purrr::reduce(bind_rows) } data_sim &lt;- sim(fit = model_counterfactual, data = tibble(median_age_std = A_seq), vars = c(&quot;marriage_std&quot;, &quot;divorce_std&quot;)) %&gt;% unpack_sim() data_sim_pi &lt;- data_sim %&gt;% group_by(row_idx, parameter) %&gt;% summarise(mean = mean(value), PI_lower = PI(value)[1], PI_upper = PI(value)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx), median_age_std = A_seq[row_idx]) %&gt;% arrange(parameter, median_age_std) data_sim_pi %&gt;% ggplot() + geom_smooth(aes(x = median_age_std, y = mean, ymin = PI_lower, ymax = PI_upper, color = parameter, fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(clr0d, clr3), guide = &quot;none&quot;) + labs(y = &quot;counterfactual value&quot;, title = &quot;Counterfactual effects of age at marriage on&quot;) + facet_wrap(parameter ~ .) Numerical operations (eg. simulating the causal effect of raising the median age of marriage from 20 to 30): A_seq2 &lt;- (c(20, 30) - mean(data_waffle$MedianAgeMarriage)) / sd(data_waffle$MedianAgeMarriage) data_sim_num &lt;- sim(fit = model_counterfactual, data = tibble(median_age_std = A_seq2), vars = c(&quot;marriage_std&quot;, &quot;divorce_std&quot;)) %&gt;% unpack_sim(seq = A_seq2) data_sim_num %&gt;% filter(parameter == &quot;divorce_std&quot;) %&gt;% dplyr::select(-parameter) %&gt;% mutate(pair = (row_number() + 1) %/% 2) %&gt;% pivot_wider(names_from = row_idx, values_from = value) %&gt;% mutate(effect = `2` - `1`) %&gt;% summarise(mean = mean(effect)) #&gt; # A tibble: 1 x 1 #&gt; mean #&gt; &lt;dbl&gt; #&gt; 1 -4.59 …A change of four and a half standard deviations is quite extreme! M_seq &lt;- A_seq data_sim_M &lt;- sim(fit = model_counterfactual, data = tibble(marriage_std = M_seq, median_age_std = 0), vars = c(&quot;divorce_std&quot;)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(M_seq)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;divorce_std&quot;) data_sim_M_pi &lt;- data_sim_M %&gt;% group_by(row_idx) %&gt;% summarise(mean = mean(divorce_std), PI_lower = PI(divorce_std)[1], PI_upper = PI(divorce_std)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx), marriage_std = M_seq[row_idx]) data_sim_M_pi %&gt;% ggplot() + geom_smooth(aes(x = marriage_std, y = mean, ymin = PI_lower, ymax = PI_upper), color = clr1, fill = fll1, stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(clr0d, clr3), guide = &quot;none&quot;) + labs(y = &quot;counterfactual value&quot;, title = &quot;Counterfactual effects of marriage rate on divorce rate&quot;) + lims(y = c(-2, 2)) 6.3 Masked relationship Loading the milk data data(milk) data_milk &lt;- milk %&gt;% filter(complete.cases(.)) %&gt;% as_tibble() %&gt;% mutate(`mass.log` = log(mass), across(.cols = c(`kcal.per.g`, `neocortex.perc`, `mass.log`), .fns = standardize, .names = &quot;{str_remove_all(.col, &#39;\\\\\\\\..*&#39;)}_std&quot;)) data_milk %&gt;% precis() %&gt;% as.matrix() %&gt;% as.data.frame() %&gt;% filter(!is.na(mean)) %&gt;% mutate(across(.cols = mean:`94.5%`, function(x){round(as.numeric(x), digits = 2)})) %&gt;% knitr::kable() mean sd 5.5% 94.5% histogram kcal.per.g 0.66 0.17 0.47 0.93 ▇▂▁▁▁▂▁▁▁▁▁ perc.fat 36.06 14.71 15.08 54.45 ▂▁▁▂▃▃▂▅▃▁▇▂ perc.protein 16.26 5.60 9.28 23.79 ▂▅▅▅▅▂▂▅▇▂ perc.lactose 47.68 13.59 30.35 68.31 ▂▇▅▅▂▇▅▁▅▂ mass 16.64 23.58 0.30 57.89 ▇▁▁▁▁▁▁▁ neocortex.perc 67.58 5.97 58.41 75.59 ▂▁▂▅▁▅▅▅▇▅▂▂ mass.log 1.50 1.93 -1.26 4.05 ▂▁▂▂▂▂▅▂▇▁▂▂▅▅ kcal_std 0.00 1.00 -1.09 1.55 ▃▇▁▃▁▂▂ neocortex_std 0.00 1.00 -1.54 1.34 ▁▁▂▃▁▇▃▂ mass_std 0.00 1.00 -1.43 1.32 ▁▂▂▃▃▁▇ 6.3.1 Bi-variate models Neocortex effect on caloric content of milk \\[ \\begin{array}{cccr} K_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{N} N_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.2) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{N} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_N$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] Mothers weight effect on caloric content of milk \\[ \\begin{array}{cccr} K_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{M} M_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.2) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{M} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_M$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] Model implementation (neocortex, draft) model_milk_draft &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_N * neocortex_std, alpha ~ dnorm(0, 1), beta_N ~ dnorm(0, 1), sigma ~ dexp(1) ), data = data_milk ) prior_milk_draft &lt;- extract.prior(model_milk_draft) %&gt;% as_tibble() seq_prior &lt;- c(-2, 2) prior_prediction_milk_draft &lt;- link(model_milk_draft, post = prior_milk_draft, data = tibble(neocortex_std = seq_prior)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_prior) p_draft &lt;- prior_prediction_milk_draft %&gt;% filter(row_number() &lt;= 50) %&gt;% ggplot() + geom_segment(aes(x = -2, xend = 2, y = `-2`, yend = `2`), alpha = .6, color = clr0d) Model implementation (neocortex) model_milk_cortex &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_N * neocortex_std, alpha ~ dnorm(0, .2), beta_N ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_milk ) precis(model_milk_cortex) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.15 -0.24 0.24 beta_N 0.13 0.21 -0.21 0.47 sigma 0.93 0.15 0.69 1.18 prior_milk_cortex &lt;- extract.prior(model_milk_cortex) %&gt;% as_tibble() prior_prediction_milk_cortex &lt;- link(model_milk_cortex, post = prior_milk_cortex, data = tibble(neocortex_std = seq_prior)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_prior) p_cortex &lt;- prior_prediction_milk_cortex %&gt;% filter(row_number() &lt;= 50) %&gt;% ggplot() + geom_segment(aes(x = -2, xend = 2, y = `-2`, yend = `2`), alpha = .6, color = clr0d) p_draft + p_cortex &amp; coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 2)) &amp; labs(x = &quot;neocortex_std&quot;, y = &quot;kcal_std&quot;) seq_cortex &lt;- seq(min(data_milk$neocortex_std) - .15, max(data_milk$neocortex_std) + .15, length.out = 51) model_milk_cortex_posterior_prediction_samples &lt;- link(model_milk_cortex, data = data.frame(neocortex_std = seq_cortex)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_cortex) %&gt;% pivot_longer(cols = everything(), names_to = &quot;neocortex_std&quot;, values_to = &quot;kcal_std&quot;) %&gt;% mutate(neocortex_std = as.numeric(neocortex_std)) model_milk_cortex_posterior_prediction_pi &lt;- model_milk_cortex_posterior_prediction_samples %&gt;% group_by(neocortex_std) %&gt;% summarise(mean = mean(kcal_std), PI_lower = PI(kcal_std)[1], PI_upper = PI(kcal_std)[2]) %&gt;% ungroup() p_cortex &lt;- ggplot(mapping = aes(x = neocortex_std)) + geom_smooth(data = model_milk_cortex_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr2, fill = fll2, size = .2) + geom_point(data = data_milk, aes(y = kcal_std), color = rgb(0,0,0,.5), size = 1.6) + labs(x = &quot;neocprtex_std&quot;, y = &quot;kcal_std&quot;) Model implementation (mothers weight) model_milk_weight &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_M * mass_std, alpha ~ dnorm(0, .2), beta_M ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_milk ) precis(model_milk_weight) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.15 -0.23 0.23 beta_M -0.30 0.20 -0.62 0.03 sigma 0.89 0.15 0.65 1.12 seq_weight &lt;- seq(min(data_milk$mass_std) - .15, max(data_milk$mass_std) + .15, length.out = 51) model_milk_weight_posterior_prediction_samples &lt;- link(model_milk_weight, data = data.frame(mass_std = seq_weight)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_weight) %&gt;% pivot_longer(cols = everything(), names_to = &quot;mass_std&quot;, values_to = &quot;kcal_std&quot;) %&gt;% mutate(mass_std = as.numeric(mass_std)) model_milk_weight_posterior_prediction_pi &lt;- model_milk_weight_posterior_prediction_samples %&gt;% group_by(mass_std) %&gt;% summarise(mean = mean(kcal_std), PI_lower = PI(kcal_std)[1], PI_upper = PI(kcal_std)[2]) %&gt;% ungroup() p_weight &lt;- ggplot(mapping = aes(x = mass_std)) + geom_smooth(data = model_milk_weight_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr2, fill = fll2, size = .2) + geom_point(data = data_milk, aes(y = kcal_std), color = rgb(0,0,0,.5), size = 1.6) + labs(x = &quot;mass_std&quot;, y = &quot;kcal_std&quot;) p_cortex + p_weight Model implementation (necocortex and mothers weight) \\[ \\begin{array}{cccr} K_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{N} N_{i} + \\beta_{M} M_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.2) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{N} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_N$ prior]}\\\\ \\beta_{M} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_M$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_milk_multi &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_N * neocortex_std + beta_M * mass_std, alpha ~ dnorm(0, .2), beta_N ~ dnorm(0, .5), beta_M ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_milk ) precis(model_milk_multi) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.13 -0.20 0.20 beta_N 0.64 0.23 0.27 1.01 beta_M -0.75 0.23 -1.12 -0.37 sigma 0.69 0.12 0.49 0.88 ct_milk &lt;- coeftab(model_milk_cortex, model_milk_weight, model_milk_multi, se = TRUE) plot_coeftab(ct_milk) data_milk %&gt;% dplyr::select(kcal_std, neocortex_std, mass_std) %&gt;% ggpairs(lower = list(continuous = wrap(ggally_points, colour = clr2, size = .9, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll2, color = clr2, adjust = 1)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) dag1 &lt;- dagify( K ~ M + N, N ~ M, exposure = &quot;M&quot;, outcome = &quot;K&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,1,.5), y = c(1,1, .4))) %&gt;% mutate(stage = if_else(name == &quot;K&quot;, &quot;response&quot;, if_else(name %in% c(&quot;M&quot;, &quot;N&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) dag2 &lt;- dagify( K ~ M + N, M ~ N, exposure = &quot;M&quot;, outcome = &quot;K&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,1,.5), y = c(1,1, .4))) %&gt;% mutate(stage = if_else(name == &quot;K&quot;, &quot;response&quot;, if_else(name %in% c(&quot;M&quot;, &quot;N&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) dag3 &lt;- dagify( K ~ M + N, M ~ U, N ~ U, exposure = &quot;M&quot;, outcome = &quot;K&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,1,.5, .5), y = c(1,1, 1,.4))) %&gt;% mutate(stage = if_else(name == &quot;K&quot;, &quot;response&quot;, if_else(name %in% c(&quot;M&quot;, &quot;N&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag1, clr_in = clr3) + plot_dag(dag2, clr_in = clr3) + plot_dag(dag3, clr_in = clr3) + plot_layout(nrow = 1) + plot_annotation(tag_levels = &quot;a&quot;) &amp; scale_y_continuous(limits = c(.35, 1.05)) &amp; coord_equal() &amp; theme(plot.tag = element_text(family = fnt_sel)) Counterfactual plots for DAG c) data_sim_mass &lt;- link(fit = model_milk_multi, data = tibble(mass_std = 0, neocortex_std = seq_cortex), vars = c(&quot;kcal_std&quot;)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(seq_cortex)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;kcal_std&quot;) data_sim_mass_pi &lt;- data_sim_mass %&gt;% group_by(row_idx) %&gt;% summarise(mean = mean(kcal_std), PI_lower = PI(kcal_std)[1], PI_upper = PI(kcal_std)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx), neocortex_std = seq_cortex[row_idx]) p_mass &lt;- data_sim_mass_pi %&gt;% ggplot() + geom_smooth(aes(x = neocortex_std, y = mean, ymin = PI_lower, ymax = PI_upper), color = clr2, fill = fll2, stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(clr0d, clr3), guide = &quot;none&quot;) + labs(y = &quot;counterfactual kcal&quot;, title = &quot;kcal at mass_std = 0&quot;) data_sim_cortex &lt;- link(fit = model_milk_multi, data = tibble(mass_std = seq_weight, neocortex_std = 0), vars = c(&quot;kcal_std&quot;)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(seq_weight)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;kcal_std&quot;) data_sim_cortex_pi &lt;- data_sim_cortex %&gt;% group_by(row_idx) %&gt;% summarise(mean = mean(kcal_std), PI_lower = PI(kcal_std)[1], PI_upper = PI(kcal_std)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx), mass_std = seq_weight[row_idx]) p_cortex &lt;- data_sim_cortex_pi %&gt;% ggplot() + geom_smooth(aes(x = mass_std, y = mean, ymin = PI_lower, ymax = PI_upper), color = clr2, fill = fll2, stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(clr0d, clr3), guide = &quot;none&quot;) + labs(y = &quot;counterfactual kcal&quot;, title = &quot;kcal at neocortex_std = 0&quot;) p_mass + p_cortex &amp; coord_cartesian(ylim = c(-1, 2)) 6.3.2 Simulate a masking relationship DAG a) (\\(M \\rightarrow K \\leftarrow N \\leftarrow M\\)) n &lt;- 100 data_milk_sim1 &lt;- tibble(mass_std = rnorm(n = n), neocortex_std = rnorm(n = n, mean = mass_std), kcal_std = rnorm(n = n, mean = neocortex_std - mass_std)) data_milk_sim1 %&gt;% dplyr::select(kcal_std, neocortex_std, mass_std) %&gt;% ggpairs(lower = list(continuous = wrap(ggally_points, colour = clr0d, size = .9, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll0, color = clr0d, adjust = 1)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) DAG b) (\\(N \\rightarrow M \\rightarrow K \\leftarrow N\\)) data_milk_sim2 &lt;- tibble(neocortex_std = rnorm(n = n), mass_std = rnorm(n = n, mean = neocortex_std), kcal_std = rnorm(n = n, mean = neocortex_std - mass_std)) DAG c) (\\(U \\rightarrow N \\rightarrow M \\rightarrow K \\leftarrow N \\leftarrow U\\)) data_milk_sim3 &lt;- tibble(unsampled = rnorm(n = n), neocortex_std = rnorm(n = n, mean = unsampled), mass_std = rnorm(n = n, mean = unsampled), kcal_std = rnorm(n = n, mean = neocortex_std - mass_std)) model_milk_cortex_sim &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_N * neocortex_std, alpha ~ dnorm(0, .2), beta_N ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_milk_sim1 ) model_milk_weight_sim &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_M * mass_std, alpha ~ dnorm(0, .2), beta_M ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_milk_sim1 ) model_milk_multi_sim &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_N * neocortex_std + beta_M * mass_std, alpha ~ dnorm(0, .2), beta_N ~ dnorm(0, .5), beta_M ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_milk_sim1 ) ct_milk_sim &lt;- coeftab(model_milk_cortex_sim, model_milk_weight_sim, model_milk_multi_sim, se = TRUE) plot_coeftab(ct_milk_sim) Computing the Marcov Equivalence Set dag_milk &lt;- dagitty(&quot;dag{ M -&gt; K &lt;- N M -&gt; N}&quot;) coordinates(dag_milk) &lt;- list( x = c( M = 0, N = 1, K = .5), y = c( M = 1, N = 1, K = .3)) dag_milk %&gt;% node_equivalent_dags() %&gt;% mutate(stage = &quot;predictor&quot;) %&gt;% plot_dag() + coord_cartesian(xlim = c(-.1, 1.1), ylim = c(.2, 1.1))+ facet_wrap(~ dag) 6.4 Categorical Variables 6.4.1 Indicator vs. Index variable (binary categories) Taking gender into account for the height model (but not caring about weight). data(Howell1) data_height &lt;- as_tibble(Howell1) %&gt;% mutate(sex = if_else(male == 1, 2, 1)) Modeling as dummy/indicator variable \\[ \\begin{array}{cccr} h_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{m} m_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(178, 20) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{m} &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\beta_N$ prior]}\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] Modeling as index variable \\[ \\begin{array}{ccccr} h_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; &amp;\\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha_{\\textrm{sex}[i]} &amp; &amp;\\textrm{[linear model]}\\\\ \\alpha_j &amp; \\sim &amp; Normal(178, 20) &amp; \\textrm{for}~j = 1..2 &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; &amp;\\textrm{[$\\sigma$ prior]} \\end{array} \\] Demonstrating that in the indicator variable approach, the uncertainty of estimates is higher for the male type (coded as 1), since this one is influenced by the uncertainty of two priors: indicator_prior &lt;- tibble(mu_female = rnorm(1e4, 178, 20), mu_male = rnorm(1e4, 178, 20) + rnorm(1e4, 0, 10)) indicator_prior %&gt;% precis() %&gt;% as.matrix() %&gt;% knitr::kable() mean sd 5.5% 94.5% histogram mu_female 177.7964 19.99779 145.6110 209.8597 ▁▁▁▁▂▃▇▇▇▅▃▁▁▁▁ mu_male 177.5124 22.49842 141.7337 213.0894 ▁▁▁▃▇▇▂▁▁▁ indicator_long &lt;- indicator_prior %&gt;% pivot_longer(cols = everything(), names_to = &quot;sex&quot;, values_to = &quot;height&quot;, names_transform = list(sex = function(str){str_remove(string = str, &quot;mu_&quot;)})) ggplot(indicator_long) + geom_density(data = indicator_long %&gt;% dplyr::select(-sex), aes(x = height, y = ..count..), color = clr0d, fill = fll0) + geom_density(aes(x = height, y = ..count.., color = sex, fill = after_scale(clr_alpha(color)))) + facet_wrap(sex ~ . ) + scale_color_manual(values = c(clr1, clr2), guide = &quot;none&quot;) Implementing the index variable approach: model_hight &lt;- quap( flist = alist( height ~ dnorm(mu, sigma), mu &lt;- alpha[sex], alpha[sex] ~ dnorm(178, 20), sigma ~ dunif(0,50) ), data = data_height ) precis(model_hight, depth = 2) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha[1] 134.91 1.61 132.34 137.48 alpha[2] 142.58 1.70 139.86 145.29 sigma 27.31 0.83 25.99 28.63 hight_posterior_samples &lt;- extract.samples(model_hight) %&gt;% as_tibble() %&gt;% mutate(diff_sex = alpha[ ,1] - alpha[ ,2] ) The expected difference between the considered types is called a contrast: hight_posterior_samples %&gt;% precis() %&gt;% as.matrix() %&gt;% knitr::kable() mean sd 5.5% 94.5% histogram sigma 27.307086 0.822027 25.99673 28.620850 ▁▁▁▁▁▁▃▅▇▇▃▂▁▁▁ alpha.1 134.933243 1.599303 132.39721 137.457066 ▁▁▁▂▅▇▇▅▂▁▁▁▁ alpha.2 142.592035 1.708900 139.84870 145.308164 ▁▁▁▁▁▂▃▇▇▇▃▂▁▁▁ diff_sex -7.658793 2.341238 -11.38310 -3.867645 ▁▁▁▂▇▇▃▁▁▁ p_contrast1 &lt;- hight_posterior_samples %&gt;% ggplot() + geom_density(aes(x = alpha[,1], color = &quot;female&quot;, fill = after_scale(clr_alpha(color))))+ geom_density(aes(x = alpha[,2], color = &quot;male&quot;, fill = after_scale(clr_alpha(color)))) + geom_errorbarh(data = tibble(start = median(hight_posterior_samples$alpha[,1]), end = median(hight_posterior_samples$alpha[,2])), aes(y = 0, xmin = start, xmax = end), height = .01) + scale_color_manual(values = c(clr1, clr2), guide = &quot;none&quot;) + lims(y = c(-.01,.25))+ labs(x = &quot;height&quot;) + theme(axis.title.y = element_blank(), axis.text.y = element_blank()) p_contrast2 &lt;- hight_posterior_samples %&gt;% ggplot() + geom_density(aes(x = alpha[,2] - alpha[,1]), color = clr0d, fill = fll0) + labs(x = &quot;contrast height(male-female)&quot;) + lims(y = c(-.01,.25))+ theme(axis.title.y = element_blank(), axis.text.y = element_blank()) p_contrast1 + p_contrast2 + plot_layout(widths = c(1,.66)) 6.4.2 Multiple categories Taking the broad taxonomic unit into account for the milk model (but not caring about neocortex od weight). houses &lt;- c(&quot;Gryffindor&quot;, &quot;Hufflepuff&quot;, &quot;Ravenclaw&quot;, &quot;Slytherin&quot;) set.seed(63) data_milk_clade &lt;- milk %&gt;% as_tibble() %&gt;% mutate(kcal_std = standardize(`kcal.per.g`), clade_id = as.integer(clade), house_id = sample(rep(1:4, each = 8), size = length(clade)), house = houses[house_id]) \\[ \\begin{array}{ccccr} K_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; &amp;\\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha_{\\textrm{CLADE}[i]} &amp; &amp;\\textrm{[linear model]}\\\\ \\alpha_j &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{for}~j = 1..4 &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; &amp;\\textrm{[$\\sigma$ prior]} \\end{array} \\] model_milk_clade &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha[clade_id], alpha[clade_id] ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = data_milk_clade ) precis(model_milk_clade, depth = 2, pars = &quot;alpha&quot;) %&gt;% as_tibble_rn() %&gt;% mutate(clade_id = str_remove_all(param, pattern = &quot;[a-z\\\\[\\\\]]*&quot;) %&gt;% as.integer(), clade = fct_reorder(levels(data_milk$clade)[clade_id], clade_id)) %&gt;% ggplot(aes(y = clade)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(aes(xmin = `5.5%`, xmax =`94.5%`), color = clr0d, fill = clr0) + geom_point(aes(x = mean), shape = 21, size = 3, color = clr0d, fill = clr0) + scale_y_discrete(&quot;&quot;, limits = rev(levels(data_milk$clade))) + labs(x = &quot;expected kcal_std&quot;) adding another categorical variable: \\[ \\begin{array}{ccccr} K_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; &amp;\\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha_{\\textrm{CLADE}[i]} + \\alpha_{\\textrm{HOUSE}[i]} &amp; &amp;\\textrm{[linear model]}\\\\ \\alpha_{\\textrm{CLADE},j} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{for}~j = 1..4 &amp; \\textrm{[$\\alpha_{\\textrm{CLADE}}$ prior]}\\\\ \\alpha_{\\textrm{HOUSE},j} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{for}~j = 1..4 &amp; \\textrm{[$\\alpha_{\\textrm{CLADE}}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; &amp;\\textrm{[$\\sigma$ prior]} \\end{array} \\] model_milk_house &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha_clade[clade_id] + alpha_house[house_id], alpha_clade[clade_id] ~ dnorm(0, 0.5), alpha_house[house_id] ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = data_milk_clade ) precis(model_milk_house, depth = 2, pars = &quot;alpha&quot;) %&gt;% as_tibble_rn() %&gt;% mutate(type = str_remove(param, pattern = &quot;alpha_&quot;) %&gt;% str_remove(&quot;\\\\[[0-9]\\\\]&quot;), idx = str_extract(param, &quot;[0-9]&quot;) %&gt;% as.integer(), name = if_else(type == &quot;clade&quot;, levels(data_milk$clade)[idx], houses[idx])) %&gt;% ggplot(aes(y = name, color = type)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(aes(xmin = `5.5%`, xmax =`94.5%`)) + geom_point(aes(x = mean, fill = after_scale(clr_lighten(color))), shape = 21, size = 3 ) + scale_color_manual(values = c(clade = clr0d, house = clr3), guide = &quot;none&quot;) + facet_grid(type ~ . , scales = &quot;free_y&quot;, switch = &quot;y&quot;) + labs(x = &quot;expected kcal_std&quot;) + theme(axis.title.y = element_blank(), strip.placement = &quot;outside&quot;) library(rlang) chapter5_models &lt;- env( data_waffle = data_waffle, model_age = model_age, model_marriage = model_marriage, model_waffle = model_waffle, model_multiple = model_multiple, data_divorce_sim = data_divorce_sim, model_multiple_sim = model_multiple_sim, model_age_sim = model_age_sim, model_marriage_sim = model_marriage_sim, model_multiple_sim_codep = model_multiple_sim_codep, model_age_sim_codep = model_age_sim_codep, model_marriage_sim_codep = model_marriage_sim_codep, pred_res_marriage = pred_res_marriage, residuals_marriage = residuals_marriage, pred_res_marriage_mu = pred_res_marriage_mu, pred_res_age = pred_res_age, residuals_age = residuals_age, pred_res_age_mu = pred_res_age_mu, data_spurious = data_spurious, model_spurious = model_spurious, model_counterfactual = model_counterfactual, data_milk = data_milk, model_milk_draft = model_milk_draft, model_milk_cortex = model_milk_cortex, model_milk_weight = model_milk_weight, model_milk_multi = model_milk_multi, data_milk_sim1 = data_milk_sim1, model_milk_cortex_sim = model_milk_cortex_sim, model_milk_weight_sim = model_milk_weight_sim, model_milk_multi_sim = model_milk_multi_sim, data_height = data_height, model_hight = model_hight, data_milk_clade = data_milk_clade, model_milk_clade = model_milk_clade, model_milk_house = model_milk_house ) write_rds(chapter5_models, &quot;envs/chapter5_models.rds&quot;) 6.5 Homework E1 \\[ \\begin{array}{ccclr} 1) &amp; \\mu_i &amp; = &amp; \\alpha + \\beta x_{i} &amp;\\textrm{[simple linear regression]}\\\\ 2) &amp; \\mu_i &amp; = &amp; \\beta_{x} x_{i} + \\beta_{z} z_{i} &amp;\\textrm{[multiple linear regression]}\\\\ 3) &amp; \\mu_i &amp; = &amp; \\alpha + \\beta (x_{i} - z_{i}) &amp;\\textrm{[simple linear regression]}\\\\ 4) &amp; \\mu_i &amp; = &amp; \\alpha + \\beta_{x} x_{i} + \\beta_{z} z_{i} &amp;\\textrm{[multiple linear regression]}\\\\ \\end{array} \\] E2 \\[ \\begin{array}{cclr} d_i &amp; = &amp; \\alpha + \\beta_{y} y_i + \\beta_{p} p_{i}&amp; \\textrm{[linear model]}\\\\ \\end{array} \\] E3 \\[ \\begin{array}{ccclr} 1) &amp; t_i &amp; = &amp; \\alpha_{f} + \\beta_{ff} f_i &amp; \\textrm{[linear model]}\\\\ 2) &amp; t_i &amp; = &amp; \\alpha_{s} + \\beta_{ss} s_{i} &amp; \\textrm{[linear model]}\\\\ 3) &amp; t_i &amp; = &amp; \\alpha + \\beta_{f} f_i + \\beta_{s} s_{i} &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] \\(\\beta_{f} \\ge 0\\) \\(\\beta_{ss} \\ge 0\\) \\(t \\sim f\\) (\\(\\beta_{f} \\gt \\beta_{ff}\\)) \\(t \\sim s\\) (\\(\\beta_{s} \\gt \\beta_{ss}\\)) \\(f \\sim -s\\) E4 1), 3), 4) and 5) (models should contain \\(k - 1\\) indicator variables) M1 n &lt;- 100 data_spurious2 &lt;- tibble(u = rnorm(n), x = rnorm(n, mean = u), y = rnorm(n, mean = -u), z = rnorm(n, mean = u) ) data_spurious2 %&gt;% ggpairs() model_spurious2a &lt;- quap( flist = alist( z ~ dnorm(mu, sigma), mu &lt;- alpha + beta_x * x, alpha ~ dnorm(0, .2), beta_x ~ dnorm(0, .75), sigma ~ dexp(1)), data = data_spurious2 ) model_spurious2b &lt;- quap( flist = alist( z ~ dnorm(mu, sigma), mu &lt;- alpha + beta_y * y, alpha ~ dnorm(0, .2), beta_y ~ dnorm(0, .75), sigma ~ dexp(1)), data = data_spurious2 ) model_spurious2c &lt;- quap( flist = alist( z ~ dnorm(mu, sigma), mu &lt;- alpha + beta_x * x + beta_y * y, alpha ~ dnorm(0, .2), beta_x ~ dnorm(0, .75), beta_y ~ dnorm(0, .75), sigma ~ dexp(1)), data = data_spurious2 ) ct_spur &lt;- coeftab(model_spurious2a, model_spurious2b, model_spurious2c, se = TRUE) plot_coeftab(ct_spur) M2 data_masked &lt;- tibble(u = rnorm(n), x = rnorm(n, mean = u), y = rnorm(n, mean = u), z = rnorm(n, mean = x-y) ) data_masked %&gt;% ggpairs() model_masked_a &lt;- quap( flist = alist( z ~ dnorm(mu, sigma), mu &lt;- alpha + beta_x * x, alpha ~ dnorm(0, .2), beta_x ~ dnorm(0, .75), sigma ~ dexp(1)), data = data_masked ) model_masked_b &lt;- quap( flist = alist( z ~ dnorm(mu, sigma), mu &lt;- alpha + beta_y * y, alpha ~ dnorm(0, .2), beta_y ~ dnorm(0, .75), sigma ~ dexp(1)), data = data_masked ) model_masked_c &lt;- quap( flist = alist( z ~ dnorm(mu, sigma), mu &lt;- alpha + beta_x * x + beta_y * y, alpha ~ dnorm(0, .2), beta_x ~ dnorm(0, .75), beta_y ~ dnorm(0, .75), sigma ~ dexp(1)), data = data_masked ) ct_masked &lt;- coeftab(model_masked_a, model_masked_b, model_masked_c, se = TRUE) plot_coeftab(ct_masked) M3 dag &lt;- dagify( D ~ A, M ~ A, exposure = &quot;A&quot;, outcome = &quot;M&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,.5,1), y = c(1, .4, 1))) %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag, clr_in = clr3) + scale_y_continuous(limits = c(.35, 1.05)) + coord_equal() M4 data_waffle_lds &lt;- data_waffle %&gt;% left_join(read_tsv(&quot;data/lds_by_state_2019.tsv&quot;)) %&gt;% mutate(lds_std = standardize(lds_perc), lds_perc_log10 = log10(lds_perc), lds_log10_std = standardize(lds_perc_log10)) data_waffle_lds %&gt;% dplyr::select(lds_perc, lds_perc_log10) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value)) + geom_histogram(bins = 10, color = clr0d, fill = fll0) + facet_wrap(name ~ ., scales = &quot;free&quot;) model_lds &lt;- quap( flist = alist( divorce_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_age * median_age_std + beta_marriage * marriage_std + beta_lds * lds_log10_std, alpha ~ dnorm(0, .2), beta_age ~ dnorm(0, .5), beta_marriage ~ dnorm(0, .5), beta_lds ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_waffle_lds ) precis(model_lds) #&gt; mean sd 5.5% 94.5% #&gt; alpha 6.262694e-07 0.09382055 -0.1499427 0.14994399 #&gt; beta_age -6.980543e-01 0.15085783 -0.9391543 -0.45695439 #&gt; beta_marriage 7.802884e-02 0.16280138 -0.1821592 0.33821689 #&gt; beta_lds -2.954296e-01 0.14942991 -0.5342475 -0.05661175 #&gt; sigma 7.511933e-01 0.07463517 0.6319118 0.87047467 precis(model_lds, depth = 2, pars = &quot;beta&quot;) %&gt;% as_tibble_rn() %&gt;% mutate(type = str_remove(param, pattern = &quot;beta_&quot;)) %&gt;% ggplot(aes(y = type)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(aes(xmin = `5.5%`, xmax =`94.5%`), color = clr0d) + geom_point(aes(x = mean), color = clr0d, fill = clr0, shape = 21, size = 3 ) + theme(axis.title.y = element_blank()) posterior_prediction &lt;- link(model_lds) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(data_waffle$divorce_std)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;divorce_predicted&quot;) %&gt;% group_by(row_idx) %&gt;% summarise(divorce_predicted_mean = mean(divorce_predicted), lower_pi = PI(divorce_predicted)[1], upper_pi = PI(divorce_predicted)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx)) %&gt;% left_join(data_waffle %&gt;% mutate(row_idx = row_number()), . ) posterior_simmulation &lt;- sim(model_lds) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(data_waffle$divorce_std)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;divorce_predicted&quot;) %&gt;% group_by(row_idx) %&gt;% summarise(lower_pi = PI(divorce_predicted)[1], upper_pi = PI(divorce_predicted)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx)) %&gt;% left_join(data_waffle %&gt;% mutate(row_idx = row_number()), . ) ggplot(mapping = aes(x = divorce_std)) + geom_abline(slope = 1, size = .7, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(data = posterior_prediction, aes(ymin = lower_pi, ymax = upper_pi, color = Loc %in% c(&quot;ID&quot;, &quot;UT&quot;)))+ geom_point(data = posterior_prediction, aes(y = divorce_predicted_mean, color = Loc %in% c(&quot;ID&quot;, &quot;UT&quot;), fill = after_scale(clr_lighten(color ,.5))), shape = 21, size = 1.5)+ geom_text(data = posterior_prediction %&gt;% filter(Loc %in% c(&quot;ID&quot;, &quot;ME&quot;, &quot;RI&quot;, &quot;UT&quot;)), aes(x = divorce_std - .15, y = divorce_predicted_mean, label = Loc)) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) M5 dag1 &lt;- dagify( O ~ W + E + P, W ~ P, E ~ P, exposure = &quot;P&quot;, outcome = &quot;O&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,.5,1, .5), y = c(1,1, 1,.4))) %&gt;% mutate(stage = if_else(name == &quot;O&quot;, &quot;response&quot;, if_else(name %in% c(&quot;W&quot;, &quot;E&quot;, &quot;P&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag1, clr_in = clr3) + # plot_dag(dag2, clr_in = clr3) &amp; # scale_y_continuous(limits = c(.35, 1.05)) &amp; coord_equal() with \\(o\\) as obesity rate \\(p\\) as gasoline price \\(e\\) as money spend on eating out \\(w\\) as average distance walked \\[ \\begin{array}{cclr} o_i &amp; \\sim &amp; Normal(\\mu_i, \\sigma) &amp; \\textrm{[likelyhood]}\\\\ \\mu_i &amp; = &amp; \\alpha_{p} + \\beta_{p} p_i &amp; \\textrm{[linear model (price only)]}\\\\ \\mu_i &amp; = &amp; \\alpha_{w} + \\beta_{w} w_i &amp; \\textrm{[linear model (walking)]}\\\\ \\mu_i &amp; = &amp; \\alpha_{e} + \\beta_{e} e_i &amp; \\textrm{[linear model (eating out)]}\\\\ \\mu_i &amp; = &amp; \\alpha_{m} + \\beta_{pp} + \\beta_{ww} w_i + p_i + \\beta_{ee} e_i &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] H1 dagitty(&#39;dag{ M -&gt; A -&gt; D }&#39;) %&gt;% impliedConditionalIndependencies() #&gt; D _||_ M | A This reads as conditional on \\(A\\), \\(D\\) is independent from \\(M\\). given the results from model_multiple, this seems plausible as the multiple model greatly reduces the effect of beat_M: precis(model_multiple) %&gt;% round(digits = 2) %&gt;% as.matrix() %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.10 -0.16 0.16 beta_A -0.61 0.15 -0.85 -0.37 beta_M -0.07 0.15 -0.31 0.18 sigma 0.79 0.08 0.66 0.91 plot_coeftab(ct) + scale_color_manual(values = rep(clr0d, 3), guide = &quot;none&quot;) Actually this one is a markov equivalent of the dag investigated in the main text (and all members of that set are consistent with the model): dag_h1 &lt;- dagitty(&#39;dag{ M -&gt; A -&gt; D }&#39;) coordinates(dag_h1) &lt;- list( x = c( M = 0, A = 1, D = .5), y = c( M = 1, A = 1, D = .3)) dag_h1 %&gt;% node_equivalent_dags() %&gt;% mutate(stage = &quot;predictor&quot;) %&gt;% plot_dag() + coord_equal(xlim = c(-.1, 1.1), ylim = c(.2, 1.1))+ facet_wrap(~ dag) H2 model_counterfactual_marriage &lt;- quap( flist = alist( # A -&gt; D divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ), # M -&gt; A median_age_std ~ dnorm( mu_A, sigma_A ), mu_A &lt;- alpha_A + beta_MA * marriage_std, alpha_A ~ dnorm( 0, 0.2 ), beta_MA ~ dnorm( 0, 0.5 ), sigma_A ~ dexp(1) ), data = data_waffle ) precis(model_counterfactual_marriage) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.10 -0.16 0.16 beta_A -0.57 0.11 -0.74 -0.39 sigma 0.79 0.08 0.66 0.91 alpha_A 0.00 0.09 -0.14 0.14 beta_MA -0.69 0.10 -0.85 -0.54 sigma_A 0.68 0.07 0.57 0.79 M_seq &lt;- seq(-2, 2, length.out = 30) data_sim &lt;- sim(fit = model_counterfactual_marriage, data = tibble(marriage_std = M_seq), vars = c(&quot;median_age_std&quot;, &quot;divorce_std&quot;)) %&gt;% unpack_sim() data_sim_pi &lt;- data_sim %&gt;% group_by(row_idx, parameter) %&gt;% summarise(mean = mean(value), PI_lower = PI(value)[1], PI_upper = PI(value)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx), marriage_std = M_seq[row_idx]) %&gt;% arrange(parameter, marriage_std) data_sim_pi %&gt;% ggplot() + geom_smooth(aes(x = marriage_std, y = mean, ymin = PI_lower, ymax = PI_upper, color = parameter, fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(clr0d, clr3), guide = &quot;none&quot;) + # labs(y = &quot;counterfactual value&quot;, title = &quot;Counterfactual effects of age at marriage on&quot;) + facet_wrap(parameter ~ .) M_seq2 &lt;- c(data_waffle$median_age_std, data_waffle$median_age_std/2) m_rate_california &lt;- which(data_waffle$Location == &quot;Idaho&quot;) M_seq2 &lt;- c(data_waffle$median_age_std[m_rate_california], data_waffle$median_age_std[m_rate_california]/2) data_sim2 &lt;- sim(fit = model_counterfactual_marriage, data = tibble(marriage_std = M_seq2), vars = c(&quot;median_age_std&quot;, &quot;divorce_std&quot;)) %&gt;% data.frame() %&gt;% pivot_longer(cols = everything()) %&gt;% separate(name, into = c(&quot;param&quot;, &quot;rn&quot;), sep = &#39;\\\\.&#39;, convert = TRUE) %&gt;% mutate(group = c(&quot;org&quot;, &quot;half&quot;)[1 + (rn &gt; (length(M_seq2)/2))]) %&gt;% filter(param == &quot;divorce_std&quot;) %&gt;% dplyr::select(-rn) %&gt;% # mutate(value = value * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) %&gt;% pivot_wider(names_from = group, values_from = value) %&gt;% unnest() %&gt;% mutate(diff = half - org) data_sim2 %&gt;% ggplot(aes(x = diff)) + geom_density(fill = fll0, color = clr0d) data_sim2 %&gt;% ggplot() + geom_density(aes(x = org, color = &quot;orgiginal&quot;, fill = after_scale(clr_alpha(color)))) + geom_density(aes(x = half, color = &quot;half&quot;, fill = after_scale(clr_alpha(color)))) + scale_color_manual(values = c(original = clr0d, half = clr3)) + labs(x = &quot;divorce_std&quot;) + theme(legend.position = &quot;bottom&quot;) mean(data_sim2$diff) #&gt; [1] 0.4360082 Halfing a states marriage rate would on average increase the divorce rate by ~ 0 standard deviations. H3 dag1 &lt;- dagify( K ~ M + N, N ~ M, exposure = &quot;M&quot;, outcome = &quot;K&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,1,.5), y = c(1,1,.4))) %&gt;% mutate(stage = if_else(name == &quot;K&quot;, &quot;response&quot;, if_else(name %in% c(&quot;M&quot;, &quot;N&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag1, clr_in = clr3) + coord_equal() model_counterfactual_milk &lt;- quap( flist = alist( # M -&gt; K &lt;- N kcal_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_MK * mass_std + beta_NK * neocortex_std, alpha ~ dnorm( 0, 0.2 ), beta_MK ~ dnorm( 0, 0.5 ), beta_NK ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ), # M -&gt; N neocortex_std ~ dnorm( mu_N, sigma_N ), mu_N &lt;- alpha_N + beta_MN * mass_std, alpha_N ~ dnorm( 0, 0.2 ), beta_MN ~ dnorm( 0, 0.5 ), sigma_N ~ dexp(1) ), data = data_milk ) precis(model_counterfactual_milk) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.13 -0.20 0.20 beta_MK -0.75 0.23 -1.12 -0.37 beta_NK 0.64 0.23 0.27 1.01 sigma 0.69 0.12 0.49 0.88 alpha_N 0.00 0.12 -0.19 0.19 beta_MN 0.68 0.15 0.44 0.93 sigma_N 0.63 0.11 0.46 0.80 W_seq &lt;- seq(-2, 2, length.out = 30) data_sim &lt;- sim(fit = model_counterfactual_milk, data = tibble(mass_std = W_seq), vars = c(&quot;neocortex_std&quot;, &quot;kcal_std&quot;)) %&gt;% unpack_sim() data_sim_pi &lt;- data_sim %&gt;% group_by(row_idx, parameter) %&gt;% summarise(mean = mean(value), PI_lower = PI(value)[1], PI_upper = PI(value)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx), mass_std = M_seq[row_idx]) %&gt;% arrange(parameter, mass_std) data_sim_pi %&gt;% ggplot() + geom_smooth(aes(x = mass_std, y = mean, ymin = PI_lower, ymax = PI_upper, color = parameter, fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(clr0d, clr3), guide = &quot;none&quot;) + # labs(y = &quot;counterfactual value&quot;, title = &quot;Counterfactual effects of age at marriage on&quot;) + facet_wrap(parameter ~ .) M_seq2 &lt;- (log(c(15, 30)) - mean(log(milk$mass))) / sd(log(milk$mass)) data_sim2 &lt;- sim(fit = model_counterfactual_milk, data = tibble(mass_std = M_seq2), vars = c(&quot;neocortex_std&quot;, &quot;kcal_std&quot;)) %&gt;% data.frame() %&gt;% pivot_longer(cols = everything()) %&gt;% separate(name, into = c(&quot;param&quot;, &quot;rn&quot;), sep = &#39;\\\\.&#39;, convert = TRUE) %&gt;% mutate(group = c(&quot;org&quot;, &quot;double&quot;)[1 + (rn &gt; (length(M_seq2)/2))]) %&gt;% filter(param == &quot;kcal_std&quot;) %&gt;% dplyr::select(-rn) %&gt;% # mutate(value = value * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) %&gt;% pivot_wider(names_from = group, values_from = value) %&gt;% unnest() %&gt;% mutate(diff = double - org) data_sim2 %&gt;% ggplot(aes(x = diff)) + geom_density(fill = fll0, color = clr0d) data_sim2 %&gt;% ggplot() + geom_density(aes(x = org, color = &quot;orgiginal&quot;, fill = after_scale(clr_alpha(color)))) + geom_density(aes(x = double, color = &quot;double&quot;, fill = after_scale(clr_alpha(color)))) + scale_color_manual(values = c(original = clr0d, double = clr3)) + labs(x = &quot;kcal_std&quot;) + theme(legend.position = &quot;bottom&quot;) quantile(data_sim2$diff, probs = c(.05, .5, .95)) #&gt; 5% 50% 95% #&gt; -1.9762716 -0.1036398 1.7891183 mean(data_sim2$diff) #&gt; [1] -0.0910389 Following the paths of the dag to get the causal effect. To then get to the magnitude of the contrast, scale by max - min. prec_out &lt;- precis(model_counterfactual_milk) # ((M -&gt; N) * (M -&gt; K) ) + (M -&gt; K) * delta_input (prec_out[&quot;beta_MN&quot;, &quot;mean&quot;] * prec_out[&quot;beta_NK&quot;, &quot;mean&quot;] + prec_out[&quot;beta_MK&quot;, &quot;mean&quot;] ) * diff(M_seq2) #&gt; [1] -0.126499 H4 data_south &lt;- data_waffle %&gt;% dplyr::select(Location, South, ends_with(&quot;_std&quot;)) dag &lt;- dagify( D ~ M + A + S, M ~ A, A ~ S, exposure = &quot;A&quot;, outcome = &quot;M&quot;) %&gt;% tidy_dagitty(.dagitty = ., layout = tibble(x = c(0,.5, .5, 1), y = c(1, .6, 1.4, 1))) %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M&quot;, &quot;S&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag, clr_in = clr3) + scale_y_continuous(limits = c(.5, 1.5)) + coord_equal() dagitty(&#39;dag{ D &lt;- A -&gt; M; D &lt;- S -&gt; A; M -&gt; D }&#39;) %&gt;% impliedConditionalIndependencies() #&gt; M _||_ S | A model_south_multi &lt;- quap( flist = alist( marriage_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_SD * South + beta_AD * median_age_std, alpha ~ dnorm(0, .2), beta_SD ~ dnorm(0,.5), beta_AD ~ dnorm(0,.5), sigma ~ dexp(1) ), data = data_south ) precis(model_south_multi) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.04 0.10 -0.12 0.19 beta_SD -0.17 0.19 -0.48 0.14 beta_AD -0.71 0.10 -0.87 -0.56 sigma 0.68 0.07 0.57 0.78 M could be independent of S (large spread around zero) precis(model_south_multi)[&quot;beta_SD&quot;, ] %&gt;% round(digits = 2) #&gt; mean sd 5.5% 94.5% #&gt; beta_SD -0.17 0.19 -0.48 0.14 Additional scenario (from Jake Thompson) dag_coords &lt;- tibble(name = c(&quot;S&quot;, &quot;A&quot;, &quot;M&quot;, &quot;D&quot;), x = c(1, 1, 2, 3), y = c(3, 1, 2, 1)/2) dagify(D ~ A + M, M ~ A + S, A ~ S, coords = dag_coords) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M&quot;, &quot;S&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(.3, 1.7)) + coord_equal() div_dag &lt;- dagitty(&quot;dag{S -&gt; M -&gt; D; S -&gt; A -&gt; D; A -&gt; M}&quot;) impliedConditionalIndependencies(div_dag) #&gt; D _||_ S | A, M model_south_multi2 &lt;- quap( flist = alist( divorce_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_S * South + beta_A * median_age_std + beta_M * marriage_std, alpha ~ dnorm(0, .2), beta_S ~ dnorm(0,.5), beta_A ~ dnorm(0,.5), beta_M ~ dnorm(0,.5), sigma ~ dexp(1) ), data = data_south ) precis(model_south_multi2) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha -0.08 0.11 -0.25 0.09 beta_S 0.35 0.22 0.01 0.69 beta_A -0.56 0.15 -0.80 -0.32 beta_M -0.04 0.15 -0.28 0.19 sigma 0.76 0.08 0.64 0.88 precis(model_south_multi2)[&quot;beta_S&quot;, ] %&gt;% round(digits = 2) #&gt; mean sd 5.5% 94.5% #&gt; beta_S 0.35 0.22 0.01 0.69 6.6 {brms} section 6.6.1 Age at marriage Model Note the sample_prior = TRUE to also sample from the prior (as well as from the posterior). Prior samples are extracted with prior_draws(). brms_c5_model_age &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + median_age_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, sample_prior = TRUE, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_age&quot;) brms_age_prior &lt;- prior_draws(brms_c5_model_age) %&gt;% as_tibble() brms_age_prior %&gt;% slice_sample(n = 50) %&gt;% rownames_to_column(&quot;draw&quot;) %&gt;% expand(nesting(draw, Intercept, b), a = c(-2, 2)) %&gt;% mutate(d = Intercept + b * a) %&gt;% ggplot(aes(a,d, group = draw)) + geom_line(color = clr0d %&gt;% clr_alpha()) + labs(x = &quot;median_age_std&quot;, y = &quot;divorce_rate_std&quot;) Getting to the posterior predictions with fitted(): nd &lt;- tibble(median_age_std = seq(from = -3, to = 3.2, length.out = 30)) # now use `fitted()` to get the model-implied trajectories fitted(object = brms_c5_model_age, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = median_age_std)) + geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, color = clr0d, fill = fll0) + geom_point(data = data_waffle, aes(y = divorce_std), color = clr_dark )+ labs(x = &quot;median_age_std&quot;, y = &quot;divorce_rate_std&quot;) \\(\\rightarrow\\) The posterior for median_age_std (\\(\\beta_{age}\\)) is reliably negative (look at Estimate and 95% quantiles )… print(brms_c5_model_age) #&gt; Family: gaussian #&gt; Links: mu = identity; sigma = identity #&gt; Formula: divorce_std ~ 1 + median_age_std #&gt; Data: data_waffle (Number of observations: 50) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; Intercept 0.00 0.10 -0.20 0.20 1.00 3936 2758 #&gt; median_age_std -0.57 0.11 -0.79 -0.34 1.00 3906 3129 #&gt; #&gt; Family Specific Parameters: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; sigma 0.82 0.08 0.68 1.00 1.00 4302 3021 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). 6.6.2 Marriage rate Model brms_c5_model_marriage &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + marriage_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;brms/brms_c5_model_marriage&quot;) … smaller magnitude for the marriage rate model: print(brms_c5_model_marriage) #&gt; Family: gaussian #&gt; Links: mu = identity; sigma = identity #&gt; Formula: divorce_std ~ 1 + marriage_std #&gt; Data: data_waffle (Number of observations: 50) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; Intercept 0.00 0.11 -0.22 0.22 1.00 4602 2813 #&gt; marriage_std 0.35 0.13 0.09 0.61 1.00 4325 3000 #&gt; #&gt; Family Specific Parameters: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; sigma 0.95 0.10 0.78 1.16 1.00 4404 3059 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). nd &lt;- tibble(marriage_std = seq(from = -2.5, to = 3.5, length.out = 30)) # now use `fitted()` to get the model-implied trajectories fitted(object = brms_c5_model_marriage, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = marriage_std)) + geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, color = clr0d, fill = fll0) + geom_point(data = data_waffle, aes(y = divorce_std), color = clr_dark )+ labs(x = &quot;marriage_rate_std&quot;, y = &quot;divorce_rate_std&quot;) 6.6.3 Multiple regression brms_c5_model_multiple &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + marriage_std + median_age_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_multiple&quot;) print(brms_c5_model_multiple) #&gt; Family: gaussian #&gt; Links: mu = identity; sigma = identity #&gt; Formula: divorce_std ~ 1 + marriage_std + median_age_std #&gt; Data: data_waffle (Number of observations: 50) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; Intercept 0.00 0.10 -0.19 0.19 1.00 3829 2943 #&gt; marriage_std -0.06 0.16 -0.37 0.25 1.00 3291 2718 #&gt; median_age_std -0.60 0.16 -0.92 -0.29 1.00 2859 2440 #&gt; #&gt; Family Specific Parameters: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; sigma 0.83 0.09 0.68 1.02 1.00 3553 2380 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). mixedup::summarise_model(brms_c5_model_multiple) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 0.68 0.83 0.68 1.02 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; Intercept 0.00 0.10 -0.19 0.19 #&gt; marriage_std -0.06 0.16 -0.37 0.25 #&gt; median_age_std -0.60 0.16 -0.92 -0.29 bind_cols( as_draws_df(brms_c5_model_age) %&gt;% transmute(`brms_age-beta_age` = b_median_age_std), as_draws_df(brms_c5_model_marriage) %&gt;% transmute(`brms_marriage-beta_marriage` = b_marriage_std), as_draws_df(brms_c5_model_multiple) %&gt;% transmute(`brms_multi-beta_marriage` = b_marriage_std, `brms_multi-beta_age` = b_median_age_std) ) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% separate(col = name, into = c(&quot;fit&quot;, &quot;parameter&quot;), sep = &quot;-&quot;) %&gt;% ggplot(aes(x = mean, xmin = ll, xmax = ul, y = fit)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_pointrange(color = clr0d, fill = clr0, shape = 21) + facet_wrap(~ parameter, ncol = 1, labeller = label_parsed) + theme(axis.title = element_blank()) Simulating divorce data n &lt;- 50 sim_d &lt;- tibble(age = rnorm(n, mean = 0, sd = 1), mar = rnorm(n, mean = -age, sd = 1), div = rnorm(n, mean = age, sd = 1)) brms_c5_model_age_sim &lt;- update(brms_c5_model_age, newdata = sim_d, formula = div ~ 1 + age, seed = 42, file = &quot;brms/brms_c5_model_age_sim&quot;) brms_c5_model_marriage_sim &lt;- update(brms_c5_model_marriage, newdata = sim_d, formula = div ~ 1 + mar, seed = 42, file = &quot;brms/brms_c5_model_marriage_sim&quot;) brms_c5_model_multiple_sim &lt;- update(brms_c5_model_multiple, newdata = sim_d, formula = div ~ 1 + mar + age, seed = 42, file = &quot;brms/brms_c5_model_multiple_sim&quot;) bind_cols( as_draws_df(brms_c5_model_age_sim) %&gt;% transmute(`brms_age-beta_age` = b_age), as_draws_df(brms_c5_model_marriage_sim) %&gt;% transmute(`brms_marriage-beta_marriage` = b_mar), as_draws_df(brms_c5_model_multiple_sim) %&gt;% transmute(`brms_multi-beta_marriage` = b_mar, `brms_multi-beta_age` = b_age) ) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% separate(col = name, into = c(&quot;fit&quot;, &quot;parameter&quot;), sep = &quot;-&quot;) %&gt;% ggplot(aes(x = mean, xmin = ll, xmax = ul, y = fit)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_pointrange(color = clr0d, fill = clr0, shape = 21) + facet_wrap(~ parameter, ncol = 1, labeller = label_parsed) + theme(axis.title = element_blank()) 6.6.4 Multivariate Posteriors brms_c5_model_residuals_marriage &lt;- brm( data = data_waffle, family = gaussian, marriage_std ~ 1 + median_age_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_residuals_marriage&quot;) fitted(brms_c5_model_residuals_marriage) %&gt;% data.frame() %&gt;% bind_cols(data_waffle) %&gt;% as_tibble() %&gt;% ggplot(aes(x = median_age_std, y = marriage_std)) + geom_point(color = clr_dark) + geom_segment(aes(xend = median_age_std, yend = Estimate), size = .5, linetype = 3) + geom_line(aes(y = Estimate), color = clr0d) + ggrepel::geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;WY&quot;, &quot;ND&quot;, &quot;ME&quot;, &quot;HI&quot;, &quot;DC&quot;)), aes(label = Loc), size = 3, seed = 14, family = fnt_sel) + labs(x = &quot;median_age_std&quot;, y = &quot;marriage_std&quot;) residual_data &lt;- residuals(brms_c5_model_residuals_marriage) %&gt;% as_tibble() %&gt;% bind_cols(data_waffle) brms_c5_model_residuals_data &lt;- brm( data = residual_data, family = gaussian, divorce_std ~ 1 + Estimate, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_residuals_data&quot;) nd &lt;- tibble(Estimate = seq(from = -2, to = 2, length.out = 30)) residuals_intervals &lt;- fitted(object = brms_c5_model_residuals_data, newdata = nd) %&gt;% as_tibble() %&gt;% rename(mean = &quot;Estimate&quot;) %&gt;% bind_cols(nd) residual_data %&gt;% ggplot(aes(x = Estimate, y = divorce_std)) + geom_smooth(data = residuals_intervals, aes(y = mean, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, color = clr0d, fill = fll0) + geom_vline(xintercept = 0, linetype = 3, color = clr_dark) + geom_point(color = clr_dark) + ggrepel::geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;WY&quot;, &quot;ND&quot;, &quot;ME&quot;, &quot;HI&quot;, &quot;DC&quot;)), aes(label = Loc), size = 3, seed = 5, family = fnt_sel) Don’t use residuals as input data for another model - this ignores a ton of uncertainty: residual_data %&gt;% ggplot(aes(x = Estimate, y = divorce_std)) + geom_vline(xintercept = 0, linetype = 3, color = clr_dark) + geom_pointrange(aes(xmin = `Q2.5`, xmax = `Q97.5`), color = clr0d, fill = clr0, shape = 21) + ggrepel::geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;RI&quot;, &quot;ME&quot;, &quot;UT&quot;, &quot;ID&quot;)), aes(label = Loc), size = 3, seed = 5, family = fnt_sel) Posterior prediction plot: fitted(brms_c5_model_multiple) %&gt;% as_tibble() %&gt;% bind_cols(data_waffle) %&gt;% ggplot(aes(x = divorce_std, y = Estimate)) + geom_abline(slope = 1, linetype = 3, color = clr_dark) + geom_pointrange(aes(ymin = `Q2.5`, ymax = `Q97.5`), color = clr0d, fill = clr0, shape = 21) + ggrepel::geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;RI&quot;, &quot;ME&quot;, &quot;UT&quot;, &quot;ID&quot;)), aes(label = Loc), size = 3, seed = 5, family = fnt_sel) brms_c5_model_spurious &lt;- brm( data = data_spurious, family = gaussian, y ~ 1 + x_real + x_spur, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_spurious&quot;) mixedup::extract_fixef(brms_c5_model_spurious) #&gt; # A tibble: 3 x 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Intercept 0.05 0.096 -0.135 0.242 #&gt; 2 x_real 0.902 0.146 0.619 1.18 #&gt; 3 x_spur 0.094 0.108 -0.113 0.309 6.6.5 Counterfactual plots At this point, it’s important to recognize we have two regression models. As a first step, we might specify each model separately in a bf() function and save them as objects (Estimating multivariate models with brms). divorce_model &lt;- bf(divorce.std ~ 1 + median.age.std + marriage.std) marriage_model &lt;- bf(marriage.std ~ 1 + median.age.std) divorce_model &lt;- bf(divorcestd ~ 1 + medianagestd + marriagestd) marriage_model &lt;- bf(marriagestd ~ 1 + medianagestd) Next we will combine our bf() objects with the + operator within the brm() function. For a model like this, we also specify set_rescor(FALSE) to prevent brms from adding a residual correlation between d and m. Also, notice how each prior statement includes a resp argument. This clarifies which sub-model the prior refers to. # can&#39;t use _ or . in column names in this context data_waffle_short &lt;- data_waffle %&gt;% set_names(nm = names(data_waffle) %&gt;% str_remove_all(&quot;_&quot;)) brms_c5_model_counterfactual &lt;- brm( data = data_waffle_short, family = gaussian, divorce_model + marriage_model + set_rescor(FALSE), prior = c(prior(normal(0, 0.2), class = Intercept, resp = divorcestd), prior(normal(0, 0.5), class = b, resp = divorcestd), prior(exponential(1), class = sigma, resp = divorcestd), prior(normal(0, 0.2), class = Intercept, resp = marriagestd), prior(normal(0, 0.5), class = b, resp = marriagestd), prior(exponential(1), class = sigma, resp = marriagestd)), chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_counterfactual&quot;) print(brms_c5_model_counterfactual) #&gt; Family: MV(gaussian, gaussian) #&gt; Links: mu = identity; sigma = identity #&gt; mu = identity; sigma = identity #&gt; Formula: divorcestd ~ 1 + medianagestd + marriagestd #&gt; marriagestd ~ 1 + medianagestd #&gt; Data: data_waffle_short (Number of observations: 50) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS #&gt; divorcestd_Intercept -0.00 0.10 -0.20 0.19 1.00 5659 #&gt; marriagestd_Intercept -0.00 0.09 -0.18 0.17 1.00 5705 #&gt; divorcestd_medianagestd -0.61 0.16 -0.90 -0.29 1.00 3490 #&gt; divorcestd_marriagestd -0.06 0.15 -0.36 0.25 1.00 3354 #&gt; marriagestd_medianagestd -0.69 0.10 -0.89 -0.49 1.00 4910 #&gt; Tail_ESS #&gt; divorcestd_Intercept 2695 #&gt; marriagestd_Intercept 3063 #&gt; divorcestd_medianagestd 2968 #&gt; divorcestd_marriagestd 2948 #&gt; marriagestd_medianagestd 3278 #&gt; #&gt; Family Specific Parameters: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; sigma_divorcestd 0.83 0.09 0.68 1.02 1.00 5112 3245 #&gt; sigma_marriagestd 0.71 0.08 0.58 0.89 1.00 4852 2896 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). nd &lt;- tibble(medianagestd = seq(from = -2, to = 2, length.out = 30), marriagestd = 0) predict(brms_c5_model_counterfactual, resp = &quot;divorcestd&quot;, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = medianagestd, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_smooth(stat = &quot;identity&quot;, fill = fll0, color = clr0d, size = .2) + labs(subtitle = &quot;Total counterfactual effect of A on D&quot;, x = &quot;manipulated median_age_std&quot;, y = &quot;counterfactual divorce_std&quot;) nd &lt;- tibble(marriagestd = seq(from = -2, to = 2, length.out = 30), medianagestd = 0) predict(brms_c5_model_counterfactual, resp = &quot;divorcestd&quot;, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = marriagestd, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_smooth(stat = &quot;identity&quot;, fill = fll0, color = clr0d, size = .2) + labs(subtitle = &quot;Total counterfactual effect of M on D&quot;, x = &quot;manipulated marriage_std&quot;, y = &quot;counterfactual divorce_std&quot;) 6.6.6 Masked Relationships brms_c5_model_milk_draft &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 1 + neocortex_std, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, sample_prior = TRUE, file = &quot;brms/brms_c5_model_milk_draft&quot;) set.seed(42) prior_draws(brms_c5_model_milk_draft) %&gt;% slice_sample(n = 50) %&gt;% rownames_to_column() %&gt;% expand(nesting(rowname, Intercept, b), neocortex_std = c(-2, 2)) %&gt;% mutate(kcal_std = Intercept + b * neocortex_std) %&gt;% ggplot(aes(x = neocortex_std, y = kcal_std)) + geom_line(aes(group = rowname), color = clr0d %&gt;% clr_alpha()) + coord_cartesian(ylim = c(-2, 2)) + labs(x = &quot;neocortex_std&quot;, y = &quot;kcal_std&quot;, subtitle = &quot;Intercept ~ dnorm(0, 1); b ~ dnorm(0, 1)&quot;) brms_c5_model_milk_cortex &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 1 + neocortex_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, sample_prior = TRUE, file = &quot;brms/brms_c5_model_milk_cortex&quot;) set.seed(42) prior_draws(brms_c5_model_milk_cortex) %&gt;% slice_sample(n = 50) %&gt;% rownames_to_column() %&gt;% expand(nesting(rowname, Intercept, b), neocortex_std = c(-2, 2)) %&gt;% mutate(kcal_std = Intercept + b * neocortex_std) %&gt;% ggplot(aes(x = neocortex_std, y = kcal_std)) + geom_line(aes(group = rowname), color = clr0d %&gt;% clr_alpha()) + coord_cartesian(ylim = c(-2, 2)) + labs(x = &quot;neocortex_std&quot;, y = &quot;kcal_std&quot;, subtitle = &quot;Intercept ~ dnorm(0, 0.2); b ~ dnorm(0, 0.5)&quot;) bind_rows( as_draws_df(brms_c5_model_milk_draft) %&gt;% select(b_Intercept:sigma), as_draws_df(brms_c5_model_milk_cortex) %&gt;% select(b_Intercept:sigma) ) %&gt;% mutate(fit = rep(c(&quot;milk_draft&quot;, &quot;milk_cortex&quot;), each = n() / 2)) %&gt;% pivot_longer(-fit, names_to = &quot;parameter&quot;) %&gt;% group_by(parameter, fit) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% mutate(fit = factor(fit, levels = c(&quot;milk_draft&quot;, &quot;milk_cortex&quot;))) %&gt;% ggplot(aes(x = mean, xmin = ll, xmax = ul, y = fit)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_pointrange(color = clr0d, fill = clr0, shape = 21) + facet_wrap(~ parameter, ncol = 1) + theme(axis.title = element_blank()) nd &lt;- tibble(neocortex_std = seq(from = -2.5, to = 2, length.out = 30)) fitted(brms_c5_model_milk_cortex, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = neocortex_std, y = Estimate, ymin = Q25, ymax = Q75)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = fll0) + geom_smooth(stat = &quot;identity&quot;, fill = fll0, color = clr0d, size = .2) + geom_point(data = data_milk, aes(x = neocortex_std, y = kcal_std), inherit.aes = FALSE, color = clr_dark) + labs(y = &#39;kcal_std&#39;) brms_c5_model_milk_weight &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 1 + mass_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, sample_prior = TRUE, file = &quot;brms/brms_c5_model_milk_weight&quot;) nd &lt;- tibble(mass_std = seq(from = -2.5, to = 2.5, length.out = 30)) fitted(brms_c5_model_milk_weight, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = mass_std, y = Estimate, ymin = Q25, ymax = Q75)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = fll0) + geom_smooth(stat = &quot;identity&quot;, fill = fll0, color = clr0d, size = .2) + geom_point(data = data_milk, aes(x = mass_std, y = kcal_std), inherit.aes = FALSE, color = clr_dark) + labs(y = &#39;kcal_std&#39;) brms_c5_model_milk_multi &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 1 + neocortex_std + mass_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_milk_multi&quot;) bind_cols( as_draws_df(brms_c5_model_milk_cortex) %&gt;% transmute(`cortex-beta_N` = b_neocortex_std), as_draws_df(brms_c5_model_milk_weight) %&gt;% transmute(`weight-beta_M` = b_mass_std), as_draws_df(brms_c5_model_milk_multi) %&gt;% transmute(`multi-beta_N` = b_neocortex_std, `multi-beta_M` = b_mass_std) ) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% separate(name, into = c(&quot;fit&quot;, &quot;parameter&quot;), sep = &quot;-&quot;) %&gt;% ggplot(aes(x = mean, y = fit, xmin = ll, xmax = ul)) + geom_pointrange(color = clr0d, fill = clr0, shape = 21) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + ylab(NULL) + facet_wrap(~ parameter, ncol = 1) nd &lt;- tibble(neocortex_std = seq(from = -2.5, to = 2, length.out = 30), mass_std = 0) fitted(brms_c5_model_milk_multi, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = neocortex_std, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = fll0) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = fll0, color = clr0d, size = .2) + labs(subtitle = &quot;Counterfactual holding M = 0&quot;, x = &quot;neocortex_std&quot;, y = &quot;kcal_std&quot;) nd &lt;- tibble(mass_std = seq(from = -2.5, to = 2.5, length.out = 30), neocortex_std = 0) fitted(brms_c5_model_milk_multi, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = mass_std, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = fll0) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = fll0, color = clr0d, size = .2) + labs(subtitle = &quot;Counterfactual holding M = 0&quot;, x = &quot;mass_std&quot;, y = &quot;kcal_std&quot;) brms_c5_model_milk_multi_sim &lt;- update( brms_c5_model_milk_multi, newdata = data_milk_sim1, formula = kcal_std ~ 1 + neocortex_std + mass_std, seed = 42, file = &quot;brms/brms_c5_model_milk_multi_sim&quot;) brms_c5_model_milk_cortex_sim &lt;- update( brms_c5_model_milk_cortex, formula = kcal_std ~ 1 + neocortex_std, seed = 42, file = &quot;brms/brms_c5_model_milk_cortex_sim&quot;) brms_c5_model_milk_weight_sim &lt;- update( brms_c5_model_milk_weight, formula = kcal_std ~ 1 + mass_std, seed = 42, file = &quot;brms/brms_c5_model_milk_weight_sim&quot;) mixedup::extract_fixef(brms_c5_model_milk_cortex_sim) #&gt; # A tibble: 2 x 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Intercept 0.003 0.162 -0.324 0.321 #&gt; 2 neocortex_std 0.123 0.231 -0.325 0.584 mixedup::extract_fixef(brms_c5_model_milk_weight_sim) #&gt; # A tibble: 2 x 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Intercept 0.005 0.152 -0.293 0.307 #&gt; 2 mass_std -0.283 0.221 -0.708 0.158 mixedup::extract_fixef(brms_c5_model_milk_multi_sim) #&gt; # A tibble: 3 x 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Intercept -0.047 0.081 -0.208 0.112 #&gt; 2 neocortex_std 0.982 0.096 0.794 1.17 #&gt; 3 mass_std -1.04 0.118 -1.26 -0.808 6.6.7 Categorical Variables 6.6.7.1 Binary Categories For an indicator variable, we need this to be a factor(): data_height &lt;- data_height %&gt;% mutate(sex = factor(sex)) brms_c5_model_height &lt;- brm( data = data_height, family = gaussian, height ~ 0 + sex, prior = c(prior(normal(178, 20), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_height&quot;) contrasts with {brms} library(tidybayes) as_draws_df(brms_c5_model_height) %&gt;% mutate(diff_fm = b_sex1 - b_sex2) %&gt;% gather(key, value, -`lp__`) %&gt;% group_by(key) %&gt;% mean_qi(value, .width = .89) %&gt;% filter(!grepl(key, pattern = &quot;^\\\\.&quot;)) %&gt;% knitr::kable() key value .lower .upper .width .point .interval b_sex1 134.901752 132.38783 137.448902 0.89 mean qi b_sex2 142.593033 139.91077 145.293809 0.89 mean qi diff_fm -7.691281 -11.49839 -3.924036 0.89 mean qi sigma 26.767597 25.51963 28.079138 0.89 mean qi 6.6.7.2 Many Categories brms_c5_model_milk_clade &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 0 + clade, prior = c(prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_milk_clade&quot;) library(bayesplot) (mcmc_intervals_data(brms_c5_model_milk_clade, prob = .5) %&gt;% filter(grepl(parameter, pattern = &quot;^b&quot;)) %&gt;% ggplot(aes(y = parameter)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_linerange(aes(xmin = ll, xmax = hh), lwd = .2, color = clr2) + geom_pointrange(aes(xmin = l, x = m, xmax = h), lwd = .7, shape = 21, color = clr2, fill = clr_lighten(clr2,.2))) + theme(axis.title = element_blank()) as_draws_df(brms_c5_model_milk_clade) %&gt;% select(starts_with(&quot;b&quot;)) %&gt;% as_tibble() %&gt;% set_names(x = . , nm = names(.) %&gt;% str_remove(&quot;b_clade&quot;)) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = reorder(name, value))) + # note how we used `reorder()` to arrange the coefficients geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_pointinterval(point_interval = mode_hdi, .width = .89, size = 2, shape = 21, color = clr2, fill = clr_lighten(clr2,.2)) + labs(title = &quot;My tidybayes-based coefficient plot&quot;, x = &quot;expected kcal (std)&quot;, y = NULL) naïve {brms} model fit: brms_c5_model_milk_house &lt;- brm( data = data_milk_clade, family = gaussian, kcal_std ~ 0 + clade + house, prior = c(prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_milk_house&quot;) \\(\\rightarrow\\) there are only three house levels 🤨. mixedup::extract_fixef(brms_c5_model_milk_house) #&gt; # A tibble: 7 x 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 cladeApe -0.431 0.261 -0.932 0.082 #&gt; 2 cladeNewWorldMonkey 0.326 0.253 -0.173 0.824 #&gt; 3 cladeOldWorldMonkey 0.497 0.286 -0.075 1.04 #&gt; 4 cladeStrepsirrhine -0.504 0.294 -1.04 0.088 #&gt; 5 houseHufflepuff -0.175 0.285 -0.742 0.378 #&gt; 6 houseRavenclaw -0.129 0.278 -0.667 0.413 #&gt; 7 houseSlytherin 0.489 0.293 -0.109 1.04 precis(model_milk_house, depth = 2) %&gt;% as.matrix() %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha_clade[1] -0.4205362 0.2603510 -0.8366273 -0.0044451 alpha_clade[2] 0.3836736 0.2596808 -0.0313464 0.7986937 alpha_clade[3] 0.5664463 0.2890333 0.1045153 1.0283773 alpha_clade[4] -0.5055652 0.2966455 -0.9796621 -0.0314684 alpha_house[1] -0.1025635 0.2617090 -0.5208251 0.3156981 alpha_house[2] -0.1996998 0.2754408 -0.6399074 0.2405079 alpha_house[3] -0.1603306 0.2690551 -0.5903326 0.2696713 alpha_house[4] 0.4866255 0.2875133 0.0271236 0.9461274 sigma 0.6631322 0.0881257 0.5222904 0.8039741 But there is no overall intercept, α, that stands for the expected value when all the predictors are set to 0. When we use the typical formula syntax with brms, we can suppress the overall intercept when for a single index variable with the &lt;criterion&gt; ~ 0 + &lt;index variable&gt; syntax. That’s exactly what we did with our b5.9 model. The catch is this approach only works with one index variable within brms. Even though we suppressed the default intercept with our formula, kcal_std ~ 0 + clade + house, we ended up loosing the first category of the second variable, house. […] The solution is the use the non-linear syntax. brms_c5_model_milk_house_correct_index &lt;- brm(data = data_milk_clade, family = gaussian, bf(kcal_std ~ 0 + a + h, a ~ 0 + clade, h ~ 0 + house, nl = TRUE), prior = c(prior(normal(0, 0.5), nlpar = a), prior(normal(0, 0.5), nlpar = h), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_milk_house_correct_index&quot;) mixedup::extract_fixef(brms_c5_model_milk_house_correct_index) #&gt; # A tibble: 8 x 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 a_cladeApe -0.395 0.28 -0.936 0.146 #&gt; 2 a_cladeNewWorldMonkey 0.363 0.28 -0.183 0.902 #&gt; 3 a_cladeOldWorldMonkey 0.527 0.307 -0.112 1.11 #&gt; 4 a_cladeStrepsirrhine -0.455 0.321 -1.10 0.167 #&gt; 5 h_houseGryffindor -0.097 0.284 -0.658 0.445 #&gt; 6 h_houseHufflepuff -0.196 0.298 -0.771 0.396 #&gt; 7 h_houseRavenclaw -0.159 0.285 -0.715 0.39 #&gt; 8 h_houseSlytherin 0.468 0.31 -0.138 1.07 as_draws_df(brms_c5_model_milk_house_correct_index) %&gt;% pivot_longer(starts_with(&quot;b_&quot;)) %&gt;% mutate(name = str_remove(name, &quot;b_&quot;) %&gt;% str_remove(., &quot;clade&quot;) %&gt;% str_remove(., &quot;house&quot;) %&gt;% str_replace(., &quot;World&quot;, &quot; World &quot;)) %&gt;% separate(name, into = c(&quot;predictor&quot;, &quot;level&quot;), sep = &quot;_&quot;) %&gt;% mutate(predictor = if_else(predictor == &quot;a&quot;, &quot;clade&quot;, &quot;house&quot;)) %&gt;% ggplot(aes(x = value, y = reorder(level, value))) + # note how we used `reorder()` to arrange the coefficients geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_pointinterval(point_interval = mode_hdi, .width = .89, size = 2, color = clr0d, fill = clr0, shape = 21 ) + labs(x = &quot;expected_kcal_std&quot;, y = NULL) + facet_wrap(~ predictor, scales = &quot;free_y&quot;) 6.6.8 Alternative ways to model multiple categories 6.6.8.1 Contrast Coding data_contrast &lt;- data_height %&gt;% mutate(sex_c = if_else(sex == &quot;1&quot;, -0.5, 0.5)) brms_c5_model_height_contrast &lt;- brm( data = data_contrast, family = gaussian, height ~ 1 + sex_c, prior = c(prior(normal(178, 20), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_height_contrast&quot;) Our posterior for \\(\\alpha\\), above, is designed to capture the average_of_the_group_means_in_height, not mean_height. In cases where the sample sizes in the two groups were equal, these two would be same. Since we have different numbers of males and females in our data, the two values differ a bit as_draws_df(brms_c5_model_height_contrast) %&gt;% mutate(male = b_Intercept - b_sex_c * 0.5, female = b_Intercept + b_sex_c * 0.5, `female - male` = b_sex_c) %&gt;% pivot_longer(male:`female - male`) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(.width = .95, shape = 21, fill = fll0, color = clr0d, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;height&quot;) + facet_wrap(~ name, scales = &quot;free&quot;) 6.6.9 Multilevel ANOVA (This might make sense after reading Chapter 13…) \\[ \\begin{array}{ccccr} K_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; &amp;\\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + u_{j[i]} &amp; &amp;\\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.5) &amp; &amp;\\textrm{[$\\alpha$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; &amp;\\textrm{[$\\sigma$ prior]} \\\\ u_{j[i]} &amp; \\sim &amp; Normal(0, \\sigma_{CLADE}) &amp; \\textrm{for}~j = 1..4 &amp;\\textrm{[u prior]}\\\\ \\sigma_{CLADE} &amp; \\sim &amp; Exponential(1) &amp; &amp;\\textrm{[$\\sigma_{CLADE}$ prior]} \\\\ \\end{array} \\] the four clade-specific deviations from that mean are captured by the four levels of \\(u_j\\), which are themselves modeled as normally distributed with a mean of zero (because they are deviations, after all) and a standard deviation \\(\\sigma_{CLADE}\\) brms_c5_model_milk_anova &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 1 + (1 | clade), prior = c(prior(normal(0, 0.5), class = Intercept), prior(exponential(1), class = sigma), prior(exponential(1), class = sd)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;brms/brms_c5_model_milk_anova&quot;) as_draws_df(brms_c5_model_milk_anova) %&gt;% mutate(Ape = b_Intercept + `r_clade[Ape,Intercept]`, `New World Monkey` = b_Intercept + `r_clade[New.World.Monkey,Intercept]`, `Old World Monkey` = b_Intercept + `r_clade[Old.World.Monkey,Intercept]`, Strepsirrhine = b_Intercept + `r_clade[Strepsirrhine,Intercept]`) %&gt;% pivot_longer(Ape:Strepsirrhine) %&gt;% ggplot(aes(x = value, y = reorder(name, value))) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_pointinterval(point_interval = mode_hdi, .width = .89, size = 2, color = clr0d, fill = clr0, shape = 21 ) + labs(x = &quot;expected_kcal_std&quot;, y = NULL) 6.7 pymc3 section × "],
["rethinking-chapter-6.html", "7 Rethinking: Chapter 6 7.1 Multicolliniarity 7.2 Post-treatment bias 7.3 Collider Bias 7.4 Homework 7.5 {brms} section 7.6 pymc3 section", " 7 Rethinking: Chapter 6 The Haunted DAG &amp; Causal Terror by Richard McElreath, building on the Summaries by Solomon Kurz and Jake Thompson. Simulating section-distortion (Berkson’s paradox) n &lt;- 200 p &lt;- .1 set.seed(42) data_sim &lt;- tibble(newsworthy = rnorm(n), trustworthy = rnorm(n), score = newsworthy + trustworthy, treshold = quantile(score, 1 - p), selected = score &gt;= treshold) data_sim %&gt;% ggplot(aes(x = newsworthy, y = trustworthy, color = selected)) + geom_smooth(data = data_sim %&gt;% filter(selected), method = &quot;lm&quot;, se = FALSE, fullrange = TRUE, size = .5) + geom_point(aes(fill = after_scale(clr_alpha(color))), shape = 21, size = 2.5) + scale_color_manual(values = c(`TRUE` = clr1, `FALSE` = clr0d))+ coord_cartesian(xlim = range(data_sim$newsworthy) * 1.05, ylim = range(data_sim$trustworthy) * 1.05, expand = 0) + coord_equal(ylim = range(data_sim$trustworthy) * 1.05) + theme(legend.position = &quot;bottom&quot;) 7.1 Multicolliniarity Simulating multicollinear legs library(rethinking) n &lt;- 100 set.seed(909) data_legs &lt;- tibble( height = rnorm(n = n, mean = 10, sd = 2), leg_proportion = runif(n, min = 0.4, max = 0.5), left_leg = leg_proportion * height + rnorm(n, 0, .02), right_leg = leg_proportion * height + rnorm(n, 0, .02), ) model_legs_multicollinear &lt;- quap( flist = alist( height ~ dnorm(mu, sigma), mu &lt;- alpha + beta_left * left_leg + beta_right * right_leg, alpha ~ dnorm(10, 100), beta_left ~ dnorm(2, 10), beta_right ~ dnorm(2, 10), sigma ~ dexp(1) ), data = data_legs ) precis(model_legs_multicollinear) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.98 0.28 0.53 1.44 beta_left 0.21 2.53 -3.83 4.25 beta_right 1.78 2.53 -2.26 5.83 sigma 0.62 0.04 0.55 0.69 precis(model_legs_multicollinear, depth = 2) %&gt;% as_tibble_rn() %&gt;% ggplot(aes(y = param)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(aes(xmin = `5.5%`, xmax =`94.5%`), color = clr0d) + geom_point(aes(x = mean), shape = 21, size = 3 , color = clr0d, fill = clr0) + scale_y_discrete(limits = c(&quot;sigma&quot;, &quot;beta_right&quot;, &quot;beta_left&quot;, &quot;alpha&quot;)) + theme(axis.title.y = element_blank()) leg_posterior_samples &lt;- extract.samples(model_legs_multicollinear) %&gt;% as_tibble() p_cor &lt;- leg_posterior_samples %&gt;% ggplot(aes(x = beta_right, y = beta_left)) + geom_point(color = clr0d, fill = clr0, shape = 21, alpha = .5) p_sum &lt;- leg_posterior_samples %&gt;% ggplot(aes(x = beta_right + beta_left)) + geom_vline(xintercept = 1/mean(data_legs$leg_proportion), color = clr_dark, linetype = 3) + geom_density(color = clr0d, fill = clr0, alpha = .5, adjust = .4) p_cor + p_sum Milk example data(milk) data_milk &lt;- milk %&gt;% as_tibble() %&gt;% drop_na(kcal.per.g:perc.lactose) %&gt;% mutate(across(where(is.double), standardize, .names = &quot;{str_remove(str_remove(.col,&#39;perc.&#39;),&#39;.per.g&#39;)}_std&quot;)) Model fat only model_milk_fat &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_fat * fat_std, alpha ~ dnorm(0,.2), beta_fat ~ dnorm(0,.5), sigma ~ dexp(1) ), data = data_milk ) precis(model_milk_fat) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.08 -0.12 0.12 beta_fat 0.86 0.08 0.73 1.00 sigma 0.45 0.06 0.36 0.54 Model lactose only model_milk_lactose &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_lactose * lactose_std, alpha ~ dnorm(0,.2), beta_lactose ~ dnorm(0,.5), sigma ~ dexp(1) ), data = data_milk ) precis(model_milk_lactose) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.07 -0.11 0.11 beta_lactose -0.90 0.07 -1.02 -0.79 sigma 0.38 0.05 0.30 0.46 Multicollinear model model_milk_multicollinear &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_fat * fat_std + beta_lactose * lactose_std, alpha ~ dnorm(0,.2), beta_fat ~ dnorm(0,.5), beta_lactose ~ dnorm(0,.5), sigma ~ dexp(1) ), data = data_milk ) precis(model_milk_multicollinear) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.07 -0.11 0.11 beta_fat 0.24 0.18 -0.05 0.54 beta_lactose -0.68 0.18 -0.97 -0.38 sigma 0.38 0.05 0.30 0.46 data_milk %&gt;% dplyr::select(kcal_std, fat_std, lactose_std) %&gt;% ggpairs( lower = list(continuous = wrap(ggally_points, colour = clr0d, size = 1.5, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll0, color = clr0d, adjust = .9)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;)) ) dagify(K ~ L + F, L ~ D, F ~ D, coords = tibble(name = c(&quot;K&quot;, &quot;L&quot;, &quot;F&quot;, &quot;D&quot;), x = c(.5, 0, 1, .5), y = c(.6, 1, 1, 1))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;K&quot;, &quot;response&quot;, if_else(name %in% c(&quot;L&quot;, &quot;F&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(.55, 1.05)) + scale_x_continuous(limits = c(-.05, 1.05)) + coord_equal() Simulating Multicollinearity simluate_collinearity &lt;- function(seed = 42, r = .9, data = data_milk ){ data &lt;- data %&gt;% mutate(x = rnorm(n = nrow(cur_data()), mean = `perc.fat` * r, sd = sqrt((1 - r ^ 2) * var(`perc.fat`)))) mod &lt;- lm(kcal.per.g ~ perc.fat + x, data = data) sqrt( diag( vcov(mod) ))[2] } # reapeat_simulation &lt;- function(r = .9, n = 100){ # stddev &lt;- replicate( n, simluate_collinearity(r)) # tibble(r = r, stddev_mean = mean(stddev), stddev_sd = sd(stddev)) # } n_seed &lt;- 100 n_rho &lt;- 60 simulation_means &lt;- crossing(seed = 1:n_seed, rho = seq(from = 0, to = .99, length.out = n_rho)) %&gt;% mutate(parameter_sd = purrr::map2_dbl(seed, rho, simluate_collinearity)) %&gt;% group_by(rho) %&gt;% summarise(mean = mean(parameter_sd), ll = quantile(parameter_sd, prob = .025), ul = quantile(parameter_sd, prob = .975)) simulation_means %&gt;% ggplot(aes(x = rho, y = mean, ymin = ll, ymax = ul)) + geom_smooth(stat = &#39;identity&#39;, size = .6, color = clr0d, fill = fll0) 7.2 Post-treatment bias Simulating fungus data n &lt;- 100 set.seed(71) data_fungus &lt;- tibble( h_0 = rnorm(n, 10, 2), treatment = rep(0:1, each = n/2), fungus = rbinom(n = n, size = 1, prob = .5 - treatment * .4 ), h_1 = h_0 + rnorm(n, 5 - 3 * fungus) ) precis(data_fungus) %&gt;% knit_precis() param mean sd 5.5% 94.5% histogram h_0 9.96 2.10 6.57 13.08 ▁▂▂▂▇▃▂▃▁▁▁▁ treatment 0.50 0.50 0.00 1.00 ▇▁▁▁▁▁▁▁▁▇ fungus 0.23 0.42 0.00 1.00 ▇▁▁▁▁▁▁▁▁▂ h_1 14.40 2.69 10.62 17.93 ▁▁▃▇▇▇▁▁ selecting a prior precis(tibble(sim_p = rlnorm(1e4, 0, .25))) %&gt;% knit_precis() param mean sd 5.5% 94.5% histogram sim_p 1.04 0.26 0.67 1.5 ▁▁▃▇▇▃▁▁▁▁▁▁ \\[ \\begin{array}{rclr} h_{1,i} &amp; \\sim &amp; Normal( \\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; h_{0,i} \\times p &amp; \\textrm{[linear model]}\\\\ p &amp; \\sim &amp; Log-Normal(0, 0.25) &amp; \\textrm{[$p$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] \\(\\rightarrow\\) the main mass of the prior is between 40% shrinkage and 50% growth. Model without treatment model_fungus_no_treatment &lt;- quap( flist = alist( h_1 ~ dnorm( mu, sigma ), mu &lt;- h_0 * p, p ~ dlnorm( 0, .25 ), sigma ~ dexp( 1 ) ), data = data_fungus ) precis(model_fungus_no_treatment) %&gt;% knit_precis() param mean sd 5.5% 94.5% p 1.43 0.02 1.40 1.45 sigma 1.79 0.13 1.59 1.99 Model with treatment and fungus (post-treatment variable) \\[ \\begin{array}{rclr} h_{1,i} &amp; \\sim &amp; Normal( \\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; h_{0,i} \\times p &amp; \\textrm{[linear model]}\\\\ p &amp; = &amp; \\alpha + \\beta_{T} T_{i} + \\beta_{F} F_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Log-Normal(0, 0.25) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{T} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_{T}$ prior]}\\\\ \\beta_{F} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_{F}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_fungus_post_treatment &lt;- quap( flist = alist( h_1 ~ dnorm( mu, sigma ), mu &lt;- h_0 * p, p &lt;- alpha + beta_treatment * treatment + beta_fungus * fungus, alpha ~ dlnorm( 0, .2 ), beta_treatment ~ dnorm(0,.5), beta_fungus ~ dnorm(0,.5), sigma ~ dexp( 1 ) ), data = data_fungus ) precis(model_fungus_post_treatment) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 1.48 0.02 1.44 1.52 beta_treatment 0.00 0.03 -0.05 0.05 beta_fungus -0.27 0.04 -0.33 -0.21 sigma 1.41 0.10 1.25 1.57 Model with treatment but without fungus \\[ \\begin{array}{rclr} h_{1,i} &amp; \\sim &amp; Normal( \\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; h_{0,i} \\times p &amp; \\textrm{[linear model]}\\\\ p &amp; = &amp; \\alpha + \\beta_{T} T_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Log-Normal(0, 0.25) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{T} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_{T}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_fungus_only_treatment &lt;- quap( flist = alist( h_1 ~ dnorm( mu, sigma ), mu &lt;- h_0 * p, p &lt;- a + beta_treatment * treatment, a ~ dlnorm( 0, .25 ), beta_treatment ~ dnorm(0,.5), sigma ~ dexp( 1 ) ), data = data_fungus ) precis(model_fungus_only_treatment) %&gt;% knit_precis() param mean sd 5.5% 94.5% a 1.38 0.03 1.34 1.42 beta_treatment 0.08 0.03 0.03 0.14 sigma 1.75 0.12 1.55 1.94 d-separation dagify(&quot;H_1&quot; ~ H_0 + F, F ~ T, coords = tibble(name = c(&quot;H_0&quot;, &quot;H_1&quot;, &quot;F&quot;, &quot;T&quot;), x = c(0, .5, .75, 1), y = c(0, 0, 0, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;H_1&quot;, &quot;response&quot;, if_else(name %in% c(&quot;H_0&quot;, &quot;F&quot;, &quot;T&quot;), &quot;predictor&quot;, &quot;confounds&quot;)), name = str_replace(name, &quot;([A-Z])_([0-9])&quot;, &quot;\\\\1\\\\[\\\\2\\\\]&quot;)) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(-.05, .05)) + scale_x_continuous(limits = c(-.05, 1.05)) + coord_equal() Directional Separation of H_1 and T occurs after conditioning on F: library(dagitty) impliedConditionalIndependencies(&quot;dag{ H_0 -&gt; H_1 &lt;- F &lt;- T}&quot;) #&gt; F _||_ H_0 #&gt; H_0 _||_ T #&gt; H_1 _||_ T | F dagify(H_1 ~ H_0 + M, F ~ T, F ~ M, coords = tibble(name = c(&quot;H_0&quot;, &quot;H_1&quot;, &quot;M&quot; , &quot;F&quot;, &quot;T&quot;), x = c(0, .5, .75, 1, 1.5), y = c(1, 1, .7, 1, 1))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;H_1&quot;, &quot;response&quot;, if_else(name %in% c(&quot;H_0&quot;, &quot;F&quot;, &quot;T&quot;), &quot;predictor&quot;, &quot;confounds&quot;)), name = str_replace(name, &quot;([A-Z])_([0-9])&quot;, &quot;\\\\1\\\\[\\\\2\\\\]&quot;)) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(.65, 1.05)) + scale_x_continuous(limits = c(-.05, 1.55)) + coord_equal() n &lt;- 1e4 set.seed(71) data_moisture &lt;- tibble( h_0 = rnorm(n, 10, 2), treatment = rep(0:1, each = n/2), moisture = rbern(n), fungus = rbinom(n = n, size = 1, prob = .5 - treatment * .4 + moisture * .4), h_1 = h_0 + rnorm(n, 5 + 3 * moisture) ) precis(data_moisture) %&gt;% knit_precis() param mean sd 5.5% 94.5% histogram h_0 10.04 2.01 6.81 13.22 ▁▁▁▂▇▇▂▁▁ treatment 0.50 0.50 0.00 1.00 ▇▁▁▁▁▁▁▁▁▇ moisture 0.50 0.50 0.00 1.00 ▇▁▁▁▁▁▁▁▁▇ fungus 0.50 0.50 0.00 1.00 ▇▁▁▁▁▁▁▁▁▇ h_1 16.55 2.69 12.22 20.84 ▁▁▁▃▇▇▅▂▁▁ Moisture-Model with treatment and fungus (post-treatment variable) model_moisture_post_treatment &lt;- quap( flist = alist( h_1 ~ dnorm( mu, sigma ), mu &lt;- h_0 * p, p &lt;- alpha + beta_treatment * treatment + beta_fungus * fungus, alpha ~ dlnorm( 0, .2 ), beta_treatment ~ dnorm(0,.5), beta_fungus ~ dnorm(0,.5), sigma ~ dexp( 1 ) ), data = data_moisture ) precis(model_moisture_post_treatment) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 1.53 0.00 1.52 1.54 beta_treatment 0.05 0.00 0.05 0.06 beta_fungus 0.13 0.00 0.12 0.14 sigma 2.13 0.02 2.11 2.16 Moisture-Model with treatment but without fungus model_moisture_only_treatment &lt;- quap( flist = alist( h_1 ~ dnorm( mu, sigma ), mu &lt;- h_0 * p, p &lt;- a + beta_treatment * treatment, a ~ dlnorm( 0, .25 ), beta_treatment ~ dnorm(0,.5), sigma ~ dexp( 1 ) ), data = data_moisture ) precis(model_moisture_only_treatment) %&gt;% knit_precis() param mean sd 5.5% 94.5% a 1.62 0.00 1.62 1.63 beta_treatment 0.00 0.00 0.00 0.01 sigma 2.22 0.02 2.19 2.25 7.3 Collider Bias # data_happy &lt;- sim_happiness(seed = 1977, N_years = 66) %&gt;% # as_tibble() progress_year &lt;- function(data, year, max_age = 65, n_births = 20, aom = 18){ new_cohort &lt;- tibble( age = 1, married = as.integer(0), happiness = seq(from = -2, to = 2, length.out = n_births), year_of_birth = year) data %&gt;% mutate(age = age + 1) %&gt;% bind_rows(., new_cohort) %&gt;% mutate(married = if_else(age &gt;= aom &amp; married == 0, rbern(n(), inv_logit(happiness - 4)), married )) %&gt;% filter(age &lt;= max_age) } sim_tidy &lt;- function(seed = 1977, n_years = 1000, max_age = 65, n_births = 20, aom = 18){ set.seed(seed) empty_tibble &lt;- tibble(age = double(), married = integer(), happiness = double()) 1:n_years %&gt;% reduce(.f = progress_year, .init = empty_tibble, max_age = max_age, n_births = n_births, aom = aom) } data_married &lt;- sim_tidy(seed = 2021, n_years = 65, n_births = 21) data_married %&gt;% mutate(married = factor(married, labels = c(&quot;unmarried&quot;, &quot;married&quot;))) %&gt;% ggplot(aes(x = age, y = happiness, color = married)) + geom_point(size = 1.75, shape = 21, aes(fill = after_scale(clr_alpha(color)))) + scale_color_manual(NULL, values = c(married = clr2, unmarried = clr0d)) + scale_x_continuous(expand = c(.015, .015)) + theme(panel.grid = element_blank(), legend.position = &quot;bottom&quot;) \\[ \\begin{array}{rclr} happyness_{i} &amp; \\sim &amp; Normal( \\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha_{married[i]} + \\beta_{age} age_{i} &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] data_married_adults &lt;- data_married %&gt;% filter(age &gt;= 18) %&gt;% mutate(age_trans = (age - 18)/ diff(c(18, 65)), married_idx = married + 1L) model_happy_married &lt;- quap( flist = alist( happiness ~ dnorm(mu, sigma), mu &lt;- alpha[married_idx] + beta_age * age_trans, alpha[married_idx] ~ dnorm( 0, 1 ), beta_age ~ dnorm( 0, 2 ), sigma ~ dexp( 1 ) ), data = data_married_adults ) precis(model_happy_married, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha[1] -0.21 0.06 -0.31 -0.11 alpha[2] 1.33 0.08 1.20 1.47 beta_age -0.81 0.11 -0.99 -0.64 sigma 0.97 0.02 0.94 1.01 model_happy &lt;- quap( flist = alist( happiness ~ dnorm(mu, sigma), mu &lt;- alpha + beta_age * age_trans, alpha ~ dnorm( 0, 1 ), beta_age ~ dnorm( 0, 2 ), sigma ~ dexp( 1 ) ), data = data_married_adults ) precis(model_happy, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.00 0.07 -0.12 0.12 beta_age 0.00 0.13 -0.21 0.21 sigma 1.21 0.03 1.17 1.25 7.3.1 The haunted DAG Education example (including Grandparents, Parents, Children and the unobserved Neighborhood) p_dag1 &lt;- dagify(C ~ P + G, P ~ G, coords = tibble(name = c(&quot;G&quot;, &quot;P&quot;, &quot;C&quot;), x = c(0, 1.5, 1.5), y = c(1, 1, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;C&quot;, &quot;response&quot;, if_else(name %in% c(&quot;P&quot;, &quot;G&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(-.05, 1.05)) + scale_x_continuous(limits = c(-.05, 1.55)) + coord_equal() p_dag2 &lt;- dagify(C ~ P + G + U, P ~ G + U, coords = tibble(name = c(&quot;G&quot;, &quot;P&quot;, &quot;C&quot;, &quot;U&quot;), x = c(0, 1.5, 1.5, 2), y = c(1, 1, 0, .5))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;C&quot;, &quot;response&quot;, if_else(name %in% c(&quot;P&quot;, &quot;G&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(-.05, 1.05)) + scale_x_continuous(limits = c(-.05, 2.05)) + coord_equal() p_dag1 + p_dag2 + plot_annotation(tag_levels = &quot;a&quot;) &amp; theme(plot.tag = element_text(family = fnt_sel)) n &lt;- 200 beta_gp &lt;- 1 # direct effect of G -&gt; P beta_gc &lt;- 0 # direct effect of G -&gt; C beta_pc &lt;- 1 # direct effect of P -&gt; C beta_U &lt;- 2 # direct effect of U on both C and P set.seed(1) data_education &lt;- tibble( unobserved = 2 * rbern(n, .5) - 1, grandparents = rnorm( n ), parents = rnorm( n, beta_gp * grandparents + beta_U * unobserved), children = rnorm( n, beta_gc * grandparents + beta_pc * parents + beta_U * unobserved) ) model_education &lt;- quap( flist = alist( children ~ dnorm(mu, sigma), mu &lt;- alpha + beta_pc * parents + beta_gc * grandparents, alpha ~ dnorm( 0, 1 ), c( beta_pc, beta_gc) ~ dnorm( 0, 1 ), sigma ~ dexp(1) ), data = data_education ) precis(model_education) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha -0.12 0.10 -0.28 0.04 beta_pc 1.79 0.04 1.72 1.86 beta_gc -0.84 0.11 -1.01 -0.67 sigma 1.41 0.07 1.30 1.52 data_education_plot &lt;- data_education %&gt;% mutate(across(grandparents:children, standardize, .names = &quot;{.col}_std&quot;)) %&gt;% mutate(parents_inner = between(parents_std, left = quantile(parents_std, probs = .45), right = quantile(parents_std, probs = .60))) data_education_plot %&gt;% ggplot(aes(x = grandparents_std, y = children_std)) + geom_smooth(data = data_education_plot %&gt;% filter(parents_inner), method = &quot;lm&quot;, se = FALSE, size = .5, color = clr_dark, fullrange = TRUE) + geom_point(aes(color = factor(unobserved), fill = after_scale(clr_alpha(color,.8)), shape = parents_inner), size = 2.5) + scale_color_manual(values = c(clr0d, clr2), guide = &quot;none&quot;) + scale_shape_manual(values = c(`FALSE` = 1, `TRUE` = 21), guide = &quot;none&quot;) + coord_equal() model_education_resolved &lt;- quap( flist = alist( children ~ dnorm(mu, sigma), mu &lt;- alpha + beta_pc * parents + beta_gc * grandparents + beta_u * unobserved, alpha ~ dnorm( 0, 1 ), c( beta_pc, beta_gc, beta_u ) ~ dnorm( 0, 1 ), sigma ~ dexp(1) ), data = data_education ) precis(model_education_resolved) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha -0.12 0.07 -0.24 -0.01 beta_pc 1.01 0.07 0.91 1.12 beta_gc -0.04 0.10 -0.20 0.11 beta_u 2.00 0.15 1.76 2.23 sigma 1.02 0.05 0.94 1.10 7.3.2 Shutting the Backdoor The four elements that construct DAGs: p_dag1 &lt;- dagify( X ~ Z, Y ~ Z, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), x = c(0, 1, .5), y = c(1, 1, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + ggtitle(&quot;Fork&quot;) p_dag2 &lt;- dagify( Z ~ X, Y ~ Z, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), x = c(0, 1, .5), y = c(1, 0, .5))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + ggtitle(&quot;Pipe&quot;) p_dag3 &lt;- dagify( Z ~ X + Y, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), x = c(0, 1, .5), y = c(0, 0 , 1))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + ggtitle(&quot;Collider&quot;) p_dag4 &lt;- dagify( Z ~ X + Y, D ~ Z, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, &quot;D&quot;), x = c(0, 1, .5, .5), y = c(0, 0 , 1, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, &quot;D&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + ggtitle(&quot;Collider&quot;) p_dag1 + p_dag2 + p_dag3 + p_dag4 + plot_annotation(tag_levels = &quot;a&quot;) + plot_layout(nrow = 1) &amp; coord_fixed(ratio = .6) &amp; scale_y_continuous(limits = c(-.1, 1.1)) &amp; scale_x_continuous(limits = c(-.1, 1.1)) &amp; theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) a. Fork: \\(X \\perp \\!\\!\\! \\perp Y | Z\\) b. Pipe: \\(X \\perp \\!\\!\\! \\perp Y | Z\\) c. Collider: \\(X \\perp \\!\\!\\! \\perp Y\\), but \\(X \\not\\!\\perp\\!\\!\\!\\perp Y | Z\\) d. Descendant: \\(X \\perp \\!\\!\\! \\perp Y\\), but \\(X \\not\\!\\perp\\!\\!\\!\\perp Y | Z\\) (to a lesser extent) 7.3.3 Two Roads dag_roads &lt;- dagify( U ~ A, X ~ U, Y ~ X + C, C ~ A, B ~ U + C, exposure = &quot;X&quot;, outcome = &quot;Y&quot;, coords = tibble(name = c(&quot;U&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;X&quot;, &quot;Y&quot;), x = c(0, .5, .5, 1, 0, 1), y = c(.7, 1, .4 , .7, 0, 0))) dag_roads %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;Y&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + coord_fixed(ratio = .6) &amp; scale_y_continuous(limits = c(-.1, 1.1)) &amp; scale_x_continuous(limits = c(-.1, 1.1)) &amp; theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) adjustmentSets(dag_roads,exposure = &quot;X&quot;, outcome = &quot;Y&quot;) #&gt; { C } #&gt; { A } #&gt; { U } dag_roads &lt;- dagitty( &quot;dag { U [unobserved] X -&gt; Y X &lt;- U &lt;- A -&gt; C -&gt; Y U -&gt; B &lt;- C }&quot; ) adjustmentSets(dag_roads,exposure = &quot;X&quot;, outcome = &quot;Y&quot;) #&gt; { C } #&gt; { A } 7.3.4 Backdoor Waffles dag_waffles &lt;- dagify( D ~ A + M + W, A ~ S, M ~ A + S, W ~ S, exposure = &quot;W&quot;, outcome = &quot;D&quot;, coords = tibble(name = c(&quot;S&quot;, &quot;A&quot;, &quot;M&quot;, &quot;W&quot;, &quot;D&quot;), x = c(0, 0, .5, 1, 1), y = c(1, 0, .5 , 1, 0))) dag_waffles %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;W&quot;, &quot;A&quot;, &quot;M&quot;, &quot;S&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + coord_fixed(ratio = .6) &amp; scale_y_continuous(limits = c(-.1, 1.1)) &amp; scale_x_continuous(limits = c(-.1, 1.1)) &amp; theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) adjustmentSets(dag_waffles) #&gt; { A, M } #&gt; { S } Test the implications of the DAG by investigating the conditional independencies implied by the DAG in the real data (by conditioning on the respective variables): impliedConditionalIndependencies(dag_waffles) #&gt; A _||_ W | S #&gt; D _||_ S | A, M, W #&gt; M _||_ W | S Exporting models and data for re-use library(rlang) chapter6_models &lt;- env( data_legs = data_legs, model_legs_multicollinear = model_legs_multicollinear, data_milk = data_milk, model_milk_fat = model_milk_fat, model_milk_lactose = model_milk_lactose, model_milk_multicollinear = model_milk_multicollinear, data_fungus = data_fungus, model_fungus_no_treatment = model_fungus_no_treatment, model_fungus_post_treatment = model_fungus_post_treatment, model_fungus_only_treatment = model_fungus_only_treatment, data_moisture = data_moisture, model_moisture_post_treatment = model_moisture_post_treatment, model_moisture_only_treatment = model_moisture_only_treatment, data_married_adults = data_married_adults, model_happy_married = model_happy_married, model_happy = model_happy, data_education = data_education, model_education = model_education, model_education_resolved = model_education_resolved ) write_rds(chapter6_models, &quot;envs/chapter6_models.rds&quot;) 7.4 Homework E1 multicollinearity (including two highly correlated predictors, so that a ridge of explanatory combinations prevents the precise estimate of both) post-treatment bias (masking an association by assuming all efects are caused by an intermediate descendant) collider bias (introducing an association as a selection effect) E2 Fruit mass as a function of FF and crown area FF \\(\\rightarrow\\) Fruit mass \\(\\leftarrow\\) crown area E3 In all cases, we wonder about the influence of \\(X\\) on \\(Y\\). The elemental confounds influence how conditioning on \\(Z\\) (or \\(D\\)) effects our inference. The Fork (\\(X \\leftarrow Z \\rightarrow Y\\)): \\(X \\perp \\!\\!\\! \\perp Y | Z\\), but \\(X \\not\\!\\perp\\!\\!\\!\\perp Y\\) The Pipe (\\(X \\rightarrow Z \\rightarrow Y\\)): \\(X \\perp \\!\\!\\! \\perp Y | Z\\), but \\(X \\not\\!\\perp\\!\\!\\!\\perp Y\\) The Collider (\\(X \\rightarrow Z \\leftarrow Y\\)): \\(X \\perp \\!\\!\\! \\perp Y\\), but \\(X \\not\\!\\perp\\!\\!\\!\\perp Y | Z\\) The Descendant (\\(X \\rightarrow Z \\leftarrow Y; Z \\rightarrow D\\), \\(X \\rightarrow Z \\rightarrow Y; Z \\rightarrow D\\)): conditioning on \\(D\\) has the same (slightly weaker) effect like conditioning on \\(Z\\) E4 As bias sample acts like selection for a specific value of a trait (eg. an article was selected for publication). This is implicitly conditioning on a third variable (also like eg. the 45-60 percentile of parents). M1 dag_roads_v &lt;- dagify( U ~ A, X ~ U, Y ~ X + C + V, C ~ A + V, B ~ U + C, exposure = &quot;X&quot;, outcome = &quot;Y&quot;, coords = tibble(name = c(&quot;U&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;X&quot;, &quot;Y&quot;, &quot;V&quot;), x = c(0, .5, .5, 1, 0, 1, 1.3), y = c(.7, 1, .4 , .7, 0, 0, .35))) dag_roads_v %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;Y&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + coord_fixed(ratio = .6) &amp; scale_y_continuous(limits = c(-.1, 1.4)) &amp; scale_x_continuous(limits = c(-.1, 1.4)) &amp; theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) adjustmentSets(dag_roads_v, exposure = &quot;X&quot;, outcome = &quot;Y&quot;) #&gt; { C, V } #&gt; { A } #&gt; { U } dag_roads_v &lt;- dagitty( &quot;dag { U [unobserved] V [unobserved] X -&gt; Y X &lt;- U &lt;- A -&gt; C -&gt; Y U -&gt; B &lt;- C Y &lt;- V -&gt; C }&quot; ) adjustmentSets(dag_roads_v, exposure = &quot;X&quot;, outcome = &quot;Y&quot;) #&gt; { A } … there are now \\(x + 1\\) paths. Condition on A, since conditioning on C would open the collider between A and V allowing for the flow of information through the backdoor V. M2 dagify( Z ~ X, Y ~ Z, exposure = &quot;X&quot;, outcome = &quot;Y&quot;, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), x = c(0, 1, .5), y = c(0, 0, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;Y&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;Z&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + coord_fixed(ratio = .6) &amp; scale_y_continuous(limits = c(-.1, .1)) &amp; scale_x_continuous(limits = c(-.1, 1.1)) &amp; theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) n &lt;- 1e4 data_sim_multicol &lt;- tibble(x = rnorm(n), z = rnorm(n, mean = x, sd = .05), y = rnorm(n, mean = z)) data_sim_multicol %&gt;% ggpairs( lower = list(continuous = wrap(ggally_points, colour = clr0d, size = 1.5, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll0, color = clr0d, adjust = .9)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;)) ) model_sim_multicol &lt;- quap( flist = alist( y ~ dnorm(mu, sigma), mu &lt;- alpha + beta_x * x + beta_z * z, alpha ~ dnorm( 0, .2 ), beta_x ~ dnorm( 0, .5), beta_z ~ dnorm( 0, .5), sigma ~ dexp(1) ), data = data_sim_multicol ) precis(model_sim_multicol) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha -0.01 0.01 -0.03 0.00 beta_x -0.10 0.17 -0.38 0.17 beta_z 1.12 0.17 0.84 1.39 sigma 1.00 0.01 0.98 1.01 precis(model_sim_multicol, depth = 2) %&gt;% as_tibble_rn() %&gt;% ggplot(aes(y = param)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(aes(xmin = `5.5%`, xmax =`94.5%`), color = clr0d) + geom_point(aes(x = mean), shape = 21, size = 3 , color = clr0d, fill = clr0) + scale_y_discrete(limits = c(&quot;sigma&quot;, &quot;beta_z&quot;, &quot;beta_x&quot;, &quot;alpha&quot;)) + theme(axis.title.y = element_blank()) Here the effect of \\(Z\\) on \\(Y\\) is not obscured by including \\(X\\). The difference is that here we are breaking a pipe by conditioning on \\(Z\\). M3 dag_h1 &lt;- dagify( X ~ Z, Y ~ X + Z + A, Z ~ A, exposure = &quot;X&quot;, outcome = &quot;Y&quot;, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, &quot;A&quot;), x = c(0, 1, .5, 1), y = c(0, 0, .7, .7))) dag_h2 &lt;- dagify( Z ~ X, Y ~ X + Z + A, Z ~ A, exposure = &quot;X&quot;, outcome = &quot;Y&quot;, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, &quot;A&quot;), x = c(0, 1, .5, 1), y = c(0, 0, .7, .7))) dag_h3 &lt;- dagify( X ~ A, Y ~ X, Z ~ X + Y + A, exposure = &quot;X&quot;, outcome = &quot;Y&quot;, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, &quot;A&quot;), x = c(0, 1, .5, 0), y = c(0, 0, .7, .7))) dag_h4 &lt;- dagify( X ~ A, Y ~ X + Z, Z ~ X + A, exposure = &quot;X&quot;, outcome = &quot;Y&quot;, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, &quot;A&quot;), x = c(0, 1, .5, 0), y = c(0, 0, .7, .7))) list(dag_h1, dag_h2, dag_h3, dag_h4) %&gt;% purrr::map(.f = function(dag){ dag %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;Y&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;Z&quot;, &quot;A&quot;), &quot;predictor&quot;, &quot;confounds&quot;)))}) %&gt;% purrr::map(plot_dag, clr_in = clr3) %&gt;% wrap_plots() + plot_annotation(tag_levels = &quot;a&quot;) &amp; coord_fixed(ratio = .6) &amp; scale_y_continuous(limits = c(-.1, .8)) &amp; scale_x_continuous(limits = c(-.1, 1.1)) &amp; theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) a. condition on \\(Z\\) to close both backdoor paths into \\(Y\\) b. none (the information flow through \\(Z\\) is causal and thus desired, and \\(A\\) is blocked by the collider in \\(Z\\)) c. none - there are no open backdoors into \\(Y\\) d. condition on \\(A\\) to enable the information from desired (causal) information from \\(Z\\) while removing the undesired information from \\(A\\) list(dag_h1, dag_h2, dag_h3, dag_h4) %&gt;% purrr::map(adjustmentSets) #&gt; [[1]] #&gt; { Z } #&gt; #&gt; [[2]] #&gt; {} #&gt; #&gt; [[3]] #&gt; {} #&gt; #&gt; [[4]] #&gt; { A } H1 data(WaffleDivorce) data_waffle &lt;- WaffleDivorce %&gt;% as_tibble() %&gt;% drop_na(everything()) %&gt;% mutate(across(where(is.double), standardize, .names = &quot;{str_to_lower(.col)}_std&quot;), waffle_std = standardize(WaffleHouses)) %&gt;% dplyr::select(Location,MedianAgeMarriage, Marriage, Divorce, WaffleHouses, South, medianagemarriage_std, marriage_std, divorce_std, waffle_std) dag_waffles &lt;- dagify(D ~ A + M + W, M ~ A + S, A ~ S, W ~ S, exposure = &quot;W&quot;, outcome = &quot;D&quot;, coords = tibble(name = c(&quot;S&quot;, &quot;A&quot;, &quot;M&quot;, &quot;D&quot;, &quot;W&quot;), x = c(0, 0, .5, 1, 1), y = c(1, 0, .5, 0, 1))) dag_waffles %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M&quot;, &quot;S&quot;, &quot;W&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(-.1, 1.1)) + coord_fixed(ratio = .6) adjustmentSets(dag_waffles) #&gt; { A, M } #&gt; { S } \\(\\rightarrow\\) there are four paths from \\(W\\) to \\(D\\) - the only causal one being \\(W \\rightarrow D\\). To close the other three, we can condition on \\(S\\) which will close two different forks, efectiffley removing all non-causaul backdoor paths. model_waffle &lt;- quap( flist = alist( divorce_std ~ dnorm(mu, sigma), mu &lt;- alpha[South] + beta_w * waffle_std, alpha[South] ~ dnorm(0, .5), beta_w ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_waffle ) precis(model_waffle, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha[1] 0.00 0.13 -0.21 0.21 alpha[2] 0.00 0.50 -0.80 0.80 beta_w 0.24 0.13 0.03 0.45 sigma 0.95 0.09 0.80 1.10 precis(model_waffle, depth = 2) %&gt;% as_tibble_rn() %&gt;% ggplot(aes(y = param)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(aes(xmin = `5.5%`, xmax =`94.5%`), color = clr0d) + geom_point(aes(x = mean), shape = 21, size = 3 , color = clr0d, fill = clr0) + scale_y_discrete(limits = c(&quot;sigma&quot;, &quot;beta_w&quot;, &quot;alpha[2]&quot;, &quot;alpha[1]&quot;)) + theme(axis.title.y = element_blank()) \\(\\rightarrow\\) The total causal influence of \\(W\\) on \\(D\\) should be in the order of 0.24 standard deviations. H2 impliedConditionalIndependencies(dag_waffles) #&gt; A _||_ W | S #&gt; D _||_ S | A, M, W #&gt; M _||_ W | S model_waffel_test1 &lt;- quap( flist = alist( medianagemarriage_std ~ dnorm(mu, sigma), mu &lt;- alpha[South] + beta_w * waffle_std, alpha[South] ~ dnorm(0, .2), beta_w ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_waffle ) precis(model_waffel_test1, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha[1] 0.00 0.11 -0.18 0.18 alpha[2] 0.00 0.20 -0.32 0.32 beta_w -0.11 0.13 -0.32 0.10 sigma 0.97 0.10 0.82 1.13 \\(\\rightarrow\\) the influence of \\(W\\) on \\(A\\) is moderate (-0.11), with a wide posterior distribution both on either side of zero (-0.32, 0.1). Based on this we can not find a definitive influence from \\(W\\) on \\(A\\), model_waffel_test2 &lt;- quap( flist = alist( divorce_std ~ dnorm(mu, sigma), mu &lt;- alpha[South] + beta_a * medianagemarriage_std + beta_m * marriage_std + beta_w * waffle_std, alpha[South] ~ dnorm(0, .2), c(beta_a, beta_m, beta_w) ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_waffle ) precis(model_waffel_test2, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha[1] 0.00 0.10 -0.15 0.15 alpha[2] 0.00 0.20 -0.32 0.32 beta_a -0.58 0.15 -0.82 -0.35 beta_m -0.05 0.15 -0.29 0.19 beta_w 0.18 0.11 0.01 0.35 sigma 0.77 0.08 0.64 0.89 waffel_test2_samples &lt;- extract.samples(model_waffel_test2) %&gt;% as_tibble() %&gt;% mutate(contrast = alpha[,1] - alpha[,2]) waffel_test2_samples_quantiles &lt;- tibble(x = quantile(waffel_test2_samples$contrast, prob = c(.055, .5, .945)), percentile = c(&quot;lower&quot;, &quot;median&quot;, &quot;upper&quot;)) %&gt;% pivot_wider(names_from = &quot;percentile&quot;, values_from = &quot;x&quot;) waffel_test2_samples %&gt;% ggplot(aes(x = contrast)) + geom_density(color = clr0d, fill = fll0) + geom_pointrange(data = waffel_test2_samples_quantiles, aes(xmin = lower, x = median, xmax = upper, y = 0), color = clr0d, fill = clr0, shape = 21) \\(\\rightarrow\\) the influence of \\(S\\) on \\(D\\) is moderate after conditioning on \\(A\\), \\(M\\) and \\(W\\) (median of the contrast effect: 0.0026) model_waffel_test3 &lt;- quap( flist = alist( marriage_std ~ dnorm(mu, sigma), mu &lt;- alpha[South] + beta_w * waffle_std, alpha[South] ~ dnorm(0, .2), beta_w ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_waffle ) precis(model_waffel_test3, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha[1] 0.00 0.11 -0.18 0.18 alpha[2] 0.00 0.20 -0.32 0.32 beta_w 0.03 0.13 -0.19 0.24 sigma 0.98 0.10 0.83 1.13 \\(\\rightarrow\\) the influence of \\(W\\) on \\(M\\) is moderate (0.03), with a wide posterior distribution both on either side of zero (-0.19, 0.24). Based on this we can not find a definitive influence from \\(W\\) on \\(M\\), H3 From Wikipedia: Weights range from 2.2–14 kg data(foxes) data_fox &lt;- foxes %&gt;% as_tibble() %&gt;% drop_na(everything()) %&gt;% mutate(across(-group, standardize, .names = &quot;{str_to_lower(.col)}_std&quot;)) fox_weight_range &lt;- tibble(weight = c(2.2, 14), weight_std = (weight - mean(data_fox$weight))/ sd(data_fox$weight)) dag_fox &lt;- dagify( W ~ F + G, G ~ F, F ~ A, exposure = &quot;A&quot;, outcome = &quot;W&quot;, coords = tibble(name = c(&quot;W&quot;, &quot;F&quot;, &quot;G&quot;, &quot;A&quot;), x = c(.5, 0, 1, .5), y = c(0, .5, .5, 1))) dag_fox %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;W&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;F&quot;, &quot;G&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr2) + scale_y_continuous(limits = c(-.1, 1.1)) + coord_fixed(ratio = .6) adjustmentSets(dag_fox) #&gt; {} model_fox_area &lt;- quap( flist = alist( weight_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_area * area_std, alpha ~ dnorm(0,.2), beta_area ~ dnorm(0, .25), sigma ~ dexp(1) ), data = data_fox ) extract.prior(model_fox_area) %&gt;% as_tibble() %&gt;% filter(row_number() &lt; 150) %&gt;% ggplot() + geom_abline(aes(slope = beta_area, intercept = alpha), color = clr_alpha(clr0d)) + geom_hline(data = fox_weight_range, aes(yintercept = weight_std), color = clr_dark, linetype = 3) + scale_x_continuous(limits = c(-3,3)) + scale_y_continuous(limits = c(-3,3)) + labs(x = &quot;area_std&quot;, y = &quot;weight_std&quot;) precis(model_fox_area) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.00 0.08 -0.13 0.13 beta_area 0.02 0.09 -0.12 0.16 sigma 0.99 0.06 0.89 1.09 The slope of area very close to zero with the posterior distribution heavy on either side 🤷 H4 adjustmentSets(dag_fox, exposure = &quot;F&quot;, outcome = &quot;W&quot;) #&gt; {} model_fox_food &lt;- quap( flist = alist( weight_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_food * avgfood_std, alpha ~ dnorm(0,.2), beta_food ~ dnorm(0, .25), sigma ~ dexp(1) ), data = data_fox ) precis(model_fox_food, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.00 0.08 -0.13 0.13 beta_food -0.02 0.09 -0.16 0.12 sigma 0.99 0.06 0.89 1.09 H5 adjustmentSets(dag_fox, exposure = &quot;G&quot;, outcome = &quot;W&quot;) #&gt; { F } … we need to condition on \\(F\\). model_fox_group &lt;- quap( flist = alist( weight_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_food * avgfood_std + beta_group * groupsize_std, alpha ~ dnorm(0,.2), c(beta_food, beta_group) ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_fox ) precis(model_fox_group, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.00 0.08 -0.13 0.13 beta_food 0.48 0.18 0.19 0.76 beta_group -0.57 0.18 -0.86 -0.29 sigma 0.94 0.06 0.84 1.04 precis(model_fox_group, depth = 2) %&gt;% as_tibble_rn() %&gt;% ggplot(aes(y = param)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(aes(xmin = `5.5%`, xmax =`94.5%`), color = clr0d) + geom_point(aes(x = mean), shape = 21, size = 3 , color = clr0d, fill = clr0) + scale_y_discrete(limits = c(&quot;sigma&quot;, &quot;beta_group&quot;, &quot;beta_food&quot;, &quot;alpha&quot;)) + theme(axis.title.y = element_blank()) H6 dag &lt;- dagify( OD ~ FD + GS + DD, FD ~ DD + FF, exposure = &quot;FD&quot;, outcome = &quot;OD&quot;, coords = tibble(name = c(&quot;FF&quot;, &quot;FD&quot;, &quot;DD&quot;, &quot;OD&quot;, &quot;GS&quot;), x = c(0, .5, 0, 1, 1.5), y = c(1, .5, 0, .5, .5))) dag %&gt;% tidy_dagitty() %&gt;% mutate(stage = if_else(name == &quot;OD&quot;, &quot;response&quot;, &quot;predictor&quot;)) %&gt;% plot_dag(clr_in = clr3) + coord_fixed(ratio = .6) adjustmentSets(dag) #&gt; { DD } impliedConditionalIndependencies(dag) #&gt; DD _||_ FF #&gt; DD _||_ GS #&gt; FD _||_ GS #&gt; FF _||_ GS #&gt; FF _||_ OD | DD, FD H7 n &lt;- 1e4 data_fruit &lt;- tibble( fruitfall = rnorm(n), dipteryx_density = rnorm(n), fruit_density = rnorm(n, mean = fruitfall + dipteryx_density), group_size = rnorm(n), od_area = rlnorm(n = n, meanlog = fruit_density + dipteryx_density - group_size)) %&gt;% mutate(across(everything(), standardize, .names = &quot;{.col}_std&quot;)) model_fruit &lt;- quap( flist = alist( od_area_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_fd * fruit_density_std + beta_dd * dipteryx_density_std, alpha ~ dnorm(0, .2), c(beta_fd, beta_dd) ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_fruit ) precis(model_fruit) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.00 0.01 -0.02 0.02 beta_fd 0.12 0.01 0.10 0.14 beta_dd 0.05 0.01 0.04 0.07 sigma 0.99 0.01 0.98 1.00 7.5 {brms} section 7.5.1 Multicolliniarity Legs Model brms_c6_model_legs_multicollinear &lt;- brm( data = data_legs, family = gaussian, height ~ 1 + left_leg + right_leg, prior = c(prior(normal(10, 100), class = Intercept), prior(normal(2, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, sample_prior = TRUE, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_legs_multicollinear&quot;) library(tidybayes) library(ggdist) as_draws_df(brms_c6_model_legs_multicollinear) %&gt;% dplyr::select(starts_with(&quot;b&quot;), sigma) %&gt;% as_tibble() %&gt;% set_names(x = . , nm = names(.) %&gt;% str_remove(&quot;b_&quot;)) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = reorder(name, value))) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_pointinterval(color = clr0dd, fill = clr0, shape = 21) + scale_y_discrete(limits = c(&quot;sigma&quot;, &quot;left_leg&quot;, &quot;Intercept&quot;)) + theme_minimal(base_family = fnt_sel) + theme(axis.title = element_blank()) Check various seeds to illustrate the difficulty to fit the legs jointly: simulate_leg_data &lt;- function(seed = 42){ n &lt;- 100 set.seed(seed) data_legs &lt;- tibble( height = rnorm(n = n, mean = 10, sd = 2), leg_proportion = runif(n, min = 0.4, max = 0.5), left_leg = leg_proportion * height + rnorm(n, 0, .02), right_leg = leg_proportion * height + rnorm(n, 0, .02), ) tibble(seed = seed, fit = list(update(brms_c6_model_legs_multicollinear, newdata = data_legs, seed = 42, refresh = 0))) } 1:4 %&gt;% map_dfr(simulate_leg_data) %&gt;% mutate(posterior = purrr::map(fit, as_draws_df )) %&gt;% unnest(cols = posterior) %&gt;% dplyr::select(seed, starts_with(&quot;b&quot;), sigma) %&gt;% set_names(x = . , nm = names(.) %&gt;% str_remove(&quot;b_&quot;)) %&gt;% pivot_longer(-seed) %&gt;% ggplot(aes(x = value, y = reorder(name, value))) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_halfeye(aes(fill_ramp = stat(cut_cdf_qi(cdf, .width = c(0.66, 0.95,1)))), fill = fll0dd, adjust = .7, normalize = &quot;groups&quot;, height = .8) + scale_fill_ramp_discrete(range = c(.85, 0.15), guide = &quot;none&quot;) + scale_y_discrete(limits = c(&quot;sigma&quot;, &quot;left_leg&quot;, &quot;right_leg&quot;, &quot;Intercept&quot;)) + coord_cartesian(ylim = c(0.9, 5), expand = 0) + facet_wrap(seed ~ ., ncol = 1, labeller = label_both) + theme_minimal(base_family = fnt_sel) + theme(axis.title = element_blank()) p_ridge &lt;- as_draws_df(brms_c6_model_legs_multicollinear) %&gt;% as_tibble() %&gt;% dplyr::select(starts_with(&quot;b&quot;), sigma) %&gt;% set_names(x = . , nm = names(.) %&gt;% str_remove(&quot;b_&quot;)) %&gt;% ggplot(aes(x = left_leg, y = right_leg)) + geom_point(color = clr0d, fill = fll0, shape = 21, size = .6) p_sum &lt;- as_draws_df(brms_c6_model_legs_multicollinear) %&gt;% as_tibble() %&gt;% dplyr::select(starts_with(&quot;b&quot;), sigma) %&gt;% set_names(x = . , nm = names(.) %&gt;% str_remove(&quot;b_&quot;)) %&gt;% ggplot(aes(x = left_leg + right_leg)) + stat_halfeye(aes(fill_ramp = stat(cut_cdf_qi(cdf, .width = c(0.66, 0.95,1)))), fill = fll0dd, adjust = .7, normalize = &quot;groups&quot;, height = .8) + scale_fill_ramp_discrete(range = c(.85, 0.15), guide = &quot;none&quot;) p_ridge + p_sum Model single leg brms_c6_model_model_leg &lt;- brm( data = data_legs, family = gaussian, height ~ 1 + left_leg, prior = c(prior(normal(10, 100), class = Intercept), prior(normal(2, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, sample_prior = TRUE, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_leg&quot;) as_draws_df(brms_c6_model_model_leg) %&gt;% dplyr::select(starts_with(&quot;b&quot;), sigma) %&gt;% as_tibble() %&gt;% set_names(x = . , nm = names(.) %&gt;% str_remove(&quot;b_&quot;)) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = reorder(name, value))) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_pointinterval(color = clr0dd, fill = clr0, shape = 21) + scale_y_discrete(limits = c(&quot;sigma&quot;, &quot;left_leg&quot;, &quot;Intercept&quot;)) + theme_minimal(base_family = fnt_sel) + theme(axis.title = element_blank()) Multicollinear milk model brms_c6_model_milk_multicollinear_1 &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 1 + lactose_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_milk_multicollinear1&quot; ) brms_c6_model_milk_multicollinear_2 &lt;- update( brms_c6_model_milk_multicollinear_1, newdata = data_milk, formula = kcal_std ~ 1 + fat_std, seed = 42, file = &quot;brms/brms_c6_model_milk_multicollinear2&quot; ) brms_c6_model_milk_multicollinear_3 &lt;- update( brms_c6_model_milk_multicollinear_1, newdata = data_milk, formula = kcal_std ~ 1 + lactose_std + fat_std, seed = 42, file = &quot;brms/brms_c6_model_milk_multicollinear3&quot; ) posterior_summary(brms_c6_model_milk_multicollinear_3) %&gt;% round(digits = 2) %&gt;% knitr::kable() Estimate Est.Error Q2.5 Q97.5 b_Intercept 0.00 0.07 -0.15 0.14 b_lactose_std -0.67 0.20 -1.06 -0.27 b_fat_std 0.25 0.20 -0.15 0.65 sigma 0.41 0.06 0.32 0.55 lp__ -17.30 1.51 -21.09 -15.42 7.5.2 Post-treatment bias brms_c6_model_fungus_no_treatment &lt;- brm( data = data_fungus, family = gaussian, h_1 ~ 0 + h_0, prior = c(prior(lognormal(0, 0.25), class = b, lb = 0), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_fungus_no_treatment&quot; ) mixedup::summarise_model(brms_c6_model_fungus_no_treatment) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 3.33 1.82 1.58 2.10 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; h_0 1.43 0.02 1.39 1.46 To fit the original model also in {brms}, we are translating \\[ \\begin{array}{rclr} \\mu_i &amp; = &amp; h_{0,i} \\times p &amp; \\textrm{[linear model]}\\\\ p &amp; = &amp; \\alpha + \\beta_{T} T_{i} + \\beta_{F} F_{i} &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] into \\[ \\begin{array}{rclr} \\mu_i &amp; = &amp; h_{0,i} \\times ( \\alpha + \\beta_{T} T_{i} + \\beta_{F} F_{i}) &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] \\(\\rightarrow\\) beware of the non-linear {brms} syntax used here! brms_c6_model_fungus_post_treatment &lt;- brm( data = data_fungus, family = gaussian, bf(h_1 ~ h_0 * (a + t * treatment + f * fungus), a + t + f ~ 1, nl = TRUE), prior = c(prior(lognormal(0, 0.2), nlpar = a, lb = 0), prior(normal(0, 0.5), nlpar = t), prior(normal(0, 0.5), nlpar = f), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_fungus_post_treatment&quot; ) mixedup::summarise_model(brms_c6_model_fungus_post_treatment) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 2.09 1.45 1.26 1.67 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; a_Intercept 1.48 0.03 1.43 1.53 #&gt; t_Intercept 0.00 0.03 -0.05 0.06 #&gt; f_Intercept -0.27 0.04 -0.34 -0.19 omitting fungus from the model: brms_c6_model_fungus_only_treatment &lt;- brm( data = data_fungus, family = gaussian, bf(h_1 ~ h_0 * (a + t * treatment), a + t ~ 1, nl = TRUE), prior = c(prior(lognormal(0, 0.2), nlpar = a, lb = 0), prior(normal(0, 0.5), nlpar = t), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_fungus_only_treatment&quot; ) mixedup::summarise_model(brms_c6_model_fungus_only_treatment) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 3.18 1.78 1.56 2.04 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; a_Intercept 1.38 0.03 1.33 1.43 #&gt; t_Intercept 0.09 0.04 0.02 0.16 including moisture as a fork: brms_c6_model_moisture_post_treatment &lt;- update( brms_c6_model_fungus_post_treatment, newdata = data_moisture, seed = 42, file = &quot;brms/brms_c6_model_moisture_post_treatment&quot; ) mixedup::summarise_model(brms_c6_model_moisture_post_treatment) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 4.54 2.13 2.10 2.16 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; a_Intercept 1.53 0.00 1.52 1.54 #&gt; t_Intercept 0.05 0.00 0.05 0.06 #&gt; f_Intercept 0.13 0.00 0.12 0.14 brms_c6_model_moisture_only_treatment &lt;- update( brms_c6_model_fungus_only_treatment, newdata = data_moisture, seed = 42, file = &quot;brms/brms_c6_model_moisture_only_treatment&quot; ) mixedup::summarise_model(brms_c6_model_moisture_only_treatment) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 4.93 2.22 2.19 2.25 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; a_Intercept 1.62 0.00 1.61 1.63 #&gt; t_Intercept 0.00 0.00 -0.01 0.01 7.5.3 Collider Bias data_married_factor &lt;- data_married_adults %&gt;% mutate(married_f = c(&quot;single&quot;, &quot;married&quot;)[married + 1] %&gt;% factor()) brms_c6_model_happy_married &lt;-brm( data = data_married_factor, family = gaussian, happiness ~ 0 + married_f + age_trans, prior = c(prior(normal(0, 1), class = b, coef = married_fmarried), prior(normal(0, 1), class = b, coef = married_fsingle), prior(normal(0, 2), class = b, coef = age_trans), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_happy_married&quot;) mixedup::summarise_model(brms_c6_model_happy_married) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 0.96 0.98 0.94 1.02 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; married_fmarried 1.34 0.08 1.18 1.50 #&gt; married_fsingle -0.21 0.06 -0.33 -0.09 #&gt; age_trans -0.81 0.11 -1.03 -0.61 brms_c6_model_happy &lt;-brm( data = data_married_factor, family = gaussian, happiness ~ 0 + Intercept + age_trans, prior = c(prior(normal(0, 1), class = b, coef = Intercept), prior(normal(0, 2), class = b, coef = age_trans), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_happy&quot;) mixedup::summarise_model(brms_c6_model_happy) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 1.47 1.21 1.16 1.27 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; age_trans 0.00 0.07 -0.13 0.13 The haunted DAG brms_c6_model_education &lt;-brm( data = data_education, family = gaussian, children ~ 0 + Intercept + parents + grandparents, prior = c(prior(normal(0, 1), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_education&quot;) mixedup::summarise_model(brms_c6_model_education) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 2.04 1.43 1.30 1.58 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; Intercept -0.12 0.10 -0.32 0.08 #&gt; parents 1.79 0.04 1.70 1.88 #&gt; grandparents -0.83 0.11 -1.04 -0.63 brms_c6_model_education_resolved &lt;- update( brms_c6_model_education, newdata = data_education, formula = children ~ 0 + Intercept + parents + grandparents + unobserved, seed = 42, file = &quot;brms/brms_c6_model_education_resolved&quot; ) mixedup::summarise_model(brms_c6_model_education_resolved) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 1.07 1.04 0.94 1.15 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; Intercept -0.12 0.07 -0.26 0.02 #&gt; parents 1.01 0.07 0.88 1.15 #&gt; grandparents -0.04 0.10 -0.24 0.15 #&gt; unobserved 2.00 0.15 1.70 2.27 7.5.4 Summary data_waffle &lt;- data_waffle %&gt;% mutate(south_f = factor(South)) brms_c6_model_waffle_1 &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + waffle_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_waffle_1&quot;) brms_c6_model_waffle_2 &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + waffle_std + south_f, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_waffle_2&quot;) brms_c6_model_waffle_3 &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + waffle_std + medianagemarriage_std + marriage_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_waffle_3&quot;) brms_c6_model_waffle_4 &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + waffle_std + medianagemarriage_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_waffle_4&quot;) brms_c6_model_waffle_5 &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + waffle_std + marriage_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c6_model_waffle_5&quot;) formula &lt;- c(glue(&quot;d {mth(&#39;\\U007E&#39;)} 1 + w&quot;), glue(&quot;d {mth(&#39;\\U007E&#39;)} 1 + w + s&quot;), glue(&quot;d {mth(&#39;\\U007E&#39;)} 1 + w + a + m&quot;), glue(&quot;d {mth(&#39;\\U007E&#39;)} 1 + w + a&quot;), glue(&quot;d {mth(&#39;\\U007E&#39;)} 1 + w + m&quot;)) tibble(model = 1:5, fit = str_c(&quot;brms_c6_model_waffle_&quot;, model)) %&gt;% mutate(y = str_c(model, &quot;: &quot;, formula), post = purrr::map(fit, ~get(.) %&gt;% as_draws_df() %&gt;% dplyr::select(waffle_std = b_waffle_std))) %&gt;% unnest(post) %&gt;% ggplot(aes(x = waffle_std, y = y, color = fit %in% c(&quot;brms_c6_model_waffle_2&quot;, &quot;brms_c6_model_waffle_3&quot;))) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_pointinterval(aes(fill = after_scale(clr_lighten(color))), shape = 21) + scale_color_manual(values = c(clr0dd, clr2)) + labs(x = str_c(&quot;*&quot;,mth(&quot;\\U03B2&quot;),&quot;&lt;sub&gt;w&lt;/sub&gt;*&quot;), y = NULL) + coord_cartesian(xlim = c(-0.4, 0.6)) + theme(axis.text.y = element_markdown(hjust = 0), axis.title.x = element_markdown(), legend.position = &quot;none&quot;) 7.6 pymc3 section × "],
["rethinking-chapter-7.html", "8 Rethinking: Chapter 7 8.1 The Problem with Parameters 8.2 Entropy and Accuracy 8.3 Golem taming: regularization 8.4 Model comparison 8.5 Homework 8.6 {brms} section 8.7 pymc3 section", " 8 Rethinking: Chapter 7 Ulysses’ Compass by Richard McElreath, building on the Summaries by Solomon Kurz and Jake Thompson. Figure 8.1: Between Scylla and Charybdis by Adolf Hirémy-Hirschl (1910). 8.1 The Problem with Parameters library(rethinking) data_brainsize &lt;- tibble( species = c(&quot;afarensis&quot;, &quot;africanus&quot;, &quot;habilis&quot;, &quot;boisei&quot;, &quot;rudolfensis&quot;, &quot;ergaster&quot;, &quot;sapiens&quot;), brain_size = c(438, 452, 612, 521, 752, 871, 1350), mass = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5)) %&gt;% mutate(brain_size_scl = brain_size/ max(brain_size), mass_std = standardize(mass)) data_brainsize %&gt;% ggplot(aes(x = mass, y = brain_size)) + geom_point(shape = 21, color = clr2, fill = fll2, size = 3) + ggrepel::geom_text_repel(aes(label = species), force = 30, min.segment.length = unit(.1, &quot;npc&quot;), family = fnt_sel, fontface = &quot;italic&quot;) + coord_fixed(ratio = .03) 8.1.1 The burial of R2 \\[ R^{2} = \\frac{var(outcome) - var(residuals)}{var(outcome)} = 1 - \\frac{var(residuals)}{var(outcome)} \\] 8.1.1.1 Linear Model \\[ \\begin{array}{rclr} b_{i} &amp; \\sim &amp; Normal( \\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{m} m_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0.5, 1) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{m} &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\beta_{T}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Log-Normal(0, 1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_brain_size &lt;- quap( flist = alist( brain_size_scl ~ dnorm(mu, exp(log_sigma)), mu &lt;- alpha + beta_m * mass_std, alpha ~ dnorm(0.5, 1), beta_m ~ dnorm(0, 10), log_sigma ~ dnorm( 0, 1 ) ), data = data_brainsize ) precis(model_brain_size) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.53 0.07 0.42 0.64 beta_m 0.17 0.07 0.05 0.29 log_sigma -1.71 0.29 -2.18 -1.24 extract_r2 &lt;- function(quap_fit, decimals = 5){ data &lt;- sim(quap_fit) %&gt;% as_tibble() %&gt;% set_names(nm = data_brainsize$species) %&gt;% summarise(across(everything(), mean, .names = &quot;{.col}&quot;)) %&gt;% pivot_longer(everything(), names_to = &quot;species&quot;, values_to = &quot;mean_brainsize&quot;) %&gt;% mutate(brain_size_scl = data_brainsize$brain_size_scl, diff = mean_brainsize - brain_size_scl) round(1 - var2(data$diff) / var2(data$brain_size_scl), digits = decimals) } set.seed(12) extract_r2(model_brain_size) #&gt; [1] 0.47746 8.1.2 Higher order polynomials \\[ \\begin{array}{rclcr} b_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; &amp;\\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{1} m_{i} + \\beta_{2} m_{i}^2 &amp; &amp;\\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0.5, 1) &amp; &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{j} &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{for}~j = 1..2 &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Log-Normal(0, 1) &amp; &amp;\\textrm{[$\\sigma$ prior]} \\end{array} \\] model_brain_size2 &lt;- quap( flist = alist( brain_size_scl ~ dnorm(mu, exp(log_sigma)), mu &lt;- alpha + beta_m[1] * mass_std + beta_m[2] * mass_std ^ 2, alpha ~ dnorm(0.5, 1), beta_m ~ dnorm(0, 10), log_sigma ~ dnorm( 0, 1 ) ), data = data_brainsize, start = list(beta_m = rep(0, 2)) ) model_brain_size3 &lt;- quap( flist = alist( brain_size_scl ~ dnorm(mu, exp(log_sigma)), mu &lt;- alpha + beta_m[1] * mass_std + beta_m[2] * mass_std ^ 2 + beta_m[3] * mass_std ^ 3, alpha ~ dnorm(0.5, 1), beta_m ~ dnorm(0, 10), log_sigma ~ dnorm( 0, 1 ) ), data = data_brainsize, start = list(beta_m = rep(0, 3)) ) model_brain_size4 &lt;- quap( flist = alist( brain_size_scl ~ dnorm(mu, exp(log_sigma)), mu &lt;- alpha + beta_m[1] * mass_std + beta_m[2] * mass_std ^ 2 + beta_m[3] * mass_std ^ 3 + beta_m[4] * mass_std ^ 4, alpha ~ dnorm(0.5, 1), beta_m ~ dnorm(0, 10), log_sigma ~ dnorm( 0, 1 ) ), data = data_brainsize, start = list(beta_m = rep(0, 4)) ) model_brain_size5 &lt;- quap( flist = alist( brain_size_scl ~ dnorm(mu, exp(log_sigma)), mu &lt;- alpha + beta_m[1] * mass_std + beta_m[2] * mass_std ^ 2 + beta_m[3] * mass_std ^ 3 + beta_m[4] * mass_std ^ 4 + beta_m[5] * mass_std ^ 5, alpha ~ dnorm(0.5, 1), beta_m ~ dnorm(0, 10), log_sigma ~ dnorm( 0, 1 ) ), data = data_brainsize, start = list(beta_m = rep(0, 5)) ) model_brain_size6 &lt;- quap( flist = alist( brain_size_scl ~ dnorm(mu, exp(log_sigma)), mu &lt;- alpha + beta_m[1] * mass_std + beta_m[2] * mass_std ^ 2 + beta_m[3] * mass_std ^ 3 + beta_m[4] * mass_std ^ 4 + beta_m[5] * mass_std ^ 5 + beta_m[6] * mass_std ^ 6, alpha ~ dnorm(0.5, 1), beta_m ~ dnorm(0, 10), log_sigma ~ dnorm( 0, 1 ) ), data = data_brainsize, start = list(beta_m = rep(0, 6)) ) mass_seq &lt;- seq( from = min(data_brainsize$mass_std) - .15, to = max(data_brainsize$mass_std) + .15, length.out = 101) plot_poly &lt;- function(mod, ylim){ model_posterior_samples &lt;- extract.samples(mod) %&gt;% as.data.frame() %&gt;% as_tibble() model_posterior_prediction_samples &lt;- link(mod, data = tibble(mass_std = mass_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = mass_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;mass_std&quot;, values_to = &quot;brain_size_scl&quot;) %&gt;% mutate(mass_std = as.numeric(mass_std)) model_posterior_prediction_pi &lt;- model_posterior_prediction_samples %&gt;% group_by(mass_std) %&gt;% summarise(mean = mean(brain_size_scl), PI_lower = PI(brain_size_scl)[1], PI_upper = PI(brain_size_scl)[2]) %&gt;% ungroup() p &lt;- ggplot(mapping = aes(x = mass_std * sd(data_brainsize$mass) + mean(data_brainsize$mass))) + geom_smooth(data = model_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean * max(data_brainsize$brain_size), ymin = PI_lower * max(data_brainsize$brain_size), ymax = PI_upper * max(data_brainsize$brain_size)), color = clr2, fill = fll2, size = .2) + geom_point(data = data_brainsize, aes(y = brain_size_scl * max(data_brainsize$brain_size)), color = rgb(0,0,0,.5), size = 1) + labs(x = &quot;mass&quot;, y = &quot;brain_size&quot;, title = glue(&quot;*R&lt;sup&gt;2&lt;/sup&gt;:* {extract_r2(mod, decimals = 2)}&quot;)) + coord_cartesian(ylim = ylim) + theme(plot.title = element_markdown()) if(identical(mod, model_brain_size6)) { p &lt;- p + geom_hline(yintercept = 0, color = clr_dark, linetype = 3 ) } p } list(model_brain_size, model_brain_size2,model_brain_size3, model_brain_size4, model_brain_size5, model_brain_size6) %&gt;% purrr::map2(.y = list(c(420, 1400), c(420, 1400), c(420, 1400), c(300, 1950), c(300, 1950), c(-400, 1500)), plot_poly) %&gt;% wrap_plots() + plot_annotation(tag_levels = &quot;a&quot;) 8.1.3 Underfitting Leave one out (LOO) model_loo &lt;- function(idx = 0, mod_degree = 1){ data &lt;- data_brainsize[-idx, ] if(mod_degree == 1){ current_mod &lt;- quap( flist = alist( brain_size_scl ~ dnorm(mu, exp(log_sigma)), mu &lt;- alpha + beta_m[1] * mass_std, alpha ~ dnorm(0.5, 1), beta_m ~ dnorm(0, 10), log_sigma ~ dnorm( 0, 1 ) ), data = data, start = list(beta_m = rep(0, 1)) ) } else if(mod_degree == 4) { current_mod &lt;- quap( flist = alist( brain_size_scl ~ dnorm(mu, exp(log_sigma)), mu &lt;- alpha + beta_m[1] * mass_std + beta_m[2] * mass_std ^ 2 + beta_m[3] * mass_std ^ 3 + beta_m[4] * mass_std ^ 4, alpha ~ dnorm(0.5, 1), beta_m ~ dnorm(0, 10), log_sigma ~ dnorm( 0, 1 ) ), data = data, start = list(beta_m = rep(0, 4)) ) } else { stop(&quot;`mod_degree` needs to be either 1 or 4&quot;) } model_posterior_prediction_samples &lt;- link(current_mod, data = tibble(mass_std = mass_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = mass_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;mass_std&quot;, values_to = &quot;brain_size_scl&quot;) %&gt;% mutate(mass_std = as.numeric(mass_std)) model_posterior_prediction_pi &lt;- model_posterior_prediction_samples %&gt;% group_by(mass_std) %&gt;% summarise(mean = mean(brain_size_scl)) %&gt;% ungroup() %&gt;% mutate(idx = idx, mod_degree = mod_degree) model_posterior_prediction_pi } cross_df(list(idx = seq_along(data_brainsize$species), mod_degree = c(1, 4))) %&gt;% pmap_dfr(model_loo) %&gt;% ggplot(mapping = aes(x = mass_std * sd(data_brainsize$mass) + mean(data_brainsize$mass))) + geom_line(aes(y = mean * max(data_brainsize$brain_size), group = factor(idx), color = idx)) + geom_point(data = data_brainsize %&gt;% mutate(idx = row_number()), aes(y = brain_size_scl * max(data_brainsize$brain_size), fill = idx, color = after_scale(clr_darken(fill))), size = 2, shape = 21) + labs(x = &quot;mass&quot;, y = &quot;brain_size&quot;) + facet_wrap(mod_degree ~ ., labeller = label_both) + scale_color_gradientn(colors = c(clr0dd, clr0, clr2), guide = &quot;none&quot;) + scale_fill_gradientn(colors = c(clr0dd, clr0, clr2), guide = &quot;none&quot;) + coord_cartesian(ylim = c(0, 2e3)) + theme(plot.title = element_markdown()) 8.2 Entropy and Accuracy 8.2.1 Entropy Definition of Information Entropy \\[ H(p) = - E~\\textrm{log}(p_{i}) = - \\sum_{i = 1}^n p_{i}~\\textrm{log}(p_{i}) \\] or verbally: The uncertainty contained in a probability distribution is the average log-probability of an event. which fulfills the requirements: uncertainty should be continuous uncertainty should increase with the number of possible events uncertainty should be additive Example for \\(p_1 = 0.3\\) and \\(p_2 = 0.7\\): \\[ H(p) = - \\big( p_{1} \\textrm{log}(p_{1}) + p_{2} \\textrm{log}(p_{2}) \\big) \\approx 0.61 \\] p &lt;- c( .3, .7 ) - sum( p * log(p) ) #&gt; [1] 0.6108643 Compared to Abu Dhabi (“it hardly ever rains”) p &lt;- c( .01, .99 ) - sum( p * log(p) ) #&gt; [1] 0.05600153 Entropy increases with th dimensionality of the prediction problem (eg. predicting 🌧/ 🌨 / ☀️) p &lt;- c( .15, .5, .7 ) - sum( p * log(p) ) #&gt; [1] 0.880814 8.2.2 Accuracy Divergence: The additional uncertainty induced by using probabilities from one distribution to describe another distribution. The Kullback-Leibler Divergence (KL): \\[ D_{KL}(p, q) = \\sum_{i} p_{i} \\big( \\textrm{log}(p_{i}) - \\textrm{log}(q_{i})\\big) = \\sum_{i} p_{i} \\textrm{log}\\left(\\frac{p_{i}}{q_{i}}\\right) \\] tibble(p1 = .3, p2 = .7, q1 = seq(from = .01, to = .99, by = .01), q2 = 1 - q1, d_kl = p1 * log(p1 / q1) + p2 * log(p2 / q2)) %&gt;% ggplot(aes(x = q1, y = d_kl)) + geom_line(color = clr2) + geom_vline(xintercept = .3, color = clr_dark, linetype = 3) 8.2.3 Estimating Divergence Log-probability score to compare the predictive accuracy of different models: \\[ S(q) = \\sum_{i} \\textrm(log) (q_{i}) \\] where \\(i\\) indexes each case and \\(q_{i}\\) is the likelihood for each case. A (re-scaled) equivalent is given with the deviance: \\[ D(q) = -2 \\sum_{i} \\textrm(log) (q_{i}) \\] and it’s Bayesian version the Log-pointwise-predictive density: \\[ lppd(y, \\Theta) = \\sum_{i} \\textrm{log} \\frac{1}{S} \\sum_{s} p (y_{i} | \\Theta_{s}) \\] where \\(S\\) is the number of samples and \\(\\Theta_{s}\\) is the s-th set of sampled parameter values in the posterior distribution. # lppd &lt;- function (fit, ...) { # ll &lt;- sim(fit, ll = TRUE, ...) # n &lt;- ncol(ll) # ns &lt;- nrow(ll) # f &lt;- function(i) log_sum_exp(ll[, i]) - log(ns) # lppd &lt;- sapply(1:n, f) # return(lppd) # } set.seed(1) lppd(model_brain_size, n = 1e4) #&gt; [1] 0.6098668 0.6483438 0.5496093 0.6234934 0.4648143 0.4347605 -0.8444632 8.2.4 Scoring the right data tibble(model_degree = 1:6, model = list(model_brain_size, model_brain_size2, model_brain_size3, model_brain_size4, model_brain_size5, model_brain_size6)) %&gt;% mutate(log_prob_score = map_dbl(model, .f = function(mod){sum(lppd(mod))})) #&gt; # A tibble: 6 x 3 #&gt; model_degree model log_prob_score #&gt; &lt;int&gt; &lt;list&gt; &lt;dbl&gt; #&gt; 1 1 &lt;map&gt; 2.42 #&gt; 2 2 &lt;map&gt; 2.65 #&gt; 3 3 &lt;map&gt; 3.69 #&gt; 4 4 &lt;map&gt; 5.32 #&gt; 5 5 &lt;map&gt; 14.1 #&gt; 6 6 &lt;map&gt; 39.6 n_cores &lt;- 8 run_sim &lt;- function(k, n_samples, n_sim = 1e3, b_sigma = 100){ mcreplicate(n_sim, sim_train_test(N = n_samples, k = k, b_sigma = b_sigma), mc.cores = n_cores) %&gt;% t() %&gt;% as_tibble() %&gt;% summarise(mean_p = mean(V1), mean_q = mean(V2), sd_p = sd(V1), sd_q = sd(V2)) %&gt;% mutate(k = k, n_samples = n_samples, b_sigma = b_sigma) } tictoc::tic() data_sim &lt;- cross_df(list(k = 1:5, n_samples = c(20, 100))) %&gt;% pmap_dfr(run_sim) tictoc::toc() write_rds(data_sim, &quot;data/rethinking_c6_data_sim.Rds&quot;) tictoc::tic() data_sim_var_beta &lt;- crossing(k = 1:5, n_samples = c(20, 100), b_sigma = c(1, 0.5, 0.2)) %&gt;% pmap_dfr(run_sim) tictoc::toc() write_rds(data_sim_var_beta, &quot;data/rethinking_c6_data_sim_var_beta.Rds&quot;) data_sim &lt;- read_rds(&quot;data/rethinking_c6_data_sim.Rds&quot;) x_dodge &lt;- .3 data_sim %&gt;% ggplot() + geom_pointrange(aes(x = k - .5 * x_dodge, ymin = mean_p - sd_p, y = mean_p, ymax = mean_p + sd_p, color = &quot;train&quot;, fill = after_scale(clr_lighten(color))), shape = 21) + geom_pointrange(aes(x = k + .5 * x_dodge, ymin = mean_q - sd_q, y = mean_q, ymax = mean_q + sd_q, color = &quot;test&quot;, fill = after_scale(clr_lighten(color))), shape = 21) + scale_color_manual(&quot;&quot;, values = c(train = clr0dd, test = clr2)) + facet_wrap(n_samples ~ ., scales = &quot;free&quot;, label = label_both) + labs(x = &quot;number of parameters&quot;, y = &quot;deviance&quot;) + theme(legend.position = &quot;bottom&quot;) p_curves &lt;- ggplot() + stat_function(fun = function(x){dnorm(x = x, mean = 0, sd = .2)}, color = clr0dd, linetype = 1, xlim = c(-3, 3), n = 301) + stat_function(fun = function(x){dnorm(x = x, mean = 0, sd = .5)}, color = clr0dd, linetype = 2, xlim = c(-3, 3), n = 301) + stat_function(fun = function(x){dnorm(x = x, mean = 0, sd = 1)}, color = clr0dd, linetype = 3, xlim = c(-3, 3), n = 501) + labs(x = &quot;parameter value&quot;, y = &quot;density&quot;) data_sim_var_beta &lt;- read_rds(&quot;data/rethinking_c6_data_sim_var_beta.Rds&quot;) p_lines &lt;- data_sim_var_beta %&gt;% dplyr::select(mean_p, mean_q,k:b_sigma) %&gt;% pivot_longer(cols = mean_p:mean_q, names_to = &quot;set&quot;, values_to = &quot;mean&quot;) %&gt;% ggplot() + geom_line(aes(x = k, y = mean, linetype = factor(b_sigma), color = set)) + geom_point(data = data_sim %&gt;% dplyr::select(mean_p, mean_q,k:b_sigma) %&gt;% pivot_longer(cols = mean_p:mean_q, names_to = &quot;set&quot;, values_to = &quot;mean&quot;), aes(x = k, y = mean, color = set, fill = after_scale(clr_lighten(color))), shape = 21, size = .9) + scale_color_manual(&quot;&quot;, values = c(mean_p = clr0dd, mean_q = clr2), labels = c(mean_p = &quot;test&quot;, mean_q = &quot;train&quot;)) + scale_linetype_manual(&quot;beta prior width&quot;, values = c(`1` = 3, `0.5` = 2, `0.2` = 1)) + facet_wrap(n_samples ~ ., scales = &quot;free&quot;, label = label_both) + labs(x = &quot;number of parameters&quot;, y = &quot;deviance&quot;) + theme(legend.position = &quot;bottom&quot;) p_curves + p_lines + plot_annotation(tag_levels = &quot;a&quot;) + plot_layout(widths = c(.5, 1), guides = &quot;collect&quot;) &amp; theme(legend.position = &quot;bottom&quot;) 8.3 Golem taming: regularization 8.3.1 Cross-validataion leave-one-out cross-validataion (loocv): dropping one data point in each fold (resulting in \\(n\\) test sets, so \\(n\\) refits of the model and \\(n\\) posterior distributions) Pareto-smoothed importance sampling cross-validataion (PSIS): approximates loocv by sampling from the original posterior while taking the importance/weight of each data point into account (provides feedback about it’s ow reliability, no model re-fitting necessary) \\[ s_{PSIS} = \\sqrt{N~\\textrm{var}(\\textrm{psis}_{i})} \\] The importance sampling estimate of out-of-sample lppd: \\[ lppd_{IS} = \\sum_{i=1}^{N} \\textrm{log} \\frac{\\Sigma_{s=1}^{S} r(\\theta_{s}p(y_{i}|\\theta_{s}))}{\\sigma_{s=1}^{S} r(\\theta_{s})} \\] The Pareto distribution \\[ p(r | u, \\sigma, k) = \\sigma^{-1} \\big(1 + k (r - u) \\sigma ^{-1}\\big)^{-\\frac{1}{k}-1} \\] ggplot() + stat_function(fun = function(x){dpareto(x = x, xmin = 1, alpha = 1)},aes(color = &quot;1&quot;), xlim = c(0, 5), n = 201)+ stat_function(fun = function(x){dpareto(x = x, xmin = 1, alpha = .7)},aes(color = &quot;0.7&quot;), xlim = c(0, 5), n = 201)+ stat_function(fun = function(x){dpareto(x = x, xmin = 1, alpha = .3)},aes(color = &quot;0.3&quot;), xlim = c(0, 5), n = 201)+ stat_function(fun = function(x){dpareto(x = x, xmin = 1, alpha = .1)},aes(color = &quot;0.1&quot;), xlim = c(0, 5), n = 201) + scale_color_manual(&quot;alpha&quot;, values = c(`1` = clr0dd, `0.7` = clr1, `0.3` = clr2, `0.1` = clr3)) + coord_cartesian(ylim = c(-.1, 15), expand = 0) + theme(legend.position = &quot;bottom&quot;) 8.3.2 Information Criteria 8.3.2.1 Akaike information criterion Only for legacy reasons \\[ AIC = D_{train} + 2p = -2 lppd + 2p \\] AIC is an approximation that depends on flat priors posterior distribution is \\(\\sim\\) gaussian sample size \\(N\\) is much greater than numbers of parameters \\(k\\) 8.3.2.2 Widely Applicable Information Criterion \\[ WAIC(y, \\Theta) = -2 (lppd - \\underbrace{\\sum_{i} \\textrm{var}_{\\theta}~\\textrm{log}~p(y_{i}|\\theta)}_{\\textrm{penalty term}}) \\] WAIC calculations data(cars) set.seed(94) model_cars &lt;- quap( flist = alist( dist ~ dnorm(mu, sigma), mu &lt;- alpha + beta_speed * speed, alpha ~ dnorm(0, 100), beta_speed ~ dnorm(0, 10), sigma ~ dexp(1) ), data = cars ) set.seed(94) n_samples &lt;- 1e3 n_cases &lt;- nrow(cars) cars_posterior_predictive_samples &lt;- extract.samples(model_cars, n = n_samples) %&gt;% as_tibble() logprob &lt;- sapply(1:n_samples, function(idx){ mu &lt;- cars_posterior_predictive_samples$alpha[idx] + cars_posterior_predictive_samples$beta_speed[idx] * cars$speed dnorm( cars$dist, mean = mu, sd = cars_posterior_predictive_samples$sigma[idx], log = TRUE)}) lppd &lt;- sapply(1:n_cases, function(i){log_sum_exp(logprob[i,]) - log(n_samples)}) pWAIC &lt;- sapply(1:n_cases, function(i){var(logprob[i,])}) -2 * (sum(lppd) - sum(pWAIC)) #&gt; [1] 423.3127 waic_vec &lt;- -2 * (lppd - pWAIC) sqrt(n_cases * var(waic_vec)) #&gt; [1] 17.81271 8.3.3 Comparing CV, PSIS and WAIC make_sim &lt;- function(n, k, b_sigma) { r &lt;- mcreplicate(n_sim, sim_train_test(N = n, k = k, b_sigma = b_sigma, WAIC = TRUE, LOOCV = TRUE, LOOIC = TRUE), mc.cores = n_cores) t &lt;- tibble( deviance_os = mean(unlist(r[2, ])), deviance_w = mean(unlist(r[3, ])), deviance_p = mean(unlist(r[11, ])), deviance_c = mean(unlist(r[19, ])), error_w = mean(unlist(r[7, ])), error_p = mean(unlist(r[15, ])), error_c = mean(unlist(r[20, ])) ) return(t) } n_sim &lt;- 1e3 n_cores &lt;- 8 tictoc::tic() data_sim_scores &lt;- crossing(n = c(20, 100), k = 1:5, b_sigma = c(0.5, 100)) %&gt;% mutate(sim = pmap(list(n, k, b_sigma), make_sim)) %&gt;% unnest(sim) tictoc::toc() # 119984.727 sec elapsed write_rds(data_sim_scores, &quot;data/rethinking_c6_data_sim_scores.Rds&quot;) data_sim_scores &lt;- read_rds(&quot;data/rethinking_c6_data_sim_scores.Rds&quot;) data_sim_scores %&gt;% pivot_longer(deviance_w:deviance_c) %&gt;% mutate(criteria = ifelse(name == &quot;deviance_w&quot;, &quot;WAIC&quot;, ifelse(name == &quot;deviance_p&quot;, &quot;PSIS&quot;, &quot;CV&quot;))) %&gt;% ggplot(aes(x = k)) + geom_line(aes(y = value, color = criteria)) + geom_point(aes(y = deviance_os, shape = factor(b_sigma)), size = 1.5) + scale_shape_manual(&quot;sigma&quot;, values = c(19, 1)) + scale_color_manual(values = c(clr0dd, clr1, clr2)) + labs(x = &quot;number of parameters (k)&quot;, y = &quot;average deviance&quot;) + facet_grid(n ~ b_sigma, scales = &quot;free_y&quot;, labeller = label_both)+ theme(legend.position = &quot;bottom&quot;) 8.4 Model comparison 8.4.1 Model mis-selection chapter6_models &lt;- read_rds(&quot;envs/chapter6_models.rds&quot;) set.seed(11) WAIC(chapter6_models$model_fungus_post_treatment) #&gt; WAIC lppd penalty std_err #&gt; 1 361.4511 -177.1724 3.553198 14.17033 set.seed(77) comp_waic &lt;- compare(chapter6_models$model_fungus_post_treatment, chapter6_models$model_fungus_no_treatment, chapter6_models$model_fungus_only_treatment, func = WAIC) %&gt;% as_tibble_rn() %&gt;% mutate(model = str_remove(param,&quot;.*\\\\$&quot;), mod = model %&gt;% purrr::map(.f = function(m){chapter6_models[[m]]}), deviance = mod %&gt;% purrr::map_dbl(.f = rethinking::deviance), model = str_remove(model,&quot;model_fungus_&quot;)) %&gt;% dplyr::select(-param) comp_waic %&gt;% dplyr::select(-mod) %&gt;% dplyr::select(model, everything()) #&gt; # A tibble: 3 x 8 #&gt; model WAIC SE dWAIC dSE pWAIC weight deviance #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 post_treatment 361. 14.2 0 NA 3.57 1.00e+ 0 354. #&gt; 2 only_treatment 403. 11.3 41.3 10.5 2.65 1.08e- 9 397. #&gt; 3 no_treatment 406. 11.8 44.7 12.2 1.70 1.98e-10 402. WAIC and PSIS result in similar values: compare(chapter6_models$model_fungus_post_treatment, chapter6_models$model_fungus_no_treatment, chapter6_models$model_fungus_only_treatment, func = PSIS) %&gt;% as_tibble_rn() #&gt; # A tibble: 3 x 7 #&gt; PSIS SE dPSIS dSE pPSIS weight param #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 362. 14.3 0 NA 3.93 1.00e+ 0 chapter6_models$model_fungus_post_trea… #&gt; 2 403. 11.3 40.6 10.4 2.69 1.50e- 9 chapter6_models$model_fungus_only_trea… #&gt; 3 406. 11.7 43.9 12.2 1.63 2.99e-10 chapter6_models$model_fungus_no_treatm… tibble(post_treatment = WAIC(chapter6_models$model_fungus_post_treatment, pointwise = TRUE)$WAIC, only_treatment = WAIC(chapter6_models$model_fungus_only_treatment, pointwise = TRUE)$WAIC, no_treatment = WAIC(chapter6_models$model_fungus_no_treatment, pointwise = TRUE)$WAIC) %&gt;% mutate(model_difference_post_only = post_treatment - only_treatment, model_difference_no_only = no_treatment - only_treatment) %&gt;% summarise(`post-only` = sqrt(n()[[1]] * var(model_difference_post_only)), `no-only` = sqrt(n()[[1]] * var(model_difference_no_only))) %&gt;% pivot_longer(everything(), names_to = &quot;comparison&quot;, values_to = &quot;se_of_model_differnce&quot;) #&gt; # A tibble: 2 x 2 #&gt; comparison se_of_model_differnce #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 post-only 10.5 #&gt; 2 no-only 4.90 \\(\\rightarrow\\) compare to WAIC table $dSE[[2]] estimating the model difference for a z-score of \\(\\sim\\) 2.6 (99%)\" comp_waic$dWAIC[[2]] + c(-1, 1) * comp_waic$dSE[[2]] * 2.6 #&gt; [1] 14.12572 68.47753 library(tidybayes) comp_waic %&gt;% ggplot() + geom_vline(xintercept = comp_waic$WAIC[[1]], color = clr_dark, linetype = 3) + geom_pointinterval(aes(y = model, x = WAIC, xmin = WAIC - SE, xmax = WAIC + SE, color = &quot;WAIC&quot;, fill = after_scale(clr_lighten(color))), shape = 21, size = 2.5) + geom_point(aes(y = model, x = deviance, color = &quot;WAIC&quot;)) + geom_pointinterval(data = comp_waic %&gt;% filter(row_number() &gt; 1), aes(y = as.numeric(factor(model)) + .25, x = WAIC, xmin = WAIC - dSE, xmax = WAIC + dSE, color = &quot;dSE&quot;), shape = 17, size = .9) + scale_color_manual(values = c(WAIC = clr2, dSE = clr0dd)) + theme(legend.position = &quot;bottom&quot;) set.seed(93) compare(chapter6_models$model_fungus_post_treatment, chapter6_models$model_fungus_no_treatment, chapter6_models$model_fungus_only_treatment)@dSE %&gt;% round(digits = 2) %&gt;% as_tibble() %&gt;% set_names(., nm = names(.) %&gt;% str_remove(pattern = &quot;.*model_fungus_&quot;)) %&gt;% mutate(` ` = comp_waic$model) %&gt;% dplyr::select(` `, everything()) %&gt;% knitr::kable() post_treatment no_treatment only_treatment post_treatment NA 12.22 10.49 only_treatment 12.22 NA 4.86 no_treatment 10.49 4.86 NA Weight of a model (last column of compare(), the relative support for each model): \\[ w_{i} = \\frac{\\textrm{exp}(-0.5\\Delta_{i})}{\\Sigma_{j}\\textrm{exp}(-0.5\\Delta_{j})} \\] These weights are important for model averaging. 8.4.2 Outliers and other illusions chapter5_models &lt;- read_rds(&quot;envs/chapter5_models.rds&quot;) set.seed(24071847) compare(chapter5_models$model_age, chapter5_models$model_marriage, chapter5_models$model_multiple) %&gt;% as_tibble_rn() #&gt; # A tibble: 3 x 7 #&gt; WAIC SE dWAIC dSE pWAIC weight param #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 127. 14.2 0 NA 4.41 0.679 chapter5_models$model_age #&gt; 2 129. 14.4 1.51 0.883 5.48 0.320 chapter5_models$model_multiple #&gt; 3 141. 11.0 13.5 10.4 3.71 0.000805 chapter5_models$model_marriage psis_k &lt;- tibble(waic_penalty = (function(){set.seed(set.seed(23)); WAIC(chapter5_models$model_multiple, pointwise = TRUE)$penalty})(), psis_k = (function(){set.seed(set.seed(23)); PSIS(chapter5_models$model_multiple, pointwise = TRUE)$k})(), location = chapter5_models$data_waffle$Loc) p_psis_k &lt;- psis_k %&gt;% ggplot(aes(x = psis_k ,y = waic_penalty)) + geom_vline(xintercept = .5, color = clr_dark, linetype = 3) + geom_point(shape = 21, size = 2, color = clr2, fill = fll2) + geom_text(data = psis_k %&gt;% filter(location %in% c(&quot;ME&quot;, &quot;ID&quot;)), aes(x = psis_k - .15, label = location)) p_dens &lt;- ggplot() + stat_function(fun = function(x){dnorm(x = x, sd = .8)}, xlim = c(-4, 4), n = 201, aes(color = &quot;gaussian&quot;), linetype = 3)+ stat_function(fun = function(x){dstudent(x = x, nu = 2, sigma = .55)}, xlim = c(-4, 4), n = 201, aes(color = &quot;student t&quot;)) + labs(y = &quot;density&quot;, x = &quot;value&quot;) p_logdens &lt;- ggplot() + stat_function(fun = function(x){-log(dnorm(x = x, sd = .8))}, xlim = c(-4, 4), n = 201, aes(color = &quot;gaussian&quot;), linetype = 3)+ stat_function(fun = function(x){-log(dstudent(x = x, nu = 2, sigma = .55))}, xlim = c(-4, 4), n = 201, aes(color = &quot;student t&quot;)) + labs(y = &quot;- log density&quot;, x = &quot;value&quot;) p_psis_k + p_dens + p_logdens + plot_layout(guides = &quot;collect&quot;) &amp; scale_color_manual(&quot;distribution&quot;, values = c(gaussian = clr0dd, `student t` = clr2)) &amp; theme(legend.position = &quot;bottom&quot;) chapter5_models$model_multiple #&gt; #&gt; Quadratic approximate posterior distribution #&gt; #&gt; Formula: #&gt; divorce_std ~ dnorm(mu, sigma) #&gt; mu &lt;- alpha + beta_M * marriage_std + beta_A * median_age_std #&gt; alpha ~ dnorm(0, 0.2) #&gt; beta_A ~ dnorm(0, 0.5) #&gt; beta_M ~ dnorm(0, 0.5) #&gt; sigma ~ dexp(1) #&gt; #&gt; Posterior means: #&gt; alpha beta_A beta_M sigma #&gt; -2.484974e-08 -6.135134e-01 -6.538068e-02 7.851183e-01 #&gt; #&gt; Log-likelihood: -59.24 model_multiple_sudent &lt;- quap( flist = alist( divorce_std ~ dstudent(2, mu, sigma), mu &lt;- alpha + beta_M * marriage_std + beta_A * median_age_std, alpha ~ dnorm(0, 0.2), beta_A ~ dnorm(0, 0.5), beta_M ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = chapter5_models$data_waffle ) PSIS(chapter5_models$model_multiple) #&gt; PSIS lppd penalty std_err #&gt; 1 129.9451 -64.97254 6.176689 15.34672 With the Student T distribution as a the likelihood, \\(k\\) is reduced as there is more mass in the tails of the distribution (thus, Idaho is less surprising) PSIS(model_multiple_sudent) #&gt; PSIS lppd penalty std_err #&gt; 1 134.2209 -67.11045 7.209632 11.9681 precis(chapter5_models$model_multiple) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.00 0.10 -0.16 0.16 beta_A -0.61 0.15 -0.85 -0.37 beta_M -0.07 0.15 -0.31 0.18 sigma 0.79 0.08 0.66 0.91 precis(model_multiple_sudent) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.02 0.10 -0.14 0.17 beta_A -0.70 0.13 -0.91 -0.49 beta_M 0.00 0.21 -0.33 0.34 sigma 0.55 0.08 0.42 0.68 Also, as the influence of Idaho is reduced, the estimate of beta_A is decreased in the updated model. library(rlang) chapter7_models &lt;- env( data_brainsize = data_brainsize, model_brain_size = model_brain_size, model_brain_size2 = model_brain_size2, model_brain_size3 = model_brain_size3, model_brain_size4 = model_brain_size4, model_brain_size5 = model_brain_size5, model_brain_size6 = model_brain_size6, cars = cars, model_cars = model_cars ) 8.5 Homework E1 Criteria defining information entropy The measure of uncertainty should be continuous (so there should be sudden shifts in the change of uncertainty for minor changes in the underlying probabilities) The measure of uncertainty should increase with the number of possible events (higher uncertainties are expected as the dimensionality of the underlying possibility increases) The measure of uncertainty should be additive (the sum of the uncertainty of all sub-events should result exactly in total uncertainty) E2 recall Information Entropy \\[ H(p) = - \\sum_{i=1}^{n} p_i \\textrm{log}(p_i) = -\\big( p_1 \\textrm{log}(p_1) + p_2 \\textrm{log}(p_2) \\big) \\] p_coin &lt;- c(tails = .3, heads = .7) -sum(p_coin * log(p_coin)) #&gt; [1] 0.6108643 E3 p_die &lt;- c(`1` = .2, `2` = .25, `3` = .25, `4` = .3) -sum(p_die * log(p_die)) #&gt; [1] 1.376227 E4 using L’Hôpital’s Rule for the limit \\(\\textrm{lim}_{p_{i}\\rightarrow\\infty} p_{i} \\textrm{log}(p_i) =0\\): p_die &lt;- c(`1` = 1/3, `2` = 1/3, `3` = 1/3, `4` = 0) -sum(c(p_die[1:3] * log(p_die[1:3]), 0)) #&gt; [1] 1.098612 M1 \\[ \\begin{array}{rcl} AIC &amp; = &amp; D_{train} + 2p\\\\ &amp; = &amp; -2 lppd + 2p \\\\ &amp;&amp;\\\\ WAIC(y, \\Theta) &amp; = &amp;-2 (lppd - \\underbrace{\\sum_{i} \\textrm{var}_{\\theta}~\\textrm{log}~p(y_{i}|\\theta)}_{\\textrm{penalty term}})\\\\ &amp; = &amp; -2 lppd + 2 \\sum_{i} \\textrm{var}_{\\theta}~\\textrm{log}~p(y_{i}|\\theta)\\\\ \\end{array} \\] \\(AIC = WAIC\\) if: \\[ \\begin{array}{rcl} p &amp; = &amp; \\sum_{i} \\textrm{var}_{\\theta}~\\textrm{log}~p(y_{i}|\\theta)\\\\ \\end{array} \\] We can expect the more general criterion (WAIC) to match the more specific one (AIC), if: the priors are flat and overwhelmed by the likelihood the posterior distribution is approximately gaussian the number of samples (\\(n\\)) is much larger than the number of parameters (\\(k\\)) M2 Both model selection and model comparison list the WAIC of a suite of models. For model selection however, all but the best performing models are discarded, while in model comparison the distribution of the criterion and the relative performance can be used to investigate subtle influences on the individual influences on the models. This provides much more context for the following inferences. M3 The magnitude of information criteria are dependent on the number of observations. models fitted to different data sets are thus not comparable: sim_waic &lt;- tibble(sample_size = rep(c(100, 500, 1e3), each = 100)) %&gt;% mutate(data = purrr::map( sample_size, .f = function(sample_size){ tibble(x = rnorm(n = sample_size), y = rnorm(n = sample_size, .5 + .75 * x)) %&gt;% mutate(across(everything(), standardize),) }), model = purrr::map( data, function(data){ quap( flist = alist( y ~ dnorm(mu, sigma), mu &lt;- alpha + beta * x, alpha ~ dnorm(0, .2), beta ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data, start = list(alpha = 0, beta = 0, sigma = .2)) } ), lppd = map_dbl(model, ~sum(rethinking::lppd(.x))), information_criterion = purrr:::map( model, function(model){ tibble(WAIC = rethinking::WAIC(model)$WAIC, PSIS = suppressMessages(rethinking::PSIS(model)$PSIS)) } )) %&gt;% unnest(information_criterion) sim_waic %&gt;% dplyr::select(sample_size, lppd:PSIS) %&gt;% pivot_longer(cols = lppd:PSIS, names_to = &quot;information_criterion&quot;) %&gt;% ggplot(aes(x = value, y = factor(sample_size), color = information_criterion)) + stat_slab(slab_type = &quot;pdf&quot;, aes(fill = after_scale(clr_alpha(color))), size = .5, trim = FALSE, n = 301) + scale_color_manual(values = c(clr0dd, clr2, clr3), guide = &quot;none&quot;) + labs(y = &quot;sample_size&quot;) + facet_wrap(information_criterion ~ ., scales = &quot;free&quot;) + theme(axis.title.x = element_blank()) M4 Given the WAIC formula A more concentrated prior translates to less variation in the prior (thus a smaller variance). Given the formula for \\(WAIC\\), where the effective number of parameters (\\(p_{WAIC}\\)) is represented by the penalty term \\(\\sum_{i} \\textrm{var}_{\\theta}~\\textrm{log}~p(y_{i}|\\theta)\\): \\[ WAIC(y, \\Theta) = -2 (lppd - \\underbrace{\\sum_{i} \\textrm{var}_{\\theta}~\\textrm{log}~p(y_{i}|\\theta)}_{\\textrm{penalty term}}) \\] A smaller \\(\\textrm{var}_{\\theta}\\) will also decrease the entire \\(p_{WAIC}\\) library(cli) quap_prior_var &lt;- function(df, p_idx) { if(p_idx == prior_widths[[1]]){ mod &lt;- quap( flist = alist( y ~ dnorm(mu, sigma), mu &lt;- alpha + beta_1 * x1 + beta_2 * x2 + beta_3 * x3, alpha ~ dnorm(0, .2), c(beta_1, beta_2, beta_3) ~ dnorm(0, prior_widths[[1]]), sigma ~ dexp(1) ), data = df, start = list(alpha = 0, beta_1 = 0, beta_2 = 0, beta_3 = 0, sigma = .2) ) } else if(p_idx == prior_widths[[2]]){ mod &lt;- quap( flist = alist( y ~ dnorm(mu, sigma), mu &lt;- alpha + beta_1 * x1 + beta_2 * x2 + beta_3 * x3, alpha ~ dnorm(0, .2), c(beta_1, beta_2, beta_3) ~ dnorm(0, prior_widths[[2]]), sigma ~ dexp(1) ), data = df, start = list(alpha = 0, beta_1 = 0, beta_2 = 0, beta_3 = 0, sigma = .2)) } else if(p_idx == prior_widths[[3]]){ mod &lt;- quap( flist = alist( y ~ dnorm(mu, sigma), mu &lt;- alpha + beta_1 * x1 + beta_2 * x2 + beta_3 * x3, alpha ~ dnorm(0, .2), c(beta_1, beta_2, beta_3) ~ dnorm(0, prior_widths[[3]]), sigma ~ dexp(1) ), data = df, start = list(alpha = 0, beta_1 = 0, beta_2 = 0, beta_3 = 0, sigma = .2)) } else if(p_idx == prior_widths[[4]]){ mod &lt;- quap( flist = alist( y ~ dnorm(mu, sigma), mu &lt;- alpha + beta_1 * x1 + beta_2 * x2 + beta_3 * x3, alpha ~ dnorm(0, .2), c(beta_1, beta_2, beta_3) ~ dnorm(0, prior_widths[[4]]), sigma ~ dexp(1) ), data = df, start = list(alpha = 0, beta_1 = 0, beta_2 = 0, beta_3 = 0, sigma = .2) ) } else { stop(glue::glue(&quot;p_idx needs to be one of ({prior_widths[[1]]}, {prior_widths[[2]]}, {prior_widths[[3]]} or {prior_widths[[4]]})&quot;)) } cli_progress_update(.envir = .GlobalEnv) tibble(model = list(mod)) %&gt;% mutate(lppd = sum(rethinking::lppd(mod)), WAIC = rethinking::WAIC(mod)$WAIC, PSIS = suppressMessages(rethinking::PSIS(mod)$PSIS)) } set.seed(42) prior_widths &lt;- c(.1, .32, 1, 3.2) n &lt;- 75 cli_progress_bar(&quot;Simulate | Prior Width&quot;, total = n * length(prior_widths)) prior_sim &lt;- tibble(prior_sd = rep(prior_widths, each = n)) %&gt;% mutate( sample_data = purrr::map(1:n(), function(x, prior_sd) { n &lt;- 20 tibble(x1 = rnorm(n = n), x2 = rnorm(n = n), x3 = rnorm(n = n)) %&gt;% mutate(y = rnorm(n = n, mean = 0.3 + 0.8 * x1 + 0.6 * x2 + 1.2 * x3), across(everything(), standardize)) }), mod = map2(sample_data, prior_sd, quap_prior_var)) %&gt;% unnest(mod) %&gt;% mutate(p_waic = .5 * (WAIC + 2 * lppd)) prior_sim %&gt;% ggplot(aes(x = p_waic, y = factor(prior_sd))) + stat_slab(slab_type = &quot;pdf&quot;, fill = fll0, color = clr0dd, size = .5, adjust = .5, trim = FALSE, n = 501) + labs(y = &quot;prior_sd&quot;) M5 Informative priors prevent overfitting because the prevent the model from simply encoding the data. M6 Overly informative priors result in underfitting because the likelihood has only a marginal influence on the posterior. As a result, the posterior simply resembles the prior. H1 data(Laffer) data_laffer &lt;- Laffer %&gt;% as_tibble() %&gt;% mutate(dplyr::across(everything(), standardize, .names = &quot;{.col}_std&quot;), tax_rate_std_sq = tax_rate_std ^ 2) model_laffer_0 &lt;- quap( flist = alist( tax_revenue_std ~ dnorm(mu, simga), mu &lt;- alpha + 0 * tax_rate_std, alpha ~ dnorm(0, .2), simga ~ dexp(1) ), data = data_laffer ) tax_seq &lt;- seq(from = min(data_laffer$tax_rate_std) - .05 * diff(range(data_laffer$tax_rate_std)), to = max(data_laffer$tax_rate_std) + .05 * diff(range(data_laffer$tax_rate_std)), length.out = 101) tax_model_0_posterior_prediction_samples &lt;- link(model_laffer_0, data = data.frame(tax_rate_std = tax_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = tax_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;tax_rate_std&quot;, values_to = &quot;tax_revenue_std&quot;) %&gt;% mutate(tax_rate_std = as.numeric(tax_rate_std), tax_revenue = tax_revenue_std * sd(data_laffer$tax_revenue_std) + mean(data_laffer$tax_revenue_std), tax_rate = tax_rate_std * sd(data_laffer$tax_rate) + mean(data_laffer$tax_rate)) tax_model_0_posterior_prediction_pi &lt;- tax_model_0_posterior_prediction_samples %&gt;% group_by(tax_rate) %&gt;% summarise(mean = mean(tax_revenue_std), PI_lower = PI(tax_revenue_std)[1], PI_upper = PI(tax_revenue_std)[2]) %&gt;% ungroup() model_laffer_1 &lt;- quap( flist = alist( tax_revenue_std ~ dnorm(mu, simga), mu &lt;- alpha + beta_1 * tax_rate_std, alpha ~ dnorm(0, .2), beta_1 ~ dnorm(0, .5), simga ~ dexp(1) ), data = data_laffer ) tax_model_1_posterior_prediction_samples &lt;- link(model_laffer_1, data = data.frame(tax_rate_std = tax_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = tax_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;tax_rate_std&quot;, values_to = &quot;tax_revenue_std&quot;) %&gt;% mutate(tax_rate_std = as.numeric(tax_rate_std), tax_revenue = tax_revenue_std * sd(data_laffer$tax_revenue_std) + mean(data_laffer$tax_revenue_std), tax_rate = tax_rate_std * sd(data_laffer$tax_rate) + mean(data_laffer$tax_rate)) tax_model_1_posterior_prediction_pi &lt;- tax_model_1_posterior_prediction_samples %&gt;% group_by(tax_rate) %&gt;% summarise(mean = mean(tax_revenue_std), PI_lower = PI(tax_revenue_std)[1], PI_upper = PI(tax_revenue_std)[2]) %&gt;% ungroup() model_laffer_2 &lt;- quap( flist = alist( tax_revenue_std ~ dnorm(mu, simga), mu &lt;- alpha + beta_1 * tax_rate_std + beta_2 * tax_rate_std ^ 2, alpha ~ dnorm(0, .2), c(beta_1, beta_2) ~ dnorm(0, .5), simga ~ dexp(1) ), data = data_laffer ) tax_model_2_posterior_prediction_samples &lt;- link(model_laffer_2, data = data.frame(tax_rate_std = tax_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = tax_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;tax_rate_std&quot;, values_to = &quot;tax_revenue_std&quot;) %&gt;% mutate(tax_rate_std = as.numeric(tax_rate_std), tax_revenue = tax_revenue_std * sd(data_laffer$tax_revenue_std) + mean(data_laffer$tax_revenue_std), tax_rate = tax_rate_std * sd(data_laffer$tax_rate) + mean(data_laffer$tax_rate)) tax_model_2_posterior_prediction_pi &lt;- tax_model_2_posterior_prediction_samples %&gt;% group_by(tax_rate) %&gt;% summarise(mean = mean(tax_revenue_std), PI_lower = PI(tax_revenue_std)[1], PI_upper = PI(tax_revenue_std)[2]) %&gt;% ungroup() n_knots &lt;- 5 knot_list &lt;- quantile(data_laffer$tax_rate_std, probs = seq(0, 1, length.out = n_knots)) library(splines) b_spline_laffer &lt;- bs(data_laffer$tax_rate_std, knots = knot_list[-c(1, n_knots)], degree = 3, intercept = TRUE) model_laffer_3 &lt;- quap( flist = alist( tax_revenue_std ~ dnorm(mu, sigma), mu &lt;- alpha + B %*% w, alpha ~ dnorm(0, .5), w ~ dnorm(0, .5), sigma ~ dexp(1) ), data = list(tax_revenue_std = data_laffer$tax_revenue_std, B = b_spline_laffer), start = list(w = rep(0, ncol(b_spline_laffer))) ) b_spline_tax &lt;- bs(tax_seq, knots = knot_list[-c(1, n_knots)], degree = 3, intercept = TRUE) tax_model_3_posterior_prediction_samples &lt;- link(model_laffer_3, data = list(B = b_spline_tax)) %&gt;% as_tibble() %&gt;% set_names(nm = tax_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;tax_rate_std&quot;, values_to = &quot;tax_revenue_std&quot;) %&gt;% mutate(tax_rate_std = as.numeric(tax_rate_std), tax_revenue = tax_revenue_std * sd(data_laffer$tax_revenue_std) + mean(data_laffer$tax_revenue_std), tax_rate = tax_rate_std * sd(data_laffer$tax_rate) + mean(data_laffer$tax_rate)) tax_model_3_posterior_prediction_pi &lt;- tax_model_3_posterior_prediction_samples %&gt;% group_by(tax_rate) %&gt;% summarise(mean = mean(tax_revenue_std), PI_lower = PI(tax_revenue_std)[1], PI_upper = PI(tax_revenue_std)[2]) %&gt;% ungroup() rethinking::compare(model_laffer_0, model_laffer_1, model_laffer_2, model_laffer_3, func = WAIC) %&gt;% knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight model_laffer_1 89.18 22.74 0.00 NA 6.14 0.33 model_laffer_2 89.46 25.26 0.28 3.03 7.35 0.29 model_laffer_0 90.05 21.21 0.87 3.44 5.19 0.22 model_laffer_3 90.67 23.47 1.49 2.49 7.93 0.16 First of all, all models do share a substantial share of the weight within the model comparison making it hard to opt for one in particular. Although the model with the lowest \\(WAIC\\) is in fact the quadratic one, the data falls almost exclusively inside the rising part of the parabola - the situation for the spline is similar. There might be a saturation effect after an initial increase, but since even the flat model (model_laffer_0) rrecives a substantial share of the weight (\\(\\sim\\) 22 %), the other models might just be over fitting the outliers while actually there tax revenue and tax rate might be independent. H2 model_laffer_0_student &lt;- quap( flist = alist( tax_revenue_std ~ dstudent(2, mu, sigma), mu &lt;- alpha + 0 * tax_rate_std, alpha ~ dnorm(0, 0.2), sigma ~ dexp(1) ), data = data_laffer ) tax_model_0_student_posterior_prediction_samples &lt;- link(model_laffer_0_student, data = data.frame(tax_rate_std = tax_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = tax_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;tax_rate_std&quot;, values_to = &quot;tax_revenue_std&quot;) %&gt;% mutate(tax_rate_std = as.numeric(tax_rate_std), tax_revenue = tax_revenue_std * sd(data_laffer$tax_revenue_std) + mean(data_laffer$tax_revenue_std), tax_rate = tax_rate_std * sd(data_laffer$tax_rate) + mean(data_laffer$tax_rate)) tax_model_0_student_posterior_prediction_pi &lt;- tax_model_0_student_posterior_prediction_samples %&gt;% group_by(tax_rate) %&gt;% summarise(mean = mean(tax_revenue_std), PI_lower = PI(tax_revenue_std)[1], PI_upper = PI(tax_revenue_std)[2]) %&gt;% ungroup() model_laffer_1_student &lt;- quap( flist = alist( tax_revenue_std ~ dstudent(2, mu, sigma), mu &lt;- alpha + beta_1 * tax_rate_std, alpha ~ dnorm(0, 0.2), beta_1 ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_laffer ) tax_model_1_student_posterior_prediction_samples &lt;- link(model_laffer_1_student, data = data.frame(tax_rate_std = tax_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = tax_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;tax_rate_std&quot;, values_to = &quot;tax_revenue_std&quot;) %&gt;% mutate(tax_rate_std = as.numeric(tax_rate_std), tax_revenue = tax_revenue_std * sd(data_laffer$tax_revenue_std) + mean(data_laffer$tax_revenue_std), tax_rate = tax_rate_std * sd(data_laffer$tax_rate) + mean(data_laffer$tax_rate)) tax_model_1_student_posterior_prediction_pi &lt;- tax_model_1_student_posterior_prediction_samples %&gt;% group_by(tax_rate) %&gt;% summarise(mean = mean(tax_revenue_std), PI_lower = PI(tax_revenue_std)[1], PI_upper = PI(tax_revenue_std)[2]) %&gt;% ungroup() model_laffer_2_student &lt;- quap( flist = alist( tax_revenue_std ~ dstudent(2, mu, sigma), mu &lt;- alpha + beta_1 * tax_rate_std + beta_2 * tax_rate_std ^ 2, alpha ~ dnorm(0, 0.2), c(beta_1, beta_2) ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_laffer ) tax_model_2_student_posterior_prediction_samples &lt;- link(model_laffer_2_student, data = data.frame(tax_rate_std = tax_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = tax_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;tax_rate_std&quot;, values_to = &quot;tax_revenue_std&quot;) %&gt;% mutate(tax_rate_std = as.numeric(tax_rate_std), tax_revenue = tax_revenue_std * sd(data_laffer$tax_revenue_std) + mean(data_laffer$tax_revenue_std), tax_rate = tax_rate_std * sd(data_laffer$tax_rate) + mean(data_laffer$tax_rate)) tax_model_2_student_posterior_prediction_pi &lt;- tax_model_2_student_posterior_prediction_samples %&gt;% group_by(tax_rate) %&gt;% summarise(mean = mean(tax_revenue_std), PI_lower = PI(tax_revenue_std)[1], PI_upper = PI(tax_revenue_std)[2]) %&gt;% ungroup() model_laffer_3_student &lt;- quap( flist = alist( tax_revenue_std ~ dstudent(2, mu, sigma), mu &lt;- alpha + B %*% w, alpha ~ dnorm(0, .5), w ~ dnorm(0, .5), sigma ~ dexp(1) ), data = list(tax_revenue_std = data_laffer$tax_revenue_std, B = b_spline_laffer), start = list(w = rep(0, ncol(b_spline_laffer))) ) tax_model_3_student_posterior_prediction_samples &lt;- link(model_laffer_3_student, data = list(B = b_spline_tax)) %&gt;% as_tibble() %&gt;% set_names(nm = tax_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;tax_rate_std&quot;, values_to = &quot;tax_revenue_std&quot;) %&gt;% mutate(tax_rate_std = as.numeric(tax_rate_std), tax_revenue = tax_revenue_std * sd(data_laffer$tax_revenue_std) + mean(data_laffer$tax_revenue_std), tax_rate = tax_rate_std * sd(data_laffer$tax_rate) + mean(data_laffer$tax_rate)) tax_model_3_student_posterior_prediction_pi &lt;- tax_model_3_student_posterior_prediction_samples %&gt;% group_by(tax_rate) %&gt;% summarise(mean = mean(tax_revenue_std), PI_lower = PI(tax_revenue_std)[1], PI_upper = PI(tax_revenue_std)[2]) %&gt;% ungroup() get_psis_and_waic &lt;- function(model, model_type, m_idx){ PSIS(model, pointwise = TRUE) %&gt;% as_tibble() %&gt;% dplyr::select(PSIS = PSIS, lppd_psis = lppd, `psis-k` = k) %&gt;% set_names(nm = names(.) %&gt;% str_c(., &quot;_&quot;,model_type,&quot;_m&quot;,m_idx)) %&gt;% bind_cols(WAIC(model, pointwise = TRUE) %&gt;% as_tibble() %&gt;% dplyr::select(WAIC = WAIC, lppd_waic = lppd, `waic-penalty` = penalty) %&gt;% set_names(nm = names(.) %&gt;% str_c(., &quot;_&quot;,model_type,&quot;_m&quot;,m_idx))) } data_laffer_waic &lt;- tibble(model = list(model_laffer_0, model_laffer_1, model_laffer_2, model_laffer_3, model_laffer_0_student, model_laffer_1_student, model_laffer_2_student, model_laffer_3_student), model_type = rep(c(&quot;norm&quot;, &quot;student&quot;), each = 4), m_idx = c(0:3, 0:3)) %&gt;% pmap(get_psis_and_waic) %&gt;% reduce(bind_cols, .init = data_laffer) %&gt;% mutate(rn = row_number()) p0 &lt;- ggplot(mapping = aes(x = tax_rate)) + geom_smooth(data = tax_model_0_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = &quot;normal&quot;, fill = after_scale(clr_alpha(color))), size = .2) + geom_point(data = data_laffer_waic, aes(y = tax_revenue_std, size = WAIC_norm_m0), color = clr_dark, fill = fll0, shape = 21) + geom_text(data = data_laffer_waic %&gt;% filter(rn %in% c(1, 11, 12)), aes(y = tax_revenue_std, label = rn), family = fnt_sel) + labs(subtitle = &quot;0: flat&quot;) p1 &lt;- ggplot(mapping = aes(x = tax_rate)) + geom_smooth(data = tax_model_1_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = &quot;normal&quot;, fill = after_scale(clr_alpha(color))), size = .2) + geom_point(data = data_laffer_waic, aes(y = tax_revenue_std, size = WAIC_norm_m1), color = clr_dark, fill = fll0, shape = 21) + labs(subtitle = &quot;1: linear&quot;) p2 &lt;- ggplot(mapping = aes(x = tax_rate)) + geom_smooth(data = tax_model_2_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = &quot;normal&quot;, fill = after_scale(clr_alpha(color))), size = .2) + geom_point(data = data_laffer_waic, aes(y = tax_revenue_std, size = WAIC_norm_m2), color = clr_dark, fill = fll0, shape = 21) + labs(subtitle = &quot;2: quadratic&quot;) p3 &lt;- ggplot(mapping = aes(x = tax_rate)) + geom_smooth(data = tax_model_3_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = &quot;normal&quot;, fill = after_scale(clr_alpha(color))), size = .2) + geom_point(data = data_laffer_waic, aes(y = tax_revenue_std, size = WAIC_norm_m3), color = clr_dark, fill = fll0, shape = 21) + labs(subtitle = &quot;3: spline&quot;) p0s &lt;- ggplot(mapping = aes(x = tax_rate)) + geom_smooth(data = tax_model_0_student_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = &quot;student&quot;, fill = after_scale(clr_alpha(color))), size = .2) + geom_point(data = data_laffer_waic, aes(y = tax_revenue_std, size = WAIC_student_m0), color = clr_dark, fill = fll0, shape = 21) + labs(subtitle = &quot;0: flat&quot;) p1s &lt;- ggplot(mapping = aes(x = tax_rate)) + geom_smooth(data = tax_model_1_student_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = &quot;student&quot;, fill = after_scale(clr_alpha(color))), size = .2) + geom_point(data = data_laffer_waic, aes(y = tax_revenue_std, size = WAIC_student_m1), color = clr_dark, fill = fll0, shape = 21) + labs(subtitle = &quot;1: linear&quot;) p2s &lt;- ggplot(mapping = aes(x = tax_rate)) + geom_smooth(data = tax_model_2_student_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = &quot;student&quot;, fill = after_scale(clr_alpha(color))), size = .2) + geom_point(data = data_laffer_waic, aes(y = tax_revenue_std, size = WAIC_student_m2), color = clr_dark, fill = fll0, shape = 21) + labs(subtitle = &quot;2: quadratic&quot;) p3s &lt;- ggplot(mapping = aes(x = tax_rate)) + geom_smooth(data = tax_model_3_student_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = &quot;student&quot;, fill = after_scale(clr_alpha(color))), size = .2) + geom_point(data = data_laffer_waic, aes(y = tax_revenue_std, size = WAIC_student_m3), color = clr_dark, fill = fll0, shape = 21) + labs(subtitle = &quot;3: spline&quot;) waic_range &lt;- data_laffer_waic %&gt;% pivot_longer(cols = starts_with(&quot;WAIC&quot;)) %&gt;% .$value %&gt;% range() p0 + p1 + p2 + p3 + p0s + p1s + p2s + p3s + plot_layout(nrow = 2, guides = &quot;collect&quot;) &amp; theme(plot.subtitle = element_text(hjust = .5), legend.position = &quot;bottom&quot;) &amp; scale_size_continuous(&quot;WAIC&quot;, breaks = c(5,10,15,20), range = c(.5, 7), limits = waic_range) &amp; scale_color_manual(&quot;Prior Type&quot;, values = c(normal = clr0d, student = clr1)) &amp; labs(y = &quot;tax_revenue&quot;) data_laffer_waic %&gt;% dplyr::select(c(starts_with(&quot;waic&quot;,ignore.case = FALSE), starts_with(&quot;psis&quot;,ignore.case = FALSE))) %&gt;% mutate(rn = row_number()) %&gt;% pivot_longer(-rn) %&gt;% separate(name, into = c(&quot;statistic&quot;, &quot;model_type&quot;, &quot;model&quot;), sep = &quot;_&quot;) %&gt;% pivot_wider(names_from = &quot;statistic&quot;, values_from = &quot;value&quot;) %&gt;% ggplot(aes(x = `psis-k`, y = `waic-penalty`)) + geom_vline(xintercept = .5, linetype = 3, color = clr_dark) + geom_point(aes(color = model_type, fill = after_scale(clr_alpha(color))), shape = 21, size = 1.5) + ggrepel::geom_text_repel(data = . %&gt;% filter(rn %in% c(1, 11, 12)), aes(label = rn), family = fnt_sel) + facet_grid(model_type ~ model, scales = &quot;free&quot;)+ scale_color_manual(&quot;Prior Type&quot;, values = c(normal = clr0d, student = clr1)) + theme(legend.position = &quot;bottom&quot;, panel.background = element_rect(fill = &quot;transparent&quot;, color = clr0d)) H3 data_birds &lt;- tibble( island = 1:3, species_a = c(.2, .8, .05), species_b = c(.2, .1, .15), species_c = c(.2, .05, .7), species_d = c(.2, .025, .05), species_e = c(.2, .025, .05) ) data_birds %&gt;% group_by(island) %&gt;% mutate(total = sum(across(starts_with(&quot;species&quot;))), entropy = -sum(across(starts_with(&quot;species&quot;), .fns = ~(function(x){x * log(x)})(.x)))) %&gt;% ungroup() #&gt; # A tibble: 3 x 8 #&gt; island species_a species_b species_c species_d species_e total entropy #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 0.2 0.2 0.2 0.2 0.2 1 1.61 #&gt; 2 2 0.8 0.1 0.05 0.025 0.025 1 0.743 #&gt; 3 3 0.05 0.15 0.7 0.05 0.05 1 0.984 The entropy in island 1 is highest as all bird species are equally common. In contrast island 2 has the lowest entropy which translates to the most irregular bird distribution. data_birds_compact &lt;- tibble(island = 1:3, birds = list(data_birds[1, 2:6] %&gt;% unlist(), data_birds[2, 2:6] %&gt;% unlist(), data_birds[3, 2:6] %&gt;% unlist())) kl_div &lt;- function(p,q){ sum(p * log(p/q)) } kl_birds &lt;- function(i1, i2){ kl_div(p = data_birds_compact[i1,]$birds[[1]], q = data_birds_compact[i2,]$birds[[1]]) } cross_df(list(i1 = 1:3, i2 = 1:3 )) %&gt;% mutate(kl_divergence = map2_dbl(i1, i2,.f = kl_birds)) %&gt;% ggplot(aes(x = i1, y = i2, fill = kl_divergence)) + geom_tile() + geom_text(aes(label = round(kl_divergence, digits = 2)), color = &quot;white&quot;, family = fnt_sel) + scale_fill_gradientn(colours = c(clr0d, clr1)) + coord_equal() Islands 1 and 3 predict each other reasonably well - these are the islands that have the highest entropy to begin with and which are thus not easily surprised. Using Island 1 as predictor (i2 / q) does generally produce the lowest KL divergences. H4 dagify(M ~ A + H, coords = tibble(name = c(&quot;M&quot;, &quot;A&quot;, &quot;H&quot;), x = c(.5, 0, 1), y = c(0, 1, 1))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;M&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;H&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(-.05, 1.05)) + scale_x_continuous(limits = c(-.05, 1.05)) + coord_fixed(ratio = .6) chapter6_models$model_happy #&gt; #&gt; Quadratic approximate posterior distribution #&gt; #&gt; Formula: #&gt; happiness ~ dnorm(mu, sigma) #&gt; mu &lt;- alpha + beta_age * age_trans #&gt; alpha ~ dnorm(0, 1) #&gt; beta_age ~ dnorm(0, 2) #&gt; sigma ~ dexp(1) #&gt; #&gt; Posterior means: #&gt; alpha beta_age sigma #&gt; 1.028282e-07 -1.313332e-07 1.210334e+00 #&gt; #&gt; Log-likelihood: -1623.32 chapter6_models$model_happy #&gt; #&gt; Quadratic approximate posterior distribution #&gt; #&gt; Formula: #&gt; happiness ~ dnorm(mu, sigma) #&gt; mu &lt;- alpha + beta_age * age_trans #&gt; alpha ~ dnorm(0, 1) #&gt; beta_age ~ dnorm(0, 2) #&gt; sigma ~ dexp(1) #&gt; #&gt; Posterior means: #&gt; alpha beta_age sigma #&gt; 1.028282e-07 -1.313332e-07 1.210334e+00 #&gt; #&gt; Log-likelihood: -1623.32 rethinking::compare(chapter6_models$model_happy, chapter6_models$model_happy_married, func = WAIC) %&gt;% knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight chapter6_models\\(model_happy_married | 2817.31| 41.49| 0.00| NA| 3.74| 1| |chapter6_models\\)model_happy 3252.08 28.37 434.77 38.87 2.41 0 The model comparison very strongly favors the model including age as predictor of happiness (model_happy_married). This is probably due to the effect that conditioning on marriage opens this collider and introduces a spurious correlation. This correlation can be used to predict inside the small world despite not having any causal justification. H5 data(foxes) data_fox &lt;- foxes %&gt;% as_tibble() %&gt;% drop_na(everything()) %&gt;% mutate(across(-group, standardize, .names = &quot;{str_to_lower(.col)}_std&quot;)) fox_weight_range &lt;- tibble(weight = c(2.2, 14), weight_std = (weight - mean(data_fox$weight))/ sd(data_fox$weight)) dag_fox &lt;- dagify( W ~ F + G, G ~ F, F ~ A, exposure = &quot;A&quot;, outcome = &quot;W&quot;, coords = tibble(name = c(&quot;W&quot;, &quot;F&quot;, &quot;G&quot;, &quot;A&quot;), x = c(.5, 0, 1, .5), y = c(0, .5, .5, 1))) dag_fox %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;W&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;F&quot;, &quot;G&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr2) + scale_y_continuous(limits = c(-.1, 1.1)) + coord_fixed(ratio = .6) model_fox_1 &lt;- quap( flist = alist( weight_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_food * avgfood_std + beta_groupsize * groupsize_std + beta_area * area_std, alpha ~ dnorm(0,.2), c(beta_food, beta_groupsize, beta_area) ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_fox ) model_fox_2 &lt;- quap( flist = alist( weight_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_food * avgfood_std + beta_groupsize * groupsize_std, alpha ~ dnorm(0,.2), c(beta_food, beta_groupsize) ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_fox ) model_fox_3 &lt;- quap( flist = alist( weight_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_groupsize * groupsize_std + beta_area * area_std, alpha ~ dnorm(0,.2), c(beta_groupsize, beta_area) ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_fox ) model_fox_4 &lt;- quap( flist = alist( weight_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_food * avgfood_std, alpha ~ dnorm(0,.2), beta_food ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_fox ) model_fox_5 &lt;- quap( flist = alist( weight_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_area * area_std, alpha ~ dnorm(0,.2), beta_area ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_fox ) compare(model_fox_1, model_fox_2, model_fox_3, model_fox_4, model_fox_5, func = WAIC) %&gt;% knit_precis(param_name = &quot;model&quot;) model WAIC SE dWAIC dSE pWAIC weight model_fox_1 323.46 16.32 0.00 NA 4.96 0.39 model_fox_2 323.77 16.09 0.31 3.62 3.67 0.34 model_fox_3 324.26 15.79 0.80 2.96 3.89 0.26 model_fox_4 333.54 13.85 10.08 7.22 2.46 0.00 model_fox_5 333.89 13.80 10.43 7.27 2.72 0.00 There are two groups of models (A: model_fox_1-3 ans B: model_fox_4-5). The group A closes all the backdoor paths into \\(W\\). Also in group B, conditioning on A or F is basically equivalent because F is a descendant of A and an intermediate between A and the rest of the DAG. 8.6 {brms} section 8.6.1 The problem with parameters 8.6.1.1 More parameters (almost) always improve fit. brms_c7_model_brain_size &lt;- brm( data = data_brainsize, family = gaussian, brain_size_scl ~ 1 + mass_std, prior = c(prior(normal(0.5, 1), class = Intercept), prior(normal(0, 10), class = b), prior(lognormal(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c7_model_brain_size&quot; ) mixedup::summarise_model(brms_c7_model_brain_size) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 0.07 0.27 0.13 0.60 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; Intercept 0.53 0.11 0.32 0.76 #&gt; mass_std 0.17 0.12 -0.07 0.41 brms_R2_is_bad &lt;- function(brm_fit, seed = 7, ...) { set.seed(seed) p &lt;- brms:::predict.brmsfit(brm_fit, summary = F, ...) r &lt;- apply(p, 2, mean) - data_brainsize$brain_size_scl 1 - rethinking::var2(r) / rethinking::var2(data_brainsize$brain_size_scl) } brms_R2_is_bad(brms_c7_model_brain_size) #&gt; [1] 0.4873914 brms_c7_model_brain_size2 &lt;- update( brms_c7_model_brain_size, newdata = data_brainsize, formula = brain_size_scl ~ 1 + mass_std + I(mass_std^2), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c7_model_brain_size2&quot;) brms_c7_model_brain_size3 &lt;- update( brms_c7_model_brain_size, newdata = data_brainsize, formula = brain_size_scl ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, control = list(adapt_delta = .995), file = &quot;brms/brms_c7_model_brain_size3&quot;) brms_c7_model_brain_size4 &lt;- update( brms_c7_model_brain_size, newdata = data_brainsize, formula = brain_size_scl ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^4), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, control = list(adapt_delta = .9995, max_treedepth = 15), file = &quot;brms/brms_c7_model_brain_size4&quot;) brms_c7_model_brain_size5 &lt;- update( brms_c7_model_brain_size, newdata = data_brainsize, formula = brain_size_scl ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^5), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, control = list(adapt_delta = .99995, max_treedepth = 15), file = &quot;brms/brms_c7_model_brain_size5&quot;) defining a custom response distribution (setting \\(\\sigma\\) to a constant value to be in line with the model of the sixth order polynomial). custom_normal &lt;- custom_family( &quot;custom_normal&quot;, dpars = &quot;mu&quot;, links = &quot;identity&quot;, type = &quot;real&quot; ) stan_funs &lt;- &quot;real custom_normal_lpdf(real y, real mu) { return normal_lpdf(y | mu, 0.001); } real custom_normal_rng(real mu) { return normal_rng(mu, 0.001); } &quot; stanvars &lt;- stanvar(scode = stan_funs, block = &quot;functions&quot;) brms_c7_model_brain_size6 &lt;- brm( data = data_brainsize, family = custom_normal, brain_size_scl ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^4) + I(mass_std^5) + I(mass_std^6), prior = c(prior(normal(0.5, 1), class = Intercept), prior(normal(0, 10), class = b)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, stanvars = stanvars, control = list(max_treedepth = 15), file = &quot;brms/brms_c7_model_brain_size6&quot;) Defining custom functions to work with the special case for the last model (which includes family = custom_normal) expose_functions(brms_c7_model_brain_size6, vectorize = TRUE) #&gt; Running /usr/local/lib/R/bin/R CMD SHLIB foo.c #&gt; gcc -I&quot;/usr/local/lib/R/include&quot; -DNDEBUG -I&quot;/home/kluk/R/x86_64-pc-linux-gnu-library/4.0/Rcpp/include/&quot; -I&quot;/home/kluk/R/x86_64-pc-linux-gnu-library/4.0/RcppEigen/include/&quot; -I&quot;/home/kluk/R/x86_64-pc-linux-gnu-library/4.0/RcppEigen/include/unsupported&quot; -I&quot;/home/kluk/R/x86_64-pc-linux-gnu-library/4.0/BH/include&quot; -I&quot;/home/kluk/R/x86_64-pc-linux-gnu-library/4.0/StanHeaders/include/src/&quot; -I&quot;/home/kluk/R/x86_64-pc-linux-gnu-library/4.0/StanHeaders/include/&quot; -I&quot;/home/kluk/R/x86_64-pc-linux-gnu-library/4.0/RcppParallel/include/&quot; -I&quot;/home/kluk/R/x86_64-pc-linux-gnu-library/4.0/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DBOOST_NO_AUTO_PTR -include &#39;/home/kluk/R/x86_64-pc-linux-gnu-library/4.0/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -I/usr/local/include -fpic -g -O2 -c foo.c -o foo.o #&gt; In file included from /home/kluk/R/x86_64-pc-linux-gnu-library/4.0/RcppEigen/include/Eigen/Core:88, #&gt; from /home/kluk/R/x86_64-pc-linux-gnu-library/4.0/RcppEigen/include/Eigen/Dense:1, #&gt; from /home/kluk/R/x86_64-pc-linux-gnu-library/4.0/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13, #&gt; from &lt;command-line&gt;: #&gt; /home/kluk/R/x86_64-pc-linux-gnu-library/4.0/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name ‘namespace’ #&gt; 628 | namespace Eigen { #&gt; | ^~~~~~~~~ #&gt; /home/kluk/R/x86_64-pc-linux-gnu-library/4.0/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:17: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘{’ token #&gt; 628 | namespace Eigen { #&gt; | ^ #&gt; In file included from /home/kluk/R/x86_64-pc-linux-gnu-library/4.0/RcppEigen/include/Eigen/Dense:1, #&gt; from /home/kluk/R/x86_64-pc-linux-gnu-library/4.0/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13, #&gt; from &lt;command-line&gt;: #&gt; /home/kluk/R/x86_64-pc-linux-gnu-library/4.0/RcppEigen/include/Eigen/Core:96:10: fatal error: complex: No such file or directory #&gt; 96 | #include &lt;complex&gt; #&gt; | ^~~~~~~~~ #&gt; compilation terminated. #&gt; make: *** [/usr/local/lib/R/etc/Makeconf:172: foo.o] Error 1 posterior_epred_custom_normal &lt;- function(prep) { mu &lt;- prep$dpars$mu mu } posterior_predict_custom_normal &lt;- function(i, prep, ...) { mu &lt;- prep$dpars$mu mu custom_normal_rng(mu) } log_lik_custom_normal &lt;- function(i, prep) { mu &lt;- prep$dpars$mu y &lt;- prep$data$Y[i] custom_normal_lpdf(y, mu) } make_r2_figure &lt;- function(brms_fit, ylim = range(data_brainsize$brain_size_scl)) { # compute the R2 r2 &lt;- brms_R2_is_bad(brms_fit) # define the new data nd &lt;- tibble(mass_std = seq(from = -2, to = 2, length.out = 200)) # simulate and wrangle fitted(brms_fit, newdata = nd, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% # plot! ggplot(aes(x = mass_std)) + geom_lineribbon(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5), color = clr0dd, size = 1/2, fill = fll0) + geom_point(data = data_brainsize, aes(y = brain_size_scl), color = clr_dark) + labs(subtitle = bquote(italic(R)^2==.(round(r2, digits = 2))), x = &quot;body mass (std)&quot;, y = &quot;brain volume (std)&quot;) + coord_cartesian(xlim = c(-1.2, 1.5), ylim = ylim) } tibble(brms_fit = list(brms_c7_model_brain_size, brms_c7_model_brain_size2, brms_c7_model_brain_size3, brms_c7_model_brain_size4, brms_c7_model_brain_size5, brms_c7_model_brain_size6), ylim = list(range(data_brainsize$brain_size_scl),range(data_brainsize$brain_size_scl), range(data_brainsize$brain_size_scl), c(.25, 1.1), c(.1, 1.4), c(-0.25, 1.5))) %&gt;% pmap(make_r2_figure) %&gt;% wrap_plots(nrow = 2) 8.6.1.2 Too few parameters hurts, too brain_loo_lines &lt;- function(brms_fit, row, ...) { nd &lt;- tibble(mass_std = seq(from = -2, to = 2, length.out = 200)) # refit the model new_fit &lt;- update(brms_fit, newdata = filter(data_brainsize, row_number() != row), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, refresh = 0, ...) # pull the lines values fitted(new_fit, newdata = nd) %&gt;% data.frame() %&gt;% select(Estimate) %&gt;% bind_cols(nd) } poly1_fits &lt;- tibble(row = 1:7) %&gt;% mutate(post = purrr::map(row, ~brain_loo_lines(brms_fit = brms_c7_model_brain_size, row = .))) %&gt;% unnest(post) poly4_fits &lt;- tibble(row = 1:7) %&gt;% mutate(post = purrr::map(row, ~brain_loo_lines(brms_fit = brms_c7_model_brain_size4, row = ., control = list(adapt_delta = .9995)))) %&gt;% unnest(post) p1 &lt;- poly1_fits %&gt;% ggplot(aes(x = mass_std, y = Estimate)) + coord_cartesian(xlim = range(data_brainsize$mass_std), ylim = range(data_brainsize$brain_size_scl)) + labs(subtitle = &quot;brms_c7_model_brain_size&quot;) p2 &lt;- poly4_fits %&gt;% ggplot(aes(x = mass_std, y = Estimate)) + coord_cartesian(xlim = range(data_brainsize$mass_std), ylim = c(-0.1, 1.4)) + labs(subtitle = &quot;brms_c7_model_brain_size4&quot;) p1 + p2 &amp; geom_line(aes(group = row), color = clr0d, size = 1/2, alpha = 1/2) &amp; geom_point(data = data_brainsize, aes(y = brain_size_scl), color = clr_dark) 8.6.2 Entropy and accuracy 8.6.2.1 Firing the weatherperson Current weatherperson weatherperson &lt;- tibble(prediction = rep(c(1,.6), c(3,7)), observed = rep(c(emo::ji(&quot;cloud_with_rain&quot;), emo::ji(&quot;sunny&quot;)), c(3,7))) weatherperson %&gt;% t() %&gt;% knitr::kable(col.names = 1:10) 1 2 3 4 5 6 7 8 9 10 prediction 1.0 1.0 1.0 0.6 0.6 0.6 0.6 0.6 0.6 0.6 observed 🌧 🌧 🌧 ☀️ ☀️ ☀️ ☀️ ☀️ ☀️ ☀️ newcomer newcomer &lt;- tibble(prediction = rep(0, 10), observed = rep(c(emo::ji(&quot;cloud_with_rain&quot;), emo::ji(&quot;sunny&quot;)), c(3,7))) newcomer %&gt;% t() %&gt;% knitr::kable(col.names = 1:10) 1 2 3 4 5 6 7 8 9 10 prediction 0 0 0 0 0 0 0 0 0 0 observed 🌧 🌧 🌧 ☀️ ☀️ ☀️ ☀️ ☀️ ☀️ ☀️ weather_happy &lt;- weatherperson %&gt;% rename(weatherperson = &quot;prediction&quot;) %&gt;% mutate(newcomer = newcomer$prediction, observed_code = if_else(observed == emo::ji(&quot;cloud_with_rain&quot;), 1, 0), day = row_number()) %&gt;% pivot_longer(c(weatherperson, newcomer), names_to = &quot;person&quot;, values_to = &quot;prediction&quot;) %&gt;% mutate(hit = ifelse(prediction == observed_code, 1, 1 - prediction - observed_code), happy = if_else(prediction == observed_code, if_else(observed_code == 1, -1, 0), if_else(observed_code == 1, -5 * (observed_code - prediction), -1 * (prediction - observed_code)))) weather_happy %&gt;% pivot_wider(id_cols = observed:day, names_from = &quot;person&quot;, values_from = prediction:happy) %&gt;% dplyr::select(-day) %&gt;% t() %&gt;% knitr::kable(col.names = 1:10) 1 2 3 4 5 6 7 8 9 10 observed 🌧 🌧 🌧 ☀️ ☀️ ☀️ ☀️ ☀️ ☀️ ☀️ observed_code 1 1 1 0 0 0 0 0 0 0 prediction_weatherperson 1.0 1.0 1.0 0.6 0.6 0.6 0.6 0.6 0.6 0.6 prediction_newcomer 0 0 0 0 0 0 0 0 0 0 hit_weatherperson 1.0 1.0 1.0 0.4 0.4 0.4 0.4 0.4 0.4 0.4 hit_newcomer 0 0 0 1 1 1 1 1 1 1 happy_weatherperson -1.0 -1.0 -1.0 -0.6 -0.6 -0.6 -0.6 -0.6 -0.6 -0.6 happy_newcomer -5 -5 -5 0 0 0 0 0 0 0 weather_happy %&gt;% group_by(person) %&gt;% summarise(total_hit = sum(hit), total_happy = sum(happy)) #&gt; # A tibble: 2 x 3 #&gt; person total_hit total_happy #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 newcomer 7 -15 #&gt; 2 weatherperson 5.8 -7.2 weather_happy %&gt;% count(person, hit) %&gt;% mutate(power = hit ^ n) %&gt;% group_by(person) %&gt;% summarise(joint_likelihood = prod(power)) #&gt; # A tibble: 2 x 2 #&gt; person joint_likelihood #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 newcomer 0 #&gt; 2 weatherperson 0.00164 8.6.2.2 Divergence depends upon direction tibble(direction = c(&quot;Earth to Mars&quot;, &quot;Mars to Earth&quot;), p_1 = c(.01, .7), q_1 = c(.7, .01)) %&gt;% mutate(p_2 = 1 - p_1, q_2 = 1 - q_1) %&gt;% mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2))) #&gt; # A tibble: 2 x 6 #&gt; direction p_1 q_1 p_2 q_2 d_kl #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Earth to Mars 0.01 0.7 0.99 0.3 1.14 #&gt; 2 Mars to Earth 0.7 0.01 0.3 0.99 2.62 log-pointwise-predictive-density (lppd) with {brms} (not included, we have to do it manually) (lppd_by_sepc &lt;- brms_c7_model_brain_size %&gt;% log_lik() %&gt;% as_tibble() %&gt;% set_names(pull(data_brainsize, species)) %&gt;% pivot_longer(everything(), names_to = &quot;species&quot;, values_to = &quot;log_prob&quot;) %&gt;% mutate(prob = exp(log_prob)) %&gt;% group_by(species) %&gt;% summarise(log_probability_score = mean(prob) %&gt;% log()) %&gt;% ungroup()) #&gt; # A tibble: 7 x 2 #&gt; species log_probability_score #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 afarensis 0.379 #&gt; 2 africanus 0.406 #&gt; 3 boisei 0.392 #&gt; 4 ergaster 0.225 #&gt; 5 habilis 0.325 #&gt; 6 rudolfensis 0.264 #&gt; 7 sapiens -0.593 lppd_by_sepc %&gt;% summarise(total_log_probability_score = sum(log_probability_score)) #&gt; # A tibble: 1 x 1 #&gt; total_log_probability_score #&gt; &lt;dbl&gt; #&gt; 1 1.40 8.6.2.3 Computing the lppd brms_log_prob &lt;- brms_c7_model_brain_size %&gt;% log_lik() %&gt;% as_tibble() brms_prob &lt;- brms_log_prob %&gt;% set_names(pull(data_brainsize, species)) %&gt;% # add an s iteration index, for convenience mutate(s = 1:n()) %&gt;% # make it long pivot_longer(-s, names_to = &quot;species&quot;, values_to = &quot;log_prob&quot;) %&gt;% # compute the probability scores mutate(prob = exp(log_prob)) brms_prob_score &lt;- brms_prob %&gt;% group_by(species) %&gt;% summarise(log_probability_score = mean(prob) %&gt;% log()) %&gt;% ungroup() brms_prob_score %&gt;% summarise(total_log_probability_score = sum(log_probability_score)) #&gt; # A tibble: 1 x 1 #&gt; total_log_probability_score #&gt; &lt;dbl&gt; #&gt; 1 1.40 8.6.2.4 Scoring the right data brms_lppd &lt;- function(brms_fit) { log_lik(brms_fit) %&gt;% data.frame() %&gt;% pivot_longer(everything(), values_to = &quot;log_prob&quot;) %&gt;% mutate(prob = exp(log_prob)) %&gt;% group_by(name) %&gt;% summarise(log_probability_score = mean(prob) %&gt;% log()) %&gt;% summarise(total_log_probability_score = sum(log_probability_score)) } tibble(name = str_c(&quot;brms_c7_model_brain_size&quot;, c(&quot;&quot;,2:6))) %&gt;% mutate(brms_fit = purrr::map(name, get)) %&gt;% mutate(lppd = purrr::map(brms_fit, ~brms_lppd(.))) %&gt;% unnest(lppd) #&gt; # A tibble: 6 x 3 #&gt; name brms_fit total_log_probability_score #&gt; &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; #&gt; 1 brms_c7_model_brain_size &lt;brmsfit&gt; 1.40 #&gt; 2 brms_c7_model_brain_size2 &lt;brmsfit&gt; 0.557 #&gt; 3 brms_c7_model_brain_size3 &lt;brmsfit&gt; 0.608 #&gt; 4 brms_c7_model_brain_size4 &lt;brmsfit&gt; -0.213 #&gt; 5 brms_c7_model_brain_size5 &lt;brmsfit&gt; 5.87 #&gt; 6 brms_c7_model_brain_size6 &lt;brmsfit&gt; 26.0 8.6.3 Information criteria 8.6.3.1 WAIC calculation brms_c7_model_cars &lt;- brm(data = cars, family = gaussian, dist ~ 1 + speed, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c7_model_cars&quot;) n_cases &lt;- nrow(cars) log_likelihood_i &lt;- log_lik(brms_c7_model_cars) %&gt;% as_tibble() %&gt;% set_names(str_pad(1:n_cases,width = 2, pad = 0)) dim(log_likelihood_i) #&gt; [1] 4000 50 log_mu_l &lt;- log_likelihood_i %&gt;% pivot_longer(everything(), names_to = &quot;i&quot;, values_to = &quot;log_likelihood&quot;) %&gt;% mutate(likelihood = exp(log_likelihood)) %&gt;% group_by(i) %&gt;% summarise(log_mean_likelihood = mean(likelihood) %&gt;% log()) ( cars_lppd &lt;- log_mu_l %&gt;% summarise(lppd = sum(log_mean_likelihood)) %&gt;% pull(lppd) ) #&gt; [1] -206.62 computing \\(p_{WAIC}\\) and \\(V(y_{i})\\) (varinace in the log-likelihood) v_i &lt;- log_likelihood_i %&gt;% pivot_longer(everything(), names_to = &quot;i&quot;, values_to = &quot;log_likelihood&quot;) %&gt;% group_by(i) %&gt;% summarise(var_loglikelihood = var(log_likelihood)) pwaic &lt;- v_i %&gt;% summarise(pwaic = sum(var_loglikelihood)) %&gt;% pull() pwaic #&gt; [1] 4.080994 \\(WAIC = -2 (lppd - p_{WAIC})\\) -2 * (cars_lppd - pwaic) #&gt; [1] 421.402 {brms} function: waic(brms_c7_model_cars) #&gt; #&gt; Computed from 4000 by 50 log-likelihood matrix #&gt; #&gt; Estimate SE #&gt; elpd_waic -210.7 8.2 #&gt; p_waic 4.1 1.5 #&gt; waic 421.4 16.4 #&gt; #&gt; 2 (4.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. Pointwise values: waic(brms_c7_model_cars)$pointwise %&gt;% as_tibble() #&gt; # A tibble: 50 x 3 #&gt; elpd_waic p_waic waic #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 -3.65 0.0217 7.30 #&gt; 2 -4.02 0.0949 8.05 #&gt; 3 -3.68 0.0209 7.37 #&gt; 4 -4.00 0.0597 7.99 #&gt; 5 -3.59 0.0102 7.18 #&gt; 6 -3.74 0.0210 7.48 #&gt; 7 -3.60 0.0103 7.21 #&gt; 8 -3.62 0.0110 7.23 #&gt; 9 -3.98 0.0351 7.96 #&gt; 10 -3.77 0.0171 7.54 #&gt; # … with 40 more rows 8.6.4 Model comparison 8.6.4.1 Model mis-selection brms_c6_model_fungus_post_treatment &lt;- read_rds(&quot;brms/brms_c6_model_fungus_post_treatment.rds&quot;) brms_c6_model_fungus_no_treatment &lt;- read_rds(&quot;brms/brms_c6_model_fungus_no_treatment.rds&quot;) brms_c6_model_fungus_only_treatment &lt;- read_rds(&quot;brms/brms_c6_model_fungus_only_treatment.rds&quot;) brms_c6_model_fungus_post_treatment &lt;- add_criterion(brms_c6_model_fungus_post_treatment, criterion = &quot;waic&quot;) brms_c6_model_fungus_no_treatment &lt;- add_criterion(brms_c6_model_fungus_no_treatment, criterion = &quot;waic&quot;) brms_c6_model_fungus_only_treatment &lt;- add_criterion(brms_c6_model_fungus_only_treatment, criterion = &quot;waic&quot;) brms_fungus_compare &lt;- loo_compare(brms_c6_model_fungus_post_treatment, brms_c6_model_fungus_no_treatment, brms_c6_model_fungus_only_treatment, criterion = &quot;waic&quot;) print(brms_fungus_compare, simplify = FALSE) #&gt; elpd_diff se_diff elpd_waic se_elpd_waic #&gt; brms_c6_model_fungus_post_treatment 0.0 0.0 -180.7 6.7 #&gt; brms_c6_model_fungus_only_treatment -20.6 4.9 -201.3 5.4 #&gt; brms_c6_model_fungus_no_treatment -22.3 5.8 -203.0 5.7 #&gt; p_waic se_p_waic waic se_waic #&gt; brms_c6_model_fungus_post_treatment 3.4 0.5 361.4 13.5 #&gt; brms_c6_model_fungus_only_treatment 2.5 0.3 402.7 10.8 #&gt; brms_c6_model_fungus_no_treatment 1.6 0.2 406.0 11.4 With respect to the output, notice the elpd_diff column and the adjacent se_diff column. Those are our WAIC differences in the elpd metric. The models have been rank ordered from the highest (i.e., brms_c6_model_fungus_post_treatment) to the lowest (i.e., brms_c6_model_fungus_no_treatment). The scores listed are the differences of brms_c6_model_fungus_post_treatment minus the comparison model. Since brms_c6_model_fungus_post_treatment is the comparison model in the top row, the values are naturally 0 (i.e., \\(x−x=0\\)). But now here’s another critical thing to understand: Since the {brms} version 2.8.0 update, WAIC and LOO differences are no longer reported in the \\(−2\\times x\\) metric. Remember how multiplying (lppd - pwaic) by -2 is a historic artifact associated with the frequentist \\(\\chi^2\\) test? We’ll, the makers of the {loo} package aren’t fans and they no longer support the conversion. So here’s the deal. The substantive interpretations of the differences presented in an elpd_diff metric will be the same as if presented in a WAIC metric. But if we want to compare our elpd_diff results to those in the text, we will have to multiply them by -2. And also, if we want the associated standard error in the same metric, we’ll need to multiply the se_diff column by 2 brms_c6_model_fungus_post_treatment &lt;- add_criterion(brms_c6_model_fungus_post_treatment, criterion = &quot;loo&quot;) brms_c6_model_fungus_no_treatment &lt;- add_criterion(brms_c6_model_fungus_no_treatment, criterion = &quot;loo&quot;) brms_c6_model_fungus_only_treatment &lt;- add_criterion(brms_c6_model_fungus_only_treatment, criterion = &quot;loo&quot;) loo_compare(brms_c6_model_fungus_post_treatment, brms_c6_model_fungus_no_treatment, brms_c6_model_fungus_only_treatment, criterion = &quot;loo&quot;) %&gt;% print(simplify = FALSE) #&gt; elpd_diff se_diff elpd_loo se_elpd_loo #&gt; brms_c6_model_fungus_post_treatment 0.0 0.0 -180.7 6.7 #&gt; brms_c6_model_fungus_only_treatment -20.6 4.9 -201.3 5.4 #&gt; brms_c6_model_fungus_no_treatment -22.3 5.8 -203.0 5.7 #&gt; p_loo se_p_loo looic se_looic #&gt; brms_c6_model_fungus_post_treatment 3.4 0.5 361.5 13.5 #&gt; brms_c6_model_fungus_only_treatment 2.6 0.3 402.7 10.8 #&gt; brms_c6_model_fungus_no_treatment 1.6 0.2 406.0 11.4 computing the standard error of the WAIC difference for brms_c6_model_fungus_post_treatment and brms_c6_model_fungus_only_treatment n &lt;- length(brms_c6_model_fungus_no_treatment$criteria$waic$pointwise[, &quot;waic&quot;]) tibble(waic_no_treatment = brms_c6_model_fungus_no_treatment$criteria$waic$pointwise[, &quot;waic&quot;], waic_post_treatment = brms_c6_model_fungus_post_treatment$criteria$waic$pointwise[, &quot;waic&quot;]) %&gt;% mutate(diff = waic_no_treatment - waic_post_treatment) %&gt;% summarise(diff_se = sqrt(n * var(diff))) #&gt; # A tibble: 1 x 1 #&gt; diff_se #&gt; &lt;dbl&gt; #&gt; 1 11.6 brms_fungus_compare[2, 2] * 2 #&gt; [1] 9.840665 (brms_fungus_compare[2, 1] * -2) + c(-1, 1) * (brms_fungus_compare[2, 2] * 2) * 2.6 #&gt; [1] 15.65820 66.82966 brms_fungus_compare[, 7:8] %&gt;% data.frame() %&gt;% rownames_to_column(&quot;model_name&quot;) %&gt;% as_tibble() %&gt;% mutate(model_name = fct_reorder(model_name, waic, .desc = TRUE)) %&gt;% ggplot(aes(x = waic, y = model_name, xmin = waic - se_waic, xmax = waic + se_waic)) + geom_pointrange(color = clr0dd, fill = clr0, shape = 21) + labs(title = &quot;custom WAIC plot&quot;, x = NULL, y = NULL) + theme(axis.ticks.y = element_blank()) in {brms}, weights need to be computed explicitly for the models model_weights(brms_c6_model_fungus_post_treatment, brms_c6_model_fungus_no_treatment, brms_c6_model_fungus_only_treatment, weights = &quot;waic&quot;) %&gt;% round(digits = 2) #&gt; brms_c6_model_fungus_post_treatment brms_c6_model_fungus_no_treatment #&gt; 1 0 #&gt; brms_c6_model_fungus_only_treatment #&gt; 0 8.6.5 Outliers and other illusions brms_c5_model_age &lt;- read_rds(&quot;brms/brms_c5_model_age.rds&quot;) %&gt;% add_criterion(criterion = &quot;loo&quot;) brms_c5_model_marriage &lt;- read_rds(&quot;brms/brms_c5_model_marriage.rds&quot;) %&gt;% add_criterion(criterion = &quot;loo&quot;) brms_c5_model_multiple &lt;- read_rds(&quot;brms/brms_c5_model_multiple.rds&quot;) %&gt;% add_criterion(criterion = &quot;loo&quot;) loo_compare(brms_c5_model_age, brms_c5_model_marriage, brms_c5_model_multiple, criterion = &quot;loo&quot;) %&gt;% print(simplify = FALSE) #&gt; elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo #&gt; brms_c5_model_age 0.0 0.0 -63.0 6.5 3.7 1.8 #&gt; brms_c5_model_multiple -0.8 0.4 -63.8 6.4 4.7 1.9 #&gt; brms_c5_model_marriage -6.8 4.7 -69.7 5.0 3.0 0.9 #&gt; looic se_looic #&gt; brms_c5_model_age 125.9 12.9 #&gt; brms_c5_model_multiple 127.5 12.8 #&gt; brms_c5_model_marriage 139.4 10.0 loo(brms_c5_model_multiple) #&gt; #&gt; Computed from 4000 by 50 log-likelihood matrix #&gt; #&gt; Estimate SE #&gt; elpd_loo -63.8 6.4 #&gt; p_loo 4.7 1.9 #&gt; looic 127.5 12.8 #&gt; ------ #&gt; Monte Carlo SE of elpd_loo is 0.1. #&gt; #&gt; All Pareto k estimates are good (k &lt; 0.5). #&gt; See help(&#39;pareto-k-diagnostic&#39;) for details. library(loo) loo(brms_c5_model_multiple) %&gt;% pareto_k_ids(threshold = 0.4) #&gt; [1] 13 20 chapter5_models$data_waffle %&gt;% slice(13) %&gt;% select(Location:Loc) #&gt; # A tibble: 1 x 2 #&gt; Location Loc #&gt; &lt;fct&gt; &lt;fct&gt; #&gt; 1 Idaho ID pareto_k_values(loo(brms_c5_model_multiple))[13] #&gt; [1] 0.4485035 brms_c5_model_multiple &lt;- add_criterion(brms_c5_model_multiple, &quot;waic&quot;, file = &quot;brms/brms_c5_model_multiple&quot;) p1 &lt;- tibble(pareto_k = brms_c5_model_multiple$criteria$loo$diagnostics$pareto_k, p_waic = brms_c5_model_multiple$criteria$waic$pointwise[, &quot;p_waic&quot;], Loc = pull(chapter5_models$data_waffle, Loc)) %&gt;% ggplot(aes(x = pareto_k, y = p_waic, color = Loc == &quot;ID&quot;)) + geom_vline(xintercept = .5, linetype = 3, color = clr_dark) + geom_point(aes(shape = Loc == &quot;ID&quot;)) + geom_text(data = . %&gt;% filter(p_waic &gt; 0.5), aes(x = pareto_k - 0.03, label = Loc), hjust = 1) + scale_color_manual(values = c(clr0dd, clr2)) + scale_shape_manual(values = c(1, 19)) + labs(subtitle = &quot;Gaussian model (brms_c5_model_multiple)&quot;) + theme(legend.position = &quot;none&quot;) To use the Student-t distribution in {brms} set family = student (note the inclusion of \\(\\nu\\) in bf()) brms_c7_model_multiple &lt;- brm( data = chapter5_models$data_waffle, family = student, bf(divorce_std ~ 1 + marriage_std + median_age_std, nu = 2), prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c7_model_multiple&quot;) brms_c7_model_multiple &lt;- add_criterion(brms_c7_model_multiple, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) p2 &lt;- tibble(pareto_k = brms_c7_model_multiple$criteria$loo$diagnostics$pareto_k, p_waic = brms_c7_model_multiple$criteria$waic$pointwise[, &quot;p_waic&quot;], Loc = pull(chapter5_models$data_waffle, Loc)) %&gt;% ggplot(aes(x = pareto_k, y = p_waic, color = Loc == &quot;ID&quot;)) + geom_vline(xintercept = .5, linetype = 3, color = clr_dark) + geom_point(aes(shape = Loc == &quot;ID&quot;)) + geom_text(data = . %&gt;% filter(p_waic &gt; 0.5), aes(x = pareto_k - 0.03, label = Loc), hjust = 1) + scale_color_manual(values = c(clr0dd, clr2)) + scale_shape_manual(values = c(1, 19)) + labs(subtitle = &quot;Student-t model (brms_c7_model_multiple)&quot;) + theme(legend.position = &quot;none&quot;) p1 + p2 loo_compare(brms_c5_model_multiple, brms_c7_model_multiple, criterion = &quot;waic&quot;) %&gt;% print(simplify = FALSE) #&gt; elpd_diff se_diff elpd_waic se_elpd_waic p_waic #&gt; brms_c5_model_multiple 0.0 0.0 -63.7 6.4 4.6 #&gt; brms_c7_model_multiple -2.7 3.0 -66.4 5.8 6.1 #&gt; se_p_waic waic se_waic #&gt; brms_c5_model_multiple 1.8 127.3 12.7 #&gt; brms_c7_model_multiple 1.0 132.8 11.6 bind_rows(as_draws_df(brms_c5_model_multiple), as_draws_df(brms_c7_model_multiple)) %&gt;% as_tibble() %&gt;% mutate(fit = rep(c(&quot;Gaussian (brms_c5_model_multiple)&quot;, &quot;Student-t (brms_c7_model_multiple)&quot;), each = n() / 2)) %&gt;% pivot_longer(b_Intercept:sigma) %&gt;% mutate(name = factor(name, levels = c(&quot;b_Intercept&quot;, &quot;b_median_age_std&quot;, &quot;b_marriage_std&quot;, &quot;sigma&quot;), labels = c(&quot;alpha&quot;, &quot;beta[a]&quot;, &quot;beta[m]&quot;, &quot;sigma&quot;))) %&gt;% ggplot(aes(x = value, y = fit, color = fit)) + stat_pointinterval(.width = .95, aes(fill = after_scale(clr_lighten(color))), size = 3, shape = 21) + facet_wrap(~ name, ncol = 1)+ scale_color_manual(values = c(clr0dd, clr2), guide = &quot;none&quot;) + labs(x = &quot;posterior&quot;, y = NULL) 8.6.6 {brms} \\(r^2\\) bayes_R2(brms_c5_model_multiple) %&gt;% round(digits = 3) %&gt;% as_tibble() %&gt;% knitr::kable() Estimate Est.Error Q2.5 Q97.5 0.329 0.087 0.149 0.479 rbind(bayes_R2(brms_c5_model_multiple), bayes_R2(brms_c5_model_age), bayes_R2(brms_c5_model_marriage)) %&gt;% as_tibble() %&gt;% mutate(model = c(&quot;brms_c5_model_multiple&quot;, &quot;brms_c5_model_age&quot;, &quot;brms_c5_model_marriage&quot;), r_square_posterior_mean = round(Estimate, digits = 3)) %&gt;% select(model, r_square_posterior_mean) #&gt; # A tibble: 3 x 2 #&gt; model r_square_posterior_mean #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 brms_c5_model_multiple 0.329 #&gt; 2 brms_c5_model_age 0.326 #&gt; 3 brms_c5_model_marriage 0.13 brms_r2 &lt;- cbind(bayes_R2(brms_c5_model_multiple, summary = FALSE), bayes_R2(brms_c5_model_age, summary = FALSE), bayes_R2(brms_c5_model_marriage, summary = FALSE)) %&gt;% as_tibble() %&gt;% set_names(c(&quot;multiple&quot;, &quot;age&quot;, &quot;marriage&quot;)) p1 &lt;- brms_r2 %&gt;% pivot_longer(everything(), names_to = &quot;model&quot;, values_to = &quot;r2&quot;) %&gt;% ggplot() + geom_density(aes(x = r2, color = model, fill = after_scale(clr_alpha(color))), size = .5, adjust = .5) + scale_x_continuous(NULL, limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL) + scale_color_manual(values = c(age = clr0, marriage = clr0dd, multiple = clr2)) + labs(subtitle = expression(italic(R)^2~distributions)) + facet_wrap(model ~ ., ncol = 1) + theme(legend.position = &quot;none&quot;) p2 &lt;- brms_r2 %&gt;% mutate(diff = multiple - marriage) %&gt;% ggplot(aes(x = diff, y = 0)) + stat_halfeye(point_interval = median_qi, .width = .95, shape = 21, fill = fll0, color = clr0dd) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(italic(marriage)~has~lower~italic(R)^2~than~italic(multiple)), x = expression(Delta*italic(R)^2)) p1 + p2 8.7 pymc3 section × "],
["rethinking-chapter-8.html", "9 Rethinking: Chapter 8 9.1 Building an Interaction 9.2 Symetry of Interactions 9.3 Continuous Interaction 9.4 Homework 9.5 {brms} section 9.6 pymc3 section", " 9 Rethinking: Chapter 8 Conditional Manatees by Richard McElreath, building on the Summaries by Solomon Kurz. 9.1 Building an Interaction Investigating how ruggedness influences countries GDP, conditional on whether the country is African or not. library(rethinking) p_dag1 &lt;- dagify(G ~ R + C + U, R ~ U, coords = tibble(name = c(&quot;R&quot;, &quot;G&quot;, &quot;C&quot;, &quot;U&quot;), x = c(0, .5, 1, .5), y = c(1, 1, 1, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;G&quot;, &quot;response&quot;, if_else(name %in% c(&quot;R&quot;, &quot;C&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) p_dag2 &lt;- dagify(G ~ R + H + U, R ~ U, H ~ R + C, coords = tibble(name = c(&quot;R&quot;, &quot;G&quot;, &quot;C&quot;, &quot;H&quot;, &quot;U&quot;), x = c(.33, 0, 1, .66, 0), y = c(.5, 1, .5, .5, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;G&quot;, &quot;response&quot;, if_else(name %in% c(&quot;R&quot;, &quot;C&quot;, &quot;H&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) p_dag1 + p_dag2 + plot_annotation(tag_levels = &quot;a&quot;) &amp; scale_y_continuous(limits = c(-.1, 1.1)) &amp; scale_x_continuous(limits = c(-.05, 1.05)) &amp; coord_fixed(ratio = .55) &amp; theme(plot.tag = element_text(family = fnt_sel)) \\(\\rightarrow\\) the DAGs are agnostic to interactions: all they show is that in a, \\(G\\) is a function of both \\(R\\) and \\(C\\) (\\(G = f(R, C)\\)) regardless of the existence of interaction between the two influences. Importing the ruggedness data: data(rugged) data_rugged &lt;- rugged %&gt;% as_tibble() %&gt;% mutate(log_gdp = log(rgdppc_2000)) %&gt;% filter(complete.cases(rgdppc_2000)) %&gt;% mutate(log_gdp_std = log_gdp / mean(log_gdp), rugged_std = rugged / max(rugged), cont_idx = as.integer(2 - cont_africa)) First sketch of the model (without interaction) \\[ \\begin{array}{rclr} log(y_{i}) &amp; \\sim &amp; Normal( \\mu_{i}, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_{i} &amp; = &amp; \\alpha + \\beta (r_{i} - \\overline{r}) &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(1, 1) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta &amp; \\sim &amp; Normal(0, 1) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] (rugged_mean &lt;- mean(data_rugged$rugged_std)) #&gt; [1] 0.2149601 model_rugged_draft &lt;- quap( flist = alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta * ( rugged_std - 0.215 ), alpha ~ dnorm(1, 1), beta ~ dnorm(0, 1), sigma ~ dexp(1) ), data = data_rugged ) Prior predictions set.seed(13) rugged_priors &lt;- extract.prior(model_rugged_draft) %&gt;% as_tibble() prior_prediction_range &lt;- c(-.2, 1.2) rugged_prior_predictions &lt;- link(model_rugged_draft, post = rugged_priors, data = list(rugged_std = prior_prediction_range)) %&gt;% as_tibble() %&gt;% set_names(nm = as.character(prior_prediction_range)) %&gt;% mutate(.draw = row_number()) p_prior_draft &lt;- rugged_prior_predictions %&gt;% filter(.draw &lt; 51) %&gt;% ggplot() + geom_segment(aes(x = -.2, xend = 1.2, y = `-0.2`, yend = `1.2`, group = .draw, color = .draw ==26), size = .4, alpha = .6) + labs(title = glue(&quot;{mth(&#39;\\U03B1 \\U007E&#39;)} Normal(1, 1)&lt;br&gt;{mth(&#39;\\U03B2 \\U007E&#39;)} Normal(0, 1)&quot;), x = &quot;ruggedness_std&quot;, y = &quot;log GDP (prop of mean)&quot;) + scale_color_manual(values = c(`FALSE` = clr0d, `TRUE` = clr1), guide = &quot;none&quot;) Proportion of extreme slopes (\\(\\gt 0.6\\)) within the prior: sum(abs(rugged_priors$beta) &gt; .6) / length(rugged_priors$beta) #&gt; [1] 0.534 Restricting the priors for \\(\\alpha\\) and \\(\\beta\\) to more reasonable ranges: \\[ \\begin{array}{rclr} \\alpha &amp; \\sim &amp; Normal(1, 0.1) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta &amp; \\sim &amp; Normal(0, 0.3) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\end{array} \\] model_rugged_restricted &lt;- quap( flist = alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta * ( rugged_std - 0.215 ), alpha ~ dnorm(1, 0.1), beta ~ dnorm(0, 0.3), sigma ~ dexp(1) ), data = data_rugged ) rugged_restricted_priors &lt;- extract.prior(model_rugged_restricted) %&gt;% as_tibble() rugged_restricted_prior_predictions &lt;- link(model_rugged_restricted, post = rugged_restricted_priors, data = list(rugged_std = prior_prediction_range)) %&gt;% as_tibble() %&gt;% set_names(nm = as.character(prior_prediction_range)) %&gt;% mutate(.draw = row_number()) p_prior_restricted &lt;- rugged_restricted_prior_predictions %&gt;% filter(.draw &lt; 51) %&gt;% ggplot() + geom_segment(aes(x = -.2, xend = 1.2, y = `-0.2`, yend = `1.2`), color = clr0d, size = .4, alpha = .6) + labs(title = glue(&quot;{mth(&#39;\\U03B1 \\U007E&#39;)} Normal(1, 0.1)&lt;br&gt;{mth(&#39;\\U03B2 \\U007E&#39;)} Normal(0, 0.3)&quot;), x = &quot;ruggedness_std&quot;, y = &quot;log GDP (prop of mean)&quot;) p_prior_draft + p_prior_restricted + plot_annotation(tag_levels = &quot;a&quot;) &amp; geom_hline(yintercept = range(data_rugged$log_gdp_std), linetype = 3, color = clr_dark) &amp; coord_cartesian(ylim = c(.5, 1.5), xlim = c(-.2, 1.2)) &amp; theme(plot.title = element_markdown(), plot.tag = element_text(family = fnt_sel)) precis(model_rugged_restricted) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 1.00 0.01 0.98 1.02 beta 0.00 0.05 -0.09 0.09 sigma 0.14 0.01 0.12 0.15 9.1.1 Adding an index variable is not enough Updating the model to include a index variable: \\[ \\begin{array}{rclr} \\mu_{i} &amp; = &amp; \\alpha_{CID[i]} + \\beta (r_{i} - \\overline{r}) &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] model_rugged_index &lt;- quap( flist = alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;- alpha[cont_idx] + beta * ( rugged_std - 0.215 ), alpha[cont_idx] ~ dnorm(1, 0.1), beta ~ dnorm(0, 0.3), sigma ~ dexp(1) ), data = data_rugged ) compare(model_rugged_restricted, model_rugged_index) #&gt; WAIC SE dWAIC dSE pWAIC #&gt; model_rugged_index -252.2759 15.30586 0.00000 NA 4.248819 #&gt; model_rugged_restricted -188.8157 13.20582 63.46027 15.12848 2.665296 #&gt; weight #&gt; model_rugged_index 1.00000e+00 #&gt; model_rugged_restricted 1.65874e-14 precis(model_rugged_index, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha[1] 0.88 0.02 0.85 0.91 alpha[2] 1.05 0.01 1.03 1.07 beta -0.05 0.05 -0.12 0.03 sigma 0.11 0.01 0.10 0.12 rugged_index_posterior &lt;- extract.samples(model_rugged_index) %&gt;% as_tibble() %&gt;% mutate(diff = alpha[,1 ] - alpha[,2]) PI(rugged_index_posterior$diff) #&gt; 5% 94% #&gt; -0.1996424 -0.1382548 rugged_range &lt;- seq(from = -.01, to = 1.01, length.out = 51) draw_posterior_samples &lt;- function(idx, model){ link(model, data = tibble(cont_idx = idx, rugged_std = rugged_range)) %&gt;% as_tibble() %&gt;% set_names(nm = as.character(rugged_range)) %&gt;% pivot_longer(everything(), values_to = &quot;log_gdp_std&quot;, names_to = &quot;rugged_std&quot;) %&gt;% mutate(cont_idx = idx, rugged_std = as.numeric(rugged_std)) } rugged_index_posterior &lt;- bind_rows(draw_posterior_samples(1, model = model_rugged_index), draw_posterior_samples(2, model = model_rugged_index)) rugged_index_posterior_pi &lt;- rugged_index_posterior %&gt;% group_by(rugged_std, cont_idx) %&gt;% summarise(mean = mean(log_gdp_std), PI_lower = PI(log_gdp_std, prob = .97)[1], PI_upper = PI(log_gdp_std, prob = .97)[2]) %&gt;% ungroup() ggplot(mapping = aes(x = rugged_std)) + geom_smooth(data = rugged_index_posterior_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = factor(cont_idx), fill = after_scale(clr_alpha(clr_lighten(color)))), size = .2) + geom_point(data = data_rugged, aes(y = log_gdp_std, color = factor(cont_idx), fill = after_scale(clr_alpha(color))), size = 1.5, shape = 21) + labs(y = &quot;log GDP (prop of mean)&quot;) + scale_color_manual(&quot;Continent&quot;, values = c(`1` = clr1, `2` = clr0dd), labels = c(`1` = &quot;Africa&quot;, `2` = &quot;Other&quot;)) 9.1.2 Adding an interaction does work Updating the model to also include a random slope: \\[ \\begin{array}{rclr} \\mu_{i} &amp; = &amp; \\alpha_{CID[i]} + \\beta_{CID[i]} (r_{i} - \\overline{r}) &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] model_rugged_slope &lt;- quap( flist = alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;- alpha[cont_idx] + beta[cont_idx] * ( rugged_std - 0.215 ), alpha[cont_idx] ~ dnorm(1, 0.1), beta[cont_idx] ~ dnorm(0, 0.3), sigma ~ dexp(1) ), data = data_rugged ) precis(model_rugged_slope, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha[1] 0.89 0.02 0.86 0.91 alpha[2] 1.05 0.01 1.03 1.07 beta[1] 0.13 0.07 0.01 0.25 beta[2] -0.14 0.05 -0.23 -0.06 sigma 0.11 0.01 0.10 0.12 compare(model_rugged_restricted, model_rugged_index, model_rugged_slope, func = PSIS) %&gt;% knit_precis(param_name = &quot;model&quot;) model PSIS SE dPSIS dSE pPSIS weight model_rugged_slope -258.72 15.27 0.00 NA 5.38 0.96 model_rugged_index -252.31 15.31 6.41 6.91 4.21 0.04 model_rugged_restricted -188.75 13.39 69.97 15.51 2.71 0.00 library(ggdist) set.seed(42) (data_rugged_psis &lt;- PSIS(model_rugged_slope, pointwise = TRUE) %&gt;% as_tibble() %&gt;% bind_cols(data_rugged)) %&gt;% ggplot(aes(x = factor(cont_idx), y = k)) + geom_boxplot(#adjust = 1, aes(color = factor(cont_idx), fill = after_scale(clr_alpha(color))))+ geom_text(data = . %&gt;% filter(k &gt; .25), aes(x = cont_idx + .1, label = country), hjust = 0, family = fnt_sel) + scale_color_manual(&quot;Continent&quot;, values = c(`1` = clr1, `2` = clr0dd), labels = c(`1` = &quot;Africa&quot;, `2` = &quot;Other&quot;)) + labs(x = &quot;continent&quot;) rugged_slope_posterior &lt;- bind_rows(draw_posterior_samples(1, model = model_rugged_slope), draw_posterior_samples(2, model = model_rugged_slope)) rugged_slope_posterior_pi &lt;- rugged_slope_posterior %&gt;% group_by(rugged_std, cont_idx) %&gt;% summarise(mean = mean(log_gdp_std), PI_lower = PI(log_gdp_std, prob = .97)[1], PI_upper = PI(log_gdp_std, prob = .97)[2]) %&gt;% ungroup() ggplot(mapping = aes(x = rugged_std)) + geom_smooth(data = rugged_slope_posterior_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper, color = factor(cont_idx), fill = after_scale(clr_alpha(clr_lighten(color)))), size = .2) + geom_point(data = data_rugged, aes(y = log_gdp_std, color = factor(cont_idx), fill = after_scale(clr_alpha(color))), size = 1.5, shape = 21) + ggrepel::geom_text_repel(data = data_rugged_psis %&gt;% filter(k &gt; .25), aes(y = log_gdp_std, x = rugged_std + .1, label = country), force = 20, hjust = 0, family = fnt_sel) + labs(y = &quot;log GDP (prop of mean)&quot;) + facet_wrap(cont_idx ~ .) + scale_color_manual(&quot;Continent&quot;, values = c(`1` = clr1, `2` = clr0dd), labels = c(`1` = &quot;Africa&quot;, `2` = &quot;Other&quot;)) 9.2 Symetry of Interactions Rewriting the linear model to highlight the symmetry between conditional slope and conditional intercept: \\[ \\mu_{i} = \\underbrace{(2 - CID_{i}) \\big( \\alpha_{1} + \\beta_{1} (r_{i} - \\overline{r}) \\big)}_{CID[i] = 1} + \\underbrace{(CID_{i} - 1) \\big( \\alpha_{2} + \\beta_{2} (r_{i} - \\overline{r}) \\big)}_{CID[i] = 2} \\] Plotting a counterfactual effect of comparing the association of log GPD with being in Africa while holding ruggedness constant: rugged_slope_posterior_delta &lt;- rugged_slope_posterior %&gt;% group_by(cont_idx, rugged_std) %&gt;% mutate(.draw = row_number()) %&gt;% ungroup() %&gt;% pivot_wider(names_from = cont_idx, values_from = log_gdp_std, names_prefix = &quot;log_gdp_std_&quot;) %&gt;% mutate(delta_log_gdp_std = log_gdp_std_1 - log_gdp_std_2) rugged_slope_posterior_delta_pi &lt;- rugged_slope_posterior_delta %&gt;% group_by(rugged_std) %&gt;% summarise(mean = mean(delta_log_gdp_std), PI_lower = PI(delta_log_gdp_std, prob = .97)[1], PI_upper = PI(delta_log_gdp_std, prob = .97)[2]) %&gt;% ungroup() ggplot(mapping = aes(x = rugged_std)) + geom_hline(yintercept = 0, color = clr_dark, linetype = 3) + geom_smooth(data = rugged_slope_posterior_delta_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr0d, fill = fll0, size = .2) + geom_text(data = tibble(y = .03 * c(-1,1), lab = c(&quot;Africa lower GDP&quot;, &quot;Africa higher GDP&quot;)), aes(x = .01, y = y, label = lab), family = fnt_sel, hjust = 0) + labs(y = &quot;expected difference log GDP&quot;) … it it simultaneously consistent with the data and the model, that the influence of ruggedness depends on the continent and that the influence of the continent depends on ruggedness. 9.3 Continuous Interaction data(tulips) precis(tulips) %&gt;% knit_precis() param mean sd 5.5% 94.5% histogram bed NaN NA NA NA water 2.00 0.83 1.00 3.00 ▇▁▁▁▇▁▁▁▁▇ shade 2.00 0.83 1.00 3.00 ▇▁▁▁▇▁▁▁▁▇ blooms 128.99 92.68 4.31 280.79 ▅▇▇▂▃▁▁▁ data_tulips &lt;- tulips %&gt;% as_tibble() %&gt;% mutate(blooms_std = blooms / max(blooms), water_cent = water - mean(water), shade_cent = shade - mean(shade)) dagify(B ~ S + W, coords = tibble(name = c(&quot;B&quot;, &quot;W&quot;, &quot;S&quot;), x = c(.5, 0, 1), y = c(1, 1, 1))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;B&quot;, &quot;response&quot;, if_else(name %in% c(&quot;W&quot;, &quot;S&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + coord_fixed(ratio = .6, xlim = c(-.05, 1.05), ylim = c(.9, 1.1)) We are going to build two models, one without any interaction and one with. 9.3.1 Model without Interaction \\[ \\begin{array}{rclr} B_{i} &amp; \\sim &amp; Normal( \\mu_{i}, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_{i} &amp; = &amp; \\alpha + \\beta_{W} (W_{i} - \\overline{W}) + \\beta_{S} (S_{i} - \\overline{S}) &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0.5, 0.25) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{W} &amp; \\sim &amp; Normal(0, 0.25) &amp; \\textrm{[$\\beta_{W}$ prior]}\\\\ \\beta_{S} &amp; \\sim &amp; Normal(0, 0.25) &amp; \\textrm{[$\\beta_{S}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] Considerations for the priors: Ranges assigned if we used \\(alpha = Normal(0.5, 1)\\) instead: alpha &lt;- rnorm(1e4, 0.5, 1); sum(alpha &lt; 0 | alpha &gt; 1) / length(alpha) #&gt; [1] 0.6251 updating to a more restrictive prior (used in the model): alpha &lt;- rnorm(1e4, 0.5, .25); sum(alpha &lt; 0 | alpha &gt; 1) / length(alpha) #&gt; [1] 0.0498 model_tulips_simple &lt;- quap( flist = alist( blooms_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_w * water_cent + beta_s * shade_cent, alpha ~ dnorm(.5, .25), beta_w ~ dnorm(0,.25), beta_s ~ dnorm(0,.25), sigma ~ dexp(1) ), data = data_tulips ) precis(model_tulips_simple) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.36 0.03 0.31 0.41 beta_w 0.21 0.04 0.15 0.26 beta_s -0.11 0.04 -0.17 -0.05 sigma 0.16 0.02 0.12 0.19 9.3.2 Model with Interaction Implementing the continuous interaction \\[ \\begin{array}{rcl} \\mu_{i} &amp; = &amp; \\alpha + \\gamma_{W,i} \\beta_{W} W_{i} + \\beta_{S} S_{i} \\\\ \\gamma_{W,i} &amp; = &amp; \\beta_{W} + \\beta_{WS} S_{i}\\\\ \\end{array} \\] which allows the substitution \\[ \\begin{array}{rcl} \\mu_{i} &amp; = &amp; \\alpha + \\underbrace{(\\beta_{W} + \\beta_{WS} S_{i})}_{\\gamma_{W,i} } \\beta_{W} W_{i} + \\beta_{S} S_{i} \\\\ &amp; = &amp; \\alpha + \\beta_{W} W_{i} + \\beta_{S} S_{i} + \\beta_{WS} W_{i} S_{i}\\\\ \\end{array} \\] Using this for the complete model: \\[ \\begin{array}{rclr} B_{i} &amp; \\sim &amp; Normal( \\mu_{i}, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_{i} &amp; = &amp; \\alpha + \\beta_{W} W_{i} + \\beta_{S} S_{i} + \\beta_{WS} W_{i} S_{i} &amp; \\textrm{[linear model]} \\\\ \\alpha &amp; \\sim &amp; Normal(0.5, 0.25) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{W} &amp; \\sim &amp; Normal(0, 0.25) &amp; \\textrm{[$\\beta_{W}$ prior]}\\\\ \\beta_{S} &amp; \\sim &amp; Normal(0, 0.25) &amp; \\textrm{[$\\beta_{S}$ prior]}\\\\ \\beta_{WS} &amp; \\sim &amp; Normal(0, 0.25) &amp; \\textrm{[$\\beta_{S}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_tulips_interaction &lt;- quap( flist = alist( blooms_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_w * water_cent + beta_s * shade_cent + beta_ws * water_cent * shade_cent, alpha ~ dnorm(.5, .25), beta_w ~ dnorm(0,.25), beta_s ~ dnorm(0,.25), beta_ws ~ dnorm(0,.25), sigma ~ dexp(1) ), data = data_tulips ) precis(model_tulips_interaction) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.36 0.02 0.32 0.40 beta_w 0.21 0.03 0.16 0.25 beta_s -0.11 0.03 -0.16 -0.07 beta_ws -0.14 0.04 -0.20 -0.09 sigma 0.12 0.02 0.10 0.15 9.3.3 Plotting the Posterior Predictions tulip_range &lt;- cross_df(list(water_cent = -1:1, shade_cent = -1:1)) %&gt;% mutate(rn = row_number()) tulips_simple_posterior &lt;- link(model_tulips_simple, data = tulip_range) %&gt;% as_tibble() %&gt;% set_names(nm = 1:length(names(.))) %&gt;% mutate(.draw = row_number()) %&gt;% pivot_longer(-.draw, names_to = &quot;rn&quot;, values_to = &quot;blooms&quot;) %&gt;% mutate(rn = as.numeric(rn), model = &quot;simple&quot;) %&gt;% left_join(tulip_range) tulips_interaction_posterior &lt;- link(model_tulips_interaction, data = tulip_range) %&gt;% as_tibble() %&gt;% set_names(nm = 1:length(names(.))) %&gt;% mutate(.draw = row_number()) %&gt;% pivot_longer(-.draw, names_to = &quot;rn&quot;, values_to = &quot;blooms&quot;) %&gt;% mutate(rn = as.numeric(rn), model = &quot;interaction&quot;) %&gt;% left_join(tulip_range) tulips_posterior &lt;- tulips_simple_posterior %&gt;% bind_rows(tulips_interaction_posterior) %&gt;% mutate(model = factor(model, levels = c(&quot;simple&quot;, &quot;interaction&quot;))) tulips_posterior_pi &lt;- tulips_posterior %&gt;% group_by(model, rn,water_cent, shade_cent ) %&gt;% summarise(mean = mean(blooms), PI_lower = PI(blooms, prob = .97)[1], PI_upper = PI(blooms, prob = .97)[2]) %&gt;% ungroup() tulips_posterior %&gt;% ggplot(aes(x = water_cent)) + geom_line(data = . %&gt;% filter(.draw &lt; 21), aes(y = blooms , group = .draw, color = .draw == 20)) + geom_point(data = data_tulips, aes(y = blooms_std), color = clr_dark) + facet_grid(model ~ shade_cent, labeller = label_both,switch = &quot;y&quot;) + scale_color_manual(values = c(`FALSE` = fll0, `TRUE` = fll1)) + theme(legend.position = &quot;bottom&quot;, strip.placement = &quot;outside&quot;) tulips_posterior_pi %&gt;% ggplot(aes(x = water_cent)) + geom_point(data = data_tulips, aes(y = blooms_std), color = clr_dark) + geom_smooth(stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr0dd, fill = clr_alpha(clr0dd), size = .5) + facet_grid(model ~ shade_cent, labeller = label_both,switch = &quot;y&quot;) + theme(legend.position = &quot;bottom&quot;, strip.placement = &quot;outside&quot;) 9.3.4 Plotting the Prior Predictions set.seed(5) tulips_simple_prior &lt;- extract.prior(model_tulips_simple, data = tulip_range) %&gt;% as_tibble() %&gt;% mutate(.draw = row_number(), grid = rep(list(cross_df(list(water_cent = -1:1, shade_cent = -1:1))), n()), model = &quot;simple&quot;) %&gt;% unnest(grid) %&gt;% mutate(blooms = alpha + beta_w * water_cent + beta_s * shade_cent) tulips_interaction_prior &lt;- extract.prior(model_tulips_interaction, data = tulip_range) %&gt;% as_tibble() %&gt;% mutate(.draw = row_number(), grid = rep(list(cross_df(list(water_cent = -1:1, shade_cent = -1:1))), n()), model = &quot;interaction&quot;) %&gt;% unnest(grid) %&gt;% mutate(blooms = alpha + beta_w * water_cent + beta_s * shade_cent + beta_ws * water_cent * shade_cent) tulips_prior &lt;- tulips_simple_prior %&gt;% bind_rows(tulips_interaction_prior) %&gt;% mutate(model = factor(model, levels = c(&quot;simple&quot;, &quot;interaction&quot;))) tulips_prior_pi &lt;- tulips_prior %&gt;% group_by(model, water_cent, shade_cent ) %&gt;% summarise(mean = mean(blooms), PI_lower = PI(blooms, prob = .97)[1], PI_upper = PI(blooms, prob = .97)[2]) %&gt;% ungroup() tulips_prior %&gt;% ggplot(aes(x = water_cent)) + geom_line(data = . %&gt;% filter(.draw &lt; 21), aes(y = blooms , group = .draw, color = .draw == 20)) + geom_hline(yintercept = c(0, 1), linetype = 3, color = clr_dark) + facet_grid(model ~ shade_cent, labeller = label_both, switch = &quot;y&quot;) + scale_color_manual(values = c(`FALSE` = clr_alpha(clr0d), `TRUE` = fll1)) + theme(legend.position = &quot;bottom&quot;, strip.placement = &quot;outside&quot;) + coord_cartesian(ylim = c(-.35,1.35)) tulips_prior_pi %&gt;% ggplot(aes(x = water_cent)) + geom_smooth(stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr0dd, fill = clr_alpha(clr0dd), size = .5) + geom_hline(yintercept = c(0, 1), linetype = 3, color = clr_dark) + facet_grid(model ~ shade_cent, labeller = label_both,switch = &quot;y&quot;) + theme(legend.position = &quot;bottom&quot;, strip.placement = &quot;outside&quot;) library(rlang) chapter8_models &lt;- env( data_rugged = data_rugged, model_rugged_draft = model_rugged_draft, model_rugged_restricted = model_rugged_restricted, model_rugged_index = model_rugged_index, model_rugged_slope = model_rugged_slope, data_tulips = data_tulips ) write_rds(chapter8_models, &quot;envs/chapter8_models.rds&quot;) 9.4 Homework E1 E2 E3 E4 M1 M2 M3 M4 M5 M6 H1 H2 H3 H4 H5 9.5 {brms} section 9.5.1 Building an Interaction data_rugged_centered &lt;- data_rugged %&gt;% mutate(rugged_std_centered = rugged_std - mean(rugged_std), cont_idx = factor(cont_idx)) brms_c8_model_rugged_draft &lt;- brm( data = data_rugged_centered, family = gaussian, log_gdp_std ~ 1 + rugged_std_centered, prior = c(prior(normal(1, 1), class = Intercept), prior(normal(0, 1), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, sample_prior = TRUE, file = &quot;brms/brms_c8_model_rugged_draft&quot;) prior_rugged_draft &lt;- prior_draws(brms_c8_model_rugged_draft) %&gt;% as_tibble() set.seed(8) p1 &lt;- prior_rugged_draft %&gt;% slice_sample(n = 50) %&gt;% rownames_to_column() %&gt;% expand(nesting(rowname, Intercept, b), rugged_std_centered = c(-2, 2)) %&gt;% mutate(log_gdp_std = Intercept + b * rugged_std_centered, rugged_std = rugged_std_centered + mean(data_rugged_centered$rugged_std)) %&gt;% ggplot(aes(x = rugged_std, y = log_gdp_std, group = rowname)) + geom_hline(yintercept = range(data_rugged_centered$log_gdp_std), linetype = 3, color = clr_dark) + geom_line(color = clr_alpha(clr0d)) + geom_abline(intercept = 1.3, slope = -0.6, color = clr0dd) + labs(subtitle = glue(&quot;{mth(&#39;\\U03B1 \\U007E&#39;)} dnorm(1, 1)&lt;br&gt;{mth(&#39;\\U03B2 \\U007E&#39;)} dnorm(0, 1)&quot;), x = &quot;ruggedness&quot;, y = &quot;log GDP (prop of mean)&quot;) + coord_cartesian(xlim = c(0, 1), ylim = c(0.5, 1.5)) + theme(plot.subtitle = element_markdown()) prior_rugged_draft %&gt;% summarise(a = sum(abs(b) &gt; abs(-0.6)) / nrow(prior_rugged_draft)) #&gt; # A tibble: 1 x 1 #&gt; a #&gt; &lt;dbl&gt; #&gt; 1 0.552 brms_c8_model_rugged_restricted &lt;- brm( data = data_rugged_centered, family = gaussian, log_gdp_std ~ 1 + rugged_std_centered, prior = c(prior(normal(1, 0.1), class = Intercept), prior(normal(0, .3), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, sample_prior = TRUE, file = &quot;brms/brms_c8_model_rugged_restricted&quot;) prior_rugged_restricted &lt;- prior_draws(brms_c8_model_rugged_restricted) %&gt;% as_tibble() set.seed(8) p2 &lt;- prior_rugged_restricted %&gt;% slice_sample(n = 50) %&gt;% rownames_to_column() %&gt;% expand(nesting(rowname, Intercept, b), rugged_std_centered = c(-2, 2)) %&gt;% mutate(log_gdp_std = Intercept + b * rugged_std_centered, rugged_std = rugged_std_centered + mean(data_rugged_centered$rugged_std)) %&gt;% ggplot(aes(x = rugged_std, y = log_gdp_std, group = rowname)) + geom_hline(yintercept = range(data_rugged_centered$log_gdp_std), linetype = 3, color = clr_dark) + geom_line(color = clr_alpha(clr0d)) + labs(subtitle = glue(&quot;{mth(&#39;\\U03B1 \\U007E&#39;)} dnorm(1, 0.1)&lt;br&gt;{mth(&#39;\\U03B2 \\U007E&#39;)} dnorm(0, 0.3)&quot;), x = &quot;ruggedness&quot;, y = &quot;log GDP (prop of mean)&quot;) + coord_cartesian(xlim = c(0, 1), ylim = c(0.5, 1.5)) + theme(plot.subtitle = element_markdown()) prior_rugged_restricted %&gt;% summarise(a = sum(abs(b) &gt; abs(-0.6)) / nrow(prior_rugged_draft)) #&gt; # A tibble: 1 x 1 #&gt; a #&gt; &lt;dbl&gt; #&gt; 1 0.0382 p1 + p2 mixedup::summarize_model(brms_c8_model_rugged_restricted) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 0.02 0.14 0.12 0.15 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; Intercept 1.00 0.01 0.98 1.02 #&gt; rugged_std_centered 0.00 0.06 -0.11 0.12 9.5.1.1 Adding an indicator variable is not enough brms_c8_model_rugged_index &lt;- brm( data = data_rugged_centered, family = gaussian, log_gdp_std ~ 0 + cont_idx + rugged_std_centered, prior = c(prior(normal(1, 0.1), class = b, coef = cont_idx1), prior(normal(1, 0.1), class = b, coef = cont_idx2), prior(normal(0, 0.3), class = b, coef = rugged_std_centered), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c8_model_rugged_index&quot;) brms_c8_model_rugged_restricted &lt;- add_criterion(brms_c8_model_rugged_restricted, &quot;waic&quot;) brms_c8_model_rugged_index &lt;- add_criterion(brms_c8_model_rugged_index, &quot;waic&quot;) loo_compare(brms_c8_model_rugged_restricted, brms_c8_model_rugged_index, criterion = &quot;waic&quot;) %&gt;% print(simplify = FALSE) %&gt;% as.data.frame() %&gt;% knit_precis(param_name = &quot;model&quot;) #&gt; elpd_diff se_diff elpd_waic se_elpd_waic p_waic #&gt; brms_c8_model_rugged_index 0.0 0.0 126.2 7.4 4.1 #&gt; brms_c8_model_rugged_restricted -31.8 7.3 94.4 6.5 2.6 #&gt; se_p_waic waic se_waic #&gt; brms_c8_model_rugged_index 0.8 -252.4 14.8 #&gt; brms_c8_model_rugged_restricted 0.3 -188.8 13.0 model elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic brms_c8_model_rugged_index 0.00 0.00 126.21 7.41 4.08 0.80 -252.42 14.81 brms_c8_model_rugged_restricted -31.81 7.32 94.40 6.48 2.61 0.29 -188.80 12.95 model_weights(brms_c8_model_rugged_restricted, brms_c8_model_rugged_index, weights = &quot;waic&quot;) %&gt;% round(digits = 3) #&gt; brms_c8_model_rugged_restricted brms_c8_model_rugged_index #&gt; 0 1 mixedup::summarize_model(brms_c8_model_rugged_index) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 0.01 0.11 0.10 0.13 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; cont_idx1 0.88 0.02 0.85 0.91 #&gt; cont_idx2 1.05 0.01 1.03 1.07 #&gt; rugged_std_centered -0.05 0.05 -0.14 0.04 posterior_rugged_index &lt;- as_draws_df(brms_c8_model_rugged_index) %&gt;% as_tibble() %&gt;% mutate(diff = b_cont_idx1 - b_cont_idx2) library(tidybayes) qi(posterior_rugged_index$diff, .width = .89) %&gt;% as_tibble() #&gt; # A tibble: 1 x 2 #&gt; V1 V2 #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 -0.200 -0.138 new_data &lt;- crossing(cont_idx = 1:2, rugged_std = seq(from = -0.2, to = 1.2, length.out = 30)) %&gt;% mutate(rugged_std_centered = rugged_std - mean(data_rugged_centered$rugged_std)) fitted_rugged_index &lt;- fitted( brms_c8_model_rugged_index, newdata = new_data, probs = c(.015, .985)) %&gt;% as_tibble() %&gt;% bind_cols(new_data) %&gt;% mutate(cont_africa = c(&quot;Africa&quot;, &quot;not Africa&quot;)[cont_idx]) p1 &lt;- data_rugged_centered %&gt;% mutate(cont_africa = c(&quot;Africa&quot;, &quot;not Africa&quot;)[cont_idx]) %&gt;% ggplot(aes(x = rugged_std, fill = cont_africa, color = cont_africa)) + geom_smooth(data = fitted_rugged_index, aes(y = Estimate, ymin = Q1.5, ymax = Q98.5), stat = &quot;identity&quot;, alpha = .2, size = .3) + geom_point(aes(y = log_gdp_std), size = 1) + scale_fill_manual(values = c(Africa = clr0dd, `not Africa` = clr0d)) + scale_colour_manual(values = c(Africa = clr0dd, `not Africa` = clr0d)) + labs(subtitle = &quot;brms_c8_model_rugged_index&quot;, x = &quot;ruggedness (standardized)&quot;, y = &quot;log GDP (as proportion of mean)&quot;) + coord_cartesian(xlim = c(0, 1)) + theme(legend.background = element_blank(), legend.position = &quot;bottom&quot;, legend.title = element_blank()) Using stat_lineribbon() since ‘boundaries are meaningless’: p2 &lt;- fitted(brms_c8_model_rugged_index, newdata = new_data, summary = FALSE) %&gt;% as_tibble() %&gt;% pivot_longer(everything()) %&gt;% bind_cols( expand(new_data, iter = 1:4000, nesting(cont_idx, rugged_std)) ) %&gt;% mutate(cont_africa = c(&quot;Africa&quot;, &quot;not Africa&quot;)[cont_idx]) %&gt;% ggplot(aes(x = rugged_std, y = value, fill = cont_africa, color = cont_africa)) + stat_lineribbon(.width = seq(from = .03, to = .99, by = .03), alpha = .1, size = 0) + geom_point(data = data_rugged_centered %&gt;% mutate(cont_africa = c(&quot;Africa&quot;, &quot;not Africa&quot;)[cont_idx]), aes(y = log_gdp_std), size = 1) + scale_fill_manual(values = c(Africa = clr0dd, `not Africa` = clr0d), guide = &quot;none&quot;) + scale_colour_manual(values = c(Africa = clr0dd, `not Africa` = clr0d), guide = &quot;none&quot;) + labs(subtitle = &quot;brms_c8_model_rugged_index&quot;, x = &quot;ruggedness (standardized)&quot;, y = &quot;log GDP (as proportion of mean)&quot;) + coord_cartesian(xlim = c(0, 1)) + theme(legend.background = element_blank(), legend.position = &quot;bottom&quot;, legend.title = element_blank()) p1 + p2 + plot_layout(guides = &quot;collect&quot;) &amp; theme(legend.position = &quot;bottom&quot;) 9.5.1.2 Adding an interaction does work Remeber the non-linear syntax for {brms} to use with index variables. brms_c8_model_rugged_slope &lt;- brm( data = data_rugged_centered, family = gaussian, bf(log_gdp_std ~ 0 + a + b * rugged_std_centered, a ~ 0 + cont_idx, b ~ 0 + cont_idx, nl = TRUE), prior = c(prior(normal(1, 0.1), class = b, coef = cont_idx1, nlpar = a), prior(normal(1, 0.1), class = b, coef = cont_idx2, nlpar = a), prior(normal(0, 0.3), class = b, coef = cont_idx1, nlpar = b), prior(normal(0, 0.3), class = b, coef = cont_idx2, nlpar = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c8_model_rugged_slope&quot;) mixedup::summarise_model(brms_c8_model_rugged_slope) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 0.01 0.11 0.10 0.12 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; a_cont_idx1 0.89 0.02 0.86 0.92 #&gt; a_cont_idx2 1.05 0.01 1.03 1.07 #&gt; b_cont_idx1 0.13 0.07 -0.01 0.28 #&gt; b_cont_idx2 -0.14 0.06 -0.25 -0.03 brms_c8_model_rugged_restricted &lt;- add_criterion(brms_c8_model_rugged_restricted, &quot;loo&quot;) brms_c8_model_rugged_index &lt;- add_criterion(brms_c8_model_rugged_index, &quot;loo&quot;) brms_c8_model_rugged_slope &lt;- add_criterion(brms_c8_model_rugged_slope, c(&quot;loo&quot;, &quot;waic&quot;)) loo_compare(brms_c8_model_rugged_restricted, brms_c8_model_rugged_index, brms_c8_model_rugged_slope, criterion = &quot;loo&quot;) %&gt;% print(simplify = FALSE) #&gt; elpd_diff se_diff elpd_loo se_elpd_loo p_loo #&gt; brms_c8_model_rugged_slope 0.0 0.0 129.7 7.3 4.9 #&gt; brms_c8_model_rugged_index -3.5 3.2 126.2 7.4 4.1 #&gt; brms_c8_model_rugged_restricted -35.3 7.5 94.4 6.5 2.6 #&gt; se_p_loo looic se_looic #&gt; brms_c8_model_rugged_slope 0.9 -259.3 14.7 #&gt; brms_c8_model_rugged_index 0.8 -252.4 14.8 #&gt; brms_c8_model_rugged_restricted 0.3 -188.8 13.0 model_weights(brms_c8_model_rugged_restricted, brms_c8_model_rugged_index, brms_c8_model_rugged_slope, weights = &quot;loo&quot;) %&gt;% round(digits = 2) #&gt; brms_c8_model_rugged_restricted brms_c8_model_rugged_index #&gt; 0.00 0.03 #&gt; brms_c8_model_rugged_slope #&gt; 0.97 tibble(k = brms_c8_model_rugged_slope$criteria$loo$diagnostics$pareto_k, row = 1:170) %&gt;% arrange(desc(k)) #&gt; # A tibble: 170 x 2 #&gt; k row #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 0.512 145 #&gt; 2 0.494 93 #&gt; 3 0.262 35 #&gt; 4 0.240 133 #&gt; 5 0.240 36 #&gt; 6 0.195 27 #&gt; 7 0.182 144 #&gt; 8 0.149 67 #&gt; 9 0.143 99 #&gt; 10 0.139 107 #&gt; # … with 160 more rows Robust model variant using a student-t distribution as prior brms_c8_model_rugged_student &lt;- brm( data = data_rugged_centered, family = student, bf(log_gdp_std ~ 0 + a + b * rugged_std_centered, a ~ 0 + cont_idx, b ~ 0 + cont_idx, nl = TRUE), prior = c(prior(normal(1, 0.1), class = b, coef = cont_idx1, nlpar = a), prior(normal(1, 0.1), class = b, coef = cont_idx2, nlpar = a), prior(normal(0, 0.3), class = b, coef = cont_idx1, nlpar = b), prior(normal(0, 0.3), class = b, coef = cont_idx2, nlpar = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c8_model_rugged_student&quot;) brms_c8_model_rugged_student &lt;- add_criterion(brms_c8_model_rugged_student, c(&quot;loo&quot;, &quot;waic&quot;)) loo_compare(brms_c8_model_rugged_slope, brms_c8_model_rugged_student, criterion = &quot;loo&quot;) %&gt;% print(simplify = FALSE) #&gt; elpd_diff se_diff elpd_loo se_elpd_loo p_loo #&gt; brms_c8_model_rugged_slope 0.0 0.0 129.7 7.3 4.9 #&gt; brms_c8_model_rugged_student -1.3 0.3 128.4 7.5 5.0 #&gt; se_p_loo looic se_looic #&gt; brms_c8_model_rugged_slope 0.9 -259.3 14.7 #&gt; brms_c8_model_rugged_student 0.8 -256.7 14.9 tibble(Normal = brms_c8_model_rugged_slope$criteria$loo$diagnostics$pareto_k, `Student-t` = brms_c8_model_rugged_student$criteria$loo$diagnostics$pareto_k) %&gt;% pivot_longer(everything(), values_to = &quot;pareto_k&quot;) %&gt;% ggplot(aes(x = pareto_k, y = name)) + geom_vline(xintercept = .5, linetype = 3, color = clr_dark) + stat_dots(slab_fill = clr0, slab_color = clr0d) + annotate(geom = &quot;text&quot;, x = .485, y = 1.5, label = &quot;threshold&quot;, angle = 90, family = fnt_sel, color = clr_dark) + ylab(NULL) + coord_cartesian(ylim = c(1.5, 2.4)) fixef(brms_c8_model_rugged_slope) %&gt;% round(digits = 2) %&gt;% data.frame() %&gt;% knit_precis() param Estimate Est.Error Q2.5 Q97.5 a_cont_idx1 0.89 0.02 0.86 0.92 a_cont_idx2 1.05 0.01 1.03 1.07 b_cont_idx1 0.13 0.07 -0.01 0.28 b_cont_idx2 -0.14 0.06 -0.25 -0.03 fixef(brms_c8_model_rugged_student) %&gt;% round(digits = 2) %&gt;% data.frame() %&gt;% knit_precis() param Estimate Est.Error Q2.5 Q97.5 a_cont_idx1 0.88 0.02 0.85 0.92 a_cont_idx2 1.05 0.01 1.03 1.07 b_cont_idx1 0.13 0.08 -0.02 0.29 b_cont_idx2 -0.15 0.06 -0.26 -0.03 9.5.2 Plotting the Interaction countries &lt;- c(&quot;Equatorial Guinea&quot;, &quot;South Africa&quot;, &quot;Seychelles&quot;, &quot;Swaziland&quot;, &quot;Lesotho&quot;, &quot;Rwanda&quot;, &quot;Burundi&quot;, &quot;Luxembourg&quot;, &quot;Greece&quot;, &quot;Switzerland&quot;, &quot;Lebanon&quot;, &quot;Yemen&quot;, &quot;Tajikistan&quot;, &quot;Nepal&quot;) fitted_rugged_slope &lt;- fitted(brms_c8_model_rugged_slope, newdata = new_data, probs = c(.015, .985)) %&gt;% data.frame() %&gt;% bind_cols(new_data) %&gt;% mutate(cont_africa = c(&quot;Africa&quot;, &quot;not Africa&quot;)[cont_idx]) data_rugged_centered %&gt;% mutate(cont_africa = c(&quot;Africa&quot;, &quot;not Africa&quot;)[cont_idx]) %&gt;% ggplot(aes(x = rugged_std, y = log_gdp_std, fill = cont_africa, color = cont_africa)) + geom_smooth(data = fitted_rugged_slope, aes(y = Estimate, ymin = Q1.5, ymax = Q98.5), stat = &quot;identity&quot;, alpha = .2, size = .4) + ggrepel::geom_text_repel(data = . %&gt;% filter(country %in% countries), aes(label = country), size = 3, seed = 8, segment.color = clr_dark, min.segment.length = 0) + geom_point(aes(y = log_gdp_std), size = 1) + scale_fill_manual(values = c(Africa = clr0dd, `not Africa` = clr0d)) + scale_colour_manual(values = c(Africa = clr0dd, `not Africa` = clr0d)) + labs(x = &quot;ruggedness (standardized)&quot;, y = &quot;log GDP (as proportion of mean)&quot;) + facet_wrap(. ~ cont_africa) + coord_cartesian(xlim = c(0, 1)) + theme(legend.position = &quot;none&quot;) 9.5.3 Symetry of Interactions fitted(brms_c8_model_rugged_slope, newdata = new_data, summary = FALSE) %&gt;% data.frame() %&gt;% pivot_longer(everything()) %&gt;% bind_cols(expand(new_data, iter = 1:4000, nesting(cont_idx, rugged_std))) %&gt;% select(-name) %&gt;% pivot_wider(names_from = cont_idx, values_from = value) %&gt;% mutate(delta = `1` - `2`) %&gt;% ggplot(aes(x = rugged_std, y = delta)) + stat_lineribbon(.width = .95, fill = fll0, color = clr0d, size = .3) + geom_hline(yintercept = 0, linetype = 3, color = clr_dark) + annotate(geom = &quot;text&quot;, x = .2, y = 0, label = &quot;Africa higher GDP\\nAfrica lower GDP&quot;, family = fnt_sel) + labs(x = &quot;ruggedness (standardized)&quot;, y = &quot;expected difference log GDP&quot;) + coord_cartesian(xlim = c(0, 1), ylim = c(-0.3, 0.2)) 9.5.4 Continuous Interactions Tulip model without interaction brms_c8_model_tulips_simple &lt;- brm( data = data_tulips, family = gaussian, blooms_std ~ 1 + water_cent + shade_cent, prior = c(prior(normal(0.5, 0.25), class = Intercept), prior(normal(0, 0.25), class = b, coef = water_cent), prior(normal(0, 0.25), class = b, coef = shade_cent), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c8_model_tulips_simple&quot;) mixedup::summarize_model(brms_c8_model_tulips_simple) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 0.03 0.18 0.13 0.24 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; Intercept 0.36 0.03 0.29 0.43 #&gt; water_cent 0.20 0.04 0.12 0.28 #&gt; shade_cent -0.11 0.04 -0.19 -0.03 Tulip model with interaction \\[\\begin{align*} \\mu_{i} &amp; = \\alpha + \\color{#B35136}{\\gamma_{W, i}} \\text{W}_{i} + \\beta_{S} \\text{S}_i \\\\ \\color{#B35136}{\\gamma_{W, i}} &amp; = \\color{#B35136}{\\beta_{W} + \\beta_{WS} \\text{S}_{i}}, \\end{align*}\\] \\[\\begin{align*} \\mu_{i} &amp; = \\alpha + \\color{#B35136}{\\underbrace{(\\beta_{W} + \\beta_{WS} \\text{S}_i)}_{\\gamma_{W, i}}} \\text{W}_i + \\beta_{S} \\text{S}_{i} \\\\ &amp; = \\alpha + \\color{#B35136}{\\beta_{W}} \\text{W}_{i} + (\\color{#B35136}{\\beta_{WS} \\text{S}_i} \\cdot \\text{W}_{i}) + \\beta_{S} \\text{S}_{i} \\\\ &amp; = \\alpha + \\color{#B35136}{\\beta_{W}} \\text{W}_{i} + \\beta_{S} \\text{S}_{i} + \\color{#B35136}{\\beta_{WS}} (\\color{#B35136}{\\text{S}_{i}} \\cdot \\text{W}_{i}), \\end{align*}\\] brms_c8_model_tulips_interaction &lt;- brm( data = data_tulips, family = gaussian, blooms_std ~ 1 + water_cent + shade_cent + water_cent:shade_cent, prior = c(prior(normal(0.5, 0.25), class = Intercept), prior(normal(0, 0.25), class = b, coef = water_cent), prior(normal(0, 0.25), class = b, coef = shade_cent), prior(normal(0, 0.25), class = b, coef = &quot;water_cent:shade_cent&quot;), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c8_model_tulips_interaction&quot;) mixedup::summarize_model(brms_c8_model_tulips_interaction) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 0.02 0.14 0.11 0.20 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; Intercept 0.36 0.03 0.31 0.42 #&gt; water_cent 0.21 0.03 0.14 0.27 #&gt; shade_cent -0.11 0.03 -0.18 -0.04 #&gt; water_cent:shade_cent -0.14 0.04 -0.22 -0.06 9.5.5 Plotting the Posterior Predictions set.seed(42) new_data &lt;- crossing(shade_cent = -1:1, water_cent = c(-1, 1)) rbind(fitted(brms_c8_model_tulips_simple, newdata = new_data, summary = FALSE, ndraws = 20), fitted(brms_c8_model_tulips_interaction, newdata = new_data, summary = FALSE, ndraws = 20)) %&gt;% as_tibble() %&gt;% set_names(mutate(new_data, name = str_c(shade_cent, water_cent, sep = &quot;_&quot;)) %&gt;% pull()) %&gt;% mutate(row = 1:n(), model = rep(c(&quot;simple&quot;, &quot;interaction&quot;), each = n() / 2) %&gt;% factor(., levels = c(&quot;simple&quot;, &quot;interaction&quot;))) %&gt;% pivot_longer(-c(row:model), values_to = &quot;blooms_std&quot;) %&gt;% separate(name, into = c(&quot;shade_cent&quot;, &quot;water_cent&quot;), sep = &quot;_&quot;, convert = TRUE) %&gt;% ggplot(aes(x = water_cent, y = blooms_std)) + geom_point(data = data_tulips, color = clr_dark) + geom_line(aes(group = row, color = row %in% c(20,40)), size = .3) + facet_grid(model ~ shade_cent, labeller = label_both) + scale_color_manual(values = c(`FALSE` = clr_alpha(clr0d), `TRUE` = clr_alpha(clr_dark)), guide = &quot;none&quot;) + coord_cartesian(xlim = c(-1, 1), ylim = c(0, 1)) 9.5.6 Plotting Prior Predictions brms_c8_model_tulips_simple_prior &lt;- update( brms_c8_model_tulips_simple, sample_prior = &quot;only&quot;, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c8_model_tulips_simple_prior&quot;) brms_c8_model_tulips_interaction_prior &lt;- update( brms_c8_model_tulips_interaction, sample_prior = &quot;only&quot;, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c8_model_tulips_interaction_prior&quot;) set.seed(42) rbind(fitted(brms_c8_model_tulips_simple_prior, newdata = new_data, summary = FALSE, ndraws = 20), fitted(brms_c8_model_tulips_interaction_prior, newdata = new_data, summary = FALSE, ndraws = 20)) %&gt;% as_tibble() %&gt;% set_names(mutate(new_data, name = str_c(shade_cent, water_cent, sep = &quot;_&quot;)) %&gt;% pull()) %&gt;% mutate(row = rep(1:20, times = 2), model = rep(c(&quot;simple&quot;, &quot;interaction&quot;), each = n() / 2) %&gt;% factor(., levels = c(&quot;simple&quot;, &quot;interaction&quot;))) %&gt;% pivot_longer(-c(row:model), values_to = &quot;blooms_std&quot;) %&gt;% separate(name, into = c(&quot;shade_cent&quot;, &quot;water_cent&quot;), sep = &quot;_&quot;,convert = TRUE) %&gt;% ggplot(aes(x = water_cent, y = blooms_std, group = row)) + geom_hline(yintercept = 0:1, linetype = 3, color = clr_dark) + geom_line(aes(group = row, color = row %in% c(20,40)), size = .3) + facet_grid(model ~ shade_cent, labeller = label_both) + scale_color_manual(values = c(`FALSE` = clr_alpha(clr0d), `TRUE` = clr_alpha(clr_dark)), guide = &quot;none&quot;) + coord_cartesian(xlim = c(-1, 1), ylim = c(-0.5, 1.5), expand = 0) 9.5.7 {brms} conditional_effects() specifically for simple two-way interactions Simple univariate model: brms_c8_model_rugged_restricted$formula #&gt; log_gdp_std ~ 1 + rugged_std_centered c_eff &lt;- conditional_effects(brms_c8_model_rugged_restricted, plot = FALSE) p1 &lt;- plot(c_eff, line_args = list(color = clr0dd, size = .4), plot = FALSE)[[1]] p2 &lt;- plot(c_eff, line_args = list(color = clr0dd, size = .4), point_args = c(color = clr_dark), points = TRUE, plot = FALSE)[[1]] c_eff &lt;- conditional_effects(brms_c8_model_rugged_restricted, spaghetti = TRUE, ndraws = 200, plot = FALSE) p3 &lt;- plot(c_eff, points = TRUE, point_args = c(color = clr_dark), line_args = c(colour = &quot;black&quot;), spaghetti_args = c(colour = clr_alpha(clr0dd, .1)), plot = FALSE)[[1]] p1 + p2 + p3 Simple bi-variate model, no interaction: brms_c8_model_rugged_index$formula #&gt; log_gdp_std ~ 0 + cont_idx + rugged_std_centered c_eff &lt;- conditional_effects(brms_c8_model_rugged_index, plot = FALSE) p &lt;- plot(c_eff, points = TRUE, point_args = c(color = clr_dark), line_args = c(colour = &quot;black&quot;), plot = FALSE) p[[1]] + p[[2]] Non-linear interaction model: brms_c8_model_rugged_slope$formula #&gt; log_gdp_std ~ 0 + a + b * rugged_std_centered #&gt; a ~ 0 + cont_idx #&gt; b ~ 0 + cont_idx c_eff &lt;- conditional_effects(brms_c8_model_rugged_slope, plot = FALSE) p &lt;- plot(c_eff, points = TRUE, point_args = c(color = clr_dark), line_args = c(colour = &quot;black&quot;), plot = FALSE) p[[1]] + p[[2]] c_eff &lt;- conditional_effects(brms_c8_model_rugged_slope, effects = &quot;rugged_std_centered:cont_idx&quot;, plot = FALSE) p &lt;- plot(c_eff, plot = FALSE) p[[1]] + scale_color_manual(values = c(`1` = clr1, `2` = clr0dd)) + scale_fill_manual(values = c(`1` = fll1, `2` = clr_alpha(clr0dd))) + theme(legend.position = &quot;bottom&quot;) c_eff &lt;- conditional_effects(brms_c8_model_rugged_slope, effects = &quot;cont_idx:rugged_std_centered&quot;, plot = FALSE) plot(c_eff, plot = FALSE)[[1]] + scale_color_manual(values = c(clr1, clr0d, clr_dark)) + scale_fill_manual(values = c(clr1, clr0d, clr_dark) %&gt;% clr_lighten()) + theme(legend.position = &quot;bottom&quot;) brms_c8_model_rugged_slope$data %&gt;% summarize(mean = mean(rugged_std_centered), `mean + 1 sd` = mean(rugged_std_centered) + sd(rugged_std_centered), `mean - 1 sd` = mean(rugged_std_centered) - sd(rugged_std_centered)) %&gt;% mutate_all(round, digits = 2) #&gt; mean mean + 1 sd mean - 1 sd #&gt; 1 0 0.19 -0.19 Interaction models with two variables brms_c8_model_tulips_interaction$formula #&gt; blooms_std ~ 1 + water_cent + shade_cent + water_cent:shade_cent c_eff &lt;- conditional_effects(brms_c8_model_tulips_interaction, effects = &quot;water_cent:shade_cent&quot;, int_conditions = list(shade_cent = -1:1), plot = FALSE) plot(c_eff, points = TRUE, plot = FALSE)[[1]] + scale_fill_manual(values = c(clr3, clr0d, clr_dark), guide = &quot;none&quot;) + scale_colour_manual(values = c(clr3, clr0d, clr_dark), guide = &quot;none&quot;) + scale_x_continuous(breaks = -1:1) + facet_wrap(~ shade_cent, labeller = label_both) 9.6 pymc3 section × "],
["bayesian-statistics-the-fun-way.html", "10 Bayesian Statistics the Fun Way 10.1 Conditioning Probabilities 10.2 Combining Probailities based on logic 10.3 The binomial distribution 10.4 The beta distribution 10.5 Bayes’ Theorem 10.6 Parameter Estimation (I) 10.7 The normal distribution 10.8 Cummulative Density and Quantile Function 10.9 Parameter estimation with prior probabilities 10.10 Monte CarloSimulation 10.11 Posterior Odds 10.12 Parameter Estimation (II)", " 10 Bayesian Statistics the Fun Way by Will Kurt 10.1 Conditioning Probabilities \\[ \\begin{eqnarray} D &amp; = &amp; observed~data\\\\ H_{1} &amp; = &amp; Hypothesis\\\\ X &amp; = &amp;prior~belief\\\\ \\end{eqnarray} \\] Allow us to formulate the probability of the observed data given our hypothesis and our prior belief. \\[ P(D | H_{1}, X) \\] To compare different hypothesis, use the ratio of probabilities (odds): \\[ \\frac{P(D | H_{1}, X)}{P(D | H_{2}, X)} &gt; 1 \\] 10.2 Combining Probailities based on logic Rules for \\(AND\\) (\\(\\land\\)), \\(OR\\) (\\(\\lor\\)) and \\(NOT\\) (\\(\\neg\\)). \\(NOT:\\) \\[ \\begin{eqnarray} P(X) &amp; = &amp; p\\\\ \\neg P(X) &amp; = &amp; 1 - p \\end{eqnarray} \\] \\(AND\\) \\[ \\begin{eqnarray} P(Y) &amp; = &amp; q \\\\ P(X) \\land P(Y) &amp; = &amp; P(X,Y) = p \\times q \\end{eqnarray} \\] \\(OR\\) (mutually exclusive events): \\[ P(X) \\lor P(Y) = P(X),P(Y) = p + q \\] while: \\[ P(X) \\land P(Y) = 0 \\] \\(OR\\) (non-mutually exclusive events): \\[ P(X) \\lor P(Y) = P(X) + P(Y) - P(X, Y) \\] 10.3 The binomial distribution Factorial (factorial(x)): \\[ x! = x \\times x-1 \\times x -2 ... \\] The binomial coefficient (choose(n, k)): \\[ {n \\choose k} = \\frac{n!}{k! \\times (n - k)!} \\] The binomial distribution (a Probability Mass Function, PMF): \\[ B(k;n,p) = {n \\choose k} \\times p^k \\times (1 - p) ^{n-k} \\] wdh &lt;- 5000 n &lt;- 10 p = 0.5 tibble(k = rbinom(wdh, n , p)) %&gt;% group_by(k) %&gt;% count(name = &quot;nn&quot;) %&gt;% ungroup() %&gt;% ggplot(aes(x = k, y = nn/wdh)) + geom_bar(stat = &quot;identity&quot;, color = clr1, fill = fll1) + labs(x = &quot;k&quot;, y = glue(&quot;B(k; {n}, {p})&quot;)) + coord_cartesian(xlim = c(-.5, 10.5)) n &lt;- 10 p = 1/6 tibble(k = rbinom(wdh, n , p)) %&gt;% group_by(k) %&gt;% count(name = &quot;nn&quot;) %&gt;% ungroup() %&gt;% ggplot(aes(x = k, y = nn/wdh)) + geom_bar(stat = &quot;identity&quot;, color = clr1, fill = fll1) + labs(x = &quot;k&quot;, y = glue(&quot;B(k; {n}, {round(p,2)})&quot;)) + coord_cartesian(xlim = c(-.5, 10.5)) Cumulative probability to have at least \\(x\\) successes in \\(n\\) trials (pbinom(x-1, n, p, lower.tail = FALSE)): \\[ \\sum_{k=x}^n B(k;n,p) \\] Similarly, less then \\(x\\) successes in \\(n\\) trials (pbinom(x, n, p)): \\[ \\sum_{k=0}^{x-1} B(k;n,p) \\] 10.4 The beta distribution \\[ Beta(p;\\alpha,\\beta) = \\frac{p^{\\alpha -1} \\times (1 - p)^{\\beta - 1}}{beta(\\alpha, \\beta)} \\] Example for an \\(n = 41\\), with \\(\\alpha = 14\\) (successes) and \\(\\beta = 27\\) (fails). alpha &lt;- 14 beta &lt;- 27 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr1, fill = fll1, n = 500) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) Probability, that chance of sucess is less than 0.5: \\[ \\int_{0}^{0.5} Beta(p; 14, 27) \\] x_cutoff &lt;- 0.5 integrate(function(p){ dbeta(p, 14, 27) }, 0, x_cutoff) #&gt; 0.9807613 with absolute error &lt; 5.9e-06 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(xlim = c(0, x_cutoff), fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr2, fill = fll2, n = 500) + geom_vline(xintercept = x_cutoff, linetype = 3) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) alpha &lt;- 5 beta &lt;- 1195 x_cutoff &lt;- 0.005 integrate(function(p){ dbeta(p, alpha, beta) }, x_cutoff, 1) #&gt; 0.2850559 with absolute error &lt; 1e-04 ggplot(tibble(x = seq(0, .01, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(xlim = c(x_cutoff, .01), fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr2, fill = fll2, n = 500) + geom_vline(xintercept = x_cutoff, linetype = 3) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) Exercises # 1) integrate(function(p){ dbeta(p, 4, 6) }, 0.6, 1) #&gt; 0.09935258 with absolute error &lt; 1.1e-15 # 2) integrate(function(p){ dbeta(p, 9, 11) }, 0.45, 0.55) #&gt; 0.30988 with absolute error &lt; 3.4e-15 # 3) integrate(function(p){ dbeta(p, 109, 111) }, 0.45, 0.55) #&gt; 0.8589371 with absolute error &lt; 9.5e-15 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = 9, shape2 = 11) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(fun = function(x){ -dbeta(x, shape1 = 109, shape2 = 111) }, geom = &quot;area&quot;, color = clr1, fill = fll1, n = 500) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) 10.5 Bayes’ Theorem conditional probability The probability of A given B is \\(P(A | B)\\) Dependence updates the product rule of probabilities: \\[ P(A,B) = P(A) \\times P(B | A) \\] (This also holds for independend probabilities, where \\(P(B) = P(B|A)\\)) Bayes’ Theorem (reversing the condition to calculate the probability of the event we are conditioning on \\(P(A|B) \\rightarrow P(B|A)\\)) \\[ P(A|B) = \\frac{P(A)P(B|A)}{P(B)} \\] The important parts here are the posterior probability\\(P(H|D)\\): how strongly we belief in our hypothesis given the data likelyhood\\(P(D|H)\\): the probability of data if the hypothesis were true prior probability\\(P(H)\\): how likely our hypothesis is in the first place Unnormalized posterior \\[ P(H|D) \\propto P(H) \\times P(D|H) \\] 10.6 Parameter Estimation (I) Expectation / mean \\[ \\mu = \\sum_{1}^{n}p_{i}x_{i} \\] n &lt;- 150 mn &lt;- 3 tibble( y = rnorm(n = n, mean = mn), n = 1:n, cum_y = cumsum(y), mean_y = cum_y / n) %&gt;% ggplot(aes(x = n, y = mean_y)) + geom_hline(yintercept = mn, linetype = 3) + geom_point(aes(y = y), color = clr0, size = .75, alpha = .5) + geom_line(color = clr2, size = .75) Spread, mean absolute deviation \\[ MAD(x) = \\frac{1}{n} \\times \\sum_{1}^{n} | x_{i} - \\mu| \\] Spread, variation \\[ Var(x) = \\frac{1}{n} \\times \\sum_{1}^{n} (x_{i} - \\mu) ^{2} \\] Spread, standard deviation \\[ \\sigma = \\sqrt{\\frac{1}{n} \\times \\sum_{1}^{n} (x_{i} - \\mu) ^{2}} \\] Note that in R, var() and sd() uses \\(\\frac{1}{n-1}\\) as denominator for the normalization: var_k &lt;- function(x){ (1/length(x)) * sum( (x - mean(x)) ^ 2 ) } sd_k &lt;- function(x){ sqrt(var_k(x)) } x &lt;- 1:10 n &lt;- length(x) var(x) * ((n-1)/n) == var_k(x) #&gt; [1] TRUE sd(x) * sqrt((n-1)/n) == sd_k(x) #&gt; [1] TRUE 10.7 The normal distribution The probability density function (PDF) for the normal distribution (dnorm()): \\[ N(\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\times e ^{- \\frac{(x - \\mu) ^ 2}{2\\sigma^2}} \\] mu &lt;- 20.6 sigma &lt;- 1.62 x_cutoff &lt;- 18 ggplot(tibble(x = seq(mu - 4 * sigma, mu + 4 * sigma, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dnorm(x, mu, sigma) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(fun = function(x){ dnorm(x, mu, sigma) }, xlim = c(mu - 4 * sigma, x_cutoff), geom = &quot;area&quot;, color = clr2, fill = fll2, n = 500) + geom_vline(xintercept = x_cutoff, linetype = 3) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) integrate(function(x){ dnorm(x, mu, sigma) }, 0, x_cutoff) #&gt; 0.05425369 with absolute error &lt; 3.5e-05 Known probability mass under a normal distribution in terms of ist standard deviation: distance from \\(\\mu\\) probability \\(\\sigma\\) 68 % \\(2 \\sigma\\) 95 % \\(3 \\sigma\\) 99.7 % Excercises x &lt;- c(100, 99.8, 101, 100.5,99.7) mu &lt;- mean(x) sigma &lt;- sd(x) x_cutoff &lt;- 100.4 ggplot(tibble(x = seq(mu - 4 * sigma, mu + 4 * sigma, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dnorm(x, mu, sigma) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(fun = function(x){ dnorm(x, mu, sigma) }, xlim = c(x_cutoff, mu + 4 * sigma), geom = &quot;area&quot;, color = clr2, fill = fll2, n = 500) + geom_vline(xintercept = x_cutoff, linetype = 3) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) 1 - (integrate(function(x){ dnorm(x, mu, sigma) }, mu - sigma, x_cutoff)[[1]] + (1-.68)/2) #&gt; [1] 0.3550062 10.8 Cummulative Density and Quantile Function Beta distribution example Mean of Beta distribution \\[ \\mu_{Beta} = \\frac{\\alpha}{\\alpha + \\beta} \\] alpha &lt;- 300 beta &lt;- 39700 mu &lt;- alpha / (alpha + beta) med &lt;- qbeta(.5, shape1 = alpha, shape2 = beta) bound_left &lt;- .006 bound_right &lt;- .009 10.9 Parameter estimation with prior probabilities alpha_data &lt;- 2 beta_data &lt;- 3 alpha_prior &lt;- 1 beta_prior &lt;- 41 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_data, shape2 = beta_data) }, geom = &quot;line&quot;, color = clr0,linetype = 1, n = 500) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_prior, shape2 = beta_prior) }, geom = &quot;line&quot;, color = clr0,linetype = 2, n = 500) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_data + alpha_prior, shape2 = beta_data + beta_prior) }, geom = &quot;area&quot;, color = clr1, fill = fll1, size = .2, n = 500) + labs(x = &quot;Probability&quot;, y = &quot;Density&quot;, caption = &quot;**solid:** data; **dashed:** prior; **filled:** posterior&quot;) + coord_cartesian(ylim = c(0, 15)) + theme(plot.caption = ggtext::element_markdown(halign = .5, hjust = .5)) 10.10 Monte CarloSimulation alpha_a &lt;- 36 beta_a &lt;- 114 alpha_b &lt;- 50 beta_b &lt;- 100 alpha_prior &lt;- 3 beta_prior &lt;- 7 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_a + alpha_prior, shape2 = beta_a + beta_prior) }, geom = &quot;area&quot;, aes(color = &quot;a&quot;, fill = after_scale(clr_alpha(color))), size = .5, n = 500) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_b + alpha_prior, shape2 = beta_b + beta_prior) }, geom = &quot;area&quot;, aes(color = &quot;b&quot;, fill = after_scale(clr_alpha(color))), size = .5, n = 500) + labs(x = &quot;Probability&quot;, y = &quot;Density&quot;) + scale_color_manual(&quot;Variant&quot;, values = c(a = clr0, b = clr2)) n_trials &lt;- 10^5 mc_simulation &lt;- tibble(samples_a = rbeta(n_trials, alpha_a + alpha_prior, beta_a + beta_prior), samples_b = rbeta(n_trials, alpha_b + alpha_prior, beta_b + beta_prior), samples_ratio = samples_b / samples_a) p_b_superior &lt;- sum(mc_simulation$samples_b &gt; mc_simulation$samples_a)/n_trials p_b_superior #&gt; [1] 0.95979 p_hist &lt;- mc_simulation %&gt;% ggplot(aes(x = samples_ratio)) + geom_histogram(color = clr2, fill = fll2, size = .2, bins = 20,boundary = 1) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) p_ecdf &lt;- mc_simulation %&gt;% ggplot() + stat_function(fun = function(x){(ecdf(x = mc_simulation$samples_ratio))(x)}, xlim = c(range(mc_simulation$samples_ratio)), geom = &quot;area&quot;,color = clr2, fill = fll2, size = .2, n = 500) + labs(x = &quot;Variant Ratio = Improvement&quot;, y = &quot;Cumulative Probability&quot;) p_hist / p_ecdf &amp; coord_cartesian(xlim = c(0.2,3.3), expand = 0) 10.11 Posterior Odds For compering Hypotheses: ratio of posterior: \\[ posterior odds = \\frac{P(H_1) \\times P(D | H_{1})}{P(H_2) \\times P(D | H_{2})} = O(H_{1}) \\times \\frac{P(D | H_{1})}{P(D | H_{2})} \\] This consists of the Bayes factor: \\[ \\frac{P(D | H_{1})}{P(D | H_{2})} \\] and the ratio of prior probabilities \\[ O(H_{1}) = \\frac{P(H_1)}{P(H_2)} \\] rough guide to evaluate poterior odds: Posterior odds Strength of evidence 1 to 3 Interesting but not conclusive 3 to 20 Looks like we’re onto something 20 to 150 Strong evidence in favor of \\(H_1\\) &gt; 150 Overwhelming evidence 10.12 Parameter Estimation (II) dx &lt;- 0.01 bayes_factor &lt;- function(h_top, h_bottom, n_success = 24, n_total = 100){ (h_top ^ n_success * (1 - h_top) ^ (n_total - n_success)) / (h_bottom ^ n_success * (1 - h_bottom) ^ (n_total - n_success)) } bayes_fs &lt;- tibble(hypothesis = seq(0, 1, by = dx), bayes_factor = bayes_factor(hypothesis, h_bottom = 0.5)) %&gt;% mutate(prior = if_else(between(hypothesis, 0.2, 0.3), .001, 1), posterior = prior * bayes_factor, posterior_normalized = posterior / sum(posterior)) p_bayes_factor &lt;- bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = bayes_factor)) + geom_area(color = clr1, fill = fll1, size = .3) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) p_prior &lt;- bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = prior)) + geom_area(color = clr0, fill = fll0, size = .3) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) p_posterior &lt;- bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = posterior)) + geom_area(color = clr2, fill = fll2, size = .3) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) p_posterior_n &lt;- bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = posterior_normalized)) + geom_area(color = clr2, fill = fll2, size = .3) p_bayes_factor / p_prior / p_posterior / p_posterior_n hypothesis bayes_factor prior posterior posterior_normalized 0.24 1478776 0.001 1478.776 0.0004708 Probability of true chance is smaller “one in two”. bayes_fs %&gt;% filter(hypothesis &lt; 0.5) %&gt;% summarise(p_lower_than_half = sum(posterior_normalized)) #&gt; # A tibble: 1 x 1 #&gt; p_lower_than_half #&gt; &lt;dbl&gt; #&gt; 1 1.00 Expectation of the probability distribution (sum of expectations weighted by their value) sum(bayes_fs$posterior_normalized * bayes_fs$hypothesis) #&gt; [1] 0.2402704 Or (because of gap) choose most likely estimate: bayes_fs %&gt;% filter(posterior_normalized == max(posterior_normalized)) %&gt;% knitr::kable() hypothesis bayes_factor prior posterior posterior_normalized 0.19 688568.9 1 688568.9 0.2192415 bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = bayes_factor / sum(bayes_factor))) + geom_area(color = clr2, fill = fll2, size = .3) + stat_function(fun = function(x){ dbeta(x, shape1 = 24 + 1 , shape2 = 76 + 1) * dx}, geom = &quot;point&quot;, shape = 21, color = clr2, fill = fll2, size = 1.5, n = 500) Exercises bayes_fs_e1 &lt;- tibble(hypothesis = seq(0, 1, by = dx), bayes_factor = bayes_factor(hypothesis, h_bottom = 0.24)) %&gt;% mutate(prior = if_else(between(hypothesis, 0.2, 0.3), .001, 1), posterior = prior * bayes_factor, posterior_normalized = posterior / sum(posterior)) bayes_fs_e1 %&gt;% ggplot(aes(x = hypothesis, y = bayes_factor / sum(bayes_factor))) + geom_area(color = clr2, fill = fll2, size = .3) + stat_function(fun = function(x){ dbeta(x, shape1 = 24 + 1 , shape2 = 76 + 1) * dx}, geom = &quot;point&quot;, shape = 21, color = clr2, fill = fll2, size = 1.5, n = 500) bayes_fs_e2 &lt;- tibble(hypothesis = seq(0, 1, by = dx), bayes_factor = bayes_factor(hypothesis, h_bottom = 0.24)) %&gt;% mutate(prior = 1.05 ^ (seq_along(hypothesis) - 1), posterior = prior * bayes_factor, posterior_normalized = posterior / sum(posterior)) bayes_fs_e2 %&gt;% ggplot(aes(x = hypothesis, y = prior)) + geom_area(color = clr0, fill = fll0, size = .3) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) bayes_fs_e2 %&gt;% ggplot(aes(x = hypothesis, y = posterior_normalized)) + geom_area(color = clr2, fill = fll2, size = .3) × "],
["mixed-models-with-r.html", "11 Mixed Models with R 11.1 Standard regregression model 11.2 mixed nodel 11.3 Add random slope 11.4 Cross Classified models 11.5 Hierachical structure 11.6 Residual Structure 11.7 Generalized Linear Mixed Models 11.8 Issues/ Considderations 11.9 Formula summary", " 11 Mixed Models with R by Michael Clark load(&quot;data/gpa.RData&quot;) gpa &lt;- gpa %&gt;% as_tibble() 11.1 Standard regregression model \\[ gpa = b_{intercept} + b_{occ} \\times occasion + \\epsilon \\] Coefficients \\(b\\) for intercept and effect of time. The error \\(\\epsilon\\) is assumed to be normally distributed with \\(\\mu = 0\\) and some standard deviation \\(\\sigma\\). \\[ \\epsilon \\sim \\mathscr{N}(0, \\sigma) \\] alternate notation, with emphasis on the data generating process: \\[ gpa ~ \\sim \\mathscr{N}(\\mu, \\sigma)\\\\ \\mu = b_{intercept} + b_{occ} \\times occasion \\] 11.2 mixed nodel 11.2.1 student specific effect (initial depiction) \\[ gpa = b_{intercept} + b_{occ} \\times occasion + ( \\textit{effect}_{student} + \\epsilon )\\\\ \\textit{effect}_{student} \\sim \\mathscr{N}(0, \\tau) \\] focusing on the coefficients (rather than on sources of error): \\[ gpa = ( b_{intercept} + \\textit{effect}_{student} ) + b_{occ} \\times occasion + \\epsilon \\] or (shorter) \\[ gpa = b_{int\\_student} + b_{occ} \\times occasion + \\epsilon \\] \\(\\rightarrow\\) this means student specific intercepts… \\[ b_{int\\_student} \\sim \\mathscr{N}(b_{intercept}, \\tau) \\] …that are normally distributed with the mean of the overall intercept (random intercepts model) 11.2.2 as multi-level model two-part regression model (one at observation level, one at student level) (this is the same as above, just needs ‘plugging in’) \\[ gpa = b_{int\\_student} + b_{occ} \\times occasion + \\epsilon\\\\ b_{int\\_student} = b_{intercept} + \\textit{effect}_{student} \\] ! There is no student-specific effect for \\(occasion\\) (which is termed fixed effect), and there is no random component gpa_lm &lt;- lm(gpa ~ occasion, data = gpa) gpa %&gt;% ggplot(aes(x = year - 1 + as.numeric(semester)/2, y = gpa, group = student)) + geom_line(alpha = .2) + geom_abline(slope = gpa_lm$coefficients[[2]], intercept = gpa_lm$coefficients[[1]], color = clr2, size = 1) + labs(x = &quot;semester&quot;) + coord_cartesian(ylim = c(1,4), expand = 0) pander::pander(summary(gpa_lm), round = 3)   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.599 0.018 145.7 0 occasion 0.106 0.006 18.04 0 Fitting linear model: gpa ~ occasion Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 1200 0.3487 0.2136 0.2129 Student effect not taken into account. 11.2.3 Mixed Model gpa_mixed &lt;- lmer(gpa ~ occasion + (1 | student), data = gpa) (Test automatic equation creation) library(equatiomatic) # Give the results to extract_eq extract_eq(gpa_mixed,) \\[ \\begin{aligned} \\operatorname{gpa}_{i} &amp;\\sim N \\left(\\alpha_{j[i]} + \\beta_{1}(\\operatorname{occasion}), \\sigma^2 \\right) \\\\ \\alpha_{j} &amp;\\sim N \\left(\\mu_{\\alpha_{j}}, \\sigma^2_{\\alpha_{j}} \\right) \\text{, for student j = 1,} \\dots \\text{,J} \\end{aligned} \\] term value se t p_value lower_2.5 upper_97.5 Intercept 2.599 0.022 119.8 0 2.557 2.642 occasion 0.106 0.004 26.1 0 0.098 0.114 group effect variance sd var_prop student Intercept 0.064 0.252 0.523 Residual 0.058 0.241 0.477 Coefficients (fixed effects) for time and intercept are the same as lm() Getting confidence intervals from a mixed model (since \\(p\\) values are not given (== 0 ?)) confint(gpa_mixed) #&gt; 2.5 % 97.5 % #&gt; .sig01 0.22517423 0.2824604 #&gt; .sigma 0.23071113 0.2518510 #&gt; (Intercept) 2.55665145 2.6417771 #&gt; occasion 0.09832589 0.1143027 mm_cinf &lt;- mixedup::extract_vc(gpa_mixed) mm_cinf %&gt;% pander::pander() Table continues below   group effect variance sd sd_2.5 sd_(Intercept)|student student Intercept 0.064 0.252 0.225 sigma Residual 0.058 0.241 0.231   sd_97.5 var_prop sd_(Intercept)|student 0.282 0.523 sigma 0.252 0.477 student effect \\(\\tau\\) = 0.252 / 0.064 (sd / var) Percentage of student variation as share of the total variation (intraclass correlation): 0.064 / 0.122 = 0.5245902 11.2.4 Estimation of random effects Random effect mixedup::extract_random_effects(gpa_mixed) %&gt;% head(5) %&gt;% knitr::kable() group_var effect group value se lower_2.5 upper_97.5 student Intercept 1 -0.071 0.092 -0.251 0.109 student Intercept 2 -0.216 0.092 -0.395 -0.036 student Intercept 3 0.088 0.092 -0.091 0.268 student Intercept 4 -0.187 0.092 -0.366 -0.007 student Intercept 5 0.030 0.092 -0.149 0.210 Random intercept (intercept + random effect) mm_coefs &lt;- mixedup::extract_coef(gpa_mixed) mm_coefs %&gt;% head(5) %&gt;% knitr::kable() group_var effect group value se lower_2.5 upper_97.5 student Intercept 1 2.528 0.095 2.343 2.713 student Intercept 2 2.383 0.095 2.198 2.568 student Intercept 3 2.687 0.095 2.502 2.872 student Intercept 4 2.412 0.095 2.227 2.597 student Intercept 5 2.629 0.095 2.444 2.814 library(merTools) mm_intervals &lt;- predictInterval(gpa_mixed) %&gt;% as_tibble() mm_mean_sd &lt;- REsim(gpa_mixed) %&gt;% as_tibble() sd_level &lt;- .95 mm_mean_sd %&gt;% mutate(sd_leveled = sd * qnorm(1 - ((1 - sd_level)/2)), sig = (median + sd_leveled) &gt;= 0 &amp; (median - sd_leveled) &lt;= 0) %&gt;% arrange(median) %&gt;% mutate(rank = row_number()) %&gt;% arrange(groupID) %&gt;% ggplot(aes(x = rank)) + geom_segment(aes(xend = rank, y = median - sd_leveled, yend = median + sd_leveled, color = sig)) + geom_point(aes(y = median), size = .3, alpha = .5) + geom_hline(yintercept = 0, size = .4, linetype = 3) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;Student&quot;, y = &quot;Coefficient&quot;, caption = &quot;interval estimates of random effects&quot;) 11.2.5 Prediction gpa_predictions &lt;- tibble(lm = predict(gpa_lm), lmm_no_random_effects = predict(gpa_mixed, re.form = NA), lmm_with_random_effects = predict(gpa_mixed)) %&gt;% bind_cols(gpa, .) gpa_predictions %&gt;% ggplot(aes(x = lm)) + geom_point(aes(y = lmm_with_random_effects, color = &quot;with_re&quot;)) + geom_point(aes(y = lmm_no_random_effects, color = &quot;no_re&quot;)) + scale_color_manual(values = c(no_re = clr2, with_re = clr0d)) student_select &lt;- 1:2 gpa_predictions %&gt;% filter(student %in% student_select) %&gt;% ggplot(aes(x = occasion)) + geom_point(aes(y = gpa, color = student))+ geom_abline(data = tibble(slope = c(gpa_lm$coefficients[[2]], mm_fixed$value[c(2,2)]), intercept = c(gpa_lm$coefficients[[1]], mm_coefs$value[as.numeric(as.character(mm_coefs$group)) %in% student_select]), model = c(&quot;lm&quot;, as.character(mm_coefs$group[as.numeric(as.character(mm_coefs$group)) %in% student_select]))), aes(slope = slope, intercept = intercept, color = model), size = .6) + scale_color_manual(values = c(lm = clr2, `1` = clr1, `2` = clr0d), labels = c( &quot;lm&quot;, &quot;lmm (student1)&quot;, &quot;lmm (student2)&quot;)) 11.2.6 Cluster Level Covariates If a cluster level covariate is added (eg. sex), \\(b_{int\\_student}\\) turns into: \\[ b_{int\\_student} = b_{intercept} + b_{sex} \\times \\textit{sex} + \\textit{effect}_{student} \\] plugging this into the model will result in \\[ gpa = b_{intercept} + b_{occ} \\times \\textit{occasion} + b_{sex} \\times \\textit{sex} + ( \\textit{effect}_{student} + \\epsilon) \\] 11.3 Add random slope gpa_mixed2 &lt;- lmer(gpa ~ occasion + (1 + occasion | student), data = gpa) mixedup::extract_fixed_effects(gpa_mixed2) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 2.599 0.018 141.592 0 2.563 2.635 occasion 0.106 0.006 18.066 0 0.095 0.118 mixedup::extract_vc(gpa_mixed2, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop student Intercept 0.045 0.213 0.491 student occasion 0.005 0.067 0.049 Residual 0.042 0.206 0.460 gpa_mixed2_rc &lt;- mixedup::extract_random_coefs(gpa_mixed2) correlation of the intercepts and slopes (negative, so students with a low starting score tend to increase a little more) VarCorr(gpa_mixed2) %&gt;% as_tibble() %&gt;% knitr::kable() grp var1 var2 vcov sdcor student (Intercept) NA 0.0451934 0.2125875 student occasion NA 0.0045039 0.0671114 student (Intercept) occasion -0.0014016 -0.0982391 Residual NA NA 0.0423879 0.2058832 gpa_lm_separate &lt;- gpa %&gt;% group_by(student) %&gt;% nest() %&gt;% mutate(mod = map(data,function(data){lm(gpa ~ occasion, data = data)})) %&gt;% bind_cols(., summarise_model(.)) p_intercepts &lt;- ggplot() + geom_density(data = gpa_mixed2_rc %&gt;% filter(effect == &quot;Intercept&quot;), aes(x = value, color = &quot;mixed&quot;, fill = after_scale(clr_alpha(color)))) + geom_density(data = gpa_lm_separate, aes(x = intercept, color = &quot;separate&quot;, fill = after_scale(clr_alpha(color)))) + labs(x = &quot;intercept&quot;) + xlim(1.5, 4) p_slopes &lt;- ggplot() + geom_density(data = gpa_mixed2_rc %&gt;% filter(effect == &quot;occasion&quot;), aes(x = value, color = &quot;mixed&quot;, fill = after_scale(clr_alpha(color)))) + geom_density(data = gpa_lm_separate, aes(x = slope, color = &quot;separate&quot;, fill = after_scale(clr_alpha(color)))) + labs(x = &quot;slope&quot;) + xlim(-.2, .4) p_intercepts + p_slopes + plot_layout(guides = &quot;collect&quot;) &amp; scale_color_manual(&quot;model&quot;, values = c(separate = clr0d, mixed = clr2)) &amp; theme(legend.position = &quot;bottom&quot;) \\(\\rightarrow\\) mixed model intercepts and slopes are less extreme In both cases the mixed model shrinks what would have been the by-group estimate, which would otherwise overfit in this scenario. This regularizing effect is yet another bonus when using mixed models. gpa_predictions &lt;- tibble(lmm_with_random_slope = predict(gpa_mixed2)) %&gt;% bind_cols(gpa_predictions, .) gpa_mixed2_rc_wide &lt;- gpa_mixed2_rc %&gt;% dplyr::select(group_var, group, effect, value) %&gt;% pivot_wider(names_from = effect, values_from = value) student_select &lt;- 1:2 p_two_students &lt;- gpa_predictions %&gt;% filter(student %in% student_select) %&gt;% ggplot(aes(x = occasion)) + geom_point(aes(y = gpa, color = student))+ geom_abline(data = tibble(slope = c(gpa_lm$coefficients[[2]], gpa_mixed2_rc_wide$occasion[as.numeric(as.character(gpa_mixed2_rc_wide$group)) %in% student_select]), intercept = c(gpa_lm$coefficients[[1]], gpa_mixed2_rc_wide$Intercept[as.numeric(as.character(gpa_mixed2_rc_wide$group)) %in% student_select]), model = c(&quot;lm&quot;, as.character(gpa_mixed2_rc_wide$group[as.numeric(as.character(gpa_mixed2_rc_wide$group)) %in% student_select]))), aes(slope = slope, intercept = intercept, color = model), size = .6) + scale_color_manual(values = c(lm = clr2, `1` = clr1, `2` = clr0d), labels = c( &quot;lm&quot;, &quot;lmm (student1)&quot;, &quot;lmm (student2)&quot;)) p_all_mod &lt;- ggplot(data = gpa_predictions, aes(x = occasion, y = gpa)) + geom_abline(data = tibble(slope = c(gpa_mixed2_rc_wide$occasion, gpa_lm$coefficients[[2]]), intercept = c(gpa_mixed2_rc_wide$Intercept, gpa_lm$coefficients[[1]]), modeltype = c(rep(&quot;lmm (random slope)&quot;, length(gpa_mixed2_rc_wide$Intercept)), &quot;lm&quot;)), aes(slope = slope, intercept = intercept, color = modeltype), size = .6) + scale_color_manual(values = c(`lmm (random slope)` = clr_alpha(clr0d ,.6), lm = clr2)) p_two_students + p_all_mod + plot_layout(guides = &quot;collect&quot;) &amp; xlim(0,5) &amp; ylim(2.2, 4) &amp; theme(legend.position = &quot;bottom&quot;) 11.4 Cross Classified models Setups where data are grouped by several factors but these are not nested (all participants get to see all images). These are crossed random effects. load(&quot;data/pupils.RData&quot;) pupils %&gt;% head() %&gt;% knitr::kable() PUPIL primary_school_id secondary_school_id achievement sex ses primary_denominational secondary_denominational 1 1 2 6.6 female highest no no 2 1 1 5.7 male lowest no yes 3 1 17 4.5 male 2 no no 4 1 3 4.4 male 2 no no 5 1 4 5.8 male 3 no yes 6 1 4 5.0 female 4 no yes pupils_crossed &lt;- lmer( achievement ~ sex + ses + ( 1 | primary_school_id ) + ( 1 | secondary_school_id ), data = pupils ) mixedup::extract_fixed_effects(pupils_crossed) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 5.924 0.123 48.303 0.000 5.684 6.164 sexfemale 0.261 0.046 5.716 0.000 0.171 0.350 ses2 0.132 0.118 1.122 0.262 -0.098 0.362 ses3 0.098 0.110 0.890 0.373 -0.118 0.314 ses4 0.298 0.105 2.851 0.004 0.093 0.503 ses5 0.354 0.101 3.514 0.000 0.156 0.551 seshighest 0.616 0.110 5.602 0.000 0.401 0.832 mixedup::extract_vc(pupils_crossed, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop primary_school_id Intercept 0.173 0.416 0.243 secondary_school_id Intercept 0.066 0.257 0.093 Residual 0.473 0.688 0.664 pupils_varicance_components_random_effects &lt;- REsim(pupils_crossed) %&gt;% as_tibble() pupils_varicance_components_random_effects %&gt;% mutate(sd_leveled = sd * qnorm(1 - ((1 - sd_level)/2)), sig = (median + sd_leveled) &gt;= 0 &amp; (median - sd_leveled) &lt;= 0) %&gt;% group_by(groupFctr) %&gt;% arrange(groupFctr, median) %&gt;% mutate(rank = row_number()) %&gt;% arrange(groupFctr, groupID) %&gt;% ungroup() %&gt;% ggplot(aes(x = rank)) + geom_segment(aes(xend = rank, y = median - sd_leveled, yend = median + sd_leveled, color = sig)) + geom_point(aes(y = median), size = .3, alpha = .5) + geom_hline(yintercept = 0, size = .4, linetype = 3) + facet_wrap(groupFctr ~ ., scales = &quot;free_x&quot;) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;Group&quot;, y = &quot;Effect Range&quot;, caption = &quot;interval estimates of random effects&quot;) Note that we have the usual extensions here if desired. As an example, we could also do random slopes for student level characteristics. 11.5 Hierachical structure These are setups, where different grouping factors are nested within each other (eg. cities, counties, states). load(&quot;data/nurses.RData&quot;) nurses %&gt;% head() %&gt;% knitr::kable() hospital ward wardid nurse age sex experience stress wardtype hospsize treatment 1 1 11 1 36 Male 11 7 general care large Training 1 1 11 2 45 Male 20 7 general care large Training 1 1 11 3 32 Male 7 7 general care large Training 1 1 11 4 57 Female 25 6 general care large Training 1 1 11 5 46 Female 22 6 general care large Training 1 1 11 6 60 Female 22 6 general care large Training nurses_hierach &lt;- lmer( stress ~ age + sex + experience + treatment + wardtype + hospsize + ( 1 | hospital) + ( 1 | hospital:ward), # together same as ( 1 | hospital / ward) data = nurses ) mixedup::extract_fixed_effects(nurses_hierach) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 5.380 0.185 29.128 0.000 5.018 5.742 age 0.022 0.002 10.053 0.000 0.018 0.026 sexFemale -0.453 0.035 -12.952 0.000 -0.522 -0.385 experience -0.062 0.004 -13.776 0.000 -0.070 -0.053 treatmentTraining -0.700 0.120 -5.843 0.000 -0.935 -0.465 wardtypespecial care 0.051 0.120 0.424 0.671 -0.184 0.286 hospsizemedium 0.489 0.202 2.428 0.015 0.094 0.884 hospsizelarge 0.902 0.275 3.280 0.001 0.363 1.440 mixedup::extract_vc(nurses_hierach, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop hospital:ward Intercept 0.337 0.580 0.500 hospital Intercept 0.119 0.345 0.177 Residual 0.217 0.466 0.323 nurses_varicance_components_random_effects &lt;- REsim(nurses_hierach) %&gt;% as_tibble() nurses_varicance_components_random_effects %&gt;% mutate(sd_leveled = sd * qnorm(1 - ((1 - sd_level)/2)), sig = (median + sd_leveled) &gt;= 0 &amp; (median - sd_leveled) &lt;= 0) %&gt;% group_by(groupFctr) %&gt;% arrange(groupFctr, median) %&gt;% mutate(rank = row_number()) %&gt;% arrange(groupFctr, groupID) %&gt;% ungroup() %&gt;% ggplot(aes(x = rank)) + geom_segment(aes(xend = rank, y = median - sd_leveled, yend = median + sd_leveled, color = sig)) + geom_point(aes(y = median), size = .3, alpha = .5) + geom_hline(yintercept = 0, size = .4, linetype = 3) + facet_wrap(groupFctr ~ ., scales = &quot;free_x&quot;) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + ylim(-2,2) + labs(x = &quot;Group&quot;, y = &quot;Effect Range&quot;, caption = &quot;interval estimates of random effects&quot;) 11.5.1 Crossed vs. nested nurses_hierach2 &lt;- lmer( stress ~ age + sex + experience + treatment + wardtype + hospsize + ( 1 | hospital ) + ( 1 | hospital:wardid ), # needs to be wardid now because ward is duplicated over hospitals (not unique) data = nurses ) nurses_nested &lt;- lmer( stress ~ age + sex + experience + treatment + wardtype + hospsize + ( 1 | hospital ) + ( 1 | wardid ), data = nurses ) Nested: mixedup::extract_fixed_effects(nurses_hierach2) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 5.380 0.185 29.128 0.000 5.018 5.742 age 0.022 0.002 10.053 0.000 0.018 0.026 sexFemale -0.453 0.035 -12.952 0.000 -0.522 -0.385 experience -0.062 0.004 -13.776 0.000 -0.070 -0.053 treatmentTraining -0.700 0.120 -5.843 0.000 -0.935 -0.465 wardtypespecial care 0.051 0.120 0.424 0.671 -0.184 0.286 hospsizemedium 0.489 0.202 2.428 0.015 0.094 0.884 hospsizelarge 0.902 0.275 3.280 0.001 0.363 1.440 mixedup::extract_vc(nurses_hierach2, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop hospital:wardid Intercept 0.337 0.580 0.500 hospital Intercept 0.119 0.345 0.177 Residual 0.217 0.466 0.323 Crossed: mixedup::extract_fixed_effects(nurses_nested) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 5.380 0.185 29.128 0.000 5.018 5.742 age 0.022 0.002 10.053 0.000 0.018 0.026 sexFemale -0.453 0.035 -12.952 0.000 -0.522 -0.385 experience -0.062 0.004 -13.776 0.000 -0.070 -0.053 treatmentTraining -0.700 0.120 -5.843 0.000 -0.935 -0.465 wardtypespecial care 0.051 0.120 0.424 0.671 -0.184 0.286 hospsizemedium 0.489 0.202 2.428 0.015 0.094 0.884 hospsizelarge 0.902 0.275 3.280 0.001 0.363 1.440 mixedup::extract_vc(nurses_nested, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop wardid Intercept 0.337 0.580 0.500 hospital Intercept 0.119 0.345 0.177 Residual 0.217 0.466 0.323 11.6 Residual Structure rescov &lt;- function(model, data) { var.d &lt;- crossprod(getME(model,&quot;Lambdat&quot;)) Zt &lt;- getME(model,&quot;Zt&quot;) vr &lt;- sigma(model)^2 var.b &lt;- vr*(t(Zt) %*% var.d %*% Zt) sI &lt;- vr * Diagonal(nrow(data)) var.y &lt;- var.b + sI var.y %&gt;% as.matrix() %&gt;% as_tibble() %&gt;% mutate(row = row_number()) %&gt;% pivot_longer(cols = -row, names_to = &quot;column&quot;) } rescov(gpa_mixed, gpa) %&gt;% mutate(x = as.numeric(column), y = as.numeric(row)) %&gt;% filter(between(x,0,30), between(y,0,30)) %&gt;% ggplot(aes(x = x, y = y, fill = value)) + geom_tile(aes(color = after_scale(clr_darken(fill))), size = .3, width = .9, height = .9) + scale_fill_gradientn(colours = c(clr0, clr_lighten(clr1), clr_lighten(clr2))) + scale_y_reverse() + coord_equal() covariance matrix for a cluster (compound symmetry): \\[ \\Sigma = \\left[ \\begin{array}{ccc} \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 \\\\ \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 \\\\ \\tau^2 &amp; \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 \\\\ \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2\\\\ \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 \\\\ \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} \\\\ \\end{array}\\right] \\] Types of covariance structures: in a standard linear regression model, we have constant variance and no covariance: \\[ \\Sigma = \\left[ \\begin{array}{ccc} \\sigma^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2 \\\\ \\end{array}\\right] \\] next, relax the assumption of equal variances, and estimate each separately. In this case of heterogeneous variances, we might see more or less variance over time, for example. \\[ \\Sigma = \\left[ \\begin{array}{ccc} \\sigma_1^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_3^2 \\\\ \\end{array}\\right] \\] we actually want to get at the underlying covariance/correlation. I’ll switch to the correlation representation, but you can still think of the variances as constant or separately estimated. So now we have something like this, where \\(\\rho\\) represents the residual correlation among observations. \\[ \\Sigma = \\sigma^2 \\left[ \\begin{array}{ccc} 1 &amp; \\rho_1 &amp; \\rho_2 \\\\ \\rho_1 &amp; 1 &amp; \\rho_3 \\\\ \\rho_2 &amp; \\rho_3 &amp; 1 \\\\ \\end{array}\\right] \\] \\(\\rightarrow\\) unstructured / symmetric correlation structure (compound symmetry) Autocorrelation (lag of order one for residuals): \\[ \\Sigma = \\sigma^2 \\left[ \\begin{array}{cccc} 1 &amp; \\rho &amp; \\rho^2 &amp; \\rho^3 \\\\ \\rho &amp; 1 &amp; \\rho &amp; \\rho^2 \\\\ \\rho^2 &amp; \\rho &amp; 1 &amp; \\rho \\\\ \\rho^3 &amp; \\rho^2 &amp; \\rho &amp; 1 \\\\ \\end{array}\\right] \\] 11.6.1 Heterogeneous variance library(nlme) gpa_hetero_res &lt;- lme( gpa ~ occasion, data = gpa, random = ~ 1 | student, weights = varIdent(form = ~ 1 | occasion) ) mixedup::extract_fixed_effects(gpa_hetero_res) %&gt;% knitr::kable() term value se z p_value lower_2.5 upper_97.5 Intercept 2.599 0.026 99.002 0 2.547 2.650 occasion 0.106 0.004 26.317 0 0.098 0.114 mixedup::extract_vc(gpa_hetero_res, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop student Intercept 0.094 0.306 0.404 Residual 0.138 0.372 0.596 alternative approach to heterogeneous variance models: library(glmmTMB) gpa_hetero_res2 &lt;- glmmTMB( gpa ~ occasion + ( 1 | student ) + diag( 0 + occas | student ), data = gpa ) Comparing results of {nlme} and {glmmTMB} tibble(relative_val = c(1, coef(gpa_hetero_res$modelStruct$varStruct, unconstrained = FALSE))) %&gt;% mutate(absolute_val = (relative_val * gpa_hetero_res$sigma) ^ 2, `hetero_res (nlme)` = mixedup::extract_het_var(gpa_hetero_res, scale = &#39;var&#39;, digits = 5) %&gt;% unname() %&gt;% as.vector() %&gt;% t() %&gt;% .[,1], `hetero_res (glmmTMB)` = mixedup::extract_het_var(gpa_hetero_res2, scale = &#39;var&#39;, digits = 5) %&gt;% dplyr::select(-group) %&gt;% unname() %&gt;% as.vector() %&gt;% t() %&gt;% .[,1]) %&gt;% knitr::kable() relative_val absolute_val hetero_res (nlme) hetero_res (glmmTMB) 1.0000000 0.1381504 0.13815 0.13790 0.8261186 0.0942837 0.09428 0.09415 0.6272415 0.0543528 0.05435 0.05430 0.4311126 0.0256764 0.02568 0.02568 0.3484013 0.0167692 0.01677 0.01677 0.4324628 0.0258374 0.02584 0.02580 11.6.2 Autocorrelation gpa_autocorr &lt;- lme( gpa ~ occasion, data = gpa, random = ~ 1 | student, correlation = corAR1(form = ~ occasion) ) mixedup::extract_fixed_effects(gpa_autocorr) %&gt;% knitr::kable() term value se z p_value lower_2.5 upper_97.5 Intercept 2.597 0.023 113.146 0 2.552 2.642 occasion 0.107 0.005 20.297 0 0.097 0.118 mixedup::extract_vc(gpa_autocorr, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop student Intercept 0.046 0.215 0.381 Residual 0.075 0.273 0.619 gpa_autocorr2 &lt;- glmmTMB( gpa ~ occasion + ar1( 0 + occas | student ) + ( 1 | student ), # occas is cotegorical version of occasion data = gpa ) mixedup::extract_fixed_effects(gpa_autocorr2) %&gt;% knitr::kable() term value se z p_value lower_2.5 upper_97.5 Intercept 2.598 0.023 111.077 0 2.552 2.643 occasion 0.107 0.005 19.458 0 0.096 0.118 mixedup::extract_vc(gpa_autocorr2, ci_level = 0) %&gt;% knitr::kable() group variance sd var_prop student 0.093 0.305 0.159 student.1 0.000 0.000 0.000 Residual 0.028 0.167 0.048 11.7 Generalized Linear Mixed Models load(&quot;data/speed_dating.RData&quot;) sdating &lt;- glmer( decision ~ sex + samerace + attractive_sc + sincere_sc + intelligent_sc + ( 1 | iid), data = speed_dating, family = binomial ) mixedup::extract_fixed_effects(sdating) %&gt;% knitr::kable() term value se z p_value lower_2.5 upper_97.5 Intercept -0.743 0.121 -6.130 0.00 -0.981 -0.506 sexMale 0.156 0.164 0.954 0.34 -0.165 0.478 sameraceYes 0.314 0.075 4.192 0.00 0.167 0.460 attractive_sc 0.502 0.015 33.559 0.00 0.472 0.531 sincere_sc 0.089 0.016 5.747 0.00 0.059 0.120 intelligent_sc 0.143 0.017 8.232 0.00 0.109 0.177 mixedup::extract_vc(sdating, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop iid Intercept 2.708 1.645 1 11.8 Issues/ Considderations small number of clusters: problematic - similar to number of samples to compute variance / mean or similar (to get to the variance component we need enough groups). This also touches whether something should be a fixed or a random effect (random is always possible when the number of clusters is large enough) small number of observations within clusters: no problem, but might prevent random slopes (for n == 1) balanced design / missing data: not really a requirement, so as long as it is not extreme likely not an issue 11.8.1 Model comparison Using AIC can help, but should not used to make the decision—reasoning about the implications of the used models should. gpa_1 &lt;- lmer(gpa ~ occasion + (1 + occasion | student), data = gpa) gpa_2 &lt;- lmer(gpa ~ occasion + sex + (1 + occasion | student), data = gpa) gpa_3 &lt;- lmer(gpa ~ occasion + (1 | student), data = gpa) list(gpa_1 = gpa_1, gpa_2 = gpa_2, gpa_3 = gpa_3) %&gt;% map_df(function(mod) data.frame(AIC = AIC(mod)), .id = &#39;model&#39;) %&gt;% arrange(AIC) %&gt;% mutate(`Δ AIC` = AIC - min(AIC)) %&gt;% knitr::kable() model AIC Δ AIC gpa_2 269.7853 0.000000 gpa_1 272.9566 3.171217 gpa_3 416.8929 147.107536 11.9 Formula summary formula meaning (1|group) random group intercept (x|group) = (1+x|group) random slope of x within group with correlated intercept (0+x|group) = (-1+x|group) random slope of x within group: no variation in intercept (1|group) + (0+x|group) uncorrelated random intercept and random slope within group (1|site/block) = (1|site)+(1|site:block) intercept varying among sites and among blocks within sites (nested random effects) site+(1|site:block) fixed effect of sites plus random variation in intercept among blocks within sites (x|site/block) = (x|site)+(x|site:block) = (1 + x|site)+(1+x|site:block) slope and intercept varying among sites and among blocks within sites (x1|site)+(x2|block) two different effects, varying at different levels x*site+(x|site:block) fixed effect variation of slope and intercept varying among sites and random variation of slope and intercept among blocks within sites (1|group1)+(1|group2) intercept varying among crossed random effects (e.g. site, year) equation formula \\(β_0 + β_{1}X_{i} + e_{si}\\) n/a (Not a mixed-effects model) \\((β_0 + b_{S,0s}) + β_{1}X_i + e_{si}\\) ∼ X + (1∣Subject) \\((β_0 + b_{S,0s}) + (β_{1} + b_{S,1s}) X_i + e_{si}\\) ~ X + (1 + X∣Subject) \\((β_0 + b_{S,0s} + b_{I,0i}) + (β_{1} + b_{S,1s}) X_i + e_{si}\\) ∼ X + (1 + X∣Subject) + (1∣Item) As above, but \\(S_{0s}\\), \\(S_{1s}\\) independent ∼ X + (1∣Subject) + (0 + X∣ Subject) + (1∣Item) \\((β_0 + b_{S,0s} + b_{I,0i}) + β_{1}X_i + e_{si}\\) ∼ X + (1∣Subject) + (1∣Item) \\((β_0 + b_{I,0i}) + (β_{1} + b_{S,1s})X_i + e_{si}\\) ∼ X + (0 + X∣Subject) + (1∣Item) × "],
["references-and-session-info.html", "12 References and Session Info 12.1 Session Info 12.2 References", " 12 References and Session Info 12.1 Session Info 12.1.1 R settings sessionInfo() #&gt; R version 4.0.3 (2020-10-10) #&gt; Platform: x86_64-pc-linux-gnu (64-bit) #&gt; Running under: Ubuntu 20.04.3 LTS #&gt; #&gt; Matrix products: default #&gt; BLAS: /usr/local/lib/R/lib/libRblas.so #&gt; LAPACK: /usr/local/lib/R/lib/libRlapack.so #&gt; #&gt; locale: #&gt; [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C #&gt; [3] LC_TIME=de_DE.UTF-8 LC_COLLATE=en_US.UTF-8 #&gt; [5] LC_MONETARY=de_DE.UTF-8 LC_MESSAGES=en_US.UTF-8 #&gt; [7] LC_PAPER=de_DE.UTF-8 LC_NAME=C #&gt; [9] LC_ADDRESS=C LC_TELEPHONE=C #&gt; [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C #&gt; #&gt; attached base packages: #&gt; [1] parallel stats graphics grDevices utils datasets methods #&gt; [8] base #&gt; #&gt; other attached packages: #&gt; [1] reticulate_1.22 bayesplot_1.8.1 mixedup_0.3.9 #&gt; [4] tidybayes_3.0.1 rethinking_2.21 cmdstanr_0.4.0.9000 #&gt; [7] rstan_2.21.2 StanHeaders_2.21.0-7 ggdag_0.2.3.9000 #&gt; [10] GGally_1.5.0 ggtext_0.1.1 brms_2.16.1 #&gt; [13] Rcpp_1.0.6 EnvStats_2.4.0 ggraph_2.0.5.9000 #&gt; [16] tidygraph_1.2.0 glue_1.4.2 patchwork_1.1.0.9000 #&gt; [19] prismatic_1.0.0.9000 forcats_0.5.1 stringr_1.4.0 #&gt; [22] dplyr_1.0.6 purrr_0.3.4 readr_1.4.0 #&gt; [25] tidyr_1.1.3 tibble_3.1.2 ggplot2_3.3.5 #&gt; [28] tidyverse_1.3.0.9000 #&gt; #&gt; loaded via a namespace (and not attached): #&gt; [1] readxl_1.3.1 backports_1.2.1 plyr_1.8.6 #&gt; [4] igraph_1.2.6 svUnit_1.0.6 splines_4.0.3 #&gt; [7] crosstalk_1.1.1 inline_0.3.17 rstantools_2.1.1 #&gt; [10] digest_0.6.27 htmltools_0.5.1.1 viridis_0.5.1 #&gt; [13] rsconnect_0.8.24 fansi_0.5.0 magrittr_2.0.1 #&gt; [16] checkmate_2.0.0 graphlayouts_0.7.1 modelr_0.1.8 #&gt; [19] RcppParallel_5.0.2 matrixStats_0.56.0 xts_0.12.1 #&gt; [22] prettyunits_1.1.1 colorspace_2.0-2 rvest_1.0.0 #&gt; [25] ggrepel_0.9.1 ggdist_3.0.1 haven_2.3.1 #&gt; [28] xfun_0.24 callr_3.6.0 crayon_1.4.1 #&gt; [31] jsonlite_1.7.2 lme4_1.1-26 zoo_1.8-8 #&gt; [34] polyclip_1.10-0 gtable_0.3.0 V8_3.4.0 #&gt; [37] distributional_0.2.2 pkgbuild_1.2.0 shape_1.4.5 #&gt; [40] abind_1.4-5 scales_1.1.1 mvtnorm_1.1-2 #&gt; [43] DBI_1.1.1 miniUI_0.1.1.1 gridtext_0.1.4 #&gt; [46] viridisLite_0.4.0 xtable_1.8-4 stats4_4.0.3 #&gt; [49] DT_0.17 htmlwidgets_1.5.3 httr_1.4.2 #&gt; [52] threejs_0.3.3 arrayhelpers_1.1-0 RColorBrewer_1.1-2 #&gt; [55] posterior_1.1.0 ellipsis_0.3.2 reshape_0.8.8 #&gt; [58] pkgconfig_2.0.3 loo_2.4.1 farver_2.1.0 #&gt; [61] sass_0.4.0.9000 dbplyr_2.1.1 utf8_1.2.1 #&gt; [64] tidyselect_1.1.1 rlang_0.4.12 reshape2_1.4.4 #&gt; [67] later_1.1.0.1 munsell_0.5.0 cellranger_1.1.0 #&gt; [70] tools_4.0.3 cli_3.1.0 generics_0.1.0 #&gt; [73] broom_0.7.6 ggridges_0.5.2 evaluate_0.14 #&gt; [76] fastmap_1.1.0 yaml_2.2.1.99 processx_3.5.1 #&gt; [79] knitr_1.33 fs_1.5.0 nlme_3.1-149 #&gt; [82] mime_0.11 projpred_2.0.2 xml2_1.3.2 #&gt; [85] compiler_4.0.3 shinythemes_1.2.0 rstudioapi_0.13 #&gt; [88] png_0.1-7 curl_4.3 gamm4_0.2-6 #&gt; [91] reprex_2.0.0 statmod_1.4.35 tweenr_1.0.2 #&gt; [94] bslib_0.2.4 stringi_1.7.3 ps_1.6.0 #&gt; [97] Brobdingnag_1.2-6 lattice_0.20-41 Matrix_1.2-18 #&gt; [100] nloptr_1.2.2.2 markdown_1.1 shinyjs_2.0.0 #&gt; [103] tensorA_0.36.1 vctrs_0.3.8 pillar_1.6.1 #&gt; [106] lifecycle_1.0.0 jquerylib_0.1.3 bridgesampling_1.1-2 #&gt; [109] httpuv_1.5.5 R6_2.5.0 bookdown_0.19 #&gt; [112] promises_1.2.0.1 gridExtra_2.3 codetools_0.2-16 #&gt; [115] boot_1.3-25 colourpicker_1.1.1 MASS_7.3-53 #&gt; [118] gtools_3.8.2 assertthat_0.2.1 withr_2.4.2 #&gt; [121] shinystan_2.5.0 mgcv_1.8-33 hms_1.1.0 #&gt; [124] grid_4.0.3 coda_0.19-4 minqa_1.2.4 #&gt; [127] rmarkdown_2.9.6 ggforce_0.3.2.9000 shiny_1.6.0 #&gt; [130] lubridate_1.7.10 base64enc_0.1-3 dygraphs_1.1.1.6 12.1.2 Python settings import arviz as az import matplotlib.pyplot as plt import numpy as np import pymc3 as pm import scipy.stats as stats import seaborn as sns import matplotlib from matplotlib.colors import ListedColormap import matplotlib.font_manager import session_info session_info.show(html = False) #&gt; ----- #&gt; arviz 0.11.4 #&gt; matplotlib 3.5.1 #&gt; numpy 1.21.4 #&gt; pymc3 3.11.4 #&gt; scipy 1.7.3 #&gt; seaborn 0.11.2 #&gt; session_info 1.0.0 #&gt; ----- #&gt; IPython 7.29.0 #&gt; jupyter_client 6.1.2 #&gt; jupyter_core 4.6.3 #&gt; notebook 6.0.3 #&gt; ----- #&gt; Python 3.8.10 (default, Nov 26 2021, 20:14:08) [GCC 9.3.0] #&gt; Linux-5.11.0-43-generic-x86_64-with-glibc2.29 #&gt; ----- #&gt; Session information updated at 2022-01-05 00:41 12.2 References Kurt, Will. 2019. Bayesian Statistics the Fun Way : Understanding Statistics and Probability with Star Wars, LEGO, and Rubber Ducks. San Francisco: No Starch Press, Inc. McElreath, Richard. 2020. Statistical Rethinking : A Bayesian Course with Examples in R and Stan. Second. Boca Raton: Chapman &amp; Hall/CRC. "]
]
