[
["index.html", "Statistical Exploration a small notebook to keep track 1 Intro", " Statistical Exploration a small notebook to keep track Kosmas Hench 2021-10-28 1 Intro × "],
["rethinking-chapter-1.html", "2 Rethinking: Chapter 1", " 2 Rethinking: Chapter 1 The Golem of Prague by Richard McElreath, building on the Summary by Solomon Kurz × "],
["rethinking-chapter-2.html", "3 Rethinking: Chapter 2 3.1 Counting possibilities 3.2 Building a Model 3.3 Making the model go / Bayes’ Theorem 3.4 Motors: Grid Approximation 3.5 Quadratic Approximation 3.6 Marcov Chain Monte Carlo (MCMC) 3.7 Homework", " 3 Rethinking: Chapter 2 Small Worlds and Large Worlds by Richard McElreath, building on the Summary by Solomon Kurz 3.1 Counting possibilities d &lt;- tibble(p1 = 0, p2 = rep(1:0, times = c(1, 3)), p3 = rep(1:0, times = c(2, 2)), p4 = rep(1:0, times = c(3, 1)), p5 = 1) p1 p2 p3 p4 p5 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 d %&gt;% mutate(turn = 1:4)%&gt;% pivot_longer(p1:p5, names_to = &quot;prob&quot;, values_to = &quot;realization&quot;) %&gt;% arrange(prob, turn) %&gt;% mutate(prob = factor(prob, levels = c(&quot;p5&quot;, &quot;p4&quot;, &quot;p3&quot;, &quot;p2&quot;, &quot;p1&quot;)), marble = c(&quot;white&quot;, &quot;dark&quot;)[realization + 1]) %&gt;% ggplot( aes( x = turn, y = prob ) ) + geom_point(shape = 21, size = 4, aes( fill = marble, color = after_scale(clr_darken(fill)))) + geom_text(data = tibble(x = rep(c(.65, 4.45), each = 5), y = rep(str_c(&quot;p&quot;,1:5), 2), label = rep(c(&quot;[&quot;, &quot;]&quot;), each = 5), vjust = .7), aes( x = x, y = y, label = label), family = fnt_sel, size = 6)+ scale_fill_manual(values = c(white = clr0, dark = clrd)) + theme(legend.position = &quot;bottom&quot;) tibble(draws = 1:3, marbles = 4) %&gt;% mutate(possibilities = marbles ^ draws) %&gt;% flextable::flextable() .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-0d0f6e96{}.cl-0d0a7d46{font-family:'DejaVu Sans';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-0d0a9060{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-0d0abbb2{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0d0abbd0{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0d0abbe4{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} drawsmarblespossibilities14424163464 layout_round &lt;- function(round = 1, n = 4, angle = 360, start_angle = 0, p = .5, round_prefix = &quot;&quot;){ n_round &lt;- n ^ round tibble(idx_round = 1:n_round, idx_round_sacaled = scales::rescale(idx_round, from = c(.5, n_round+.5), to = c(0, 1) * angle/360 + start_angle/360), idx_draw = rep(1:n, n_round/n), idx_parent = ((idx_round - 1 ) %/% n) + 1, name_parent = str_c(round_prefix, round - 1, &quot;_&quot;, idx_parent), name = str_c(round_prefix, round, &quot;_&quot;, idx_round), x = sin(idx_round_sacaled * 2 * pi) * round, y = cos(idx_round_sacaled * 2 * pi) * round) %&gt;% mutate(marble = c(&quot;white&quot;, &quot;dark&quot;)[1 + ((idx_draw/n) &lt;= p)], round_prefix = round_prefix, round = round) } links_round &lt;- function(round = 1, n = 4, round_prefix = &quot;&quot;){ n_round &lt;- n ^ round n_prev &lt;- n ^ (round - 1) tibble(idx_round = 1:n_round, idx_parent = ((idx_round - 1 ) %/% n) + 1, from = str_c(round_prefix, round - 1, &quot;_&quot;, idx_parent), to = str_c(round_prefix,round, &quot;_&quot;, idx_round), round_prefix = round_prefix) } round_origin &lt;- origin_round &lt;- function(round_prefix = &quot;&quot;){ tibble(idx_round = 0, idx_round_sacaled = 0, idx_draw = 0, name = str_c(round_prefix, &quot;0_1&quot;), x = 0, y = 0, marble = NA) } marble_graph &lt;- function(n_rounds = 3, n_draws = 4, angle = 360,start_angle = 0, p = .5, round_prefix = &quot;&quot;){ tbl_graph(nodes = 1:n_rounds %&gt;% map_dfr(layout_round, n = n_draws, angle = angle, start_angle = start_angle, p = p, round_prefix = round_prefix) %&gt;% bind_rows(round_origin(round_prefix = round_prefix), .), edges = 1:n_rounds %&gt;% map_dfr(links_round, round_prefix = round_prefix)) %E&gt;% mutate(marble = .N()$marble[to], to_name = .N()$name[to], from_name = .N()$name[from]) } marble_graph(p = .25, n_rounds = 3, angle = 180, start_angle = -90) %&gt;% ggraph( layout = &quot;manual&quot;, x = . %N&gt;% as_tibble() %&gt;% .$x, y = . %N&gt;% as_tibble() %&gt;% .$y) + geom_node_point(aes(fill = marble, color = marble, size = -round), shape = 21) + geom_edge_link(aes(color = marble), start_cap = circle(2, &#39;mm&#39;), end_cap = circle(2, &#39;mm&#39;)) + coord_equal() + scale_color_manual(values = clr_darken(c(white = clr0, dark = clrd)), na.value = &quot;transparent&quot;, guide = &quot;none&quot;) + scale_fill_manual(values = c(white = clr0, dark = clrd), na.value = &quot;transparent&quot;) + scale_edge_color_manual(values = c(white = clr_darken(clr0, .2), dark = clrd), guide = &quot;none&quot;) + scale_size_continuous(range = c(1.5,3), guide = &quot;none&quot;) + guides(fill = guide_legend(override.aes = list(size = 4)), edge_alpha = &quot;none&quot;) + labs(title = &quot;p = 0.25&quot;) + theme(legend.position = &quot;bottom&quot;) n_deviders &lt;- 3 n_rounds &lt;- 3 dividers &lt;- tibble(x = rep(0,n_deviders), y = x, tau = seq(from = 0, to = 2*pi, length.out = n_deviders + 1)[2:(n_deviders+1)], xend = sin(tau) * (n_rounds + .1), yend = cos(tau) * (n_rounds + .1)) p_trials &lt;- c(.25, .5, .75) all_conjectures &lt;- tibble(start_angle = c(0, 120, 240), round_prefix = c(&quot;r1_&quot; ,&quot;r2_&quot;, &quot;r3_&quot;), p = p_trials) %&gt;% pmap(marble_graph, angle = 120) %&gt;% reduce(bind_graphs) na_to_false &lt;- function(x){x[is.na(x)] &lt;- FALSE; x} na_to_true &lt;- function(x){x[is.na(x)] &lt;- TRUE; x} tester &lt;- function(x){x$name[x$r1_right]} trial_sequence &lt;- c(&quot;white&quot;, &quot;dark&quot;)[c(2,1,2)] selectors &lt;- all_conjectures %N&gt;% mutate(r1_right = (round == 1 &amp; marble == trial_sequence[1]) %&gt;% na_to_true(), r2_still_in = name_parent %in% name[r1_right], r2_right = r2_still_in &amp; (round == 2 &amp; marble == trial_sequence[2]), r3_still_in = name_parent %in% name[r2_right], r3_right = r3_still_in &amp; (round == 3 &amp; marble == trial_sequence[3]), on_path = r1_right | r2_right |r3_right) %&gt;% as_tibble() %&gt;% filter(on_path) selector_results &lt;- selectors %&gt;% filter(round == n_rounds) %&gt;% group_by(round_prefix) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(tau = seq(from = 0, to = 2*pi, length.out = n_rounds + 1)[2:(n_rounds+1)] - (2*pi)/(n_rounds * 2), x = sin(tau) * (n_rounds + .5), y = cos(tau) * (n_rounds + .5)) all_conjectures %&gt;% ggraph( layout = &quot;manual&quot;, x = . %N&gt;% as_tibble() %&gt;% .$x, y = . %N&gt;% as_tibble() %&gt;% .$y) + geom_node_point(aes(fill = marble, color = marble, size = -round, alpha = name %in% selectors$name), shape = 21) + geom_edge_link(aes(color = marble, alpha = to_name %in% selectors$name), start_cap = circle(2, &#39;mm&#39;), end_cap = circle(2, &#39;mm&#39;)) + geom_segment(data = dividers, aes(x = x, y = y, xend = xend, yend = yend), color = clr_darken(&quot;white&quot;,.10)) + geom_text(data = selector_results, aes( x = x, y = y, label = n), family = fnt_sel, size = 6) + coord_equal() + scale_color_manual(values = clr_darken(c(white = clr0, dark = clrd)), na.value = &quot;transparent&quot;, guide = &quot;none&quot;) + scale_fill_manual(values = c(white = clr0, dark = clrd), na.value = &quot;transparent&quot;) + scale_edge_color_manual(values = c(white = clr_darken(clr0, .2), dark = clrd), guide = &quot;none&quot;) + scale_size_continuous(range = c(1.5,3), guide = &quot;none&quot;) + scale_alpha_manual(values = c(`TRUE` = 1, `FALSE` = .2), guide = &quot;none&quot;) + guides(fill = guide_legend(override.aes = list(size = 4)), edge_alpha = &quot;none&quot;) + labs(caption = str_c(trial_sequence,collapse = &quot;-&quot;)) + theme(legend.position = &quot;bottom&quot;) html_marbles &lt;- c( glue(&quot;&lt;span style=&#39;color:{clr0};filter:drop-shadow(0px 0px 1px black)&#39;&gt;\\U2B24&lt;/span&gt;&quot;), glue(&quot;&lt;span style=&#39;color:{clr1l};filter:drop-shadow(0px 0px 1px black)&#39;&gt;\\U2B24&lt;/span&gt;&quot;)) html_conjecture &lt;- function(x){ str_c(&quot;[ &quot;,str_c(html_marbles[c(x)+1], collapse = &quot; &quot;),&quot; ]&quot;) } tibble(conjectures = list(rep(0,4), rep(1:0, c(1,3)), rep(1:0, c(2,2)), rep(1:0, c(3,1)), rep(1,4)), conjecture = map_chr(conjectures, html_conjecture), ways = map_dbl(conjectures, sum), p = c(0, p_trials, 1), `ways data/prior counts` = c(0, selector_results$n, 0), `new count` = map2_chr( `ways data/prior counts`, ways, .f = function(x,y){glue(&quot;{x} $\\\\times$ {y} = {x * y}&quot;)}), plausibility = (`ways data/prior counts` / sum(`ways data/prior counts`)) %&gt;% round(digits = 2) ) %&gt;% rename(`ways to produce &lt;span style=&#39;color:#85769EFF;filter:drop-shadow(0px 0px 1px black)&#39;&gt;⬤&lt;/span&gt;` = &quot;ways&quot;) %&gt;% dplyr::select(-conjectures) %&gt;% knitr::kable() conjecture ways to produce ⬤ p ways data/prior counts new count plausibility [ ⬤ ⬤ ⬤ ⬤ ] 0 0.00 0 0 \\(\\times\\) 0 = 0 0.00 [ ⬤ ⬤ ⬤ ⬤ ] 1 0.25 3 3 \\(\\times\\) 1 = 3 0.15 [ ⬤ ⬤ ⬤ ⬤ ] 2 0.50 8 8 \\(\\times\\) 2 = 16 0.40 [ ⬤ ⬤ ⬤ ⬤ ] 3 0.75 9 9 \\(\\times\\) 3 = 27 0.45 [ ⬤ ⬤ ⬤ ⬤ ] 4 1.00 0 0 \\(\\times\\) 4 = 0 0.00 3.2 Building a Model d &lt;- tibble(toss = c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;), n_trials = 1:9, sequence = n_trials %&gt;% map_chr(.f = function(x, chr){str_c(chr[1:x], collapse = &quot;&quot;)}, chr = toss), n_success = cumsum(toss == &quot;w&quot;), lag_n_trials = lag(n_trials, default = 0), lag_n_success = lag(n_success, default = 0)) sequence_length &lt;- 50 d %&gt;% expand(nesting(n_trials, toss, n_success), p_water = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% group_by(p_water) %&gt;% # you can learn more about lagging here: https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/lag or here: https://dplyr.tidyverse.org/reference/lead-lag.html mutate(lagged_n_trials = lag(n_trials, k = 1), lagged_n_success = lag(n_success, k = 1)) #&gt; # A tibble: 450 x 6 #&gt; # Groups: p_water [50] #&gt; n_trials toss n_success p_water lagged_n_trials lagged_n_success #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 w 1 0 NA NA #&gt; 2 1 w 1 0.0204 NA NA #&gt; 3 1 w 1 0.0408 NA NA #&gt; 4 1 w 1 0.0612 NA NA #&gt; 5 1 w 1 0.0816 NA NA #&gt; 6 1 w 1 0.102 NA NA #&gt; 7 1 w 1 0.122 NA NA #&gt; 8 1 w 1 0.143 NA NA #&gt; 9 1 w 1 0.163 NA NA #&gt; 10 1 w 1 0.184 NA NA #&gt; # … with 440 more rows stat_binom &lt;- function(n_trials, n_success, lag_n_trials, lag_n_success, sequence, ...){ if(n_trials == 1) { g_lag &lt;- stat_function(data = tibble(n_trials = n_trials, sequence = sequence), fun = function(x){1}, xlim = c(0,1), linetype = 3) } else { g_lag &lt;- stat_function(data = tibble(n_trials = n_trials, sequence = sequence), fun = function(x){ f &lt;- function(x){ dbinom(x = lag_n_success, size = lag_n_trials, prob = x)} f(x)/ integrate(f = f,lower = 0, upper = 1)[[1]]}, xlim = c(0,1), n = 500, linetype = 3) } g_current &lt;- stat_function(data = tibble(n_trials = n_trials, sequence = sequence), fun = function(x){ f &lt;- function(x){ dbinom(x = n_success, size = n_trials, prob = x)} f(x)/ integrate(f = f,lower = 0,upper = 1)[[1]]}, xlim = c(0,1),n = 500) list( g_lag, g_current) } ggplot() + (d %&gt;% pmap(stat_binom) %&gt;% unlist()) + facet_wrap(str_c(n_trials,&quot;: &quot;, sequence) ~ .) 3.3 Making the model go / Bayes’ Theorem \\[ Pr(\\textit{p} | W, L) = \\frac{Pr(W, L | \\textit{p}) ~ Pr(\\textit{p})}{Pr(W,L)}\\\\ Posterior = \\frac{Probability~of~the~Data \\times Prior}{ Average~probability~of~the~Data} \\] f_posterior_unscaled &lt;- function(f_porior, f_like){ function(x){ f_porior(x) * f_like(x)} } f_parts &lt;- c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;) gg_posterior &lt;- function(f_porior, f_like, comp = 1){ list( stat_function(data = tibble(part = factor(&quot;prior&quot;, levels = f_parts), comp = comp), fun = f_porior, xlim = c(0,1), n = 500, geom = &quot;area&quot;, color = clr2, fill = fll2), stat_function(data = tibble(part = factor(&quot;likelihood&quot;, levels = f_parts), comp = comp), fun = f_like, xlim = c(0,1), n = 500, geom = &quot;area&quot;, color = clr2, fill = fll2), stat_function(data = tibble(part = factor(&quot;posterior&quot;, levels = f_parts), comp = comp), fun = f_posterior_unscaled(f_porior = f_porior, f_like = f_like), xlim = c(0,1), n = 500, geom = &quot;area&quot;, color = clr2, fill = fll2) ) } scale_fun &lt;- function(f){ # marginal likelihood function(x){f(x) / integrate(f = f, lower = 0, upper = 1)[[1]]} } f_like_in &lt;- function(x){dbeta(x = x, shape1 = 8, shape2 = 5)} f_uni &lt;- function(x){1} f_step &lt;- function(x){if_else(x &lt; .5, 0, 1)} f_peak &lt;- function(x){if_else(x &lt; .5, (x * 2)^3, ((1 - x) * 2)^3)} ggplot() + gg_posterior(f_porior = f_uni, f_like = f_like_in) + gg_posterior(f_porior = f_step, f_like = f_like_in, comp = 2) + gg_posterior(f_porior = f_peak, f_like = f_like_in, comp = 3) + facet_wrap(comp ~ part, scales = &quot;free_y&quot;) 3.4 Motors: Grid Approximation grid_approx &lt;- function(n_grid = 20, L = 6, W = 3, prior = function(x){rep(1, length(x))}){ tibble(p_grid = seq(0, 1, length.out = n_grid), prior = prior(p_grid), likelihood = dbinom(L, size = W + L, prob = p_grid), posterior_unstand = likelihood * prior, posterior = posterior_unstand / sum(posterior_unstand)) } plot_grid_approx &lt;- function(data){ data %&gt;% ggplot(aes(x = p_grid, y = posterior, color = posterior)) + ggforce::geom_link2() + geom_point(shape = 21, fill = &quot;white&quot;, size = 2.5) + scale_color_gradientn(colours = c(clr_darken(clr0), clr2), guide = &quot;none&quot;) + labs(title = glue(&quot;{length(data$p_grid)} grid points&quot;)) } c(4, 7, 15, 60) %&gt;% map(grid_approx) %&gt;% map(plot_grid_approx) %&gt;% wrap_plots(nrow = 1) &amp; scale_x_continuous(breaks = c(0, .5, 1)) Note how the y scale depends on the number of grid points: the peak reaches ~0.75 for 4 points, but only ~ 0.043 for 60 points. 3.5 Quadratic Approximation library(rethinking) map &lt;- purrr::map conpare_qa &lt;- function(w_in, l_in){ globe_qa &lt;- quap( alist( W ~ dbinom( W + L, p ), # binomial likelihood p ~ dunif( 0, 1 ) # uniform prior ), data = list( W = w_in, L = l_in ) ) qa_results &lt;- precis(globe_qa) %&gt;% as_tibble() %&gt;% mutate(qa = glue(&quot;W: {w_in}, L: {l_in}&quot;)) qa_results %&gt;% knitr::kable() %&gt;% print() ggplot() + stat_function(fun = function(x){ dbeta( shape1 = w_in + 1, shape2 = l_in + 1, x = x ) / integrate(f = function(x){ dbeta( shape1 = w_in + 1, shape2 = l_in + 1, x = x )}, lower = 0, upper = 1)[[1]] }, xlim = c(0,1), n = 500, geom = &quot;area&quot;, color = clr0d, fill = fll0)+ stat_function(fun = function(x){ dnorm(x = x, mean = qa_results$mean, sd = qa_results$sd )}, xlim = c(0,1), n = 500, geom = &quot;line&quot;, color = clr2, linetype = 3) + labs(title = glue(&quot;W: {w_in}, L: {l_in}, n = {w_in + l_in}&quot;), y = &quot;density&quot;, x = &quot;proportion water&quot;) } conpare_qa(w_in = 6, l_in = 3) + conpare_qa(w_in = 12, l_in = 6) + conpare_qa(w_in = 24, l_in = 12) #&gt; #&gt; #&gt; | mean| sd| 5.5%| 94.5%|qa | #&gt; |---------:|---------:|---------:|---------:|:----------| #&gt; | 0.6666668| 0.1571337| 0.4155367| 0.9177969|W: 6, L: 3 | #&gt; #&gt; #&gt; | mean| sd| 5.5%| 94.5%|qa | #&gt; |---------:|---------:|---------:|---------:|:-----------| #&gt; | 0.6666662| 0.1111104| 0.4890902| 0.8442421|W: 12, L: 6 | #&gt; #&gt; #&gt; | mean| sd| 5.5%| 94.5%|qa | #&gt; |---------:|---------:|---------:|---------:|:------------| #&gt; | 0.6666666| 0.0785669| 0.5411015| 0.7922316|W: 24, L: 12 | 3.6 Marcov Chain Monte Carlo (MCMC) n_samples &lt;- 10^4 p &lt;- rep( NA, n_samples ) p[1] &lt;- .5 W &lt;- 6 L &lt;- 3 for ( i in 2:n_samples ) { p_new &lt;- rnorm( n = 1, mean = p[ i - 1], sd = 0.1) if ( p_new &lt; 0 ){ p_new &lt;- abs( p_new ) } if ( p_new &gt; 1 ){ p_new &lt;- 2 - p_new } q0 &lt;- dbinom( W, W + L, p[ i - 1 ] ) q1 &lt;- dbinom( W, W + L, p_new ) p[i] &lt;- if_else( runif(1) &lt; q1 / q0, p_new, p[i - 1] ) } tibble(x = p) %&gt;% ggplot() + stat_function(fun = function(x){ dbeta( shape1 = W + 1, shape2 = L + 1, x = x ) / integrate(f = function(x){ dbeta( shape1 = W + 1, shape2 = L + 1, x = x )}, lower = 0, upper = 1)[[1]] }, xlim = c(0,1), n = 500, geom = &quot;area&quot;, aes(color = &quot;posterior&quot;, fill = after_scale(clr_alpha(color))))+ geom_density(aes(x = x, color = &quot;MCMC&quot;)) + scale_color_manual(&quot;approach&quot;, values = c(posterior = clr0, MCMC = clr2)) + labs(y = &quot;density&quot;, x = &quot;proportion water&quot;) + theme(legend.position = &quot;bottom&quot;) 3.7 Homework M1 plot_grid_approx &lt;- function(data){ data %&gt;% ggplot(aes(x = p_grid, y = posterior, color = posterior)) + ggforce::geom_link2() + geom_point(shape = 21, fill = &quot;white&quot;, size = 2.5) + scale_color_gradientn(colours = c(clr_darken(clr0), clr2), guide = &quot;none&quot;) + labs(title = glue(&quot;{length(data$p_grid)} grid points&quot;)) } tibble(n_grid = rep(c(4,7,20), 3), L = rep(c(0,1,2), each = 3), W = rep(c(3,3,5), each = 3)) %&gt;% pmap(grid_approx) %&gt;% map(plot_grid_approx) %&gt;% wrap_plots(nrow = 3) &amp; scale_x_continuous(breaks = c(0, .5, 1)) M2 tibble(n_grid = rep(c(4,7,20), 3), L = rep(c(0,1,2), each = 3), W = rep(c(3,3,5), each = 3)) %&gt;% pmap(grid_approx, prior = f_step ) %&gt;% map(plot_grid_approx) %&gt;% wrap_plots(nrow = 3) &amp; scale_x_continuous(breaks = c(0, .5, 1)) M3 \\[ Pr(Earth | Land) = \\frac{Pr(Land | Earth) \\times Pr(Earth)}{Pr(Land)} \\] p_l_on_earth &lt;- .3 p_l_on_mars &lt;- 1 p_earth &lt;- .5 average_p_l &lt;- .5 * (p_l_on_earth + p_l_on_mars) (p_earth_on_l &lt;- p_l_on_earth * p_earth / average_p_l) #&gt; [1] 0.2307692 M4 &amp; M5 cards &lt;- c(&quot;C1b1|C1b2&quot;, &quot;C2b1|C2w2&quot;, &quot;C3w1|C3w2&quot; ) conjectures &lt;- c(&quot;C1b1|C1b2&quot;, &quot;C1b2|C1b1&quot;, &quot;C2b1|C2w2&quot;, &quot;C2w2|C2b1&quot;, &quot;C3w1|C3w2&quot;, &quot;C3w2|C3w1&quot;) tibble(cards = c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;), ways = c(&quot;C1b1|C1b2 &amp; C1b2|C1b1&quot;, &quot;C2b1|C2w2&quot;, &quot;&quot;), ways_tor_produce_data = c(2, 1, 0), plausibility = (ways_tor_produce_data / sum(ways_tor_produce_data)) %&gt;% round(digits = 3), other_side = c(&quot;b&quot;, &quot;w&quot;, &quot;&quot;), prior = c(2,1,1), posterior = ((plausibility * prior) / sum(plausibility * prior)) %&gt;% round(digits = 3))%&gt;% knitr::kable() cards ways ways_tor_produce_data plausibility other_side prior posterior C1 C1b1|C1b2 &amp; C1b2|C1b1 2 0.667 b 2 0.8 C2 C2b1|C2w2 1 0.333 w 1 0.2 C3 0 0.000 1 0.0 M6 tibble(cards = c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;), ways = c(&quot;C1b1|C1b2 &amp; C1b2|C1b1&quot;, &quot;C2b1|C2w2&quot;, &quot;&quot;), ways_tor_produce_data = c(2, 1, 0), plausibility = (ways_tor_produce_data / sum(ways_tor_produce_data)) %&gt;% round(digits = 3), other_side = c(&quot;b&quot;, &quot;w&quot;, &quot;&quot;), prior = c(1,2,3), posterior = ((plausibility * prior) / sum(plausibility * prior)) %&gt;% round(digits = 3))%&gt;% knitr::kable() cards ways ways_tor_produce_data plausibility other_side prior posterior C1 C1b1|C1b2 &amp; C1b2|C1b1 2 0.667 b 1 0.5 C2 C2b1|C2w2 1 0.333 w 2 0.5 C3 0 0.000 3 0.0 M7 tibble(cards = c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;), ways = c(&quot;C1b1&gt;C2w2 &amp; C1b1&gt;C3w1 &amp; C1b1&gt;C3w2 &amp; C2b1&gt;C2w2 &amp; C2b1&gt;C3w1 &amp; C2b1&gt;C3w2&quot;, &quot;C2b1&gt;C3w1 &amp; C2b1&gt;C3w2&quot;, &quot;&quot;), ways_tor_produce_data = c(6, 2, 0), plausibility = (ways_tor_produce_data / sum(ways_tor_produce_data)) %&gt;% round(digits = 3))%&gt;% knitr::kable() cards ways ways_tor_produce_data plausibility C1 C1b1&gt;C2w2 &amp; C1b1&gt;C3w1 &amp; C1b1&gt;C3w2 &amp; C2b1&gt;C2w2 &amp; C2b1&gt;C3w1 &amp; C2b1&gt;C3w2 6 0.75 C2 C2b1&gt;C3w1 &amp; C2b1&gt;C3w2 2 0.25 C3 0 0.00 H1 \\[ Pr(twin | spec_a) = 0.2 \\\\ Pr(twin | spec_b) = 0.1 \\\\ Pr(twin) = 0.15 \\] \\[ Pr(spec_a | twin) = \\frac{Pr(spec_a) \\times Pr(twin | spec_a)}{Pr(twin)} \\\\ Pr(spec_b | twin) = \\frac{Pr(spec_b) \\times Pr(twin | spec_b)}{Pr(twin)} \\] pr_twn_on_a &lt;- .2 pr_twn_on_b &lt;- .1 pr_twn &lt;- (pr_twn_on_a + pr_twn_on_b) /2 prior_a &lt;- .5 pr_a_on_twn &lt;- (prior_a * pr_twn_on_a) / pr_twn pr_b_on_twn &lt;- ((1 - prior_a) * pr_twn_on_b) / pr_twn (p_next_twn &lt;- pr_a_on_twn * pr_twn_on_a + pr_b_on_twn * pr_twn_on_b) %&gt;% round(digits = 3) #&gt; [1] 0.167 H2 \\[ Pr(spec_a | twin) = \\frac{2}{3} \\] pr_a_on_twn #&gt; [1] 0.6666667 H3 \\[ Pr(single | spec_a) = Pr(\\neg twin | spec_a) = 1 - Pr(twin | spec_a) \\] \\[ Pr( spec_a | single) = \\frac{Pr(single|spec_a)Pr(spec_a)}{Pr(single)} \\] pr_sgl_on_a &lt;- 1 - pr_twn_on_a pr_sgl_on_b &lt;- 1 - pr_twn_on_b pr_sgl &lt;- weighted.mean(x = c(pr_sgl_on_a, pr_sgl_on_b), w = c(pr_a_on_twn, 1- pr_a_on_twn)) prior_a &lt;- pr_a_on_twn pr_a_on_sgl &lt;- (prior_a * pr_sgl_on_a) / pr_sgl pr_b_on_sgl &lt;- ((1 - prior_a) * pr_sgl_on_b) / pr_sgl tibble(pr_a_on_sgl = pr_a_on_sgl, pr_b_on_sgl = pr_b_on_sgl, control = pr_a_on_sgl + pr_b_on_sgl) %&gt;% round(digits = 4) %&gt;% knitr::kable() pr_a_on_sgl pr_b_on_sgl control 0.64 0.36 1 H4 \\[ Pr(spec_a | test ) = 0.8 \\\\ Pr(spec_b | test ) = 0.65 \\\\ Pr(spec_a | test ) = \\frac{Pr( test | spec_a ) \\times Pr(spec_a)}{Pr(test_positive)} \\] pr_testa_on_a &lt;- .8 pr_testb_on_b &lt;- .65 pr_testa_on_b &lt;- 1 - pr_testb_on_b prior_a &lt;- .5 pr_testa &lt;- weighted.mean(x = c(pr_testa_on_a, pr_testa_on_b), w = c(prior_a, 1- prior_a)) tibble(pr_on_testa = c(&quot;A&quot;,&quot;B&quot;), genetic_test = c((prior_a * pr_testa_on_a) / pr_testa, ((1 - prior_a) * pr_testa_on_b) / pr_testa)) #&gt; # A tibble: 2 x 2 #&gt; pr_on_testa genetic_test #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 A 0.696 #&gt; 2 B 0.304 prior_a_updated &lt;- (prior_a * pr_testa_on_a) / pr_testa pr_twn_updated &lt;- weighted.mean(x = c(pr_twn_on_a, pr_twn_on_b), w = c(prior_a_updated, 1- prior_a_updated)) pr_a_on_twn_updated &lt;- (prior_a_updated * pr_twn_on_a) / pr_twn_updated pr_b_on_twn_updated &lt;- ((1 - prior_a_updated) * pr_twn_on_b) / pr_twn_updated tibble(pr_a_on_twn_updated = pr_a_on_twn_updated, pr_b_on_twn_updated = pr_b_on_twn_updated, control = pr_a_on_twn_updated + pr_b_on_twn_updated) %&gt;% round(digits = 4) %&gt;% knitr::kable() pr_a_on_twn_updated pr_b_on_twn_updated control 0.8205 0.1795 1 × "],
["rethinking-chapter-3.html", "4 Rethinking: Chapter 3 4.1 Sampling from a grid approximate posterior 4.2 Sampling to Summarize 4.3 Point estimates 4.4 sample to simulate prediction 4.5 Homework", " 4 Rethinking: Chapter 3 Sampling the Imaginary by Richard McElreath, building on the Summary by Solomon Kurz \\[ Pr(vampire|positive) = \\frac{Pr(positive|vampire) \\times Pr(vampire)}{Pr(positive)} \\] pr_positive_on_vamp &lt;- .95 pr_positive_on_mort &lt;- .01 pr_vamp &lt;- .001 pr_positive &lt;- pr_positive_on_vamp * pr_vamp + pr_positive_on_mort * (1 - pr_vamp) (pr_vamp_on_positive &lt;- pr_positive_on_vamp * pr_vamp /pr_positive) #&gt; [1] 0.08683729 4.1 Sampling from a grid approximate posterior posterior here means simply ‘the probability of p conditional on the data’: grid_approx &lt;- function(n_grid = 20, L = 6, W = 3, prior = function(x){rep(1, length(x))}){ tibble(p_grid = seq(0, 1, length.out = n_grid), prior = prior(p_grid), likelihood = dbinom(L, size = W + L, prob = p_grid), posterior_unstand = likelihood * prior, posterior = posterior_unstand / sum(posterior_unstand)) } grid_data &lt;- grid_approx(n_grid = 10^4) samples &lt;- tibble(sample = 1:(10^4), proportion_water = sample(x = grid_data$p_grid, size = length(sample), prob = grid_data$posterior, replace = TRUE)) p_scatter &lt;- samples %&gt;% ggplot(aes(x = sample, y = proportion_water)) + geom_point(size = .75, shape = 21, color = clr_alpha(clr2,.3), fill = clr_alpha(clr2,.1)) + scale_x_continuous(expand = c(0,0)) p_dens &lt;- samples %&gt;% ggplot(aes(x = proportion_water)) + geom_density( color = clr2, fill = fll2) + scale_x_continuous(limits = c(0,1), expand = c(0, 0)) p_scatter + p_dens 4.2 Sampling to Summarize Once the posterior distribution is created, the model is done. Typical targets / questions: intervals of defined boundaries intervals of defined probability mass point estimates 4.2.1 Intervals of devined boundaries sum(grid_data$posterior[grid_data$p_grid &lt; 0.5]) #&gt; [1] 0.171875 sum(samples$proportion_water &lt; .5) / length(samples$proportion_water) #&gt; [1] 0.1797 sum(samples$proportion_water &gt; .5 &amp; samples$proportion_water &lt; .75) / length(samples$proportion_water) #&gt; [1] 0.5976 # f_post &lt;- function(x){dbeta(x = x, shape1 = 9 +1 , shape2 = 6 +1)} f_post &lt;- function(x){dbinom(x = 6, size = 9, prob = x)} f_post_norm &lt;- function(x){ f_post(x) / integrate(f = f_post,lower = 0, upper = 1)[[1]]} plot_intervals &lt;- function(x_bounds = c(0, 1), x_line = as.numeric(NA), f_posterior = f_post_norm, data = samples, ylim = c(0, 3)){ p_d &lt;- ggplot() + stat_function(fun = f_posterior, xlim = c(0,1), geom = &quot;area&quot;, color = clr0, fill = fll0) + stat_function(fun = f_posterior, xlim = x_bounds, geom = &quot;area&quot;, color = clr2, fill = fll2) + labs(y = &quot;density&quot;, x = &quot;proportion_water&quot;) p_d_emp &lt;- data %&gt;% ggplot(aes(x = proportion_water)) + geom_density( color = clr0, fill = fll0) + stat_function(fun = function(x){demp(obs = data$proportion_water, x = x)}, xlim = x_bounds, geom = &quot;area&quot;, color = clr2, fill = fll2) + labs(y = &quot;empirical density&quot;) p_d + p_d_emp &amp; geom_vline(data = tibble(x = x_line), aes(xintercept = x), linetype = 3) &amp; scale_y_continuous(limits = ylim)&amp; scale_x_continuous(limits = c(0, 1)) } plot_intervals(x_bounds = c(0, .5), x_line = .5) / plot_intervals(x_bounds = c(.5, .75), x_line = c(.5, .75)) 4.2.2 Intervals of defined mass aka.: compatibility interval credible interval percentile interval special form: highest posterior density interval (HPDI) qnt_80 &lt;- quantile(samples$proportion_water, probs = .8) qnt_80_inner &lt;- quantile(samples$proportion_water, probs = c(.1, .9)) plot_intervals(x_bounds = c(0, qnt_80), x_line = qnt_80)/ plot_intervals(x_bounds = qnt_80_inner, x_line = qnt_80_inner) library(rethinking) map &lt;- purrr::map grid_data_skew &lt;- grid_approx(L = 3, W = 0, n_grid = 10^4) samples_skew &lt;- tibble(sample = 1:(10^4), proportion_water = sample(x = grid_data_skew$p_grid, size = length(sample), prob = grid_data_skew$posterior, replace = TRUE)) f_post_skew &lt;- function(x){dbinom(x = 3, size = 3, prob = x)} f_post_norm_skew &lt;- function(x){ f_post_skew(x) / integrate(f = f_post_skew, lower = 0, upper = 1)[[1]]} qnt_50_inner &lt;- PI(samples_skew$proportion_water, prob = .5) qnt_50_high_dens &lt;- HPDI(samples_skew$proportion_water, prob = .5) plot_intervals(x_bounds = qnt_50_inner, x_line = qnt_50_inner, f_posterior = f_post_norm_skew, data = samples_skew, ylim = c(0, 4)) / plot_intervals(x_bounds = qnt_50_high_dens, x_line = qnt_50_high_dens, f_posterior = f_post_norm_skew, data = samples_skew, ylim = c(0, 4)) 4.3 Point estimates point_estimates &lt;- tibble(proportion_water= list(mean, median, chainmode) %&gt;% map_dbl(.f = function(f, vals){ f(vals) }, vals = samples_skew$proportion_water), statistic = c(&quot;mean&quot;, &quot;median&quot;, &quot;mode&quot;)) p_point_estimates &lt;- ggplot() + stat_function(fun = f_post_norm_skew, xlim = c(0,1), geom = &quot;area&quot;, color = clr0, fill = fll0) + geom_vline(data = point_estimates, aes(xintercept = proportion_water, linetype = statistic), color = clr1) + labs(y = &quot;density&quot;) f_loss &lt;- function(x){ map_dbl(x, function(x){ sum( grid_data_skew$posterior * abs( x - grid_data_skew$p_grid)) })} f_loss_quad &lt;- function(x){ map_dbl(x, function(x){ sum( grid_data_skew$posterior * ( x - grid_data_skew$p_grid) ^ 2) })} p_loss &lt;- ggplot() + stat_function(fun = f_loss, xlim = c(0,1), geom = &quot;area&quot;, color = clr0, fill = fll0) + geom_point(data = tibble(x = point_estimates$proportion_water[2], y = f_loss(point_estimates$proportion_water[2])), aes(x = x, y = y), shape = 1, size = 3, color = clr1) + labs(x = &quot;poportion_water&quot;, y = &quot;expected proportional loss&quot;) p_loss_quad &lt;- ggplot() + stat_function(fun = f_loss_quad, xlim = c(0,1), geom = &quot;area&quot;, color = clr0, fill = fll0) + geom_point(data = tibble(x = point_estimates$proportion_water[2], y = f_loss_quad(point_estimates$proportion_water[2])), aes(x = x, y = y), shape = 1, size = 3, color = clr1) + labs(x = &quot;poportion_water&quot;, y = &quot;expected proportional loss&quot;) p_point_estimates + p_loss + p_loss_quad + plot_layout(guide = &quot;collect&quot;) &amp; theme(legend.position = &quot;bottom&quot;) 4.4 sample to simulate prediction binomial likelihood \\[ Pr(W | N ,p) = \\frac{N!}{W! (N -W)!} p^{W}(1 - p)^{N-W} \\] dbinom( 0:2, size = 2, prob = .7) #&gt; [1] 0.09 0.42 0.49 rbinom( 10, size = 2, prob = .7) #&gt; [1] 2 2 1 2 2 1 1 0 1 2 create_dummy_w &lt;- function(size, prob){ tibble(x = rbinom(10^5, size = size, prob = prob), size = size, prob = prob) } dummy_w &lt;- create_dummy_w(size = 9, prob = .7) dummy_w %&gt;% group_by(x) %&gt;% count() %&gt;% ungroup() %&gt;% ggplot(aes(x = factor(x), y = n)) + geom_bar(stat = &quot;identity&quot;, color = clr1, fill = fll1, width = .6) + labs(y = &quot;count&quot;, x = &quot;dummy water count&quot;) tibble(size = rep(c(3,6,9), each = 3), prob = rep(c(.3,.6,.9),3)) %&gt;% pmap_dfr(create_dummy_w) %&gt;% group_by(x, size , prob) %&gt;% count() %&gt;% ungroup() %&gt;% ggplot(aes(x = factor(x), y = n)) + geom_bar(stat = &quot;identity&quot;, color = clr1, fill = fll1, width = .6) + facet_grid(prob ~ size, scales = &quot;free&quot;, space = &quot;free_x&quot;, labeller = label_both) + labs(y = &quot;count&quot;, x = &quot;dummy water count&quot;) + theme(panel.background = element_rect(color = clr0d, fill = clr_alpha(clr0d,.2))) f_posterior &lt;- function(x){dbinom(x = 6, size = 9, prob = x)} f_posterior_dens &lt;- function(x){ f_posterior(x) / integrate(f = f_posterior, lower = 0, upper = 1)[[1]]} grid_points &lt;- grid_points &lt;- tibble(x = seq(.1,.9, by = .1), y = f_posterior_dens(x)) p_posterior &lt;- ggplot() + stat_function(fun = f_posterior_dens, geom = &quot;area&quot;, color = clr0d, fill = fll0,xlim = c(0,1)) + geom_segment(data = grid_points, aes(x = x, xend = x, y = 0, yend = y), color = clr1, linetype = 3) + geom_point(data = grid_points, aes(x = x, y = y), color = clr1) + labs(x = &quot;probability of water&quot;, y = &quot;density&quot;) simulate_binom &lt;- function(probability, n_draws = 10^5, size = 9) { rbinom(n_draws, size = size, prob = probability) } d_small &lt;- tibble(probability = seq(from = .1, to = .9, by = .1)) %&gt;% mutate(draws = purrr::map(probability, simulate_binom)) %&gt;% unnest(draws) p_small &lt;- d_small %&gt;% ggplot(aes(x = draws)) + geom_bar(stat = &quot;count&quot;, color = clr1, fill = fll1, width = .6) + facet_wrap(probability ~ ., nrow = 1) dist_posterior &lt;- tibble(n_water = rbinom(10^4, size = 9, prob = samples$proportion_water), seq = map(n_water, .f = function(x){sample(x = rep(c(&quot;W&quot;,&quot;L&quot;), c(x, 9-x)), size = 9, replace = FALSE)}), max_run_length = map_dbl(seq,.f = function(x){rle(x)$lengths %&gt;% max()}), n_switches = map_dbl(seq,.f = function(x){(rle(x)$lengths %&gt;% length()) -1})) p_posterior_predictive &lt;- dist_posterior %&gt;% ggplot(aes(x = factor(n_water))) + geom_bar(stat = &quot;count&quot;, color = clr1, fill = fll1, width = .6) + labs(x = &quot;number of water samples&quot;) p_posterior / p_small / p_posterior_predictive globe_data &lt;- c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;) globe_run_length &lt;- rle(globe_data)$lengths %&gt;% max() globe_n_switches &lt;- (rle(globe_data)$lengths %&gt;% length()) -1 p_run_length &lt;- dist_posterior %&gt;% ggplot(aes(x = factor(max_run_length))) + geom_bar(stat = &quot;count&quot;, aes(color = max_run_length == globe_run_length, fill = after_scale(clr_alpha(color))), width = .6) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;longest run length&quot;) p_switches &lt;- dist_posterior %&gt;% ggplot(aes(x = factor(n_switches))) + geom_bar(stat = &quot;count&quot;, aes(color = n_switches == globe_n_switches, fill = after_scale(clr_alpha(color))), width = .6) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of switches&quot;) p_run_length + p_switches 4.5 Homework n &lt;- 10^4 set.seed( 100 ) easy_data &lt;- tibble(p_grid = seq( from = 0, to = 1, length.out = n ), prior = rep(1 , n), likelihood = dbinom( 6, size = 9, prob = p_grid), posterior_unscaled = likelihood * prior, posterior = posterior_unscaled / sum(posterior_unscaled), samples = sample( p_grid, prob = posterior, size = n, replace = TRUE), cummulative_posterior = cumsum(posterior)) easy_data %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = prior / sum(prior), color = &quot;prior&quot;)) + geom_line(aes(y = likelihood / sum(likelihood), color = &quot;likelihood&quot;)) + geom_line(aes(y = posterior, color = &quot;posterior&quot;), linetype = 3) + geom_line(aes(y = cummulative_posterior / sum(cummulative_posterior), color = &quot;cummulative_posterior&quot;), linetype = 3) + scale_color_manual(values = c(prior = clr1, likelihood = clr0d, posterior = clr2, cummulative_posterior = &quot;black&quot;)) + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), legend.position = &quot;bottom&quot;) E1 sum( easy_data$posterior[easy_data$p_grid &lt; .2] ) #&gt; [1] 0.0008635326 E2 sum( easy_data$posterior[easy_data$p_grid &gt; .8] ) #&gt; [1] 0.120821 E3 sum( easy_data$posterior[easy_data$p_grid &gt; .2 &amp; easy_data$p_grid &lt; .8] ) #&gt; [1] 0.8783154 E4 quantile(easy_data$samples, probs = .2) #&gt; 20% #&gt; 0.5145315 max( easy_data$p_grid[easy_data$cummulative_posterior &lt;= .2] ) #&gt; [1] 0.5162516 E5 quantile(easy_data$samples, probs = .8) #&gt; 80% #&gt; 0.7618962 min( easy_data$p_grid[easy_data$cummulative_posterior &gt;= .8] ) #&gt; [1] 0.7605761 E6 HPDI(easy_data$samples, prob = .66) #&gt; |0.66 0.66| #&gt; 0.5138514 0.7886789 p_e6 &lt;- easy_data %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior, color = &quot;posterior&quot;)) + geom_vline(data = tibble(x = HPDI(easy_data$samples, prob = .66)), aes(xintercept = x), linetype = 3) + scale_color_manual(values = c(posterior = clr2), guide = &quot;none&quot;) + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), legend.position = &quot;bottom&quot;) E7 PI(easy_data$samples, prob = .66) #&gt; 17% 83% #&gt; 0.4972327 0.7745775 p_e7 &lt;- easy_data %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior, color = &quot;posterior&quot;)) + geom_vline(data = tibble(x = PI(easy_data$samples, prob = .66)), aes(xintercept = x), linetype = 3) + scale_color_manual(values = c(posterior = clr2), guide = &quot;none&quot;) + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), legend.position = &quot;bottom&quot;) p_e6 + p_e7 M1 grid_data &lt;- grid_approx(n_grid = 10^4, L = 8, W = 7) grid_data %&gt;% ggplot(aes(x = p_grid))+ geom_line(aes(y = posterior, color = &quot;posterior&quot;)) + scale_color_manual(values = c(posterior = clr2), guide = &quot;none&quot;) + theme(legend.position = &quot;bottom&quot;) M2 samples &lt;- tibble(sample = 1:(10^5), proportion_water = sample(x = grid_data$p_grid, size = length(sample), prob = grid_data$posterior, replace = TRUE)) HPDI(samples$proportion_water, prob = .9) #&gt; |0.9 0.9| #&gt; 0.3325333 0.7212721 M3 f_posterior &lt;- function(x){dbinom(x = 8, size = 15, prob = x)} f_posterior_dens &lt;- function(x){ f_posterior(x) / integrate(f = f_posterior, lower = 0, upper = 1)[[1]]} grid_points &lt;- grid_points &lt;- tibble(x = seq(.1, .9, by = .1), y = f_posterior_dens(x)) p_posterior &lt;- ggplot() + stat_function(fun = f_posterior_dens, geom = &quot;area&quot;, color = clr0d, fill = fll0,xlim = c(0,1)) + geom_segment(data = grid_points, aes(x = x, xend = x, y = 0, yend = y), color = clr1, linetype = 3) + geom_point(data = grid_points, aes(x = x, y = y), color = clr1) + labs(x = &quot;probability of water&quot;, y = &quot;density&quot;) simulate_binom &lt;- function(probability, n_draws = 10^5, size = 15) { rbinom(n_draws, size = size, prob = probability) } d_small &lt;- tibble(probability = seq(from = .1, to = .9, by = .1)) %&gt;% mutate(draws = purrr::map(probability, simulate_binom)) %&gt;% unnest(draws) p_small &lt;- d_small %&gt;% ggplot(aes(x = draws)) + geom_bar(stat = &quot;count&quot;, color = clr1, fill = fll1, width = .6) + facet_wrap(probability ~ ., nrow = 1) dist_posterior &lt;- tibble(n_water = rbinom(10^4, size = 15, prob = samples$proportion_water), seq = map(n_water, .f = function(x){sample(x = rep(c(&quot;W&quot;,&quot;L&quot;), c(x, 15-x)), size = 15, replace = FALSE)}), max_run_length = map_dbl(seq,.f = function(x){rle(x)$lengths %&gt;% max()}), n_switches = map_dbl(seq,.f = function(x){(rle(x)$lengths %&gt;% length()) -1})) p_posterior_predictive &lt;- dist_posterior %&gt;% ggplot(aes(x = factor(n_water))) + geom_bar(stat = &quot;count&quot;, aes(color = n_water == 8 , fill = after_scale(clr_alpha(color, .3))), width = .6) + scale_color_manual(values = c(`TRUE` = clr1, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of water samples&quot;) p_posterior / p_small / p_posterior_predictive sum( dist_posterior$n_water == 8 ) / length( dist_posterior$n_water ) #&gt; [1] 0.1462 M4 dist_posterior &lt;- tibble(n_water = rbinom(10^4, size = 9, prob = samples$proportion_water), seq = map(n_water, .f = function(x){sample(x = rep(c(&quot;W&quot;,&quot;L&quot;), c(x, 9-x)), size = 9, replace = FALSE)}), max_run_length = map_dbl(seq,.f = function(x){rle(x)$lengths %&gt;% max()}), n_switches = map_dbl(seq,.f = function(x){(rle(x)$lengths %&gt;% length()) -1})) dist_posterior %&gt;% ggplot(aes(x = factor(n_water))) + geom_bar(stat = &quot;count&quot;, aes(color = n_water == 6 , fill = after_scale(clr_alpha(color, .3))), width = .6) + scale_color_manual(values = c(`TRUE` = clr1, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of water samples&quot;) sum( dist_posterior$n_water == 6 ) / length( dist_posterior$n_water ) #&gt; [1] 0.175 M5 grid_data &lt;- grid_approx(n_grid = 10^4, L = 8, W = 7, prior = function(x){if_else(x &lt; .5, 0, 1)}) grid_data %&gt;% ggplot(aes(x = p_grid))+ geom_line(aes(y = posterior, color = &quot;posterior&quot;)) + scale_color_manual(values = c(posterior = clr1), guide = &quot;none&quot;) + theme(legend.position = &quot;bottom&quot;) samples &lt;- tibble(sample = 1:(10^5), proportion_water = sample(x = grid_data$p_grid, size = length(sample), prob = grid_data$posterior, replace = TRUE)) HPDI(samples$proportion_water, prob = .9) #&gt; |0.9 0.9| #&gt; 0.5000500 0.7127713 #&lt;&lt;CURRENT STATUS&gt;&gt;# f_posterior &lt;- function(x){dbinom(x = 8, size = 15, prob = x)} f_posterior_dens &lt;- function(x){ f_posterior(x) / integrate(f = f_posterior, lower = 0, upper = 1)[[1]]} grid_points &lt;- grid_points &lt;- tibble(x = seq(.1, .9, by = .1), y = f_posterior_dens(x)) p_posterior &lt;- ggplot() + stat_function(fun = f_posterior_dens, geom = &quot;area&quot;, color = clr0d, fill = fll0,xlim = c(0,1)) + geom_segment(data = grid_points, aes(x = x, xend = x, y = 0, yend = y), color = clr2, linetype = 3) + geom_point(data = grid_points, aes(x = x, y = y), color = clr2) + labs(x = &quot;probability of water&quot;, y = &quot;density&quot;) simulate_binom &lt;- function(probability, n_draws = 10^5, size = 15) { rbinom(n_draws, size = size, prob = probability) } d_small &lt;- tibble(probability = seq(from = .1, to = .9, by = .1)) %&gt;% mutate(draws = purrr::map(probability, simulate_binom)) %&gt;% unnest(draws) p_small &lt;- d_small %&gt;% ggplot(aes(x = draws)) + geom_bar(stat = &quot;count&quot;, color = clr2, fill = fll2, width = .6) + facet_wrap(probability ~ ., nrow = 1) dist_posterior &lt;- tibble(n_water = rbinom(10^4, size = 15, prob = samples$proportion_water), seq = map(n_water, .f = function(x){sample(x = rep(c(&quot;W&quot;,&quot;L&quot;), c(x, 15-x)), size = 15, replace = FALSE)}), max_run_length = map_dbl(seq,.f = function(x){rle(x)$lengths %&gt;% max()}), n_switches = map_dbl(seq,.f = function(x){(rle(x)$lengths %&gt;% length()) -1})) p_posterior_predictive &lt;- dist_posterior %&gt;% ggplot(aes(x = factor(n_water))) + geom_bar(stat = &quot;count&quot;, aes(color = n_water == 8 , fill = after_scale(clr_alpha(color, .3))), width = .6) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of water samples&quot;) p_posterior / p_small / p_posterior_predictive sum( dist_posterior$n_water == 8 ) / length( dist_posterior$n_water ) #&gt; [1] 0.1561 × "],
["rethinking-chapter-4.html", "5 Rethinking: Chapter 4", " 5 Rethinking: Chapter 4 Geocentric Models by Richard McElreath, building on the Summary by Solomon Kurz × "],
["bayesian-statistics-the-fun-way.html", "6 Bayesian Statistics the Fun Way 6.1 Conditioning Probabilities 6.2 Combining Probailities based on logic 6.3 The binomial distribution 6.4 The beta distribution 6.5 Bayes’ Theorem 6.6 Parameter Estimation (I) 6.7 The normal distribution 6.8 Cummulative Density and Quantile Function 6.9 Parameter estimation with prior probabilities 6.10 Monte CarloSimulation 6.11 Posterior Odds 6.12 Parameter Estimation (II)", " 6 Bayesian Statistics the Fun Way by Will Kurt 6.1 Conditioning Probabilities \\[ \\begin{eqnarray} D &amp; = &amp; observed~data\\\\ H_{1} &amp; = &amp; Hypothesis\\\\ X &amp; = &amp;prior~belief\\\\ \\end{eqnarray} \\] Allow us to formulate the probability of the observed data given our hypothesis and our prior belief. \\[ P(D | H_{1}, X) \\] To compare different hypothesis, use the ratio of probabilities (odds): \\[ \\frac{P(D | H_{1}, X)}{P(D | H_{2}, X)} &gt; 1 \\] 6.2 Combining Probailities based on logic Rules for \\(AND\\) (\\(\\land\\)), \\(OR\\) (\\(\\lor\\)) and \\(NOT\\) (\\(\\neg\\)). \\(NOT:\\) \\[ \\begin{eqnarray} P(X) &amp; = &amp; p\\\\ \\neg P(X) &amp; = &amp; 1 - p \\end{eqnarray} \\] \\(AND\\) \\[ \\begin{eqnarray} P(Y) &amp; = &amp; q \\\\ P(X) \\land P(Y) &amp; = &amp; P(X,Y) = p \\times q \\end{eqnarray} \\] \\(OR\\) (mutually exclusive events): \\[ P(X) \\lor P(Y) = P(X),P(Y) = p + q \\] while: \\[ P(X) \\land P(Y) = 0 \\] \\(OR\\) (non-mutually exclusive events): \\[ P(X) \\lor P(Y) = P(X) + P(Y) - P(X, Y) \\] 6.3 The binomial distribution Factorial (factorial(x)): \\[ x! = x \\times x-1 \\times x -2 ... \\] The binomial coefficient (choose(n, k)): \\[ {n \\choose k} = \\frac{n!}{k! \\times (n - k)!} \\] The binomial distribution (a Probability Mass Function, PMF): \\[ B(k;n,p) = {n \\choose k} \\times p^k \\times (1 - p) ^{n-k} \\] wdh &lt;- 5000 n &lt;- 10 p = 0.5 tibble(k = rbinom(wdh, n , p)) %&gt;% group_by(k) %&gt;% count(name = &quot;nn&quot;) %&gt;% ungroup() %&gt;% ggplot(aes(x = k, y = nn/wdh)) + geom_bar(stat = &quot;identity&quot;, color = clr1, fill = fll1) + labs(x = &quot;k&quot;, y = glue(&quot;B(k; {n}, {p})&quot;)) + coord_cartesian(xlim = c(-.5, 10.5)) n &lt;- 10 p = 1/6 tibble(k = rbinom(wdh, n , p)) %&gt;% group_by(k) %&gt;% count(name = &quot;nn&quot;) %&gt;% ungroup() %&gt;% ggplot(aes(x = k, y = nn/wdh)) + geom_bar(stat = &quot;identity&quot;, color = clr1, fill = fll1) + labs(x = &quot;k&quot;, y = glue(&quot;B(k; {n}, {round(p,2)})&quot;)) + coord_cartesian(xlim = c(-.5, 10.5)) Cumulative probability to have at least \\(x\\) successes in \\(n\\) trials (pbinom(x-1, n, p, lower.tail = FALSE)): \\[ \\sum_{k=x}^n B(k;n,p) \\] Similarly, less then \\(x\\) successes in \\(n\\) trials (pbinom(x, n, p)): \\[ \\sum_{k=0}^{x-1} B(k;n,p) \\] 6.4 The beta distribution \\[ Beta(p;\\alpha,\\beta) = \\frac{p^{\\alpha -1} \\times (1 - p)^{\\beta - 1}}{beta(\\alpha, \\beta)} \\] Example for an \\(n = 41\\), with \\(\\alpha = 14\\) (successes) and \\(\\beta = 27\\) (fails). alpha &lt;- 14 beta &lt;- 27 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr1, fill = fll1, n = 500) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) Probability, that chance of sucess is less than 0.5: \\[ \\int_{0}^{0.5} Beta(p; 14, 27) \\] x_cutoff &lt;- 0.5 integrate(function(p){ dbeta(p, 14, 27) }, 0, x_cutoff) #&gt; 0.9807613 with absolute error &lt; 5.9e-06 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(xlim = c(0, x_cutoff), fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr2, fill = fll2, n = 500) + geom_vline(xintercept = x_cutoff, linetype = 3) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) alpha &lt;- 5 beta &lt;- 1195 x_cutoff &lt;- 0.005 integrate(function(p){ dbeta(p, alpha, beta) }, x_cutoff, 1) #&gt; 0.2850559 with absolute error &lt; 1e-04 ggplot(tibble(x = seq(0, .01, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(xlim = c(x_cutoff, .01), fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr2, fill = fll2, n = 500) + geom_vline(xintercept = x_cutoff, linetype = 3) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) Exercises # 1) integrate(function(p){ dbeta(p, 4, 6) }, 0.6, 1) #&gt; 0.09935258 with absolute error &lt; 1.1e-15 # 2) integrate(function(p){ dbeta(p, 9, 11) }, 0.45, 0.55) #&gt; 0.30988 with absolute error &lt; 3.4e-15 # 3) integrate(function(p){ dbeta(p, 109, 111) }, 0.45, 0.55) #&gt; 0.8589371 with absolute error &lt; 9.5e-15 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = 9, shape2 = 11) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(fun = function(x){ -dbeta(x, shape1 = 109, shape2 = 111) }, geom = &quot;area&quot;, color = clr1, fill = fll1, n = 500) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) 6.5 Bayes’ Theorem conditional probability The probability of A given B is \\(P(A | B)\\) Dependence updates the product rule of probabilities: \\[ P(A,B) = P(A) \\times P(B | A) \\] (This also holds for independend probabilities, where \\(P(B) = P(B|A)\\)) Bayes’ Theorem (reversing the condition to calculate the probability of the event we are conditioning on \\(P(A|B) \\rightarrow P(B|A)\\)) \\[ P(A|B) = \\frac{P(A)P(B|A)}{P(B)} \\] The important parts here are the posterior probability\\(P(H|D)\\): how strongly we belief in our hypothesis given the data likelyhood\\(P(D|H)\\): the probability of data if the hypothesis were true prior probability\\(P(H)\\): how likely our hypothesis is in the first place Unnormalized posterior \\[ P(H|D) \\propto P(H) \\times P(D|H) \\] 6.6 Parameter Estimation (I) Expectation / mean \\[ \\mu = \\sum_{1}^{n}p_{i}x_{i} \\] n &lt;- 150 mn &lt;- 3 tibble( y = rnorm(n = n, mean = mn), n = 1:n, cum_y = cumsum(y), mean_y = cum_y / n) %&gt;% ggplot(aes(x = n, y = mean_y)) + geom_hline(yintercept = mn, linetype = 3) + geom_point(aes(y = y), color = clr0, size = .75, alpha = .5) + geom_line(color = clr2, size = .75) Spread, mean absolute deviation \\[ MAD(x) = \\frac{1}{n} \\times \\sum_{1}^{n} | x_{i} - \\mu| \\] Spread, variation \\[ Var(x) = \\frac{1}{n} \\times \\sum_{1}^{n} (x_{i} - \\mu) ^{2} \\] Spread, standard deviation \\[ \\sigma = \\sqrt{\\frac{1}{n} \\times \\sum_{1}^{n} (x_{i} - \\mu) ^{2}} \\] Note that in R, var() and sd() uses \\(\\frac{1}{n-1}\\) as denominator for the normalization: var_k &lt;- function(x){ (1/length(x)) * sum( (x - mean(x)) ^ 2 ) } sd_k &lt;- function(x){ sqrt(var_k(x)) } x &lt;- 1:10 n &lt;- length(x) var(x) * ((n-1)/n) == var_k(x) #&gt; [1] TRUE sd(x) * sqrt((n-1)/n) == sd_k(x) #&gt; [1] TRUE 6.7 The normal distribution The probability density function (PDF) for the normal distribution (dnorm()): \\[ N(\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\times e ^{- \\frac{(x - \\mu) ^ 2}{2\\sigma^2}} \\] mu &lt;- 20.6 sigma &lt;- 1.62 x_cutoff &lt;- 18 ggplot(tibble(x = seq(mu - 4 * sigma, mu + 4 * sigma, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dnorm(x, mu, sigma) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(fun = function(x){ dnorm(x, mu, sigma) }, xlim = c(mu - 4 * sigma, x_cutoff), geom = &quot;area&quot;, color = clr2, fill = fll2, n = 500) + geom_vline(xintercept = x_cutoff, linetype = 3) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) integrate(function(x){ dnorm(x, mu, sigma) }, 0, x_cutoff) #&gt; 0.05425369 with absolute error &lt; 3.5e-05 Known probability mass under a normal distribution in terms of ist standard deviation: distance from \\(\\mu\\) probability \\(\\sigma\\) 68 % \\(2 \\sigma\\) 95 % \\(3 \\sigma\\) 99.7 % Excercises x &lt;- c(100, 99.8, 101, 100.5,99.7) mu &lt;- mean(x) sigma &lt;- sd(x) x_cutoff &lt;- 100.4 ggplot(tibble(x = seq(mu - 4 * sigma, mu + 4 * sigma, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dnorm(x, mu, sigma) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(fun = function(x){ dnorm(x, mu, sigma) }, xlim = c(x_cutoff, mu + 4 * sigma), geom = &quot;area&quot;, color = clr2, fill = fll2, n = 500) + geom_vline(xintercept = x_cutoff, linetype = 3) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) 1 - (integrate(function(x){ dnorm(x, mu, sigma) }, mu - sigma, x_cutoff)[[1]] + (1-.68)/2) #&gt; [1] 0.3550062 6.8 Cummulative Density and Quantile Function Beta distribution example Mean of Beta distribution \\[ \\mu_{Beta} = \\frac{\\alpha}{\\alpha + \\beta} \\] alpha &lt;- 300 beta &lt;- 39700 mu &lt;- alpha / (alpha + beta) med &lt;- qbeta(.5, shape1 = alpha, shape2 = beta) bound_left &lt;- .006 bound_right &lt;- .009 6.9 Parameter estimation with prior probabilities alpha_data &lt;- 2 beta_data &lt;- 3 alpha_prior &lt;- 1 beta_prior &lt;- 41 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_data, shape2 = beta_data) }, geom = &quot;line&quot;, color = clr0,linetype = 1, n = 500) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_prior, shape2 = beta_prior) }, geom = &quot;line&quot;, color = clr0,linetype = 2, n = 500) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_data + alpha_prior, shape2 = beta_data + beta_prior) }, geom = &quot;area&quot;, color = clr1, fill = fll1, size = .2, n = 500) + labs(x = &quot;Probability&quot;, y = &quot;Density&quot;, caption = &quot;**solid:** data; **dashed:** prior; **filled:** posterior&quot;) + coord_cartesian(ylim = c(0, 15)) + theme(plot.caption = ggtext::element_markdown(halign = .5, hjust = .5)) 6.10 Monte CarloSimulation alpha_a &lt;- 36 beta_a &lt;- 114 alpha_b &lt;- 50 beta_b &lt;- 100 alpha_prior &lt;- 3 beta_prior &lt;- 7 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_a + alpha_prior, shape2 = beta_a + beta_prior) }, geom = &quot;area&quot;, aes(color = &quot;a&quot;, fill = after_scale(clr_alpha(color))), size = .5, n = 500) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_b + alpha_prior, shape2 = beta_b + beta_prior) }, geom = &quot;area&quot;, aes(color = &quot;b&quot;, fill = after_scale(clr_alpha(color))), size = .5, n = 500) + labs(x = &quot;Probability&quot;, y = &quot;Density&quot;) + scale_color_manual(&quot;Variant&quot;, values = c(a = clr0, b = clr2)) n_trials &lt;- 10^5 mc_simulation &lt;- tibble(samples_a = rbeta(n_trials, alpha_a + alpha_prior, beta_a + beta_prior), samples_b = rbeta(n_trials, alpha_b + alpha_prior, beta_b + beta_prior), samples_ratio = samples_b / samples_a) p_b_superior &lt;- sum(mc_simulation$samples_b &gt; mc_simulation$samples_a)/n_trials p_b_superior #&gt; [1] 0.96005 p_hist &lt;- mc_simulation %&gt;% ggplot(aes(x = samples_ratio)) + geom_histogram(color = clr2, fill = fll2, size = .2, bins = 20,boundary = 1) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) p_ecdf &lt;- mc_simulation %&gt;% ggplot() + stat_function(fun = function(x){(ecdf(x = mc_simulation$samples_ratio))(x)}, xlim = c(range(mc_simulation$samples_ratio)), geom = &quot;area&quot;,color = clr2, fill = fll2, size = .2, n = 500) + labs(x = &quot;Variant Ratio = Improvement&quot;, y = &quot;Cumulative Probability&quot;) p_hist / p_ecdf &amp; coord_cartesian(xlim = c(0.2,3.3), expand = 0) 6.11 Posterior Odds For compering Hypotheses: ratio of posterior: \\[ posterior odds = \\frac{P(H_1) \\times P(D | H_{1})}{P(H_2) \\times P(D | H_{2})} = O(H_{1}) \\times \\frac{P(D | H_{1})}{P(D | H_{2})} \\] This consists of the Bayes factor: \\[ \\frac{P(D | H_{1})}{P(D | H_{2})} \\] and the ratio of prior probabilities \\[ O(H_{1}) = \\frac{P(H_1)}{P(H_2)} \\] rough guide to evaluate poterior odds: Posterior odds Strength of evidence 1 to 3 Interesting but not conclusive 3 to 20 Looks like we’re onto something 20 to 150 Strong evidence in favor of \\(H_1\\) &gt; 150 Overwhelming evidence 6.12 Parameter Estimation (II) dx &lt;- 0.01 bayes_factor &lt;- function(h_top, h_bottom, n_success = 24, n_total = 100){ (h_top ^ n_success * (1 - h_top) ^ (n_total - n_success)) / (h_bottom ^ n_success * (1 - h_bottom) ^ (n_total - n_success)) } bayes_fs &lt;- tibble(hypothesis = seq(0, 1, by = dx), bayes_factor = bayes_factor(hypothesis, h_bottom = 0.5)) %&gt;% mutate(prior = if_else(between(hypothesis, 0.2, 0.3), .001, 1), posterior = prior * bayes_factor, posterior_normalized = posterior / sum(posterior)) p_bayes_factor &lt;- bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = bayes_factor)) + geom_area(color = clr1, fill = fll1, size = .3) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) p_prior &lt;- bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = prior)) + geom_area(color = clr0, fill = fll0, size = .3) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) p_posterior &lt;- bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = posterior)) + geom_area(color = clr2, fill = fll2, size = .3) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) p_posterior_n &lt;- bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = posterior_normalized)) + geom_area(color = clr2, fill = fll2, size = .3) p_bayes_factor / p_prior / p_posterior / p_posterior_n hypothesis bayes_factor prior posterior posterior_normalized 0.24 1478776 0.001 1478.776 0.0004708 Probability of true chance is smaller “one in two”. bayes_fs %&gt;% filter(hypothesis &lt; 0.5) %&gt;% summarise(p_lower_than_half = sum(posterior_normalized)) #&gt; # A tibble: 1 x 1 #&gt; p_lower_than_half #&gt; &lt;dbl&gt; #&gt; 1 1.00 Expectation of the probability distribution (sum of expectations weighted by their value) sum(bayes_fs$posterior_normalized * bayes_fs$hypothesis) #&gt; [1] 0.2402704 Or (because of gap) choose most likely estimate: bayes_fs %&gt;% filter(posterior_normalized == max(posterior_normalized)) %&gt;% knitr::kable() hypothesis bayes_factor prior posterior posterior_normalized 0.19 688568.9 1 688568.9 0.2192415 bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = bayes_factor / sum(bayes_factor))) + geom_area(color = clr2, fill = fll2, size = .3) + stat_function(fun = function(x){ dbeta(x, shape1 = 24 + 1 , shape2 = 76 + 1) * dx}, geom = &quot;point&quot;, shape = 21, color = clr2, fill = fll2, size = 1.5, n = 500) Exercises bayes_fs_e1 &lt;- tibble(hypothesis = seq(0, 1, by = dx), bayes_factor = bayes_factor(hypothesis, h_bottom = 0.24)) %&gt;% mutate(prior = if_else(between(hypothesis, 0.2, 0.3), .001, 1), posterior = prior * bayes_factor, posterior_normalized = posterior / sum(posterior)) bayes_fs_e1 %&gt;% ggplot(aes(x = hypothesis, y = bayes_factor / sum(bayes_factor))) + geom_area(color = clr2, fill = fll2, size = .3) + stat_function(fun = function(x){ dbeta(x, shape1 = 24 + 1 , shape2 = 76 + 1) * dx}, geom = &quot;point&quot;, shape = 21, color = clr2, fill = fll2, size = 1.5, n = 500) bayes_fs_e2 &lt;- tibble(hypothesis = seq(0, 1, by = dx), bayes_factor = bayes_factor(hypothesis, h_bottom = 0.24)) %&gt;% mutate(prior = 1.05 ^ (seq_along(hypothesis) - 1), posterior = prior * bayes_factor, posterior_normalized = posterior / sum(posterior)) bayes_fs_e2 %&gt;% ggplot(aes(x = hypothesis, y = prior)) + geom_area(color = clr0, fill = fll0, size = .3) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) bayes_fs_e2 %&gt;% ggplot(aes(x = hypothesis, y = posterior_normalized)) + geom_area(color = clr2, fill = fll2, size = .3) × "],
["mixed-models-with-r.html", "7 Mixed Models with R 7.1 Standard regregression model 7.2 mixed nodel 7.3 Add random slope 7.4 Cross Classified models 7.5 Hierachical structure 7.6 Residual Structure 7.7 Generalized Linear Mixed Models 7.8 Issues/ Considderations 7.9 Formula summary", " 7 Mixed Models with R by Michael Clark load(&quot;data/gpa.RData&quot;) gpa &lt;- gpa %&gt;% as_tibble() 7.1 Standard regregression model \\[ gpa = b_{intercept} + b_{occ} \\times occasion + \\epsilon \\] Coefficients \\(b\\) for intercept and effect of time. The error \\(\\epsilon\\) is assumed to be normally distributed with \\(\\mu = 0\\) and some standard deviation \\(\\sigma\\). \\[ \\epsilon \\sim \\mathscr{N}(0, \\sigma) \\] alternate notation, with emphasis on the data generating process: \\[ gpa ~ \\sim \\mathscr{N}(\\mu, \\sigma)\\\\ \\mu = b_{intercept} + b_{occ} \\times occasion \\] 7.2 mixed nodel 7.2.1 student specific effect (initial depiction) \\[ gpa = b_{intercept} + b_{occ} \\times occasion + ( \\textit{effect}_{student} + \\epsilon )\\\\ \\textit{effect}_{student} \\sim \\mathscr{N}(0, \\tau) \\] focusing on the coefficients (rather than on sources of error): \\[ gpa = ( b_{intercept} + \\textit{effect}_{student} ) + b_{occ} \\times occasion + \\epsilon \\] or (shorter) \\[ gpa = b_{int\\_student} + b_{occ} \\times occasion + \\epsilon \\] \\(\\rightarrow\\) this means student specific intercepts… \\[ b_{int\\_student} \\sim \\mathscr{N}(b_{intercept}, \\tau) \\] …that are normally distributed with the mean of the overall intercept (random intercepts model) 7.2.2 as multi-level model two-part regression model (one at observation level, one at student level) (this is the same as above, just needs ‘plugging in’) \\[ gpa = b_{int\\_student} + b_{occ} \\times occasion + \\epsilon\\\\ b_{int\\_student} = b_{intercept} + \\textit{effect}_{student} \\] ! There is no student-specific effect for \\(occasion\\) (which is termed fixed effect), and there is no random component gpa_lm &lt;- lm(gpa ~ occasion, data = gpa) gpa %&gt;% ggplot(aes(x = year - 1 + as.numeric(semester)/2, y = gpa, group = student)) + geom_line(alpha = .2) + geom_abline(slope = gpa_lm$coefficients[[2]], intercept = gpa_lm$coefficients[[1]], color = clr2, size = 1) + labs(x = &quot;semester&quot;) + coord_cartesian(ylim = c(1,4), expand = 0) pander::pander(summary(gpa_lm), round = 3)   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.599 0.018 145.7 0 occasion 0.106 0.006 18.04 0 Fitting linear model: gpa ~ occasion Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 1200 0.3487 0.2136 0.2129 Student effect not taken into account. 7.2.3 Mixed Model gpa_mixed &lt;- lmer(gpa ~ occasion + (1 | student), data = gpa) (Test automatic equation creation) library(equatiomatic) # Give the results to extract_eq extract_eq(gpa_mixed,) \\[ \\begin{aligned} \\operatorname{gpa}_{i} &amp;\\sim N \\left(\\alpha_{j[i]} + \\beta_{1}(\\operatorname{occasion}), \\sigma^2 \\right) \\\\ \\alpha_{j} &amp;\\sim N \\left(\\mu_{\\alpha_{j}}, \\sigma^2_{\\alpha_{j}} \\right) \\text{, for student j = 1,} \\dots \\text{,J} \\end{aligned} \\] term value se t p_value lower_2.5 upper_97.5 Intercept 2.599 0.022 119.8 0 2.557 2.642 occasion 0.106 0.004 26.1 0 0.098 0.114 group effect variance sd var_prop student Intercept 0.064 0.252 0.523 Residual 0.058 0.241 0.477 Coefficients (fixed effects) for time and intercept are the same as lm() Getting confidence intervals from a mixed model (since \\(p\\) values are not given (== 0 ?)) confint(gpa_mixed) #&gt; 2.5 % 97.5 % #&gt; .sig01 0.22517423 0.2824604 #&gt; .sigma 0.23071113 0.2518510 #&gt; (Intercept) 2.55665145 2.6417771 #&gt; occasion 0.09832589 0.1143027 mm_cinf &lt;- mixedup::extract_vc(gpa_mixed) mm_cinf %&gt;% pander::pander() Table continues below   group effect variance sd sd_2.5 sd_(Intercept)|student student Intercept 0.064 0.252 0.225 sigma Residual 0.058 0.241 0.231   sd_97.5 var_prop sd_(Intercept)|student 0.282 0.523 sigma 0.252 0.477 student effect \\(\\tau\\) = 0.252 / 0.064 (sd / var) Percentage of student variation as share of the total variation (intraclass correlation): 0.064 / 0.122 = 0.5245902 7.2.4 Estimation of random effects Random effect mixedup::extract_random_effects(gpa_mixed) %&gt;% head(5) %&gt;% knitr::kable() group_var effect group value se lower_2.5 upper_97.5 student Intercept 1 -0.071 0.092 -0.251 0.109 student Intercept 2 -0.216 0.092 -0.395 -0.036 student Intercept 3 0.088 0.092 -0.091 0.268 student Intercept 4 -0.187 0.092 -0.366 -0.007 student Intercept 5 0.030 0.092 -0.149 0.210 Random intercept (intercept + random effect) mm_coefs &lt;- mixedup::extract_coef(gpa_mixed) mm_coefs %&gt;% head(5) %&gt;% knitr::kable() group_var effect group value se lower_2.5 upper_97.5 student Intercept 1 2.528 0.095 2.343 2.713 student Intercept 2 2.383 0.095 2.198 2.568 student Intercept 3 2.687 0.095 2.502 2.872 student Intercept 4 2.412 0.095 2.227 2.597 student Intercept 5 2.629 0.095 2.444 2.814 library(merTools) mm_intervals &lt;- predictInterval(gpa_mixed) %&gt;% as_tibble() mm_mean_sd &lt;- REsim(gpa_mixed) %&gt;% as_tibble() sd_level &lt;- .95 mm_mean_sd %&gt;% mutate(sd_leveled = sd * qnorm(1 - ((1 - sd_level)/2)), sig = (median + sd_leveled) &gt;= 0 &amp; (median - sd_leveled) &lt;= 0) %&gt;% arrange(median) %&gt;% mutate(rank = row_number()) %&gt;% arrange(groupID) %&gt;% ggplot(aes(x = rank)) + geom_segment(aes(xend = rank, y = median - sd_leveled, yend = median + sd_leveled, color = sig)) + geom_point(aes(y = median), size = .3, alpha = .5) + geom_hline(yintercept = 0, size = .4, linetype = 3) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;Student&quot;, y = &quot;Coefficient&quot;, caption = &quot;interval estimates of random effects&quot;) 7.2.5 Prediction gpa_predictions &lt;- tibble(lm = predict(gpa_lm), lmm_no_random_effects = predict(gpa_mixed, re.form = NA), lmm_with_random_effects = predict(gpa_mixed)) %&gt;% bind_cols(gpa, .) gpa_predictions %&gt;% ggplot(aes(x = lm)) + geom_point(aes(y = lmm_with_random_effects, color = &quot;with_re&quot;)) + geom_point(aes(y = lmm_no_random_effects, color = &quot;no_re&quot;)) + scale_color_manual(values = c(no_re = clr2, with_re = clr0d)) student_select &lt;- 1:2 gpa_predictions %&gt;% filter(student %in% student_select) %&gt;% ggplot(aes(x = occasion)) + geom_point(aes(y = gpa, color = student))+ geom_abline(data = tibble(slope = c(gpa_lm$coefficients[[2]], mm_fixed$value[c(2,2)]), intercept = c(gpa_lm$coefficients[[1]], mm_coefs$value[as.numeric(as.character(mm_coefs$group)) %in% student_select]), model = c(&quot;lm&quot;, as.character(mm_coefs$group[as.numeric(as.character(mm_coefs$group)) %in% student_select]))), aes(slope = slope, intercept = intercept, color = model), size = .6) + scale_color_manual(values = c(lm = clr2, `1` = clr1, `2` = clr0d), labels = c( &quot;lm&quot;, &quot;lmm (student1)&quot;, &quot;lmm (student2)&quot;)) 7.2.6 Cluster Level Covariates If a cluster level covariate is added (eg. sex), \\(b_{int\\_student}\\) turns into: \\[ b_{int\\_student} = b_{intercept} + b_{sex} \\times \\textit{sex} + \\textit{effect}_{student} \\] plugging this into the model will result in \\[ gpa = b_{intercept} + b_{occ} \\times \\textit{occasion} + b_{sex} \\times \\textit{sex} + ( \\textit{effect}_{student} + \\epsilon) \\] 7.3 Add random slope gpa_mixed2 &lt;- lmer(gpa ~ occasion + (1 + occasion | student), data = gpa) mixedup::extract_fixed_effects(gpa_mixed2) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 2.599 0.018 141.592 0 2.563 2.635 occasion 0.106 0.006 18.066 0 0.095 0.118 mixedup::extract_vc(gpa_mixed2, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop student Intercept 0.045 0.213 0.491 student occasion 0.005 0.067 0.049 Residual 0.042 0.206 0.460 gpa_mixed2_rc &lt;- mixedup::extract_random_coefs(gpa_mixed2) correlation of the intercepts and slopes (negative, so students with a low starting score tend to increase a little more) VarCorr(gpa_mixed2) %&gt;% as_tibble() %&gt;% knitr::kable() grp var1 var2 vcov sdcor student (Intercept) NA 0.0451934 0.2125875 student occasion NA 0.0045039 0.0671114 student (Intercept) occasion -0.0014016 -0.0982391 Residual NA NA 0.0423879 0.2058832 gpa_lm_separate &lt;- gpa %&gt;% group_by(student) %&gt;% nest() %&gt;% mutate(mod = map(data,function(data){lm(gpa ~ occasion, data = data)})) %&gt;% bind_cols(., summarise_model(.)) p_intercepts &lt;- ggplot() + geom_density(data = gpa_mixed2_rc %&gt;% filter(effect == &quot;Intercept&quot;), aes(x = value, color = &quot;mixed&quot;, fill = after_scale(clr_alpha(color)))) + geom_density(data = gpa_lm_separate, aes(x = intercept, color = &quot;separate&quot;, fill = after_scale(clr_alpha(color)))) + labs(x = &quot;intercept&quot;) + xlim(1.5, 4) p_slopes &lt;- ggplot() + geom_density(data = gpa_mixed2_rc %&gt;% filter(effect == &quot;occasion&quot;), aes(x = value, color = &quot;mixed&quot;, fill = after_scale(clr_alpha(color)))) + geom_density(data = gpa_lm_separate, aes(x = slope, color = &quot;separate&quot;, fill = after_scale(clr_alpha(color)))) + labs(x = &quot;slope&quot;) + xlim(-.2, .4) p_intercepts + p_slopes + plot_layout(guides = &quot;collect&quot;) &amp; scale_color_manual(&quot;model&quot;, values = c(separate = clr0d, mixed = clr2)) &amp; theme(legend.position = &quot;bottom&quot;) \\(\\rightarrow\\) mixed model intercepts and slopes are less extreme In both cases the mixed model shrinks what would have been the by-group estimate, which would otherwise overfit in this scenario. This regularizing effect is yet another bonus when using mixed models. gpa_predictions &lt;- tibble(lmm_with_random_slope = predict(gpa_mixed2)) %&gt;% bind_cols(gpa_predictions, .) gpa_mixed2_rc_wide &lt;- gpa_mixed2_rc %&gt;% dplyr::select(group_var, group, effect, value) %&gt;% pivot_wider(names_from = effect, values_from = value) student_select &lt;- 1:2 p_two_students &lt;- gpa_predictions %&gt;% filter(student %in% student_select) %&gt;% ggplot(aes(x = occasion)) + geom_point(aes(y = gpa, color = student))+ geom_abline(data = tibble(slope = c(gpa_lm$coefficients[[2]], gpa_mixed2_rc_wide$occasion[as.numeric(as.character(gpa_mixed2_rc_wide$group)) %in% student_select]), intercept = c(gpa_lm$coefficients[[1]], gpa_mixed2_rc_wide$Intercept[as.numeric(as.character(gpa_mixed2_rc_wide$group)) %in% student_select]), model = c(&quot;lm&quot;, as.character(gpa_mixed2_rc_wide$group[as.numeric(as.character(gpa_mixed2_rc_wide$group)) %in% student_select]))), aes(slope = slope, intercept = intercept, color = model), size = .6) + scale_color_manual(values = c(lm = clr2, `1` = clr1, `2` = clr0d), labels = c( &quot;lm&quot;, &quot;lmm (student1)&quot;, &quot;lmm (student2)&quot;)) p_all_mod &lt;- ggplot(data = gpa_predictions, aes(x = occasion, y = gpa)) + geom_abline(data = tibble(slope = c(gpa_mixed2_rc_wide$occasion, gpa_lm$coefficients[[2]]), intercept = c(gpa_mixed2_rc_wide$Intercept, gpa_lm$coefficients[[1]]), modeltype = c(rep(&quot;lmm (random slope)&quot;, length(gpa_mixed2_rc_wide$Intercept)), &quot;lm&quot;)), aes(slope = slope, intercept = intercept, color = modeltype), size = .6) + scale_color_manual(values = c(`lmm (random slope)` = clr_alpha(clr0d ,.6), lm = clr2)) p_two_students + p_all_mod + plot_layout(guides = &quot;collect&quot;) &amp; xlim(0,5) &amp; ylim(2.2, 4) &amp; theme(legend.position = &quot;bottom&quot;) 7.4 Cross Classified models Setups where data are grouped by several factors but these are not nested (all participants get to see all images). These are crossed random effects. load(&quot;data/pupils.RData&quot;) pupils %&gt;% head() %&gt;% knitr::kable() PUPIL primary_school_id secondary_school_id achievement sex ses primary_denominational secondary_denominational 1 1 2 6.6 female highest no no 2 1 1 5.7 male lowest no yes 3 1 17 4.5 male 2 no no 4 1 3 4.4 male 2 no no 5 1 4 5.8 male 3 no yes 6 1 4 5.0 female 4 no yes pupils_crossed &lt;- lmer( achievement ~ sex + ses + ( 1 | primary_school_id ) + ( 1 | secondary_school_id ), data = pupils ) mixedup::extract_fixed_effects(pupils_crossed) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 5.924 0.123 48.303 0.000 5.684 6.164 sexfemale 0.261 0.046 5.716 0.000 0.171 0.350 ses2 0.132 0.118 1.122 0.262 -0.098 0.362 ses3 0.098 0.110 0.890 0.373 -0.118 0.314 ses4 0.298 0.105 2.851 0.004 0.093 0.503 ses5 0.354 0.101 3.514 0.000 0.156 0.551 seshighest 0.616 0.110 5.602 0.000 0.401 0.832 mixedup::extract_vc(pupils_crossed, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop primary_school_id Intercept 0.173 0.416 0.243 secondary_school_id Intercept 0.066 0.257 0.093 Residual 0.473 0.688 0.664 pupils_varicance_components_random_effects &lt;- REsim(pupils_crossed) %&gt;% as_tibble() pupils_varicance_components_random_effects %&gt;% mutate(sd_leveled = sd * qnorm(1 - ((1 - sd_level)/2)), sig = (median + sd_leveled) &gt;= 0 &amp; (median - sd_leveled) &lt;= 0) %&gt;% group_by(groupFctr) %&gt;% arrange(groupFctr, median) %&gt;% mutate(rank = row_number()) %&gt;% arrange(groupFctr, groupID) %&gt;% ungroup() %&gt;% ggplot(aes(x = rank)) + geom_segment(aes(xend = rank, y = median - sd_leveled, yend = median + sd_leveled, color = sig)) + geom_point(aes(y = median), size = .3, alpha = .5) + geom_hline(yintercept = 0, size = .4, linetype = 3) + facet_wrap(groupFctr ~ ., scales = &quot;free_x&quot;) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;Group&quot;, y = &quot;Effect Range&quot;, caption = &quot;interval estimates of random effects&quot;) Note that we have the usual extensions here if desired. As an example, we could also do random slopes for student level characteristics. 7.5 Hierachical structure These are setups, where different grouping factors are nested within each other (eg. cities, counties, states). load(&quot;data/nurses.RData&quot;) nurses %&gt;% head() %&gt;% knitr::kable() hospital ward wardid nurse age sex experience stress wardtype hospsize treatment 1 1 11 1 36 Male 11 7 general care large Training 1 1 11 2 45 Male 20 7 general care large Training 1 1 11 3 32 Male 7 7 general care large Training 1 1 11 4 57 Female 25 6 general care large Training 1 1 11 5 46 Female 22 6 general care large Training 1 1 11 6 60 Female 22 6 general care large Training nurses_hierach &lt;- lmer( stress ~ age + sex + experience + treatment + wardtype + hospsize + ( 1 | hospital) + ( 1 | hospital:ward), # together same as ( 1 | hospital / ward) data = nurses ) mixedup::extract_fixed_effects(nurses_hierach) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 5.380 0.185 29.128 0.000 5.018 5.742 age 0.022 0.002 10.053 0.000 0.018 0.026 sexFemale -0.453 0.035 -12.952 0.000 -0.522 -0.385 experience -0.062 0.004 -13.776 0.000 -0.070 -0.053 treatmentTraining -0.700 0.120 -5.843 0.000 -0.935 -0.465 wardtypespecial care 0.051 0.120 0.424 0.671 -0.184 0.286 hospsizemedium 0.489 0.202 2.428 0.015 0.094 0.884 hospsizelarge 0.902 0.275 3.280 0.001 0.363 1.440 mixedup::extract_vc(nurses_hierach, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop hospital:ward Intercept 0.337 0.580 0.500 hospital Intercept 0.119 0.345 0.177 Residual 0.217 0.466 0.323 nurses_varicance_components_random_effects &lt;- REsim(nurses_hierach) %&gt;% as_tibble() nurses_varicance_components_random_effects %&gt;% mutate(sd_leveled = sd * qnorm(1 - ((1 - sd_level)/2)), sig = (median + sd_leveled) &gt;= 0 &amp; (median - sd_leveled) &lt;= 0) %&gt;% group_by(groupFctr) %&gt;% arrange(groupFctr, median) %&gt;% mutate(rank = row_number()) %&gt;% arrange(groupFctr, groupID) %&gt;% ungroup() %&gt;% ggplot(aes(x = rank)) + geom_segment(aes(xend = rank, y = median - sd_leveled, yend = median + sd_leveled, color = sig)) + geom_point(aes(y = median), size = .3, alpha = .5) + geom_hline(yintercept = 0, size = .4, linetype = 3) + facet_wrap(groupFctr ~ ., scales = &quot;free_x&quot;) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + ylim(-2,2) + labs(x = &quot;Group&quot;, y = &quot;Effect Range&quot;, caption = &quot;interval estimates of random effects&quot;) 7.5.1 Crossed vs. nested nurses_hierach2 &lt;- lmer( stress ~ age + sex + experience + treatment + wardtype + hospsize + ( 1 | hospital ) + ( 1 | hospital:wardid ), # needs to be wardid now because ward is duplicated over hospitals (not unique) data = nurses ) nurses_nested &lt;- lmer( stress ~ age + sex + experience + treatment + wardtype + hospsize + ( 1 | hospital ) + ( 1 | wardid ), data = nurses ) Nested: mixedup::extract_fixed_effects(nurses_hierach2) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 5.380 0.185 29.128 0.000 5.018 5.742 age 0.022 0.002 10.053 0.000 0.018 0.026 sexFemale -0.453 0.035 -12.952 0.000 -0.522 -0.385 experience -0.062 0.004 -13.776 0.000 -0.070 -0.053 treatmentTraining -0.700 0.120 -5.843 0.000 -0.935 -0.465 wardtypespecial care 0.051 0.120 0.424 0.671 -0.184 0.286 hospsizemedium 0.489 0.202 2.428 0.015 0.094 0.884 hospsizelarge 0.902 0.275 3.280 0.001 0.363 1.440 mixedup::extract_vc(nurses_hierach2, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop hospital:wardid Intercept 0.337 0.580 0.500 hospital Intercept 0.119 0.345 0.177 Residual 0.217 0.466 0.323 Crossed: mixedup::extract_fixed_effects(nurses_nested) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 5.380 0.185 29.128 0.000 5.018 5.742 age 0.022 0.002 10.053 0.000 0.018 0.026 sexFemale -0.453 0.035 -12.952 0.000 -0.522 -0.385 experience -0.062 0.004 -13.776 0.000 -0.070 -0.053 treatmentTraining -0.700 0.120 -5.843 0.000 -0.935 -0.465 wardtypespecial care 0.051 0.120 0.424 0.671 -0.184 0.286 hospsizemedium 0.489 0.202 2.428 0.015 0.094 0.884 hospsizelarge 0.902 0.275 3.280 0.001 0.363 1.440 mixedup::extract_vc(nurses_nested, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop wardid Intercept 0.337 0.580 0.500 hospital Intercept 0.119 0.345 0.177 Residual 0.217 0.466 0.323 7.6 Residual Structure rescov &lt;- function(model, data) { var.d &lt;- crossprod(getME(model,&quot;Lambdat&quot;)) Zt &lt;- getME(model,&quot;Zt&quot;) vr &lt;- sigma(model)^2 var.b &lt;- vr*(t(Zt) %*% var.d %*% Zt) sI &lt;- vr * Diagonal(nrow(data)) var.y &lt;- var.b + sI var.y %&gt;% as.matrix() %&gt;% as_tibble() %&gt;% mutate(row = row_number()) %&gt;% pivot_longer(cols = -row, names_to = &quot;column&quot;) } rescov(gpa_mixed, gpa) %&gt;% mutate(x = as.numeric(column), y = as.numeric(row)) %&gt;% filter(between(x,0,30), between(y,0,30)) %&gt;% ggplot(aes(x = x, y = y, fill = value)) + geom_tile(aes(color = after_scale(clr_darken(fill))), size = .3, width = .9, height = .9) + scale_fill_gradientn(colours = c(clr0, clr_lighten(clr1), clr_lighten(clr2))) + scale_y_reverse() + coord_equal() covariance matrix for a cluster (compound symmetry): \\[ \\Sigma = \\left[ \\begin{array}{ccc} \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 \\\\ \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 \\\\ \\tau^2 &amp; \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 \\\\ \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2\\\\ \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 \\\\ \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} \\\\ \\end{array}\\right] \\] Types of covariance structures: in a standard linear regression model, we have constant variance and no covariance: \\[ \\Sigma = \\left[ \\begin{array}{ccc} \\sigma^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2 \\\\ \\end{array}\\right] \\] next, relax the assumption of equal variances, and estimate each separately. In this case of heterogeneous variances, we might see more or less variance over time, for example. \\[ \\Sigma = \\left[ \\begin{array}{ccc} \\sigma_1^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_3^2 \\\\ \\end{array}\\right] \\] we actually want to get at the underlying covariance/correlation. I’ll switch to the correlation representation, but you can still think of the variances as constant or separately estimated. So now we have something like this, where \\(\\rho\\) represents the residual correlation among observations. \\[ \\Sigma = \\sigma^2 \\left[ \\begin{array}{ccc} 1 &amp; \\rho_1 &amp; \\rho_2 \\\\ \\rho_1 &amp; 1 &amp; \\rho_3 \\\\ \\rho_2 &amp; \\rho_3 &amp; 1 \\\\ \\end{array}\\right] \\] \\(\\rightarrow\\) unstructured / symmetric correlation structure (compound symmetry) Autocorrelation (lag of order one for residuals): \\[ \\Sigma = \\sigma^2 \\left[ \\begin{array}{cccc} 1 &amp; \\rho &amp; \\rho^2 &amp; \\rho^3 \\\\ \\rho &amp; 1 &amp; \\rho &amp; \\rho^2 \\\\ \\rho^2 &amp; \\rho &amp; 1 &amp; \\rho \\\\ \\rho^3 &amp; \\rho^2 &amp; \\rho &amp; 1 \\\\ \\end{array}\\right] \\] 7.6.1 Heterogeneous variance library(nlme) gpa_hetero_res &lt;- lme( gpa ~ occasion, data = gpa, random = ~ 1 | student, weights = varIdent(form = ~ 1 | occasion) ) mixedup::extract_fixed_effects(gpa_hetero_res) %&gt;% knitr::kable() term value se z p_value lower_2.5 upper_97.5 Intercept 2.599 0.026 99.002 0 2.547 2.650 occasion 0.106 0.004 26.317 0 0.098 0.114 mixedup::extract_vc(gpa_hetero_res, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop student Intercept 0.094 0.306 0.404 Residual 0.138 0.372 0.596 alternative approach to heterogeneous variance models: library(glmmTMB) gpa_hetero_res2 &lt;- glmmTMB( gpa ~ occasion + ( 1 | student ) + diag( 0 + occas | student ), data = gpa ) Comparing results of {nlme} and {glmmTMB} tibble(relative_val = c(1, coef(gpa_hetero_res$modelStruct$varStruct, unconstrained = FALSE))) %&gt;% mutate(absolute_val = (relative_val * gpa_hetero_res$sigma) ^ 2, `hetero_res (nlme)` = mixedup::extract_het_var(gpa_hetero_res, scale = &#39;var&#39;, digits = 5) %&gt;% unname() %&gt;% as.vector() %&gt;% t() %&gt;% .[,1], `hetero_res (glmmTMB)` = mixedup::extract_het_var(gpa_hetero_res2, scale = &#39;var&#39;, digits = 5) %&gt;% dplyr::select(-group) %&gt;% unname() %&gt;% as.vector() %&gt;% t() %&gt;% .[,1]) %&gt;% knitr::kable() relative_val absolute_val hetero_res (nlme) hetero_res (glmmTMB) 1.0000000 0.1381504 0.13815 0.13790 0.8261186 0.0942837 0.09428 0.09415 0.6272415 0.0543528 0.05435 0.05430 0.4311126 0.0256764 0.02568 0.02568 0.3484013 0.0167692 0.01677 0.01677 0.4324628 0.0258374 0.02584 0.02580 7.6.2 Autocorrelation gpa_autocorr &lt;- lme( gpa ~ occasion, data = gpa, random = ~ 1 | student, correlation = corAR1(form = ~ occasion) ) mixedup::extract_fixed_effects(gpa_autocorr) %&gt;% knitr::kable() term value se z p_value lower_2.5 upper_97.5 Intercept 2.597 0.023 113.146 0 2.552 2.642 occasion 0.107 0.005 20.297 0 0.097 0.118 mixedup::extract_vc(gpa_autocorr, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop student Intercept 0.046 0.215 0.381 Residual 0.075 0.273 0.619 gpa_autocorr2 &lt;- glmmTMB( gpa ~ occasion + ar1( 0 + occas | student ) + ( 1 | student ), # occas is cotegorical version of occasion data = gpa ) mixedup::extract_fixed_effects(gpa_autocorr2) %&gt;% knitr::kable() term value se z p_value lower_2.5 upper_97.5 Intercept 2.598 0.023 111.077 0 2.552 2.643 occasion 0.107 0.005 19.458 0 0.096 0.118 mixedup::extract_vc(gpa_autocorr2, ci_level = 0) %&gt;% knitr::kable() group variance sd var_prop student 0.093 0.305 0.159 student.1 0.000 0.000 0.000 Residual 0.028 0.167 0.048 7.7 Generalized Linear Mixed Models load(&quot;data/speed_dating.RData&quot;) sdating &lt;- glmer( decision ~ sex + samerace + attractive_sc + sincere_sc + intelligent_sc + ( 1 | iid), data = speed_dating, family = binomial ) mixedup::extract_fixed_effects(sdating) %&gt;% knitr::kable() term value se z p_value lower_2.5 upper_97.5 Intercept -0.743 0.121 -6.130 0.00 -0.981 -0.506 sexMale 0.156 0.164 0.954 0.34 -0.165 0.478 sameraceYes 0.314 0.075 4.192 0.00 0.167 0.460 attractive_sc 0.502 0.015 33.559 0.00 0.472 0.531 sincere_sc 0.089 0.016 5.747 0.00 0.059 0.120 intelligent_sc 0.143 0.017 8.232 0.00 0.109 0.177 mixedup::extract_vc(sdating, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop iid Intercept 2.708 1.645 1 7.8 Issues/ Considderations small number of clusters: problematic - similar to number of samples to compute variance / mean or similar (to get to the variance component we need enough groups). This also touches whether something should be a fixed or a random effect (random is always possible when the number of clusters is large enough) small number of observations within clusters: no problem, but might prevent random slopes (for n == 1) balanced design / missing data: not really a requirement, so as long as it is not extreme likely not an issue 7.8.1 Model comparison Using AIC can help, but should not used to make the decision—reasoning about the implications of the used models should. gpa_1 &lt;- lmer(gpa ~ occasion + (1 + occasion | student), data = gpa) gpa_2 &lt;- lmer(gpa ~ occasion + sex + (1 + occasion | student), data = gpa) gpa_3 &lt;- lmer(gpa ~ occasion + (1 | student), data = gpa) list(gpa_1 = gpa_1, gpa_2 = gpa_2, gpa_3 = gpa_3) %&gt;% map_df(function(mod) data.frame(AIC = AIC(mod)), .id = &#39;model&#39;) %&gt;% arrange(AIC) %&gt;% mutate(`Δ AIC` = AIC - min(AIC)) %&gt;% knitr::kable() model AIC Δ AIC gpa_2 269.7853 0.000000 gpa_1 272.9566 3.171217 gpa_3 416.8929 147.107536 7.9 Formula summary formula meaning (1|group) random group intercept (x|group) = (1+x|group) random slope of x within group with correlated intercept (0+x|group) = (-1+x|group) random slope of x within group: no variation in intercept (1|group) + (0+x|group) uncorrelated random intercept and random slope within group (1|site/block) = (1|site)+(1|site:block) intercept varying among sites and among blocks within sites (nested random effects) site+(1|site:block) fixed effect of sites plus random variation in intercept among blocks within sites (x|site/block) = (x|site)+(x|site:block) = (1 + x|site)+(1+x|site:block) slope and intercept varying among sites and among blocks within sites (x1|site)+(x2|block) two different effects, varying at different levels x*site+(x|site:block) fixed effect variation of slope and intercept varying among sites and random variation of slope and intercept among blocks within sites (1|group1)+(1|group2) intercept varying among crossed random effects (e.g. site, year) equation formula \\(β_0 + β_{1}X_{i} + e_{si}\\) n/a (Not a mixed-effects model) \\((β_0 + b_{S,0s}) + β_{1}X_i + e_{si}\\) ∼ X + (1∣Subject) \\((β_0 + b_{S,0s}) + (β_{1} + b_{S,1s}) X_i + e_{si}\\) ~ X + (1 + X∣Subject) \\((β_0 + b_{S,0s} + b_{I,0i}) + (β_{1} + b_{S,1s}) X_i + e_{si}\\) ∼ X + (1 + X∣Subject) + (1∣Item) As above, but \\(S_{0s}\\), \\(S_{1s}\\) independent ∼ X + (1∣Subject) + (0 + X∣ Subject) + (1∣Item) \\((β_0 + b_{S,0s} + b_{I,0i}) + β_{1}X_i + e_{si}\\) ∼ X + (1∣Subject) + (1∣Item) \\((β_0 + b_{I,0i}) + (β_{1} + b_{S,1s})X_i + e_{si}\\) ∼ X + (0 + X∣Subject) + (1∣Item) × "],
["references-and-session-info.html", "8 References and Session Info 8.1 Session Info 8.2 References", " 8 References and Session Info 8.1 Session Info sessionInfo() #&gt; R version 4.0.3 (2020-10-10) #&gt; Platform: x86_64-pc-linux-gnu (64-bit) #&gt; Running under: Ubuntu 20.04.3 LTS #&gt; #&gt; Matrix products: default #&gt; BLAS: /usr/local/lib/R/lib/libRblas.so #&gt; LAPACK: /usr/local/lib/R/lib/libRlapack.so #&gt; #&gt; locale: #&gt; [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C #&gt; [3] LC_TIME=de_DE.UTF-8 LC_COLLATE=en_US.UTF-8 #&gt; [5] LC_MONETARY=de_DE.UTF-8 LC_MESSAGES=en_US.UTF-8 #&gt; [7] LC_PAPER=de_DE.UTF-8 LC_NAME=C #&gt; [9] LC_ADDRESS=C LC_TELEPHONE=C #&gt; [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C #&gt; #&gt; attached base packages: #&gt; [1] stats graphics grDevices utils datasets methods base #&gt; #&gt; other attached packages: #&gt; [1] EnvStats_2.4.0 ggraph_2.0.5.9000 tidygraph_1.2.0 #&gt; [4] glue_1.4.2 patchwork_1.1.0.9000 prismatic_1.0.0.9000 #&gt; [7] forcats_0.5.1 stringr_1.4.0 dplyr_1.0.6 #&gt; [10] purrr_0.3.4 readr_1.4.0 tidyr_1.1.3 #&gt; [13] tibble_3.1.2 ggplot2_3.3.4.9000 tidyverse_1.3.0.9000 #&gt; #&gt; loaded via a namespace (and not attached): #&gt; [1] ggrepel_0.9.1 Rcpp_1.0.6 lubridate_1.7.10 assertthat_0.2.1 #&gt; [5] digest_0.6.27 utf8_1.2.1 ggforce_0.3.2.9000 R6_2.5.0 #&gt; [9] cellranger_1.1.0 backports_1.2.1 reprex_2.0.0 evaluate_0.14 #&gt; [13] httr_1.4.2 pillar_1.6.1 rlang_0.4.12 readxl_1.3.1 #&gt; [17] rstudioapi_0.13 jquerylib_0.1.3 rmarkdown_2.9.6 polyclip_1.10-0 #&gt; [21] igraph_1.2.6 munsell_0.5.0 broom_0.7.6 compiler_4.0.3 #&gt; [25] modelr_0.1.8 xfun_0.24 pkgconfig_2.0.3 htmltools_0.5.1.1 #&gt; [29] tidyselect_1.1.1 gridExtra_2.3 bookdown_0.19 graphlayouts_0.7.1 #&gt; [33] viridisLite_0.4.0 fansi_0.5.0 crayon_1.4.1 dbplyr_2.1.1 #&gt; [37] withr_2.4.2 MASS_7.3-53 grid_4.0.3 jsonlite_1.7.2 #&gt; [41] gtable_0.3.0 lifecycle_1.0.0 DBI_1.1.1 magrittr_2.0.1 #&gt; [45] scales_1.1.1 cli_2.5.0 stringi_1.7.3 farver_2.1.0 #&gt; [49] viridis_0.5.1 fs_1.5.0 xml2_1.3.2 bslib_0.2.4 #&gt; [53] ellipsis_0.3.2 generics_0.1.0 vctrs_0.3.8 tools_4.0.3 #&gt; [57] tweenr_1.0.2 hms_1.1.0 yaml_2.2.1.99 colorspace_2.0-2 #&gt; [61] rvest_1.0.0 knitr_1.33 haven_2.3.1 sass_0.4.0.9000 8.2 References "]
]
