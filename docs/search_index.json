[
["index.html", "Statistical Exploration a small notebook to keep track 1 Intro", " Statistical Exploration a small notebook to keep track Kosmas Hench 2021-12-03 1 Intro × "],
["rethinking-chapter-1.html", "2 Rethinking: Chapter 1", " 2 Rethinking: Chapter 1 The Golem of Prague by Richard McElreath, building on the Summary by Solomon Kurz × "],
["rethinking-chapter-2.html", "3 Rethinking: Chapter 2 3.1 Counting possibilities 3.2 Building a Model 3.3 Making the model go / Bayes’ Theorem 3.4 Motors: Grid Approximation 3.5 Quadratic Approximation 3.6 Marcov Chain Monte Carlo (MCMC) 3.7 Homework 3.8 {brms} section 3.9 pymc3 section", " 3 Rethinking: Chapter 2 Small Worlds and Large Worlds by Richard McElreath, building on the Summary by Solomon Kurz 3.1 Counting possibilities d &lt;- tibble(p1 = 0, p2 = rep(1:0, times = c(1, 3)), p3 = rep(1:0, times = c(2, 2)), p4 = rep(1:0, times = c(3, 1)), p5 = 1) p1 p2 p3 p4 p5 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 d %&gt;% mutate(turn = 1:4)%&gt;% pivot_longer(p1:p5, names_to = &quot;prob&quot;, values_to = &quot;realization&quot;) %&gt;% arrange(prob, turn) %&gt;% mutate(prob = factor(prob, levels = c(&quot;p5&quot;, &quot;p4&quot;, &quot;p3&quot;, &quot;p2&quot;, &quot;p1&quot;)), marble = c(&quot;white&quot;, &quot;dark&quot;)[realization + 1]) %&gt;% ggplot( aes( x = turn, y = prob ) ) + geom_point(shape = 21, size = 4, aes( fill = marble, color = after_scale(clr_darken(fill)))) + geom_text(data = tibble(x = rep(c(.65, 4.45), each = 5), y = rep(str_c(&quot;p&quot;,1:5), 2), label = rep(c(&quot;[&quot;, &quot;]&quot;), each = 5), vjust = .7), aes( x = x, y = y, label = label), family = fnt_sel, size = 6)+ scale_fill_manual(values = c(white = clr0, dark = clrd)) + theme(legend.position = &quot;bottom&quot;) tibble(draws = 1:3, marbles = 4) %&gt;% mutate(possibilities = marbles ^ draws) %&gt;% flextable::flextable() .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c995cd76{}.cl-c990b700{font-family:'DejaVu Sans';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c990ca88{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c990f77e{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c990f79c{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c990f7a6{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} drawsmarblespossibilities14424163464 layout_round &lt;- function(round = 1, n = 4, angle = 360, start_angle = 0, p = .5, round_prefix = &quot;&quot;){ n_round &lt;- n ^ round tibble(idx_round = 1:n_round, idx_round_sacaled = scales::rescale(idx_round, from = c(.5, n_round+.5), to = c(0, 1) * angle/360 + start_angle/360), idx_draw = rep(1:n, n_round/n), idx_parent = ((idx_round - 1 ) %/% n) + 1, name_parent = str_c(round_prefix, round - 1, &quot;_&quot;, idx_parent), name = str_c(round_prefix, round, &quot;_&quot;, idx_round), x = sin(idx_round_sacaled * 2 * pi) * round, y = cos(idx_round_sacaled * 2 * pi) * round) %&gt;% mutate(marble = c(&quot;white&quot;, &quot;dark&quot;)[1 + ((idx_draw/n) &lt;= p)], round_prefix = round_prefix, round = round) } links_round &lt;- function(round = 1, n = 4, round_prefix = &quot;&quot;){ n_round &lt;- n ^ round n_prev &lt;- n ^ (round - 1) tibble(idx_round = 1:n_round, idx_parent = ((idx_round - 1 ) %/% n) + 1, from = str_c(round_prefix, round - 1, &quot;_&quot;, idx_parent), to = str_c(round_prefix,round, &quot;_&quot;, idx_round), round_prefix = round_prefix) } round_origin &lt;- origin_round &lt;- function(round_prefix = &quot;&quot;){ tibble(idx_round = 0, idx_round_sacaled = 0, idx_draw = 0, name = str_c(round_prefix, &quot;0_1&quot;), x = 0, y = 0, marble = NA) } marble_graph &lt;- function(n_rounds = 3, n_draws = 4, angle = 360,start_angle = 0, p = .5, round_prefix = &quot;&quot;){ tbl_graph(nodes = 1:n_rounds %&gt;% map_dfr(layout_round, n = n_draws, angle = angle, start_angle = start_angle, p = p, round_prefix = round_prefix) %&gt;% bind_rows(round_origin(round_prefix = round_prefix), .), edges = 1:n_rounds %&gt;% map_dfr(links_round, round_prefix = round_prefix)) %E&gt;% mutate(marble = .N()$marble[to], to_name = .N()$name[to], from_name = .N()$name[from]) } marble_graph(p = .25, n_rounds = 3, angle = 180, start_angle = -90) %&gt;% ggraph( layout = &quot;manual&quot;, x = . %N&gt;% as_tibble() %&gt;% .$x, y = . %N&gt;% as_tibble() %&gt;% .$y) + geom_node_point(aes(fill = marble, color = marble, size = -round), shape = 21) + geom_edge_link(aes(color = marble), start_cap = circle(2, &#39;mm&#39;), end_cap = circle(2, &#39;mm&#39;)) + coord_equal() + scale_color_manual(values = clr_darken(c(white = clr0, dark = clrd)), na.value = &quot;transparent&quot;, guide = &quot;none&quot;) + scale_fill_manual(values = c(white = clr0, dark = clrd), na.value = &quot;transparent&quot;) + scale_edge_color_manual(values = c(white = clr_darken(clr0, .2), dark = clrd), guide = &quot;none&quot;) + scale_size_continuous(range = c(1.5,3), guide = &quot;none&quot;) + guides(fill = guide_legend(override.aes = list(size = 4)), edge_alpha = &quot;none&quot;) + labs(title = &quot;p = 0.25&quot;) + theme(legend.position = &quot;bottom&quot;) n_deviders &lt;- 3 n_rounds &lt;- 3 dividers &lt;- tibble(x = rep(0,n_deviders), y = x, tau = seq(from = 0, to = 2*pi, length.out = n_deviders + 1)[2:(n_deviders+1)], xend = sin(tau) * (n_rounds + .1), yend = cos(tau) * (n_rounds + .1)) p_trials &lt;- c(.25, .5, .75) all_conjectures &lt;- tibble(start_angle = c(0, 120, 240), round_prefix = c(&quot;r1_&quot; ,&quot;r2_&quot;, &quot;r3_&quot;), p = p_trials) %&gt;% pmap(marble_graph, angle = 120) %&gt;% reduce(bind_graphs) na_to_false &lt;- function(x){x[is.na(x)] &lt;- FALSE; x} na_to_true &lt;- function(x){x[is.na(x)] &lt;- TRUE; x} tester &lt;- function(x){x$name[x$r1_right]} trial_sequence &lt;- c(&quot;white&quot;, &quot;dark&quot;)[c(2,1,2)] selectors &lt;- all_conjectures %N&gt;% mutate(r1_right = (round == 1 &amp; marble == trial_sequence[1]) %&gt;% na_to_true(), r2_still_in = name_parent %in% name[r1_right], r2_right = r2_still_in &amp; (round == 2 &amp; marble == trial_sequence[2]), r3_still_in = name_parent %in% name[r2_right], r3_right = r3_still_in &amp; (round == 3 &amp; marble == trial_sequence[3]), on_path = r1_right | r2_right |r3_right) %&gt;% as_tibble() %&gt;% filter(on_path) selector_results &lt;- selectors %&gt;% filter(round == n_rounds) %&gt;% group_by(round_prefix) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(tau = seq(from = 0, to = 2*pi, length.out = n_rounds + 1)[2:(n_rounds+1)] - (2*pi)/(n_rounds * 2), x = sin(tau) * (n_rounds + .5), y = cos(tau) * (n_rounds + .5)) all_conjectures %&gt;% ggraph( layout = &quot;manual&quot;, x = . %N&gt;% as_tibble() %&gt;% .$x, y = . %N&gt;% as_tibble() %&gt;% .$y) + geom_node_point(aes(fill = marble, color = marble, size = -round, alpha = name %in% selectors$name), shape = 21) + geom_edge_link(aes(color = marble, alpha = to_name %in% selectors$name), start_cap = circle(2, &#39;mm&#39;), end_cap = circle(2, &#39;mm&#39;)) + geom_segment(data = dividers, aes(x = x, y = y, xend = xend, yend = yend), color = clr_darken(&quot;white&quot;,.10)) + geom_text(data = selector_results, aes( x = x, y = y, label = n), family = fnt_sel, size = 6) + coord_equal() + scale_color_manual(values = clr_darken(c(white = clr0, dark = clrd)), na.value = &quot;transparent&quot;, guide = &quot;none&quot;) + scale_fill_manual(values = c(white = clr0, dark = clrd), na.value = &quot;transparent&quot;) + scale_edge_color_manual(values = c(white = clr_darken(clr0, .2), dark = clrd), guide = &quot;none&quot;) + scale_size_continuous(range = c(1.5,3), guide = &quot;none&quot;) + scale_alpha_manual(values = c(`TRUE` = 1, `FALSE` = .2), guide = &quot;none&quot;) + guides(fill = guide_legend(override.aes = list(size = 4)), edge_alpha = &quot;none&quot;) + labs(caption = str_c(trial_sequence,collapse = &quot;-&quot;)) + theme(legend.position = &quot;bottom&quot;) html_marbles &lt;- c( glue(&quot;&lt;span style=&#39;color:{clr0};filter:drop-shadow(0px 0px 1px black)&#39;&gt;\\U2B24&lt;/span&gt;&quot;), glue(&quot;&lt;span style=&#39;color:{clr1l};filter:drop-shadow(0px 0px 1px black)&#39;&gt;\\U2B24&lt;/span&gt;&quot;)) html_conjecture &lt;- function(x){ str_c(&quot;[ &quot;,str_c(html_marbles[c(x)+1], collapse = &quot; &quot;),&quot; ]&quot;) } tibble(conjectures = list(rep(0,4), rep(1:0, c(1,3)), rep(1:0, c(2,2)), rep(1:0, c(3,1)), rep(1,4)), conjecture = map_chr(conjectures, html_conjecture), ways = map_dbl(conjectures, sum), p = c(0, p_trials, 1), `ways data/prior counts` = c(0, selector_results$n, 0), `new count` = map2_chr( `ways data/prior counts`, ways, .f = function(x,y){glue(&quot;{x} $\\\\times$ {y} = {x * y}&quot;)}), plausibility = (`ways data/prior counts` / sum(`ways data/prior counts`)) %&gt;% round(digits = 2) ) %&gt;% rename(`ways to produce &lt;span style=&#39;color:#85769EFF;filter:drop-shadow(0px 0px 1px black)&#39;&gt;⬤&lt;/span&gt;` = &quot;ways&quot;) %&gt;% dplyr::select(-conjectures) %&gt;% knitr::kable() conjecture ways to produce ⬤ p ways data/prior counts new count plausibility [ ⬤ ⬤ ⬤ ⬤ ] 0 0.00 0 0 \\(\\times\\) 0 = 0 0.00 [ ⬤ ⬤ ⬤ ⬤ ] 1 0.25 3 3 \\(\\times\\) 1 = 3 0.15 [ ⬤ ⬤ ⬤ ⬤ ] 2 0.50 8 8 \\(\\times\\) 2 = 16 0.40 [ ⬤ ⬤ ⬤ ⬤ ] 3 0.75 9 9 \\(\\times\\) 3 = 27 0.45 [ ⬤ ⬤ ⬤ ⬤ ] 4 1.00 0 0 \\(\\times\\) 4 = 0 0.00 3.2 Building a Model d &lt;- tibble(toss = c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;), n_trials = 1:9, sequence = n_trials %&gt;% map_chr(.f = function(x, chr){str_c(chr[1:x], collapse = &quot;&quot;)}, chr = toss), n_success = cumsum(toss == &quot;w&quot;), lag_n_trials = lag(n_trials, default = 0), lag_n_success = lag(n_success, default = 0)) sequence_length &lt;- 50 d %&gt;% expand(nesting(n_trials, toss, n_success), p_water = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% group_by(p_water) %&gt;% # you can learn more about lagging here: https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/lag or here: https://dplyr.tidyverse.org/reference/lead-lag.html mutate(lagged_n_trials = lag(n_trials, k = 1), lagged_n_success = lag(n_success, k = 1)) #&gt; # A tibble: 450 x 6 #&gt; # Groups: p_water [50] #&gt; n_trials toss n_success p_water lagged_n_trials lagged_n_success #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 w 1 0 NA NA #&gt; 2 1 w 1 0.0204 NA NA #&gt; 3 1 w 1 0.0408 NA NA #&gt; 4 1 w 1 0.0612 NA NA #&gt; 5 1 w 1 0.0816 NA NA #&gt; 6 1 w 1 0.102 NA NA #&gt; 7 1 w 1 0.122 NA NA #&gt; 8 1 w 1 0.143 NA NA #&gt; 9 1 w 1 0.163 NA NA #&gt; 10 1 w 1 0.184 NA NA #&gt; # … with 440 more rows stat_binom &lt;- function(n_trials, n_success, lag_n_trials, lag_n_success, sequence, ...){ if(n_trials == 1) { g_lag &lt;- stat_function(data = tibble(n_trials = n_trials, sequence = sequence), fun = function(x){1}, xlim = c(0,1), linetype = 3) } else { g_lag &lt;- stat_function(data = tibble(n_trials = n_trials, sequence = sequence), fun = function(x){ f &lt;- function(x){ dbinom(x = lag_n_success, size = lag_n_trials, prob = x)} f(x)/ integrate(f = f,lower = 0, upper = 1)[[1]]}, xlim = c(0,1), n = 500, linetype = 3) } g_current &lt;- stat_function(data = tibble(n_trials = n_trials, sequence = sequence), fun = function(x){ f &lt;- function(x){ dbinom(x = n_success, size = n_trials, prob = x)} f(x)/ integrate(f = f,lower = 0,upper = 1)[[1]]}, xlim = c(0,1),n = 500) list( g_lag, g_current) } ggplot() + (d %&gt;% pmap(stat_binom) %&gt;% unlist()) + facet_wrap(str_c(n_trials,&quot;: &quot;, sequence) ~ .) 3.3 Making the model go / Bayes’ Theorem \\[ Pr(\\textit{p} | W, L) = \\frac{Pr(W, L | \\textit{p}) ~ Pr(\\textit{p})}{Pr(W,L)}\\\\ Posterior = \\frac{Probability~of~the~Data \\times Prior}{ Average~probability~of~the~Data} \\] f_posterior_unscaled &lt;- function(f_porior, f_like){ function(x){ f_porior(x) * f_like(x)} } f_parts &lt;- c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;) gg_posterior &lt;- function(f_porior, f_like, comp = 1){ list( stat_function(data = tibble(part = factor(&quot;prior&quot;, levels = f_parts), comp = comp), fun = f_porior, xlim = c(0,1), n = 500, geom = &quot;area&quot;, color = clr2, fill = fll2), stat_function(data = tibble(part = factor(&quot;likelihood&quot;, levels = f_parts), comp = comp), fun = f_like, xlim = c(0,1), n = 500, geom = &quot;area&quot;, color = clr2, fill = fll2), stat_function(data = tibble(part = factor(&quot;posterior&quot;, levels = f_parts), comp = comp), fun = f_posterior_unscaled(f_porior = f_porior, f_like = f_like), xlim = c(0,1), n = 500, geom = &quot;area&quot;, color = clr2, fill = fll2) ) } scale_fun &lt;- function(f){ # marginal likelihood function(x){f(x) / integrate(f = f, lower = 0, upper = 1)[[1]]} } f_like_in &lt;- function(x){dbeta(x = x, shape1 = 8, shape2 = 5)} f_uni &lt;- function(x){1} f_step &lt;- function(x){if_else(x &lt; .5, 0, 1)} f_peak &lt;- function(x){if_else(x &lt; .5, (x * 2)^3, ((1 - x) * 2)^3)} ggplot() + gg_posterior(f_porior = f_uni, f_like = f_like_in) + gg_posterior(f_porior = f_step, f_like = f_like_in, comp = 2) + gg_posterior(f_porior = f_peak, f_like = f_like_in, comp = 3) + facet_wrap(comp ~ part, scales = &quot;free_y&quot;) 3.4 Motors: Grid Approximation grid_approx &lt;- function(n_grid = 20, W = 6, L = 3, prior = function(x){rep(1, length(x))}){ tibble(p_grid = seq(0, 1, length.out = n_grid), prior = prior(p_grid), likelihood = dbinom(W, size = W + L, prob = p_grid), posterior_unstand = likelihood * prior, posterior = posterior_unstand / sum(posterior_unstand)) } plot_grid_approx &lt;- function(data){ data %&gt;% ggplot(aes(x = p_grid, y = posterior, color = posterior)) + ggforce::geom_link2() + geom_point(shape = 21, fill = &quot;white&quot;, size = 2.5) + scale_color_gradientn(colours = c(clr_darken(clr0), clr2), guide = &quot;none&quot;) + labs(title = glue(&quot;{length(data$p_grid)} grid points&quot;)) } c(4, 7, 15, 60) %&gt;% map(grid_approx) %&gt;% map(plot_grid_approx) %&gt;% wrap_plots(nrow = 1) &amp; scale_x_continuous(breaks = c(0, .5, 1)) Note how the y scale depends on the number of grid points: the peak reaches ~0.75 for 4 points, but only ~ 0.043 for 60 points. 3.5 Quadratic Approximation library(rethinking) map &lt;- purrr::map conpare_qa &lt;- function(w_in, l_in){ globe_qa &lt;- quap( alist( W ~ dbinom( W + L, p ), # binomial likelihood p ~ dunif( 0, 1 ) # uniform prior ), data = list( W = w_in, L = l_in ) ) qa_results &lt;- precis(globe_qa) %&gt;% as_tibble() %&gt;% mutate(qa = glue(&quot;W: {w_in}, L: {l_in}&quot;)) qa_results %&gt;% knitr::kable() %&gt;% print() ggplot() + stat_function(fun = function(x){ dbeta( shape1 = w_in + 1, shape2 = l_in + 1, x = x ) / integrate(f = function(x){ dbeta( shape1 = w_in + 1, shape2 = l_in + 1, x = x )}, lower = 0, upper = 1)[[1]] }, xlim = c(0,1), n = 500, geom = &quot;area&quot;, color = clr0d, fill = fll0)+ stat_function(fun = function(x){ dnorm(x = x, mean = qa_results$mean, sd = qa_results$sd )}, xlim = c(0,1), n = 500, geom = &quot;line&quot;, color = clr2, linetype = 3) + labs(title = glue(&quot;W: {w_in}, L: {l_in}, n = {w_in + l_in}&quot;), y = &quot;density&quot;, x = &quot;proportion water&quot;) } conpare_qa(w_in = 6, l_in = 3) + conpare_qa(w_in = 12, l_in = 6) + conpare_qa(w_in = 24, l_in = 12) #&gt; #&gt; #&gt; | mean| sd| 5.5%| 94.5%|qa | #&gt; |---------:|---------:|---------:|---------:|:----------| #&gt; | 0.6666668| 0.1571337| 0.4155367| 0.9177969|W: 6, L: 3 | #&gt; #&gt; #&gt; | mean| sd| 5.5%| 94.5%|qa | #&gt; |---------:|---------:|---------:|---------:|:-----------| #&gt; | 0.6666662| 0.1111104| 0.4890902| 0.8442421|W: 12, L: 6 | #&gt; #&gt; #&gt; | mean| sd| 5.5%| 94.5%|qa | #&gt; |---------:|---------:|---------:|---------:|:------------| #&gt; | 0.6666663| 0.0785669| 0.5411011| 0.7922314|W: 24, L: 12 | 3.6 Marcov Chain Monte Carlo (MCMC) n_samples &lt;- 1e4 p_init &lt;- rep( NA, n_samples ) p_init[1] &lt;- .5 manual_mcmc &lt;- function(p, W = 6, L = 3){ for ( i in 2:n_samples ) { p_new &lt;- rnorm( n = 1, mean = p[ i - 1], sd = 0.1) if ( p_new &lt; 0 ){ p_new &lt;- abs( p_new ) } if ( p_new &gt; 1 ){ p_new &lt;- 2 - p_new } q0 &lt;- dbinom( W, W + L, p[ i - 1 ] ) q1 &lt;- dbinom( W, W + L, p_new ) p[i] &lt;- if_else( runif(1) &lt; q1 / q0, p_new, p[i - 1] ) } p } p &lt;- manual_mcmc(p_init) p_36 &lt;- manual_mcmc(p_init, W = 24, L = 12) p_p &lt;- tibble(x = p) %&gt;% ggplot() + stat_function(fun = function(x){ dbeta( shape1 = 6 + 1, shape2 = 3 + 1, x = x ) / integrate(f = function(x){ dbeta( shape1 = 6 + 1, shape2 = 3 + 1, x = x )}, lower = 0, upper = 1)[[1]] }, xlim = c(0,1), n = 500, geom = &quot;area&quot;, aes(color = &quot;posterior&quot;, fill = after_scale(clr_alpha(color))))+ geom_density(aes(x = x, color = &quot;MCMC&quot;)) + scale_color_manual(&quot;approach&quot;, values = c(posterior = clr0, MCMC = clr2)) + labs(y = &quot;density&quot;, x = &quot;proportion water&quot;, title = &quot;n = 12&quot;) + theme(legend.position = &quot;bottom&quot;) p_p36 &lt;- tibble(x = p_36) %&gt;% ggplot() + stat_function(fun = function(x){ dbeta( shape1 = 24 + 1, shape2 = 12 + 1, x = x ) / integrate(f = function(x){ dbeta( shape1 = 24 + 1, shape2 = 12 + 1, x = x )}, lower = 0, upper = 1)[[1]] }, xlim = c(0,1), n = 500, geom = &quot;area&quot;, aes(color = &quot;posterior&quot;, fill = after_scale(clr_alpha(color))))+ geom_density(aes(x = x, color = &quot;MCMC&quot;)) + scale_color_manual(&quot;approach&quot;, values = c(posterior = clr0, MCMC = clr2)) + labs(y = &quot;density&quot;, x = &quot;proportion water&quot;, title = &quot;n = 36&quot;) + theme(legend.position = &quot;bottom&quot;) p_p + p_p36 3.7 Homework M1 plot_grid_approx &lt;- function(data){ data %&gt;% ggplot(aes(x = p_grid, y = posterior, color = posterior)) + ggforce::geom_link2() + geom_point(shape = 21, fill = &quot;white&quot;, size = 2.5) + scale_color_gradientn(colours = c(clr_darken(clr0), clr2), guide = &quot;none&quot;) + labs(title = glue(&quot;{length(data$p_grid)} grid points&quot;)) } tibble(n_grid = rep(c(4,7,20), 3), L = rep(c(0,1,2), each = 3), W = rep(c(3,3,5), each = 3)) %&gt;% pmap(grid_approx) %&gt;% map(plot_grid_approx) %&gt;% wrap_plots(nrow = 3) &amp; scale_x_continuous(breaks = c(0, .5, 1)) M2 tibble(n_grid = rep(c(4,7,20), 3), L = rep(c(0,1,2), each = 3), W = rep(c(3,3,5), each = 3)) %&gt;% pmap(grid_approx, prior = f_step ) %&gt;% map(plot_grid_approx) %&gt;% wrap_plots(nrow = 3) &amp; scale_x_continuous(breaks = c(0, .5, 1)) M3 \\[ Pr(Earth | Land) = \\frac{Pr(Land | Earth) \\times Pr(Earth)}{Pr(Land)} \\] p_l_on_earth &lt;- .3 p_l_on_mars &lt;- 1 p_earth &lt;- .5 average_p_l &lt;- .5 * (p_l_on_earth + p_l_on_mars) (p_earth_on_l &lt;- p_l_on_earth * p_earth / average_p_l) #&gt; [1] 0.2307692 M4 &amp; M5 cards &lt;- c(&quot;C1b1|C1b2&quot;, &quot;C2b1|C2w2&quot;, &quot;C3w1|C3w2&quot; ) conjectures &lt;- c(&quot;C1b1|C1b2&quot;, &quot;C1b2|C1b1&quot;, &quot;C2b1|C2w2&quot;, &quot;C2w2|C2b1&quot;, &quot;C3w1|C3w2&quot;, &quot;C3w2|C3w1&quot;) tibble(cards = c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;), ways = c(&quot;C1b1|C1b2 &amp; C1b2|C1b1&quot;, &quot;C2b1|C2w2&quot;, &quot;&quot;), ways_tor_produce_data = c(2, 1, 0), plausibility = (ways_tor_produce_data / sum(ways_tor_produce_data)) %&gt;% round(digits = 3), other_side = c(&quot;b&quot;, &quot;w&quot;, &quot;&quot;), prior = c(2,1,1), posterior = ((plausibility * prior) / sum(plausibility * prior)) %&gt;% round(digits = 3))%&gt;% knitr::kable() cards ways ways_tor_produce_data plausibility other_side prior posterior C1 C1b1|C1b2 &amp; C1b2|C1b1 2 0.667 b 2 0.8 C2 C2b1|C2w2 1 0.333 w 1 0.2 C3 0 0.000 1 0.0 M6 tibble(cards = c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;), ways = c(&quot;C1b1|C1b2 &amp; C1b2|C1b1&quot;, &quot;C2b1|C2w2&quot;, &quot;&quot;), ways_tor_produce_data = c(2, 1, 0), plausibility = (ways_tor_produce_data / sum(ways_tor_produce_data)) %&gt;% round(digits = 3), other_side = c(&quot;b&quot;, &quot;w&quot;, &quot;&quot;), prior = c(1,2,3), posterior = ((plausibility * prior) / sum(plausibility * prior)) %&gt;% round(digits = 3))%&gt;% knitr::kable() cards ways ways_tor_produce_data plausibility other_side prior posterior C1 C1b1|C1b2 &amp; C1b2|C1b1 2 0.667 b 1 0.5 C2 C2b1|C2w2 1 0.333 w 2 0.5 C3 0 0.000 3 0.0 M7 tibble(cards = c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;), ways = c(&quot;C1b1&gt;C2w2 &amp; C1b1&gt;C3w1 &amp; C1b1&gt;C3w2 &amp; C2b1&gt;C2w2 &amp; C2b1&gt;C3w1 &amp; C2b1&gt;C3w2&quot;, &quot;C2b1&gt;C3w1 &amp; C2b1&gt;C3w2&quot;, &quot;&quot;), ways_tor_produce_data = c(6, 2, 0), plausibility = (ways_tor_produce_data / sum(ways_tor_produce_data)) %&gt;% round(digits = 3))%&gt;% knitr::kable() cards ways ways_tor_produce_data plausibility C1 C1b1&gt;C2w2 &amp; C1b1&gt;C3w1 &amp; C1b1&gt;C3w2 &amp; C2b1&gt;C2w2 &amp; C2b1&gt;C3w1 &amp; C2b1&gt;C3w2 6 0.75 C2 C2b1&gt;C3w1 &amp; C2b1&gt;C3w2 2 0.25 C3 0 0.00 H1 \\[ Pr(twin | spec_a) = 0.2 \\\\ Pr(twin | spec_b) = 0.1 \\\\ Pr(twin) = 0.15 \\] \\[ Pr(spec_a | twin) = \\frac{Pr(spec_a) \\times Pr(twin | spec_a)}{Pr(twin)} \\\\ Pr(spec_b | twin) = \\frac{Pr(spec_b) \\times Pr(twin | spec_b)}{Pr(twin)} \\] pr_twn_on_a &lt;- .1 pr_twn_on_b &lt;- .2 pr_twn &lt;- (pr_twn_on_a + pr_twn_on_b) /2 prior_a &lt;- .5 pr_a_on_twn &lt;- (prior_a * pr_twn_on_a) / pr_twn pr_b_on_twn &lt;- ((1 - prior_a) * pr_twn_on_b) / pr_twn (p_next_twn &lt;- pr_a_on_twn * pr_twn_on_a + pr_b_on_twn * pr_twn_on_b) %&gt;% round(digits = 3) #&gt; [1] 0.167 H2 \\[ Pr(spec_a | twin) = \\frac{1}{3} \\] pr_a_on_twn #&gt; [1] 0.3333333 H3 \\[ Pr(single | spec_a) = Pr(\\neg twin | spec_a) = 1 - Pr(twin | spec_a) \\] \\[ Pr( spec_a | single) = \\frac{Pr(single|spec_a)Pr(spec_a)}{Pr(single)} \\] pr_sgl_on_a &lt;- 1 - pr_twn_on_a pr_sgl_on_b &lt;- 1 - pr_twn_on_b pr_sgl &lt;- weighted.mean(x = c(pr_sgl_on_a, pr_sgl_on_b), w = c(pr_a_on_twn, 1- pr_a_on_twn)) prior_a &lt;- pr_a_on_twn pr_a_on_sgl &lt;- (prior_a * pr_sgl_on_a) / pr_sgl pr_b_on_sgl &lt;- ((1 - prior_a) * pr_sgl_on_b) / pr_sgl tibble(pr_a_on_sgl = pr_a_on_sgl, pr_b_on_sgl = pr_b_on_sgl, control = pr_a_on_sgl + pr_b_on_sgl) %&gt;% round(digits = 4) %&gt;% knitr::kable() pr_a_on_sgl pr_b_on_sgl control 0.36 0.64 1 H4 \\[ Pr(spec_a | test ) = 0.8 \\\\ Pr(spec_b | test ) = 0.65 \\\\ Pr(spec_a | test ) = \\frac{Pr( test | spec_a ) \\times Pr(spec_a)}{Pr(test_positive)} \\] pr_testa_on_a &lt;- .8 pr_testb_on_b &lt;- .65 pr_testa_on_b &lt;- 1 - pr_testb_on_b prior_a &lt;- .5 pr_testa &lt;- (prior_a * pr_testa_on_a) + ((1- prior_a) * pr_testa_on_b) tibble(pr_on_testa = c(&quot;A&quot;,&quot;B&quot;), genetic_test = c((prior_a * pr_testa_on_a) / pr_testa, ((1 - prior_a) * pr_testa_on_b) / pr_testa)) #&gt; # A tibble: 2 x 2 #&gt; pr_on_testa genetic_test #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 A 0.696 #&gt; 2 B 0.304 prior_a_updated &lt;- pr_a_on_sgl pr_testa_updated &lt;- (prior_a_updated * pr_testa_on_a) + ((1- prior_a_updated) * pr_testa_on_b) tibble(pr_on_testa = c(&quot;A&quot;,&quot;B&quot;), genetic_test = c((prior_a_updated * pr_testa_on_a) / pr_testa_updated, ((1 - prior_a_updated) * pr_testa_on_b) / pr_testa_updated)) #&gt; # A tibble: 2 x 2 #&gt; pr_on_testa genetic_test #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 A 0.562 #&gt; 2 B 0.438 3.8 {brms} section brms_c2_36_tosses &lt;- brm( data = list(w = 24), family = binomial(link = &quot;identity&quot;), w | trials(36) ~ 0 + Intercept, prior(beta(1, 1), class = b, lb = 0, ub = 1), seed = 42, file = &quot;brms/brms_c2_36_tosses&quot; ) brms_c2_36_tosses %&gt;% summary() #&gt; Family: binomial #&gt; Links: mu = identity #&gt; Formula: w | trials(36) ~ 0 + Intercept #&gt; Data: list(w = 24) (Number of observations: 1) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; Intercept 0.66 0.08 0.49 0.80 1.00 1123 1813 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). posterior_summary(brms_c2_36_tosses) %&gt;% round(digits = 3) %&gt;% knitr::kable() Estimate Est.Error Q2.5 Q97.5 b_Intercept 0.657 0.078 0.492 0.798 lp__ -3.994 0.744 -6.055 -3.465 as_draws_df(brms_c2_36_tosses) %&gt;% as_tibble() %&gt;% ggplot(aes(x = b_Intercept)) + geom_density(color = clr1, fill = fll1) + scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) + labs(y = &quot;density&quot;, x = &quot;proportion water&quot;, title = &quot;{brms} posterior probability (n = 36&quot;) 3.9 pymc3 section Taken from from pymc-devs… import arviz as az import matplotlib.pyplot as plt import numpy as np import pymc3 as pm import scipy.stats as stats import seaborn as sns import matplotlib from matplotlib.colors import ListedColormap import matplotlib.font_manager matplotlib.font_manager._rebuild() ways = np.array([0, 3, 8, 9, 0]) ways / ways.sum() #&gt; array([0. , 0.15, 0.4 , 0.45, 0. ]) \\[ Pr(w | n, p) = \\frac{n!}{w! (n - w)!}p^{w}(1 - p)^{n-w} \\] stats.binom.pmf(6, n = 9, p = 0.5) #&gt; 0.16406250000000003 Computing the posterior using a grid approximation inside a python function. def posterior_grid_approx(grid_points = 5, success = 6, tosses = 9): &quot;&quot;&quot;&quot;&quot;&quot; # define grid p_grid = np.linspace(0, 1, grid_points) # define prior prior = np.repeat(5, grid_points) # uniform # prior = (p_grid &gt;= 0.5).astype(int) # truncated # prior = np.exp(- 5 * abs(p_grid - 0.5)) # double exp # compute likelihood at each point in the grid likelihood = stats.binom.pmf(success, tosses, p_grid) # compute product of likelihood and prior unstd_posterior = likelihood * prior # standardize the posterior, so it sums to 1 posterior = unstd_posterior / unstd_posterior.sum() return p_grid, posterior w, n = 6, 9 fig, ax = plt.subplots(1, 3, figsize = (12, 4)) points = (5, 10, 20) for idx, ps in enumerate(points): p_grid, posterior = posterior_grid_approx(ps, w, n) ax[idx].plot(p_grid, posterior, &quot;o-&quot;, color = r.clr3, label = f&quot;successes = {w}\\ntosses = {n}&quot;) ax[idx].set_xlabel(&quot;probability of water&quot;) ax[idx].set_ylabel(&quot;posterior probability&quot;) ax[idx].set_title(f&quot;{ps} points&quot;) ax[idx].legend(loc = 0) ax[idx].spines[&#39;left&#39;].set_visible(False) ax[idx].spines[&#39;right&#39;].set_visible(False) ax[idx].spines[&#39;top&#39;].set_visible(False) ax[idx].spines[&#39;bottom&#39;].set_visible(False) ax[idx].grid(linestyle = &#39;dotted&#39;) np.repeat((0, 1), (3, 6)) #&gt; array([0, 0, 0, 1, 1, 1, 1, 1, 1]) data = np.repeat((0, 1), (3, 6)) with pm.Model() as normal_approximation: p = pm.Uniform(&quot;p&quot;, 0, 1) # uniform priors w = pm.Binomial(&quot;w&quot;, n=len(data), p=p, observed=data.sum()) # binomial likelihood mean_q = pm.find_MAP() std_q = ((1 / pm.find_hessian(mean_q, vars=[p])) ** 0.5)[0] # display summary of quadratic approximation #&gt; █ print(&quot;Mean\\tStandard deviation\\np {:.2}\\t{:.2}&quot;.format(mean_q[&quot;p&quot;], std_q[0])) #&gt; Mean Standard deviation #&gt; p 0.67 0.16 # Compute the 89% percentile interval norm = stats.norm(mean_q, std_q) prob = 0.89 z = stats.norm.ppf([(1 - prob) / 2, (1 + prob) / 2]) pi = mean_q[&quot;p&quot;] + std_q * z print(&quot;5.5%, 94.5% \\n{:.2}, {:.2}&quot;.format(pi[0], pi[1])) #&gt; 5.5%, 94.5% #&gt; 0.42, 0.92 # analytical calculation w, n = 6, 9 x = np.linspace(0, 1, 100) plt.plot(x, stats.beta.pdf(x, w + 1, n - w + 1), label=&quot;True posterior&quot;, color = r.clr0d) # quadratic approximation plt.plot(x, stats.norm.pdf(x, mean_q[&quot;p&quot;], std_q), label=&quot;Quadratic approximation&quot;, color = r.clr3) plt.legend(loc = 0) plt.title(f&quot;n = {n}&quot;) plt.xlabel(&quot;Proportion water&quot;); plt.gca().spines[&#39;left&#39;].set_visible(False) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(False) plt.grid(linestyle = &#39;dotted&#39;) plt.show() x = np.linspace(0, 1, 100) w, n = [6, 12, 24], [9, 18, 36] fig, ax = plt.subplots(1, 3, figsize=(12, 4)) for idx, ps in enumerate(zip(w, n)): data = np.repeat((0, 1), (ps[1] - ps[0], ps[0])) with pm.Model() as normal_approximation: p = pm.Uniform(&quot;p&quot;, 0, 1) # uniform priors w = pm.Binomial(&quot;w&quot;, n=len(data), p=p, observed=data.sum()) # binomial likelihood mean_q = pm.find_MAP() std_q = ((1 / pm.find_hessian(mean_q, vars=[p])) ** 0.5)[0] ax[idx].plot(x, stats.beta.pdf(x, ps[0] + 1, ps[1] - ps[0] + 1), label = &quot;True posterior&quot;, color = r.clr0d) ax[idx].plot(x, stats.norm.pdf(x, mean_q[&quot;p&quot;], std_q), label = &quot;Quadratic approximation&quot;, color = r.clr3) ax[idx].set_xlabel(&quot;probability of water&quot;) ax[idx].set_ylabel(&quot;density&quot;) ax[idx].set_title(r&quot;$n={}$&quot;.format(ps[1])) ax[idx].legend(loc=&quot;upper left&quot;) ax[idx].spines[&#39;left&#39;].set_visible(False) ax[idx].spines[&#39;right&#39;].set_visible(False) ax[idx].spines[&#39;top&#39;].set_visible(False) ax[idx].spines[&#39;bottom&#39;].set_visible(False) ax[idx].grid(linestyle = &#39;dotted&#39;) n_samples = 1000 p = np.zeros(n_samples) p[0] = 0.5 W = 6 L = 3 for i in range(1, n_samples): p_new = stats.norm(p[i - 1], 0.1).rvs(1) if p_new &lt; 0: p_new = -p_new if p_new &gt; 1: p_new = 2 - p_new q0 = stats.binom.pmf(W, n = W + L, p = p[i - 1]) q1 = stats.binom.pmf(W, n = W + L, p = p_new) if stats.uniform.rvs(0, 1) &lt; q1 / q0: p[i] = p_new else: p[i] = p[i - 1] az.plot_kde(p, label = &quot;Metropolis approximation&quot;, plot_kwargs = {&quot;color&quot;: r.clr3}) x = np.linspace(0, 1, 100) plt.plot(x, stats.beta.pdf(x, W + 1, L + 1), &quot;C1&quot;, label = &quot;True posterior&quot;, color = r.clr0d) plt.legend() plt.gca().spines[&#39;left&#39;].set_visible(False) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(False) plt.grid(linestyle = &#39;dotted&#39;) plt.show() × "],
["rethinking-chapter-3.html", "4 Rethinking: Chapter 3 4.1 Sampling from a grid approximate posterior 4.2 Sampling to Summarize 4.3 Point estimates 4.4 sample to simulate prediction 4.5 Homework 4.6 {brms} section 4.7 pymc3 section", " 4 Rethinking: Chapter 3 Sampling the Imaginary by Richard McElreath, building on the Summary by Solomon Kurz \\[ Pr(vampire|positive) = \\frac{Pr(positive|vampire) \\times Pr(vampire)}{Pr(positive)} \\] pr_positive_on_vamp &lt;- .95 pr_positive_on_mort &lt;- .01 pr_vamp &lt;- .001 pr_positive &lt;- pr_positive_on_vamp * pr_vamp + pr_positive_on_mort * (1 - pr_vamp) (pr_vamp_on_positive &lt;- pr_positive_on_vamp * pr_vamp /pr_positive) #&gt; [1] 0.08683729 4.1 Sampling from a grid approximate posterior posterior here means simply ‘the probability of p conditional on the data’: grid_approx &lt;- function(n_grid = 20, L = 6, W = 3, prior = function(x){rep(1, length(x))}){ tibble(p_grid = seq(0, 1, length.out = n_grid), prior = prior(p_grid), likelihood = dbinom(L, size = W + L, prob = p_grid), posterior_unstand = likelihood * prior, posterior = posterior_unstand / sum(posterior_unstand)) } grid_data &lt;- grid_approx(n_grid = 10^4) samples &lt;- tibble(sample = 1:(10^4), proportion_water = sample(x = grid_data$p_grid, size = length(sample), prob = grid_data$posterior, replace = TRUE)) p_scatter &lt;- samples %&gt;% ggplot(aes(x = sample, y = proportion_water)) + geom_point(size = .75, shape = 21, color = clr_alpha(clr2,.3), fill = clr_alpha(clr2,.1)) + scale_x_continuous(expand = c(0,0)) p_dens &lt;- samples %&gt;% ggplot(aes(x = proportion_water)) + geom_density( color = clr2, fill = fll2) + scale_x_continuous(limits = c(0,1), expand = c(0, 0)) p_scatter + p_dens 4.2 Sampling to Summarize Once the posterior distribution is created, the model is done. Typical targets / questions: intervals of defined boundaries intervals of defined probability mass point estimates 4.2.1 Intervals of devined boundaries sum(grid_data$posterior[grid_data$p_grid &lt; 0.5]) #&gt; [1] 0.171875 sum(samples$proportion_water &lt; .5) / length(samples$proportion_water) #&gt; [1] 0.1728 sum(samples$proportion_water &gt; .5 &amp; samples$proportion_water &lt; .75) / length(samples$proportion_water) #&gt; [1] 0.6089 # f_post &lt;- function(x){dbeta(x = x, shape1 = 9 +1 , shape2 = 6 +1)} f_post &lt;- function(x){dbinom(x = 6, size = 9, prob = x)} f_post_norm &lt;- function(x){ f_post(x) / integrate(f = f_post,lower = 0, upper = 1)[[1]]} plot_intervals &lt;- function(x_bounds = c(0, 1), x_line = as.numeric(NA), f_posterior = f_post_norm, data = samples, ylim = c(0, 3)){ p_d &lt;- ggplot() + stat_function(fun = f_posterior, xlim = c(0,1), geom = &quot;area&quot;, color = clr0, fill = fll0) + stat_function(fun = f_posterior, xlim = x_bounds, geom = &quot;area&quot;, color = clr2, fill = fll2) + labs(y = &quot;density&quot;, x = &quot;proportion_water&quot;) p_d_emp &lt;- data %&gt;% ggplot(aes(x = proportion_water)) + geom_density( color = clr0, fill = fll0) + stat_function(fun = function(x){demp(obs = data$proportion_water, x = x)}, xlim = x_bounds, geom = &quot;area&quot;, color = clr2, fill = fll2) + labs(y = &quot;empirical density&quot;) p_d + p_d_emp &amp; geom_vline(data = tibble(x = x_line), aes(xintercept = x), linetype = 3) &amp; scale_y_continuous(limits = ylim)&amp; scale_x_continuous(limits = c(0, 1)) } plot_intervals(x_bounds = c(0, .5), x_line = .5) / plot_intervals(x_bounds = c(.5, .75), x_line = c(.5, .75)) 4.2.2 Intervals of defined mass aka.: compatibility interval credible interval percentile interval special form: highest posterior density interval (HPDI) qnt_80 &lt;- quantile(samples$proportion_water, probs = .8) qnt_80_inner &lt;- quantile(samples$proportion_water, probs = c(.1, .9)) plot_intervals(x_bounds = c(0, qnt_80), x_line = qnt_80)/ plot_intervals(x_bounds = qnt_80_inner, x_line = qnt_80_inner) library(rethinking) map &lt;- purrr::map grid_data_skew &lt;- grid_approx(L = 3, W = 0, n_grid = 10^4) samples_skew &lt;- tibble(sample = 1:(10^4), proportion_water = sample(x = grid_data_skew$p_grid, size = length(sample), prob = grid_data_skew$posterior, replace = TRUE)) f_post_skew &lt;- function(x){dbinom(x = 3, size = 3, prob = x)} f_post_norm_skew &lt;- function(x){ f_post_skew(x) / integrate(f = f_post_skew, lower = 0, upper = 1)[[1]]} qnt_50_inner &lt;- PI(samples_skew$proportion_water, prob = .5) qnt_50_high_dens &lt;- HPDI(samples_skew$proportion_water, prob = .5) plot_intervals(x_bounds = qnt_50_inner, x_line = qnt_50_inner, f_posterior = f_post_norm_skew, data = samples_skew, ylim = c(0, 4)) / plot_intervals(x_bounds = qnt_50_high_dens, x_line = qnt_50_high_dens, f_posterior = f_post_norm_skew, data = samples_skew, ylim = c(0, 4)) 4.3 Point estimates point_estimates &lt;- tibble(proportion_water= list(mean, median, chainmode) %&gt;% map_dbl(.f = function(f, vals){ f(vals) }, vals = samples_skew$proportion_water), statistic = c(&quot;mean&quot;, &quot;median&quot;, &quot;mode&quot;)) p_point_estimates &lt;- ggplot() + stat_function(fun = f_post_norm_skew, xlim = c(0,1), geom = &quot;area&quot;, color = clr0, fill = fll0) + geom_vline(data = point_estimates, aes(xintercept = proportion_water, linetype = statistic), color = clr1) + labs(y = &quot;density&quot;) f_loss &lt;- function(x){ map_dbl(x, function(x){ sum( grid_data_skew$posterior * abs( x - grid_data_skew$p_grid)) })} f_loss_quad &lt;- function(x){ map_dbl(x, function(x){ sum( grid_data_skew$posterior * ( x - grid_data_skew$p_grid) ^ 2) })} p_loss &lt;- ggplot() + stat_function(fun = f_loss, xlim = c(0,1), geom = &quot;area&quot;, color = clr0, fill = fll0) + geom_point(data = tibble(x = point_estimates$proportion_water[2], y = f_loss(point_estimates$proportion_water[2])), aes(x = x, y = y), shape = 1, size = 3, color = clr1) + labs(x = &quot;poportion_water&quot;, y = &quot;expected proportional loss&quot;) p_loss_quad &lt;- ggplot() + stat_function(fun = f_loss_quad, xlim = c(0,1), geom = &quot;area&quot;, color = clr0, fill = fll0) + geom_point(data = tibble(x = point_estimates$proportion_water[2], y = f_loss_quad(point_estimates$proportion_water[2])), aes(x = x, y = y), shape = 1, size = 3, color = clr1) + labs(x = &quot;poportion_water&quot;, y = &quot;expected proportional loss&quot;) p_point_estimates + p_loss + p_loss_quad + plot_layout(guide = &quot;collect&quot;) &amp; theme(legend.position = &quot;bottom&quot;) 4.4 sample to simulate prediction binomial likelihood \\[ Pr(W | N ,p) = \\frac{N!}{W! (N -W)!} p^{W}(1 - p)^{N-W} \\] dbinom( 0:2, size = 2, prob = .7) #&gt; [1] 0.09 0.42 0.49 rbinom( 10, size = 2, prob = .7) #&gt; [1] 1 1 2 2 1 2 1 1 2 2 create_dummy_w &lt;- function(size, prob){ tibble(x = rbinom(10^5, size = size, prob = prob), size = size, prob = prob) } dummy_w &lt;- create_dummy_w(size = 9, prob = .7) dummy_w %&gt;% group_by(x) %&gt;% count() %&gt;% ungroup() %&gt;% ggplot(aes(x = factor(x), y = n)) + geom_bar(stat = &quot;identity&quot;, color = clr1, fill = fll1, width = .6) + labs(y = &quot;count&quot;, x = &quot;dummy water count&quot;) tibble(size = rep(c(3,6,9), each = 3), prob = rep(c(.3,.6,.9), 3)) %&gt;% pmap_dfr(create_dummy_w) %&gt;% group_by(x, size , prob) %&gt;% count() %&gt;% ungroup() %&gt;% ggplot(aes(x = factor(x), y = n)) + geom_bar(stat = &quot;identity&quot;, color = clr1, fill = fll1, width = .6) + facet_grid(prob ~ size, scales = &quot;free&quot;, space = &quot;free_x&quot;, labeller = label_both) + labs(y = &quot;count&quot;, x = &quot;dummy water count&quot;) + theme(panel.background = element_rect(color = clr0d, fill = clr_alpha(clr0d,.2))) grid_data &lt;- grid_approx(n_grid = 10^4 + 1, L = 6, W = 3, prior = function(x){rep(1, length(x))}) %&gt;% mutate(idx = 1:(10^4 + 1)) grid_data %&gt;% ggplot(aes(x = p_grid))+ geom_line(aes(y = posterior, color = &quot;posterior&quot;)) + scale_color_manual(values = c(posterior = clr1), guide = &quot;none&quot;) + theme(legend.position = &quot;bottom&quot;) samples &lt;- grid_data %&gt;% slice_sample(n = 10^5 , weight_by = posterior, replace = TRUE) %&gt;% mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 9), seq = map(w, .f = function(x){sample(x = rep(c(&quot;W&quot;,&quot;L&quot;), c(x, 9-x)), size = 9, replace = FALSE)}), max_run_length = map_dbl(seq,.f = function(x){rle(x)$lengths %&gt;% max()}), n_switches = map_dbl(seq,.f = function(x){(rle(x)$lengths %&gt;% length()) -1})) p_posterior &lt;- grid_data %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_area(color = clr0d, fill = fll0) + geom_segment(data = grid_data %&gt;% filter(idx %in% (1+ (1:9)*1000)), aes(xend = p_grid, yend = 0, size = posterior), color = fll1) + geom_point(data = grid_data %&gt;% filter(idx %in% (1+ (1:9)*1000)), color = clr1) + scale_size_continuous(range = c(.1, 1), guide = &quot;none&quot;) + labs(x = &quot;probability of water&quot;, y = &quot;density&quot;, title = &quot;Posterior Probability&quot;) simulate_binom &lt;- function(probability, n_draws = 10^5, size = 9) { rbinom(n_draws, size = size, prob = probability) } d_small &lt;- tibble(probability = seq(from = .1, to = .9, by = .1)) %&gt;% mutate(draws = purrr::map(probability, simulate_binom)) %&gt;% unnest(draws) p_small &lt;- d_small %&gt;% ggplot(aes(x = draws)) + geom_bar(stat = &quot;count&quot;, color = clr1, fill = fll1, width = .6) + facet_wrap(probability ~ ., nrow = 1)+ labs(title = &quot;Sampling Distributions&quot;) p_posterior_predictive &lt;- samples %&gt;% ggplot(aes(x = factor(w))) + geom_bar(stat = &quot;count&quot;, aes(color = w == 6 , fill = after_scale(clr_alpha(color, .3))), width = .6) + scale_color_manual(values = c(`TRUE` = clr1, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of water samples&quot;, title = &quot;Posterior Predictive Distribution&quot;) p_posterior / p_small / p_posterior_predictive sum( samples$w == 6 ) / length( samples$w ) #&gt; [1] 0.20148 globe_data &lt;- c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;) globe_run_length &lt;- rle(globe_data)$lengths %&gt;% max() globe_n_switches &lt;- (rle(globe_data)$lengths %&gt;% length()) -1 p_run_length &lt;- samples %&gt;% ggplot(aes(x = factor(max_run_length))) + geom_bar(stat = &quot;count&quot;, aes(color = max_run_length == globe_run_length, fill = after_scale(clr_alpha(color))), width = .6) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;longest run length&quot;) p_switches &lt;- samples %&gt;% ggplot(aes(x = factor(n_switches))) + geom_bar(stat = &quot;count&quot;, aes(color = n_switches == globe_n_switches, fill = after_scale(clr_alpha(color))), width = .6) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of switches&quot;) p_run_length + p_switches 4.5 Homework n &lt;- 1e4 easy_data &lt;- tibble(p_grid = seq( from = 0, to = 1, length.out = n ), prior = rep(1 , n), likelihood = dbinom( 6, size = 9, prob = p_grid), posterior_unscaled = likelihood * prior, posterior = posterior_unscaled / sum(posterior_unscaled), cummulative_posterior = cumsum(posterior)) easy_data %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = prior / sum(prior), color = &quot;prior&quot;)) + geom_line(aes(y = likelihood / sum(likelihood), color = &quot;likelihood&quot;)) + geom_line(aes(y = posterior, color = &quot;posterior&quot;), linetype = 3) + geom_line(aes(y = cummulative_posterior / sum(cummulative_posterior), color = &quot;cummulative_posterior&quot;), linetype = 3) + scale_color_manual(values = c(prior = clr1, likelihood = clr0d, posterior = clr2, cummulative_posterior = &quot;black&quot;)) + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), legend.position = &quot;bottom&quot;) set.seed( 100 ) easy_samples &lt;- easy_data %&gt;% slice_sample(n = n, weight_by = posterior, replace = TRUE) easy_samples %&gt;% ggplot(aes(x = p_grid)) + geom_density(color = clr0d, fill = fll0) + scale_x_continuous(limits = c(0,1)) + geom_vline(data = tibble(x = c(.2, .8)), aes(xintercept = x), linetype = 3) E1 mean(easy_samples$p_grid &lt;= .2) #&gt; [1] 5e-04 E2 mean(easy_samples$p_grid &gt; .8) #&gt; [1] 0.1219 E3 mean( easy_samples$p_grid &gt; .2 &amp; easy_samples$p_grid &lt; .8) #&gt; [1] 0.8776 E4 quantile(easy_samples$p_grid , probs = .2) #&gt; 20% #&gt; 0.5145315 E5 quantile(easy_samples$p_grid , probs = .8) #&gt; 80% #&gt; 0.7618962 E6 HPDI(easy_samples$p_grid, prob = .66) #&gt; |0.66 0.66| #&gt; 0.5138514 0.7886789 p_e6 &lt;- easy_samples %&gt;% ggplot(aes(x = p_grid)) + geom_density(color = clr0d, fill = fll0) + stat_function(fun = function(x){demp(x = x, obs = easy_samples$p_grid)}, geom = &quot;area&quot;, xlim = HPDI(easy_samples$p_grid, prob = .66), color = clr2, fill = fll2) + scale_color_manual(values = c(posterior = clr2), guide = &quot;none&quot;) + theme(legend.position = &quot;bottom&quot;) E7 PI(easy_samples$p_grid, prob = .66) #&gt; 17% 83% #&gt; 0.4972327 0.7745775 p_e7 &lt;- easy_samples %&gt;% ggplot(aes(x = p_grid)) + geom_density(color = clr0d, fill = fll0) + stat_function(fun = function(x){demp(x = x, obs = easy_samples$p_grid)}, geom = &quot;area&quot;, xlim = PI(easy_samples$p_grid, prob = .66), color = clr2, fill = fll2) + scale_color_manual(values = c(posterior = clr2), guide = &quot;none&quot;) + theme(legend.position = &quot;bottom&quot;) p_e6 + p_e7 M1 grid_data &lt;- grid_approx(n_grid = 10^4 + 1, L = 8, W = 7, prior = function(x){rep(1, length(x))}) %&gt;% mutate(idx = 1:(10^4 + 1)) grid_data %&gt;% ggplot(aes(x = p_grid))+ geom_line(aes(y = posterior, color = &quot;posterior&quot;)) + scale_color_manual(values = c(posterior = clr2), guide = &quot;none&quot;) + theme(legend.position = &quot;bottom&quot;) M2 samples &lt;- grid_data %&gt;% slice_sample(n = 10^5 , weight_by = posterior, replace = TRUE) %&gt;% mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 15)) HPDI(samples$p_grid, prob = .9) #&gt; |0.9 0.9| #&gt; 0.3329 0.7218 M3 p_posterior &lt;- grid_data %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_area(color = clr0d, fill = fll0) + geom_segment(data = grid_data %&gt;% filter(idx %in% (1+ (1:9)*1000)), aes(xend = p_grid, yend = 0, size = posterior), color = fll2) + geom_point(data = grid_data %&gt;% filter(idx %in% (1+ (1:9)*1000)), color = clr2) + scale_size_continuous(range = c(.1, 1), guide = &quot;none&quot;) + labs(x = &quot;probability of water&quot;, y = &quot;density&quot;, title = &quot;Posterior Probability&quot;) d_small &lt;- tibble(probability = seq(from = .1, to = .9, by = .1)) %&gt;% mutate(draws = purrr::map(probability, simulate_binom, size = 15)) %&gt;% unnest(draws) p_small &lt;- d_small %&gt;% ggplot(aes(x = draws)) + geom_bar(stat = &quot;count&quot;, color = clr2, fill = fll2, width = .6) + facet_wrap(probability ~ ., nrow = 1)+ labs(title = &quot;Sampling Distributions&quot;) p_posterior_predictive &lt;- samples %&gt;% ggplot(aes(x = factor(w))) + geom_bar(stat = &quot;count&quot;, aes(color = w == 8 , fill = after_scale(clr_alpha(color, .3))), width = .6) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of water samples&quot;, title = &quot;Posterior Predictive Distribution&quot;) p_posterior / p_small / p_posterior_predictive sum( samples$w == 8 ) / length( samples$w ) #&gt; [1] 0.14738 M4 samples &lt;- grid_data %&gt;% slice_sample(n = 10^5 , weight_by = posterior, replace = TRUE) %&gt;% mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 9)) samples %&gt;% ggplot(aes(x = factor(w))) + geom_bar(stat = &quot;count&quot;, aes(color = w == 6 , fill = after_scale(clr_alpha(color, .3))), width = .6) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of water samples&quot;, title = &quot;Posterior Predictive Distribution&quot;) sum( samples$w == 6 ) / length( samples$w ) #&gt; [1] 0.17634 M5 grid_data &lt;- grid_approx(n_grid = 10^4 + 1, L = 8, W = 7, prior = function(x){if_else(x &lt; .5, 0, 1)}) %&gt;% mutate(idx = 1:(10^4 + 1)) samples &lt;- grid_data %&gt;% slice_sample(n = 10^5 , weight_by = posterior, replace = TRUE) %&gt;% mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 15)) HPDI(samples$p_grid, prob = .9) #&gt; |0.9 0.9| #&gt; 0.5000 0.7117 p_posterior &lt;- grid_data %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_area(color = clr0d, fill = fll0) + geom_segment(data = grid_data %&gt;% filter(idx %in% (1+ (1:9)*1000)), aes(xend = p_grid, yend = 0, size = posterior), color = fll2) + geom_point(data = grid_data %&gt;% filter(idx %in% (1+ (1:9)*1000)), color = clr2) + scale_size_continuous(range = c(.1, 1), guide = &quot;none&quot;) + labs(x = &quot;probability of water&quot;, y = &quot;density&quot;, title = &quot;Posterior Probability&quot;) d_small &lt;- tibble(probability = seq(from = .1, to = .9, by = .1)) %&gt;% mutate(draws = purrr::map(probability, simulate_binom, size = 15)) %&gt;% unnest(draws) p_small &lt;- d_small %&gt;% ggplot(aes(x = draws)) + geom_bar(stat = &quot;count&quot;, color = clr2, fill = fll2, width = .6) + facet_wrap(probability ~ ., nrow = 1)+ labs(title = &quot;Sampling Distributions&quot;) p_posterior_predictive &lt;- samples %&gt;% ggplot(aes(x = factor(w))) + geom_bar(stat = &quot;count&quot;, aes(color = w == 8 , fill = after_scale(clr_alpha(color, .3))), width = .6) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;number of water samples&quot;, title = &quot;Posterior Predictive Distribution&quot;) p_posterior / p_small / p_posterior_predictive sum( samples$w == 8 ) / length( samples$w ) #&gt; [1] 0.15846 M6 random_tosses &lt;- function(n, n_grid = 1e4, n_posterior_sample = 1e4, prior = function(x){rep(1, length(x))}){ grid_data &lt;- tibble(p_grid = seq(0, 1, length.out = n_grid), prior = prior(p_grid), likelihood = dbinom(rbinom(1, size = n, prob = .7), size = n, prob = p_grid), posterior_unstand = likelihood * prior, posterior = posterior_unstand / sum(posterior_unstand)) samples &lt;- grid_data %&gt;% slice_sample(n = n_posterior_sample, weight_by = posterior, replace = TRUE) tibble(n = n, grid_data = list(grid_data), samples = list(samples), hpdi = list(HPDI(samples[[1]]$p_grid, prob = .99)), hpdi_width = diff(hpdi[[1]])) } c(c(1:10),((1:100) *30)) %&gt;% map_dfr(random_tosses) %&gt;% ggplot(aes(x = n, y = hpdi_width)) + geom_point(aes(color = hpdi_width &lt; .05)) + geom_hline(yintercept = .05, color = &quot;black&quot;, linetype = 3) + scale_color_manual(values = c(`FALSE` = clr0d, `TRUE` = clr2), guide = &quot;none&quot;) H1 library(rethinking) data(homeworkch3) n_grid &lt;- 1e4 + 1 grid_data &lt;- tibble(p_grid = seq(0, 1, length.out = n_grid), prior = (function(x){rep(1, length(x))})(p_grid), likelihood = dbinom(sum(birth1 + birth2), size = length(c(birth1, birth2)), prob = p_grid), posterior_unstand = likelihood * prior, posterior = posterior_unstand / sum(posterior_unstand)) samples &lt;- grid_data %&gt;% slice_sample(n = 1e5, weight_by = posterior, replace = TRUE) (mode_posterior &lt;- chainmode(samples$p_grid)) #&gt; [1] 0.5547754 grid_data %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_area(color = clr0d, fill = fll0, size = .5) + geom_vline(xintercept = mode_posterior, color = clr2, linetype = 3) H2 (percentile_intervals &lt;- tibble(boundary = c(&quot;lower&quot;, &quot;upper&quot;), p50 = HPDI(samples$p_grid, prob = .5), p89 = HPDI(samples$p_grid, prob = .89), p97 = HPDI(samples$p_grid, prob = .97))) %&gt;% knitr::kable() boundary p50 p89 p97 lower 0.5306 0.4977 0.4775 upper 0.5778 0.6090 0.6274 grid_data %&gt;% ggplot(aes(x = p_grid, y = posterior)) + stat_function(fun = function(x){demp(x = x, obs = samples$p_grid)}, geom = &quot;line&quot;, color = clr0d, xlim = c(0,1), n = 501) + stat_function(fun = function(x){demp(x = x, obs = samples$p_grid)}, geom = &quot;area&quot;, color = clr2, fill = clr_alpha(fll2,.2), xlim = percentile_intervals$p50, n = 501) + stat_function(fun = function(x){demp(x = x, obs = samples$p_grid)}, geom = &quot;area&quot;, color = clr2, fill = clr_alpha(fll2,.2), xlim = percentile_intervals$p89, n = 501) + stat_function(fun = function(x){demp(x = x, obs = samples$p_grid)}, geom = &quot;area&quot;, color = clr2, fill = clr_alpha(fll2,.2), xlim = percentile_intervals$p97, n = 501) H3 random_births &lt;- grid_data %&gt;% slice_sample(n = 1e4, weight_by = posterior, replace = TRUE) %&gt;% mutate(births = map(p_grid, .f = function(x){rbinom(n = 200, size = 1, prob = x)}), n_boys = map_dbl(births, sum), n_girls = 200 - n_boys, n_boys_firstborn = map_dbl(births, function(x){ sum(x[1:100]) })) sum(random_births$n_boys &lt; sum(birth1 + birth2)) / sum(birth1 + birth2) #&gt; [1] 43.45946 p_all &lt;- random_births %&gt;% ggplot(aes(x = n_boys)) + geom_density(color = clr0d, fill = fll0) + geom_vline(xintercept = sum(birth1 + birth2), color = clr2, linetype = 3) + scale_x_continuous(limits = c(0, 200)) p_first &lt;- random_births %&gt;% ggplot(aes(x = n_boys_firstborn)) + geom_density(color = clr0d, fill = fll0) + geom_vline(xintercept = sum(birth1), color = clr2, linetype = 3) + scale_x_continuous(limits = c(0, 100)) p_all + p_first H5 births_first_girl &lt;- tibble(birth1 = birth1, birth2 = birth2) %&gt;% filter(birth1 == 0) n_first_girl &lt;- length(births_first_girl$birth2) random_births &lt;- grid_data %&gt;% slice_sample(n = 1e4, weight_by = posterior, replace = TRUE) %&gt;% mutate(births = map(p_grid, .f = function(x){rbinom(n = n_first_girl, size = 1, prob = x)}), n_boys = map_dbl(births, sum), n_girls = n_first_girl - n_boys) sum(random_births$n_boys &lt; sum(births_first_girl$birth2)) / 1e4 #&gt; [1] 0.9991 random_births %&gt;% ggplot(aes(x = n_boys)) + geom_density(color = clr0d, fill = fll0) + geom_vline(xintercept = sum(births_first_girl$birth2), color = clr2, linetype = 3) + scale_x_continuous(limits = c(0, n_first_girl), expand = c(0, 0)) 4.6 {brms} section brms_c3_6in9 &lt;- brm(data = list(w = 6), family = binomial(link = &quot;identity&quot;), w | trials(9) ~ 0 + Intercept, # this is a flat prior prior(beta(1, 1), class = b, lb = 0, ub = 1), iter = 5000, warmup = 1000, seed = 42, file = &quot;brms/brms_c3_6in9&quot;) posterior_summary(brms_c3_6in9) %&gt;% round(digits = 3) %&gt;% knitr::kable() Estimate Est.Error Q2.5 Q97.5 b_Intercept 0.637 0.140 0.346 0.879 lp__ -3.310 0.748 -5.392 -2.780 n_trials &lt;- 9 samples_brms &lt;- fitted(brms_c3_6in9, summary = FALSE, scale = &quot;linear&quot;) %&gt;% as_tibble() %&gt;% set_names(nm = &quot;p&quot;) %&gt;% mutate(w = rbinom(n(), size = n_trials, prob = p)) p_brms_posterior &lt;- samples_brms %&gt;% ggplot(aes(x = p)) + geom_density(color = clr1, fill = fll1) + scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) + labs(y = &quot;density&quot;, x = &quot;proportion water&quot;, title = &quot;{brms} posterior probability (6 in 9)&quot;) p_brms_posterior_predictive &lt;- samples_brms %&gt;% ggplot(aes(x = factor(w))) + geom_bar(stat = &quot;count&quot;, aes(color = w == 6 , fill = after_scale(clr_alpha(color, .3))), width = .6) + scale_color_manual(values = c(`TRUE` = clr1, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(y = &quot;count&quot;, x = &quot;number of water samples&quot;, title = &quot;posterior predictive distribution&quot;) p_brms_posterior + p_brms_posterior_predictive 4.7 pymc3 section Taken from pymc-devs… import arviz as az import matplotlib.pyplot as plt import numpy as np import pymc3 as pm import scipy.stats as stats import seaborn as sns import matplotlib from matplotlib.colors import ListedColormap import matplotlib.font_manager matplotlib.font_manager._rebuild() def posterior_grid_approx(grid_points = 5, success = 6, tosses = 9): &quot;&quot;&quot;&quot;&quot;&quot; # define grid p_grid = np.linspace(0, 1, grid_points) # define prior prior = np.repeat(5, grid_points) # uniform # prior = (p_grid &gt;= 0.5).astype(int) # truncated # prior = np.exp(- 5 * abs(p_grid - 0.5)) # double exp # compute likelihood at each point in the grid likelihood = stats.binom.pmf(success, tosses, p_grid) # compute product of likelihood and prior unstd_posterior = likelihood * prior # standardize the posterior, so it sums to 1 posterior = unstd_posterior / unstd_posterior.sum() return p_grid, posterior PrPV = 0.95 PrPM = 0.01 PrV = 0.001 PrP = PrPV * PrV + PrPM * (1 - PrV) PrVP = PrPV * PrV / PrP PrVP #&gt; 0.08683729433272395 p_grid, posterior = posterior_grid_approx(grid_points = 100, success = 6, tosses = 9) samples = np.random.choice(p_grid, p = posterior, size = int(1e4), replace = True) fig, (ax0, ax1) = plt.subplots(1, 2, figsize = (12, 4)) ax0.plot(samples, &quot;o&quot;, alpha = 0.2, color = r.clr3) ax0.set_xlabel(&quot;sample number&quot;) ax0.set_ylabel(&quot;proportion water (p)&quot;) ax0.spines[&#39;left&#39;].set_visible(False) ax0.spines[&#39;right&#39;].set_visible(False) ax0.spines[&#39;top&#39;].set_visible(False) ax0.spines[&#39;bottom&#39;].set_visible(False) ax0.grid(linestyle = &#39;dotted&#39;) az.plot_kde(samples, ax = ax1, plot_kwargs = {&quot;color&quot;: r.clr3}) ax1.set_xlabel(&quot;proportion water (p)&quot;) ax1.set_ylabel(&quot;density&quot;) ax1.spines[&#39;left&#39;].set_visible(False) ax1.spines[&#39;right&#39;].set_visible(False) ax1.spines[&#39;top&#39;].set_visible(False) ax1.spines[&#39;bottom&#39;].set_visible(False) ax1.grid(linestyle = &#39;dotted&#39;) plt.show() sum(posterior[p_grid &lt; 0.5]) #&gt; 0.17183313110747475 sum(samples &lt; 0.5) / 1e4 #&gt; 0.1767 sum((samples &gt; 0.5) &amp; (samples &lt; 0.75)) / 1e4 #&gt; 0.6113 np.percentile(samples, 80) #&gt; 0.7575757575757577 np.percentile(samples, [10, 90]) #&gt; array([0.44444444, 0.80808081]) p_grid, posterior = posterior_grid_approx(grid_points = 100, success = 3, tosses = 3) plt.plot(p_grid, posterior, color = r.clr3) plt.xlabel(&quot;proportion water (p)&quot;) plt.ylabel(&quot;Density&quot;) plt.gca().spines[&#39;left&#39;].set_visible(False) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(False) plt.grid(linestyle = &#39;dotted&#39;) plt.show() samples = np.random.choice(p_grid, p = posterior, size = int(1e4), replace = True) np.percentile(samples, [25, 75]) #&gt; array([0.71717172, 0.93939394]) az.hdi(samples, hdi_prob = 0.5) #&gt; array([0.84848485, 1. ]) p_grid[posterior == max(posterior)] #&gt; array([1.]) stats.mode(samples)[0] #&gt; array([1.]) np.mean(samples), np.median(samples) #&gt; (0.8061979797979799, 0.8484848484848485) sum(posterior * abs(0.5 - p_grid)) #&gt; 0.31626874808692995 loss = [sum(posterior * abs(p - p_grid)) for p in p_grid] p_grid[loss == min(loss)] #&gt; array([0.84848485]) stats.binom.pmf(range(3), n = 2, p = 0.7) #&gt; array([0.09, 0.42, 0.49]) stats.binom.rvs(n = 2, p = 0.7, size = 1) #&gt; array([1]) stats.binom.rvs(n = 2, p = 0.7, size = 10) #&gt; array([2, 2, 1, 2, 1, 0, 2, 1, 2, 2]) dummy_w = stats.binom.rvs(n = 2, p = 0.7, size = int(1e5)) [(dummy_w == i).mean() for i in range(3)] #&gt; [0.08832, 0.42272, 0.48896] dummy_w = stats.binom.rvs(n = 9, p = 0.7, size = int(1e5)) # dummy_w = stats.binom.rvs(n=9, p=0.6, size=int(1e4)) # dummy_w = stats.binom.rvs(n=9, p=samples) bar_width = 0.7 plt.hist(dummy_w, bins = np.arange(0, 11) - bar_width / 2, width = bar_width, color = r.clr3) #&gt; (array([2.0000e+00, 4.2000e+01, 3.9200e+02, 2.1170e+03, 7.4230e+03, #&gt; 1.7153e+04, 2.6752e+04, 2.6557e+04, 1.5640e+04, 3.9220e+03]), array([-0.35, 0.65, 1.65, 2.65, 3.65, 4.65, 5.65, 6.65, 7.65, #&gt; 8.65, 9.65]), &lt;a list of 10 Patch objects&gt;) plt.xlim(0, 9.5) #&gt; (0, 9.5) plt.xlabel(&quot;dummy water count&quot;) plt.ylabel(&quot;Frequency&quot;) plt.gca().spines[&#39;left&#39;].set_visible(False) plt.gca().spines[&#39;right&#39;].set_visible(False) plt.gca().spines[&#39;top&#39;].set_visible(False) plt.gca().spines[&#39;bottom&#39;].set_visible(False) plt.grid(linestyle = &#39;dotted&#39;) plt.show() p_grid, posterior = posterior_grid_approx(grid_points = 100, success = 6, tosses = 9) np.random.seed(100) samples = np.random.choice(p_grid, p = posterior, size = int(1e4), replace = True) birth1 = np.array([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1]) birth2 = np.array([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0]) sum(birth1) + sum(birth2) #&gt; 111 × "],
["rethinking-chapter-4.html", "5 Rethinking: Chapter 4 5.1 Why normal distributions are normal 5.2 Normal by multiplication and by log-multiplication 5.3 A language for describing models 5.4 Linear Prediction 5.5 Curves from lines 5.6 Splines 5.7 Homework 5.8 {brms} section 5.9 pymc3 section", " 5 Rethinking: Chapter 4 Geocentric Models by Richard McElreath, building on the Summaries by Solomon Kurz and Jake Thompson. 5.1 Why normal distributions are normal 5.1.1 Normal by addition n_people &lt;- 1e3 position &lt;- crossing(person = 1:n_people, step = 0:16) %&gt;% mutate(deviation = map_dbl(step, ~if_else(. == 0, 0, runif(1, -1, 1)))) %&gt;% group_by(person) %&gt;% mutate(position = cumsum(deviation)) %&gt;% ungroup() p_all_steps &lt;- position %&gt;% ggplot(aes(x = step, y = position, group = person)) + geom_line(aes(color = person == n_people)) + geom_point(data = position %&gt;% filter(person == n_people), aes(color = &quot;TRUE&quot;), size = 1) + geom_vline(data = tibble(step = c(4, 8, 16)), aes(xintercept = step), linetype = 3, color = rgb(0,0,0,.5)) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr_alpha(clr0d, .05)), guide = &quot;none&quot;) + scale_x_continuous(breaks = c(0,4,8,16)) + theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) plot_steps &lt;- function(step_nr, data = position, add_ideal = FALSE){ data_step &lt;- data %&gt;% filter(step == step_nr) p &lt;- data_step %&gt;% ggplot(aes(x = position)) + geom_density(adjust = .2, color = clr0d, fill = fll0) + scale_x_continuous(limits = c(-6, 6)) + labs(title = glue(&quot;{step_nr} steps&quot;)) if(add_ideal){p &lt;- p + stat_function(fun = function(x){dnorm(x, mean = 0, sd = sd(data_step$position))}, n = 501, color = clr2, linetype = 3)} p } p_all_steps / (plot_steps(step_nr = 4) + plot_steps(step_nr = 8) + plot_steps(step_nr = 16, add_ideal = TRUE)) 5.2 Normal by multiplication and by log-multiplication normal_by_multiplication &lt;- function(effect_size = 0.1, x_scale = ggplot2::scale_x_continuous(), x_lab = &quot;normal&quot;){ tibble(person = 1:n_people, growth = replicate(length(person), prod(1 + runif(12, 0, effect_size)))) %&gt;% ggplot(aes(x = growth)) + geom_density(color = clr0d, fill = fll0) + labs(title = glue(&quot;effect size: {effect_size}&quot;), x = glue(&quot;growth ({x_lab})&quot;)) + x_scale } normal_by_multiplication(effect_size = .01) + normal_by_multiplication(effect_size = .1) + normal_by_multiplication(effect_size = .5)+ normal_by_multiplication(effect_size = .5, x_scale = scale_x_log10(), x_lab = &quot;log10&quot;) + plot_layout(nrow = 1) 5.2.1 using the Gaussian distribution part of the exponential family probability density function \\(\\mu\\): mean \\(\\sigma\\): standard deviation \\(\\tau\\): precision \\[ p( y | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp \\left( \\frac{(y-\\mu)^2}{2\\sigma^2} \\right)\\\\ \\tau = 1 / \\sigma^2 \\\\ p( y | \\mu, \\tau) = \\sqrt{\\frac{\\tau}{2\\pi}}exp(-\\tfrac{1}{2}\\tau(y - \\mu)^2) \\] 5.3 A language for describing models *The first line defines the likelihood used in Bayes’ theorem, the other lines describe the priors used. The tilde means that the relationships are **stochastic*.** re-describing the globe-toss model: The count \\(W\\) is distributed binomially with a sample size \\(N\\) and the probabiliy \\(p\\). The prior for \\(p\\) is assumed to be uniform between zero and one \\[ W \\sim Binomial(N, p)\\\\ p \\sim Uniform(0, 1) \\] Substituting in Bayes’ theorem: \\[ Pr(p | w, n) = \\frac{Binomial(w|n,p)~Uniform(p|0,1)}{\\int Binomial(w|n,p)~Uniform(p|0,1) dp} \\] w &lt;- 6 n &lt;- 9 grid_data &lt;- tibble(p_grid = seq(0,1, length.out = 101), likelihood = dbinom(w, n, p_grid), prior = dunif(p_grid, 0, 1), posterior_unstand = likelihood * prior, posterior = posterior_unstand / sum(posterior_unstand)) grid_data %&gt;% pivot_longer(cols = c(prior, likelihood, posterior), names_to = &quot;bayes_part&quot;, values_to = &quot;p&quot;) %&gt;% mutate(bayes_part = factor(bayes_part, levels = names(clr_bayes))) %&gt;% ggplot(aes(x = p_grid)) + geom_area(aes(y = p, color = bayes_part, fill = after_scale(clr_alpha(color)))) + scale_color_manual(values = clr_bayes, guide = &quot;none&quot;) + facet_wrap(bayes_part ~ ., scales = &quot;free_y&quot;) 5.3.1 Gaussian model of height 5.3.1.1 The data library(rethinking) data(Howell1) (data &lt;- as_tibble(Howell1)) %&gt;% precis() %&gt;% as_tibble(rownames = NA) %&gt;% knitr::kable() mean sd 5.5% 94.5% histogram height 138.2635963 27.6024476 81.108550 165.73500 ▁▁▁▁▁▁▁▂▁▇▇▅▁ weight 35.6106176 14.7191782 9.360721 54.50289 ▁▂▃▂▂▂▂▅▇▇▃▂▁ age 29.3443934 20.7468882 1.000000 66.13500 ▇▅▅▃▅▂▂▁▁ male 0.4724265 0.4996986 0.000000 1.00000 ▇▁▁▁▁▁▁▁▁▇ (data_adults &lt;- data %&gt;% filter(age &gt;= 18)) %&gt;% precis() %&gt;% as_tibble(rownames = NA) %&gt;% knitr::kable() mean sd 5.5% 94.5% histogram height 154.59709 7.7423321 142.8750 167.00500 ▁▃▇▇▅▇▂▁▁ weight 44.99049 6.4567081 35.1375 55.76588 ▁▅▇▇▃▂▁ age 41.13849 15.9678551 20.0000 70.00000 ▂▅▇▅▃▇▃▃▂▂▂▁▁▁▁ male 0.46875 0.4997328 0.0000 1.00000 ▇▁▁▁▁▁▁▁▁▇ data_adults %&gt;% ggplot(aes(x = height)) + geom_density(adjust = .5, color = clr0d, fill = fll0) + scale_x_continuous(limits = c(130,185)) 5.3.1.2 The model \\[ \\begin{array}{cccr} h_i &amp; \\stackrel{iid}{\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu &amp; \\sim &amp; Normal(178, 20) &amp; \\textrm{[$\\mu$ prior]}\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] where, \\(iid\\) means “independent and identically distributed”. Prior predictive simulation (‘what does the model think before seeing the data?’) n_samples &lt;- 1e4 prior_simulation &lt;- tibble( sample_mu = rnorm(n_samples, 178, 20), sample_sigma = runif(n_samples, 0, 50), prior_h = rnorm(n_samples, sample_mu, sample_sigma), bad_mu = rnorm(n_samples, 178, 100), bad_prior = rnorm(n_samples, bad_mu, sample_sigma) ) p_mu &lt;- ggplot() + stat_function(fun = function(x){dnorm(x = x, mean = 178, sd = 20)}, xlim = c(100,250), color = clr0d, fill = fll0, geom = &quot;area&quot;) + labs(title = glue(&quot;*\\U03BC* {mth(&#39;\\U007E&#39;)} dnorm( 178, 20 )&quot;), y = &quot;density&quot;, x = &quot;*\\U03BC*&quot;) p_sigma &lt;- ggplot() + stat_function(fun = function(x){dunif(x = x, min = 0, max = 50)}, xlim = c(-5, 55), color = clr1, fill = fll1, geom = &quot;area&quot;) + labs(title = glue(&quot;*{mth(&#39;\\U03C3&#39;)}* {mth(&#39;\\U007E&#39;)} dunif( 0, 50 )&quot;), y = &quot;density&quot;, x = glue(&quot;*{mth(&#39;\\U03C3&#39;)}*&quot;)) p_prior_sim &lt;- prior_simulation %&gt;% ggplot(aes(x = prior_h)) + geom_density(color = clr2, fill = fll2, adjust = .4) + scale_x_continuous(limits = c(0,356), breaks = c(0,73,178,283)) + labs(title = glue(&quot;*h&lt;sub&gt;i&lt;/sub&gt;* {mth(&#39;\\U007E&#39;)} dnorm( *\\U03BC*, {mth(&#39;\\U03C3&#39;)} )&quot;), x = &quot;height&quot;) p_bad_prior &lt;- prior_simulation %&gt;% ggplot(aes(x = bad_prior)) + geom_density(color = clr2, fill = fll2, adjust = .4) + scale_x_continuous(limits = c(-222,578), breaks = c(-128,0,178,484), expand = c(0,0)) + geom_vline(data = tibble(h = c(0,272)), aes(xintercept = h), linetype = 3)+ labs(title = glue(&quot;*h&lt;sub&gt;i&lt;/sub&gt;* {mth(&#39;\\U007E&#39;)} dnorm( *\\U03BC*, {mth(&#39;\\U03C3&#39;)} )&lt;br&gt;*\\U03BC* {mth(&#39;\\U007E&#39;)} dnorm( 178, 100 )&quot;), x = &quot;height&quot;) p_mu + p_sigma + p_prior_sim + p_bad_prior &amp; theme(plot.title = element_markdown(), axis.title.x = element_markdown()) 5.3.1.3 grid approximation of the posterior distribution n_grid &lt;- 101 grid_data &lt;- cross_df(list(mu = seq(from = 152, to = 157, length.out = n_grid), sigma = seq(from = 6.5, to = 9, length.out = n_grid))) %&gt;% mutate(log_likelihood = map2_dbl(.x = mu, .y = sigma, .f = function(x, y){ dnorm(x = data_adults$height, mean = x, sd = y, log = TRUE) %&gt;% sum() }), prior_mu = dnorm(mu, mean = 178, sd = 20, log = TRUE), prior_sigma = dunif(sigma, min = 0, max = 50, log = TRUE), product = log_likelihood + prior_mu + prior_sigma, probability = exp(product - max(product))) grid_data %&gt;% ggplot(aes(x = mu, y = sigma, z = probability)) + geom_raster(aes(fill = probability)) + geom_contour(color = rgb(1,1,1,.1)) + coord_cartesian(xlim = range(grid_data$mu), ylim = range(grid_data$sigma)) + scale_fill_gradientn(colours = clr_grd5 %&gt;% clr_alpha(alpha = .8), limits = c(0,1)) + coord_cartesian(xlim = range(grid_data$mu), ylim = range(grid_data$sigma), expand = 0) + guides(fill = guide_colorbar(title.position = &quot;top&quot;, barwidth = unit(.9,&quot;npc&quot;), barheight = unit(5, &quot;pt&quot;))) + labs(x = &quot; *\\U03BC*&quot;, y = glue(&quot;*{mth(&#39;\\U03C3&#39;)}*&quot;))+ theme(legend.position = &quot;bottom&quot;, axis.title.x = element_markdown(), axis.title.y = element_markdown()) Sampling from the posterior distribution n_posterior_sample &lt;- 1e4 samples &lt;- grid_data %&gt;% slice_sample(n = n_posterior_sample, weight_by = probability, replace = TRUE) p_samples &lt;- samples %&gt;% group_by(mu, sigma) %&gt;% count() %&gt;% ungroup() %&gt;% ggplot(aes(x = mu, y = sigma, color = n)) + geom_point(size = .4) + scale_color_gradientn(colours = clr_grd5 %&gt;% clr_alpha(alpha = .8)) + coord_cartesian(xlim = buffer_range(grid_data$mu), ylim = buffer_range(grid_data$sigma), expand = 0) + guides(color = guide_colorbar(title.position = &quot;top&quot;, barwidth = unit(.2,&quot;npc&quot;), barheight = unit(5, &quot;pt&quot;))) + labs(x = &quot; *\\U03BC*&quot;, y = glue(&quot;*{mth(&#39;\\U03C3&#39;)}*&quot;))+ theme(legend.position = &quot;bottom&quot;, axis.title.x = element_markdown(), axis.title.y = element_markdown()) p_mu_dens &lt;- samples %&gt;% ggplot(aes(x = mu)) + geom_density(color = clr0d, fill = fll0) + scale_x_continuous(limits = buffer_range(grid_data$mu), expand = c(0, 0)) + labs(y = &quot;marginal&lt;br&gt;density&quot;) + theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.title.y = element_markdown()) p_sigma_dens &lt;- samples %&gt;% ggplot(aes(x = sigma)) + geom_density(color = clr0d, fill = fll0) + scale_x_continuous(limits = buffer_range(grid_data$sigma), expand = c(0, 0)) + labs(y = &quot;marginal density&quot;) + coord_flip() + theme(axis.title.y = element_blank(), axis.text.y = element_blank()) p_mu_dens + patchwork::guide_area() + p_samples + p_sigma_dens + plot_layout(guides = &quot;collect&quot;, widths = c(1,.3), heights = c(.3,1)) Exploration of long tail for \\(\\sigma\\) when original sample size is small: heights_subset &lt;- sample(data_adults$height, size = 20) grid_data_subset &lt;- cross_df(list(mu = seq(from = 145, to = 165, length.out = n_grid), sigma = seq(from = 4.5, to = 16, length.out = n_grid))) %&gt;% mutate(log_likelihood = map2_dbl(.x = mu, .y = sigma, .f = function(x, y){ dnorm(x = heights_subset, mean = x, sd = y, log = TRUE) %&gt;% sum() }), prior_mu = dnorm(mu, mean = 178, sd = 20, log = TRUE), prior_sigma = dunif(sigma, min = 0, max = 50, log = TRUE), product = log_likelihood + prior_mu + prior_sigma, probability = exp(product - max(product))) samples_subset &lt;- grid_data_subset %&gt;% slice_sample(n = n_posterior_sample, weight_by = probability, replace = TRUE) p_samples &lt;- samples_subset %&gt;% group_by(mu, sigma) %&gt;% count() %&gt;% ungroup() %&gt;% ggplot(aes(x = mu, y = sigma, color = n)) + geom_point(size = .4) + scale_color_gradientn(colours = clr_grd4 %&gt;% clr_alpha(alpha = .8)) + coord_cartesian(xlim = buffer_range(grid_data_subset$mu), ylim = buffer_range(grid_data_subset$sigma), expand = 0) + guides(color = guide_colorbar(title.position = &quot;top&quot;, barwidth = unit(.2,&quot;npc&quot;), barheight = unit(5, &quot;pt&quot;))) + labs(x = &quot; *\\U03BC*&quot;, y = glue(&quot;*{mth(&#39;\\U03C3&#39;)}*&quot;))+ theme(legend.position = &quot;bottom&quot;, axis.title.x = element_markdown(), axis.title.y = element_markdown()) p_mu_dens &lt;- samples_subset %&gt;% ggplot(aes(x = mu)) + geom_density(color = clr0d, fill = fll0) + scale_x_continuous(limits = buffer_range(grid_data_subset$mu), expand = c(0, 0)) + labs(y = &quot;marginal&lt;br&gt;density&quot;) + theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.title.y = element_markdown()) p_sigma_dens &lt;- samples_subset %&gt;% ggplot(aes(x = sigma)) + geom_density(color = clr0d, fill = fll0) + scale_x_continuous(limits = buffer_range(grid_data_subset$sigma), expand = c(0, 0)) + labs(y = &quot;marginal density&quot;) + coord_flip() + theme(axis.title.y = element_blank(), axis.text.y = element_blank()) p_mu_dens + patchwork::guide_area() + p_samples + p_sigma_dens + plot_layout(guides = &quot;collect&quot;, widths = c(1,.3), heights = c(.3,1)) 5.3.1.4 Quadratic approximation of the posterior distribution \\[ \\begin{array}{cccr} h_i &amp; \\stackrel{iid}{\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\verb|height ~ dnorm(mu, sigma)|\\\\ \\mu &amp; \\sim &amp; Normal(178, 20) &amp; \\verb|mu ~ dnorm(178, 20)|\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; \\verb|sigma ~ dunif(0, 50)| \\end{array} \\] model_spec &lt;- alist( height ~ dnorm(mu, sigma), mu ~ dnorm(178, 20), sigma ~ dunif(0, 50) ) # &quot;maximum a priori estimate&quot; map_starting_points &lt;- list( mu = mean(data_adults$height), sigma = sd(data_adults$height) ) model_heights_quap_weak_prior &lt;- quap(flist = model_spec, data = data_adults, start = map_starting_points) precis(model_heights_quap_weak_prior) %&gt;% as_tibble(rownames = NA) %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% 154.61 0.41 153.95 155.27 7.73 0.29 7.27 8.20 Comparing how a stronger prior for \\(\\mu\\) (narrower distribution) forces a larger estimate of \\(\\sigma\\) to compensate for this. quap( flist = alist( height ~ dnorm( mu , sigma ), mu ~ dnorm( 178, 0.1 ), sigma ~ dunif( 0, 50 ) ), data = data_adults, start = map_starting_points) %&gt;% precis() %&gt;% as_tibble(rownames = NA) %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% 177.86 0.10 177.70 178.02 24.52 0.93 23.03 26.00 The variance-covariance matrix of the quadratic aprroximation for sampling the multi-dimensional gaussian distribution: vcov_mod_heights &lt;- vcov(model_heights_quap_weak_prior) vcov_mod_heights %&gt;% round(digits = 6) %&gt;% knitr::kable() mu sigma mu 0.169740 0.000218 sigma 0.000218 0.084906 diag(vcov_mod_heights) #&gt; mu sigma #&gt; 0.16973961 0.08490582 round(cov2cor(vcov_mod_heights), digits = 5) \\[\\begin{bmatrix} 1 &amp;0.00182 \\\\0.00182 &amp;1 \\\\ \\end{bmatrix}\\] sampling from the multi-dimensional posterior distribution posterior_sample &lt;- extract.samples(model_heights_quap_weak_prior, n = 1e4) %&gt;% as_tibble() precis(posterior_sample) %&gt;% as_tibble() %&gt;% knitr::kable() mean sd 5.5% 94.5% histogram 154.605300 0.4101637 153.949499 155.265720 ▁▁▅▇▂▁▁ 7.727751 0.2919396 7.256454 8.195782 ▁▁▁▁▂▅▇▇▃▁▁▁ 5.4 Linear Prediction ggplot(data_adults, aes(height, weight)) + geom_point(shape = 21, size = 1.5, color = clr1, fill = fll1) \\[ \\begin{array}{cccr} h_i &amp; {\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta (x_i - \\bar{x}) &amp; \\textrm{[linear model]}\\\\ % alternatively \\overline{x} \\alpha &amp; \\sim &amp; Normal(178, 20) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] The current prior for \\(\\beta\\) is a bad choice, because it allows negative as well as unreasonably high and low dependencies of \\(h\\) (height) on \\(x\\) (weight): set.seed(2971) N &lt;- 100 linear_priors &lt;- tibble(n = 1:N, alpha = rnorm( n = N, mean = 178, sd = 20 ), beta_1 = rnorm( n = N, mean = 0, sd = 10), beta_2 = rlnorm( n = N, mean = 0, sd = 1)) %&gt;% expand(nesting(n, alpha, beta_1, beta_2), weight = range(data_adults$weight)) %&gt;% mutate(height_1 = alpha + beta_1 * (weight - mean(data_adults$weight)), height_2 = alpha + beta_2 * (weight - mean(data_adults$weight))) p_lin_pr &lt;- ggplot(linear_priors, aes(x = weight, y = height_1, group = n)) + labs(title = glue(&quot;{mth(&#39;*\\U03B2* ~&#39;)} Normal(0, 10)&quot;), y = &quot;height&quot;) p_log_pr &lt;- ggplot(linear_priors, aes(x = weight, y = height_2, group = n)) + labs(title = glue(&quot;{mth(&#39;*\\U03B2* ~&#39;)} Log-Normal(0, 1)&quot;), y = &quot;height&quot;) p_lnorm &lt;- ggplot() + stat_function(fun = function(x){dlnorm(x = x, meanlog = 0, sdlog = 1)}, xlim = c(0,5), geom = &quot;area&quot;, color = clr2, fill = fll2, n = 501) + labs(title = &quot;Log-Norm(0, 0.1)&quot;, y = &quot;density&quot;) (p_lin_pr + p_log_pr &amp; geom_hline(data = tibble(height = c(0, 272), type = 1:2), aes(yintercept = height, linetype = factor(type)), size = .4) &amp; geom_line(color = clr2, alpha = .25) &amp; scale_linetype_manual(values = c(`1` = 3, `2` = 1), guide = &quot;none&quot;) &amp; coord_cartesian(xlim = range(data_adults$weight), ylim = c(-100, 400)) &amp; theme(plot.title = element_markdown())) + p_lnorm The log-normal prior seems more sensible, so we update the model priors as such: \\[ \\begin{array}{cccr} \\beta &amp; \\sim &amp; Log-Normal(0, 1) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\end{array} \\] 5.4.1 Finding the posterior Distribution xbar &lt;- mean(data_adults$weight) model_hight &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * ( weight - xbar ), alpha ~ dnorm( 178, 20 ), beta ~ dlnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_adults ) Table of marginal distributions of the parameters after training the model on the data centered_remember_hw &lt;- precis(model_hight) %&gt;% round(digits = 3) %&gt;% as_tibble(rownames = NA) centered_remember_hw %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 154.602 0.270 154.170 155.034 beta 0.903 0.042 0.836 0.970 sigma 5.072 0.191 4.766 5.377 We also need thevariance-covariance matrix to fully describe the audratic approximation completely: model_hight %&gt;% vcov() %&gt;% round(digits = 2) %&gt;% as.data.frame(row.names = row.names(.)) %&gt;% knitr::kable() alpha beta sigma alpha 0.07 0 0.00 beta 0.00 0 0.00 sigma 0.00 0 0.04 model_hight_smp &lt;- extract.samples(model_hight) %&gt;% as_tibble() model_hight_smp_mean &lt;- model_hight_smp %&gt;% summarise(across(.cols = everything(), mean)) model_hight_smp %&gt;% ggpairs( lower = list(continuous = wrap(ggally_points, colour = clr1, size = .2, alpha = .1)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll1, color = clr1, adjust = .5)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) Plotting the posterior distribution against the data ggplot(data_adults, aes(x = weight, y = height)) + geom_point(color = clr0d) + stat_function(fun = function(x){model_hight_smp_mean$alpha + model_hight_smp_mean$beta * (x - xbar)}, color = clr2, n = 2) A demonstration of the the effect of sample size on the uncertainty of the linear fit sub_model &lt;- function(N = 10){ data_inner &lt;- data_adults[1:N,] xbar &lt;- mean(data_inner$weight) model_hight_inner &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * ( weight - xbar ), alpha ~ dnorm( 178, 20 ), beta ~ dlnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_inner ) model_hight_smp_inner &lt;- extract.samples(model_hight_inner) %&gt;% as_tibble() %&gt;% sample_n(20) ggplot(data_inner, aes(x = weight, y = height)) + geom_point(color = clr0d) + (purrr::map(1:20, function(i){stat_function( fun = function(x){model_hight_smp_inner$alpha[i] + model_hight_smp_inner$beta[i] * (x - xbar)}, color = clr2, n = 2, alpha = .1)})) + labs(title = glue(&quot;N: {N}&quot;)) } sub_model(10) + sub_model(50) + sub_model(150) + sub_model(352) adding intervals mu_at_50 &lt;- model_hight_smp %&gt;% mutate(mu_at_50 = alpha + beta * (50 - xbar)) p_density &lt;- mu_at_50 %&gt;% ggplot(aes(x = mu_at_50)) + geom_density(adjust = .5, color = clr0d, fill = fll0) + stat_function(fun = function(x){demp(x, obs = mu_at_50$mu_at_50, density.arg.list = list(adjust = .5))}, xlim = mu_at_50$mu_at_50 %&gt;% PI(), geom = &quot;area&quot;, fill = fll2, color = clr2) + geom_vline(data = tibble(weights = mu_at_50$mu_at_50 %&gt;% PI()), aes(xintercept = weights), linetype = 3)+ scale_x_continuous(glue(&quot;{mth(&#39;*\\U03BC*&#39;)} | weight = 50&quot;), limits = c(157.7, 160.8)) + theme(axis.title.x = element_markdown()) mu_at_50$mu_at_50 %&gt;% PI() #&gt; 5% 94% #&gt; 158.5857 159.6717 weight_seq &lt;- seq(from = 25, to = 70, by = 1) model_hight_mu &lt;- link(model_hight, data = data.frame(weight = weight_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) p_dots &lt;- model_hight_mu %&gt;% ggplot(aes(x = weight, y = height)) + geom_point(aes(color = weight == 50), alpha = .1, size = .3) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) model_hight_mu_interval &lt;- model_hight_mu %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() p_interval &lt;- model_hight_mu_interval %&gt;% ggplot(aes(x = weight)) + geom_point(data = data_adults, aes(y = height), color = clr0, size = .3) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr2, alpha = .35) + geom_line(aes(y = mean)) p_density + p_dots + p_interval Prediction intervals model_hight_sd &lt;- sim(model_hight, data = data.frame(weight = weight_seq), n = 1e4) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq) %&gt;% pivot_longer(cols = `25`:`70`, names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) model_hight_sd %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = weight)) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data_adults, aes(y = height), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = model_hight_mu_interval, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_hight_mu_interval, aes(y = mean)) 5.5 Curves from lines The full data (including kids) is clearly curved in shape: ggplot(data = data, aes(x = weight, y = height)) + geom_point(color = clr0d) We will work on standardized \\(x\\) values to prevent “numerical glitches” by transforming \\(x\\) via \\(x_s = (\\frac{x - \\bar{x}}{sd(x)})\\): quadratic polynomial fit \\[ \\begin{array}{cccr} h_i &amp; {\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_1 x_i + \\beta_2 x_i ^ 2&amp; \\textrm{[linear model]}\\\\ % alternatively \\overline{x} \\alpha &amp; \\sim &amp; Normal(178, 20) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_1 &amp; \\sim &amp; Log-Normal(0, 1) &amp; \\textrm{[$\\beta_1$ prior]}\\\\ \\beta_2 &amp; \\sim &amp; Normal(0, 1) &amp; \\textrm{[$\\beta_2$ prior]}\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] cubic polynomial fit \\[ \\begin{array}{cccr} h_i &amp; {\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_1 x_i + \\beta_2 x_i ^ 2 + \\beta_3 x_i ^ 3 &amp; \\textrm{[linear model]}\\\\ % alternatively \\overline{x} \\alpha &amp; \\sim &amp; Normal(178, 20) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_1 &amp; \\sim &amp; Log-Normal(0, 1) &amp; \\textrm{[$\\beta_1$ prior]}\\\\ \\beta_2 &amp; \\sim &amp; Normal(0, 1) &amp; \\textrm{[$\\beta_2$ prior]}\\\\ \\beta_3 &amp; \\sim &amp; Normal(0, 1) &amp; \\textrm{[$\\beta_3$ prior]}\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] plot_model_intervals &lt;- function(mod, data, weight_seq = list(weight_s = seq(from = min(data_model$weight_s), to = max(data_model$weight_s), length.out = 70))){ model_hight_mu_interval &lt;- link(mod, data = weight_seq) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq$weight_s ) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight_s&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight_s = as.numeric(weight_s)) %&gt;% group_by(weight_s) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() model_hight_sd &lt;- sim(mod, data = weight_seq, n = 1e4) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq$weight_s) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight_s&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight_s = as.numeric(weight_s)) model_hight_sd %&gt;% group_by(weight_s) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = weight_s)) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data, aes(y = height), color = rgb(0,0,0,.25), size = .4) + geom_ribbon(data = model_hight_mu_interval, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_hight_mu_interval, aes(y = mean)) } data_model &lt;- data %&gt;% mutate(weight_s = (weight - mean(weight))/sd(weight), weight_s2 = weight_s ^ 2, weight_s3 = weight_s ^ 3) model_hight_s1 &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * weight_s , alpha ~ dnorm( 178, 20 ), beta ~ dlnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_model ) model_hight_s2 &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta1 * weight_s + beta2 * weight_s2, alpha ~ dnorm( 178, 20 ), beta1 ~ dlnorm( 0, 1 ), beta2 ~ dnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_model ) model_hight_s3 &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta1 * weight_s + beta2 * weight_s2 + beta3 * weight_s3, alpha ~ dnorm( 178, 20 ), beta1 ~ dlnorm( 0, 1 ), beta2 ~ dnorm( 0, 1 ), beta3 ~ dnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_model ) plot_model_intervals(model_hight_s1, data_model) + plot_model_intervals(model_hight_s2, data_model, weight_seq = tibble(weight_s = seq(from = min(data_model$weight_s), to = max(data_model$weight_s), length.out = 70), weight_s2 = weight_s ^ 2)) + plot_model_intervals(model_hight_s3, data_model, weight_seq = tibble(weight_s = seq(from = min(data_model$weight_s), to = max(data_model$weight_s), length.out = 70), weight_s2 = weight_s ^ 2, weight_s3 = weight_s ^ 3)) plot_model_intervals(model_hight_s3, data_model, weight_seq = tibble(weight_s = seq(from = min(data_model$weight_s), to = max(data_model$weight_s), length.out = 70), weight_s2 = weight_s ^ 2, weight_s3 = weight_s ^ 3)) + scale_x_continuous(&quot;weight [kg]&quot;, breaks = (seq(5,65, length.out = 5) - mean(data_model$weight)) / sd(data_model$weight), labels = seq(5,65, length.out = 5)) + labs(y = &quot;height [cm]&quot;) 5.6 Splines Loading the Hanami data (花見), containing the historical dates of first annual cherry tree blossom. data(cherry_blossoms) precis(cherry_blossoms) %&gt;% as_tibble() %&gt;% knitr::kable() mean sd 5.5% 94.5% histogram 1408.000000 350.8845964 867.77000 1948.23000 ▇▇▇▇▇▇▇▇▇▇▇▇▁ 104.540508 6.4070362 94.43000 115.00000 ▁▂▅▇▇▃▁▁ 6.141886 0.6636479 5.15000 7.29470 ▁▃▅▇▃▂▁▁ 7.185151 0.9929206 5.89765 8.90235 ▁▂▅▇▇▅▂▂▁▁▁▁▁▁▁ 5.098941 0.8503496 3.78765 6.37000 ▁▁▁▁▁▁▁▃▅▇▃▂▁▁▁ cherry_blossoms %&gt;% ggplot(aes(x = year, y = doy)) + geom_point(color = clr2, alpha = .3) + labs(y = &quot;Day of first blossom&quot;) data_cherry &lt;- cherry_blossoms %&gt;% filter(complete.cases(doy)) %&gt;% as_tibble() n_knots &lt;- 15 knot_list &lt;- quantile(data_cherry$year, probs = seq(0, 1, length.out = n_knots)) library(splines) b_spline_cherry &lt;- bs(data_cherry$year, knots = knot_list[-c(1, n_knots)], degree = 3, intercept = TRUE) b_spline_tib &lt;- b_spline_cherry %&gt;% as_tibble() %&gt;% set_names(nm = str_pad(1:17, width = 2, pad = 0)) %&gt;% bind_cols(select(data_cherry, year)) %&gt;% pivot_longer(cols = -year, names_to = &quot;bias_function&quot;, values_to = &quot;bias&quot;) ggplot() + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = &quot;black&quot;) + geom_line(data = b_spline_tib, aes(x = year, y = bias, color = as.numeric(bias_function) , group = bias_function), size = 1, alpha = .75) + scale_color_gradientn(colours = c(&quot;black&quot;, clr0d, clr2), guide = &quot;none&quot;)+ theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) B-spline model: $$ \\[\\begin{array}{cccr} D_i &amp; \\sim &amp; Normal( \\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\sum_{k=1}^K w_k B_{k,i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(100, 10) &amp; \\textrm{[$\\alpha$ prior]}\\\\ w_i &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[w prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array}\\] $$ model_cherry &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- a + B %*% w, a ~ dnorm(100, 10), w ~ dnorm(0, 10), sigma ~ dexp(1) ), data = list(D = data_cherry$doy, B = b_spline_cherry), start = list(w = rep(0, ncol(b_spline_cherry))) ) precis(model_cherry, depth = 2) %&gt;% round(digits = 2) %&gt;% as_tibble(rownames = NA) %&gt;% knitr::kable() mean sd 5.5% 94.5% w[1] -3.02 3.86 -9.19 3.15 w[2] -0.83 3.87 -7.01 5.36 w[3] -1.06 3.58 -6.79 4.67 w[4] 4.85 2.88 0.25 9.44 w[5] -0.84 2.87 -5.43 3.76 w[6] 4.32 2.91 -0.33 8.98 w[7] -5.32 2.80 -9.79 -0.84 w[8] 7.85 2.80 3.37 12.33 w[9] -1.00 2.88 -5.61 3.60 w[10] 3.04 2.91 -1.61 7.69 w[11] 4.67 2.89 0.05 9.29 w[12] -0.15 2.87 -4.74 4.43 w[13] 5.56 2.89 0.95 10.18 w[14] 0.72 3.00 -4.08 5.51 w[15] -0.80 3.29 -6.06 4.46 w[16] -6.96 3.38 -12.36 -1.57 w[17] -7.67 3.22 -12.82 -2.52 a 103.35 2.37 99.56 107.13 sigma 5.88 0.14 5.65 6.11 cherry_samples &lt;- extract.samples(model_cherry) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;a&quot;, &quot;sigma&quot;, str_pad(1:17, 2,pad = 0))) cherry_samples_mu &lt;- cherry_samples %&gt;% summarise(across(everything(), mean)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;bias_function&quot;, values_to =&quot;weight&quot;) ggplot() + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = &quot;black&quot;) + geom_line(data = b_spline_tib %&gt;% left_join(cherry_samples_mu), aes(x = year, y = bias * weight, color = as.numeric(bias_function) , group = bias_function), size = 1, alpha = .75) + scale_color_gradientn(colours = c(&quot;black&quot;, clr0d, clr2), guide = &quot;none&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) model_cherry_samples &lt;- link(model_cherry) %&gt;% as_tibble()%&gt;% set_names(nm = data_cherry$year) %&gt;% pivot_longer(cols = everything(), names_to = &quot;year&quot;, values_to = &quot;doy&quot;) %&gt;% mutate(year = as.numeric(year)) %&gt;% arrange(year) model_cherry_stats &lt;- model_cherry_samples %&gt;% group_by(year) %&gt;% nest() %&gt;% mutate(mean = map_dbl(data, function(data){mean(data$doy)}), PI_lower = map_dbl(data, function(data){PI(data$doy)[1]}), PI_upper = map_dbl(data, function(data){PI(data$doy)[2]})) model_cherry_stats %&gt;% ggplot(aes(x = year)) + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = &quot;black&quot;) + geom_point(data = cherry_blossoms, aes(y = doy), color = clr2, alpha = .3) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr2, alpha = .35) + geom_line(aes(y = mean)) + labs(y = &quot;Day of first blossom&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) 5.7 Homework E1 \\[ \\begin{array}{cccr} y_i &amp; \\sim &amp; Normal( \\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\mu$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] E2 There are two parameters: \\(\\mu\\) \\(\\sigma\\) E3 \\[ \\begin{array}{rcl} Pr( \\mu, \\sigma | y ) &amp; = &amp; \\frac{Normal( y | \\mu, \\sigma ) Pr(y)}{Pr(\\mu, \\sigma)} \\\\ Pr( \\mu, \\sigma | y ) &amp; = &amp; \\frac{\\prod_i Pr( y_i | \\mu, \\sigma) Normal( \\mu | 0, 10) Exponential(\\sigma | 1) }{ \\int\\int\\prod_i Pr( y_i | \\mu, \\sigma) Normal( \\mu | 0, 10) Exponential(\\sigma | 1)d\\mu d\\sigma} \\end{array} \\] E4 \\[ \\begin{array}{cccr} y_i &amp; \\sim &amp; Normal( \\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta x_i &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta &amp; \\sim &amp; Normal(0, 1) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(2) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] E5 There are three parameters \\(\\alpha\\) \\(\\beta\\) \\(\\sigma\\) M1 \\[ \\begin{array}{cccr} y_i &amp; \\sim &amp; Normal( \\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\mu$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] Using grid approximation n &lt;- 5e3 sample_data &lt;- tibble(y = rnorm(n = n, # sample size mean = rnorm(n = n, mean = 0, sd = 10), # mu prior sd = rexp(n = n, rate = 1))) # sigma prior sample_data %&gt;% ggplot(aes(x = y)) + geom_density(color = clr0d, fill = fll0) + stat_function(fun = function(x){dnorm(x = x, mean = 0, sd = 10 )}, geom = &quot;line&quot;, linetype = 3, color = clr1) + scale_x_continuous(limits = c(-50, 50)) + labs(y = &quot;density&quot;) M2 quap_formula &lt;- alist( y ~ dnorm(mu, sigma), # likelihood mu ~ dnorm(0, 10), # mu prior sigma ~ exp(1) # sigma prior ) M3 \\[ \\begin{array}{cccr} y_i &amp; \\sim &amp; Normal( \\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta x_i &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta &amp; \\sim &amp; Uniform(0, 1) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] M4 \\[ \\begin{array}{cccr} h_i &amp; \\sim &amp; Normal( \\mu, \\sigma) &amp; \\textrm{[likelihood]} \\\\ \\mu_i &amp; = &amp; \\alpha + \\beta h_i &amp; \\textrm{[linear model]} \\\\ \\alpha &amp; \\sim &amp; Normal(150, 5) &amp; \\textrm{[$\\alpha$ prior, starting size]} \\\\ \\beta &amp; \\sim &amp; Uniform(0, 10) &amp; \\textrm{[$\\beta$ prior, yearly growth]} \\\\ \\sigma &amp; \\sim &amp; Normal(0, 8) &amp; \\textrm{[$\\sigma$ prior, size variation]} \\end{array} \\] M5 No, the chosen prior for \\(\\beta\\) already covers this information: \\(\\beta \\sim Uniform(0, 10)\\) is always positive, forcing a positive growth per year. M6 Limiting the variance of height to 64cm could be done in different ways: by choosing a uniform prior with fixed boundaries [eg. \\(Uniform(0,64)\\)], or by limiting the variance of an unbound distribution [eg. \\(\\sigma\\) for a normal distribution. 99.7% of the mass is within \\(3 \\sigma\\), so \\(Normal(32, 10)\\) would do as well]. M7 model_uncentered &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * weight, alpha ~ dnorm( 178, 20), beta ~ dlnorm( 0, 1 ), sigma ~ dunif( 0, 50 ) ), data = data_adults ) precis(model_uncentered) %&gt;% round(digits = 3) %&gt;% as_tibble(rownames = NA) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 114.534 1.898 111.501 117.567 beta 0.891 0.042 0.824 0.957 sigma 5.073 0.191 4.767 5.378 model_uncentered %&gt;% vcov() %&gt;% round(digits = 2) %&gt;% as.data.frame(row.names = row.names(.)) %&gt;% knitr::kable() alpha beta sigma alpha 3.60 -0.08 0.01 beta -0.08 0.00 0.00 sigma 0.01 0.00 0.04 compare to the centered version: centered_remember_hw %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 154.602 0.270 154.170 155.034 beta 0.903 0.042 0.836 0.970 sigma 5.072 0.191 4.766 5.377 model_hight %&gt;% vcov() %&gt;% round(digits = 2) %&gt;% as.data.frame(row.names = row.names(.)) %&gt;% knitr::kable() alpha beta sigma alpha 0.07 0 0.00 beta 0.00 0 0.00 sigma 0.00 0 0.04 The un-centered model shows higher covariances between \\(\\alpha\\) and all other parameters. model_uncentered_mu &lt;- link(model_uncentered, data = data.frame(weight = weight_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) model_uncentered_mu_interval &lt;- model_uncentered_mu %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() model_uncentered_sd &lt;- sim(model_uncentered, data = data.frame(weight = weight_seq), n = 5e3) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq) %&gt;% pivot_longer(cols = `25`:`70`, names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) p_1 &lt;- model_uncentered_sd %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = weight)) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data_adults, aes(y = height), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = model_uncentered_mu_interval, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_uncentered_mu_interval, aes(y = mean))+ labs(title = &quot; uncentered&quot;) p_2 &lt;- model_hight_sd %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = weight)) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data_adults, aes(y = height), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = model_hight_mu_interval, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_hight_mu_interval, aes(y = mean))+ labs(title = &quot; centered&quot;) p_1 + p_2 Hmm 🤔`: I can’t see a difference - maybe that is the point? M8 spline_check &lt;- function(n_knots = 15, inner = TRUE){ knot_list &lt;- quantile(data_cherry$year, probs = seq(0, 1, length.out = n_knots)) b_spline_cherry &lt;- bs(data_cherry$year, knots = knot_list[-c(1, n_knots)], degree = 3, intercept = TRUE) b_spline_tib &lt;- b_spline_cherry %&gt;% as_tibble() %&gt;% set_names(nm = str_pad(1:(n_knots+2), width = 2, pad = 0)) %&gt;% bind_cols(select(data_cherry, year)) %&gt;% pivot_longer(cols = -year, names_to = &quot;bias_function&quot;, values_to = &quot;bias&quot;) model_cherry &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- a + B %*% w, a ~ dnorm(100, prior_sd_a), w ~ dnorm(0, prior_sd_w), sigma ~ dexp(1) ), data = list(D = data_cherry$doy, B = b_spline_cherry), start = list(w = rep(0, ncol(b_spline_cherry))) ) cherry_samples &lt;- extract.samples(model_cherry) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;a&quot;, &quot;sigma&quot;, str_pad(1:(n_knots+2), 2,pad = 0))) cherry_samples_mu &lt;- cherry_samples %&gt;% summarise(across(everything(), mean)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;bias_function&quot;, values_to =&quot;weight&quot;) model_cherry_samples &lt;- link(model_cherry) %&gt;% as_tibble()%&gt;% set_names(nm = data_cherry$year) %&gt;% pivot_longer(cols = everything(), names_to = &quot;year&quot;, values_to = &quot;doy&quot;) %&gt;% mutate(year = as.numeric(year)) %&gt;% arrange(year) model_cherry_stats &lt;- model_cherry_samples %&gt;% group_by(year) %&gt;% nest() %&gt;% mutate(mean = map_dbl(data, function(data){mean(data$doy)}), PI_lower = map_dbl(data, function(data){PI(data$doy)[1]}), PI_upper = map_dbl(data, function(data){PI(data$doy)[2]})) p_splines_pure &lt;- ggplot() + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = rgb(0,0,0,.5)) + geom_line(data = b_spline_tib %&gt;% left_join(cherry_samples_mu), aes(x = year, y = bias * weight, color = as.numeric(bias_function) , group = bias_function), size = .3, alpha = .75) + scale_color_gradientn(colours = c(&quot;black&quot;, clr0d, clr1), guide = &quot;none&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.title.x = element_blank(), axis.text.x = element_blank()) + labs(title = glue(&quot;{n_knots} kn, sd a: {prior_sd_a}, sd w: {prior_sd_w}&quot;)) p_splines_fitted &lt;- model_cherry_stats %&gt;% ggplot(aes(x = year)) + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = rgb(0,0,0,.5)) + geom_point(data = cherry_blossoms, aes(y = doy), color = clr1, alpha = .1, size = .2) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .65) + geom_line(aes(y = mean)) + labs(y = &quot;Day of first blossom&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), axis.title.x = element_blank(), axis.text.x = element_blank()) if(inner){ p_splines_pure &lt;- p_splines_pure + theme(axis.title.y = element_blank()) p_splines_fitted &lt;- p_splines_fitted + theme(axis.title.y = element_blank()) } p_splines_pure + p_splines_fitted + plot_layout(ncol = 1, heights = c(.5, 1)) } set.seed(14) prior_sd_a = 10 prior_sd_w &lt;- 10 p1 &lt;- spline_check(n_knots = 3, inner = FALSE) prior_sd_w &lt;- 50 p2 &lt;- spline_check(n_knots = 3, inner = TRUE) set.seed(42) prior_sd_w &lt;- 100 p3 &lt;- spline_check(n_knots = 3, inner = TRUE) p1 | p2 | p3 set.seed(41) prior_sd_w &lt;- 10 p1 &lt;- spline_check(n_knots = 10, inner = FALSE) prior_sd_w &lt;- 50 p2 &lt;- spline_check(n_knots = 10, inner = TRUE) prior_sd_w &lt;- 100 p3 &lt;- spline_check(n_knots = 10, inner = TRUE) p1 | p2 | p3 set.seed(42) prior_sd_w &lt;- 10 p1 &lt;- spline_check(n_knots = 30, inner = FALSE) prior_sd_w &lt;- 50 p2 &lt;- spline_check(n_knots = 30, inner = TRUE) prior_sd_w &lt;- 100 p3 &lt;- spline_check(n_knots = 30, inner = TRUE) p1 | p2 | p3 They control the division of data and the initial scale for the weighting H1 model_hight_smp %&gt;% mutate(mu_at_50 = alpha + beta * (50 - xbar), `46.95` = alpha + beta * (46.95 - xbar), `43.72` = alpha + beta * (43.72 - xbar), `64.78` = alpha + beta * (64.78 - xbar), `32.59` = alpha + beta * (32.59 - xbar), `54.63` = alpha + beta * (54.63 - xbar)) %&gt;% dplyr::select(`46.95`:`54.63` ) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% group_by(weight) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(median_weight = map_dbl(data, function(x){median(x$height)}), mean_weight = map_dbl(data, function(x){mean(x$height)}), lower_89 = map_dbl(data, function(x){PI(x$height)[[1]]}), upper_89 = map_dbl(data, function(x){PI(x$height)[[2]]}), individual = 1:5) %&gt;% dplyr::select(individual, weight,median_weight:upper_89) %&gt;% mutate(across(everything(), .fns = ~ round(as.numeric(.x), digits = 5))) %&gt;% knitr::kable() individual weight median_weight mean_weight lower_89 upper_89 1 46.95 156.3742 156.3739 155.9302 156.8173 2 43.72 153.4580 153.4585 153.0147 153.8970 3 64.78 172.4641 172.4671 171.0703 173.8520 4 32.59 143.4179 143.4127 142.4567 144.3474 5 54.63 163.3039 163.3058 162.5306 164.0879 H2 data_children &lt;- data %&gt;% filter(age &lt; 18) ggplot(data_children, aes(weight, height)) + geom_point(shape = 21, size = 1.5, color = clr1, fill = fll1) xbar_children &lt;- mean(data_children$weight) model_hight_children &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * ( weight - xbar_children ), alpha ~ dnorm( 178, 20 ), beta ~ dlnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_children ) precis(model_hight_children) %&gt;% round(digits = 3) %&gt;% as_tibble(rownames = NA) #&gt; # A tibble: 3 x 4 #&gt; mean sd `5.5%` `94.5%` #&gt; * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 108. 0.609 107. 109. #&gt; 2 2.72 0.068 2.61 2.83 #&gt; 3 8.44 0.431 7.75 9.13 model_hight_children %&gt;% vcov() %&gt;% cov2cor() %&gt;% round(digits = 2) %&gt;% as.data.frame(row.names = row.names(.)) %&gt;% knitr::kable() alpha beta sigma alpha 1.00 0.00 0.01 beta 0.00 1.00 -0.01 sigma 0.01 -0.01 1.00 model_hight_smp_children &lt;- extract.samples(model_hight_children) %&gt;% as_tibble() model_hight_smp_mean_children &lt;- model_hight_smp_children %&gt;% summarise(across(.cols = everything(), mean)) ggplot(data_children, aes(x = weight, y = height)) + geom_point(color = clr0d) + stat_function(fun = function(x){model_hight_smp_mean_children$alpha + model_hight_smp_mean_children$beta * (x - xbar_children)}, color = clr2, n = 2) model_hight_smp_mean_children %&gt;% knitr::kable() alpha beta sigma 108.3763 2.716742 8.44416 A child get 27.1674225128912 cm taller per 10 kg weight. weight_seq_children &lt;- seq(from = 2, to = 45, by = 1) model_hight_mu_children &lt;- link(model_hight_children, data = data.frame(weight = weight_seq_children)) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq_children) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) model_hight_mu_interval_children &lt;- model_hight_mu_children %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() model_hight_sd_children &lt;- sim(model_hight_children, data = data.frame(weight = weight_seq_children), n = 1e4) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq_children) %&gt;% pivot_longer(cols = `2`:`45`, names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight = as.numeric(weight)) model_hight_sd_children %&gt;% group_by(weight) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = weight)) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data_children, aes(y = height), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = model_hight_mu_interval_children, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_hight_mu_interval_children, aes(y = mean)) The model seems to be systematically overestimate the height for the more extreme weights (very light and rather heavy). The relationship does not appear to be linear in the first place, so a non-lnear fit would be better - ideally one that is biologically motivated. H3 data_log &lt;- data %&gt;% mutate(weight_log = log10(weight)) xbar_log &lt;- mean(data_log$weight_log) model_hight_log &lt;- quap( flist = alist( height ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * ( weight_log - xbar_log ), alpha ~ dnorm( 179, 20 ), beta ~ dlnorm( 0, 1 ), sigma ~ dunif( 0, 50) ), data = data_log ) precis(model_hight_log) %&gt;% round(digits = 3) %&gt;% as_tibble(rownames = NA) #&gt; # A tibble: 3 x 4 #&gt; mean sd `5.5%` `94.5%` #&gt; * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 138. 0.22 138. 139. #&gt; 2 108. 0.881 107. 110. #&gt; 3 5.14 0.156 4.89 5.38 model_hight_log %&gt;% vcov() %&gt;% cov2cor() %&gt;% round(digits = 2) %&gt;% as.data.frame(row.names = row.names(.)) %&gt;% knitr::kable() alpha beta sigma alpha 1 0 0 beta 0 1 0 sigma 0 0 1 model_hight_smp_log &lt;- extract.samples(model_hight_log) %&gt;% as_tibble() model_hight_smp_mean_log &lt;- model_hight_smp_log %&gt;% summarise(across(.cols = everything(), mean)) ggplot(data_log, aes(x = weight_log, y = height)) + geom_point(color = clr0d) + stat_function(fun = function(x){model_hight_smp_mean_log$alpha + model_hight_smp_mean_log$beta * (x - xbar_log)}, color = clr2, n = 2) weight_seq_log &lt;- log10(seq(from = 2, to = 70, by = 1)) model_hight_mu_log &lt;- link(model_hight_log, data = data.frame(weight_log = weight_seq_log)) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq_log) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight_log&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight_log = as.numeric(weight_log)) model_hight_mu_interval_log &lt;- model_hight_mu_log %&gt;% group_by(weight_log) %&gt;% summarise(mean = mean(height), PI_lower = PI(height, prob = .97)[1], PI_upper = PI(height, prob = .97)[2]) %&gt;% ungroup() model_hight_sd_log &lt;- sim(model_hight_log, data = data.frame(weight_log = weight_seq_log), n = 1e4) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq_log) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight_log&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight_log = as.numeric(weight_log)) model_hight_sd_log %&gt;% group_by(weight_log) %&gt;% summarise(mean = mean(height), PI_lower = PI(height, prob = .97)[1], PI_upper = PI(height, prob = .97)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = 10^(weight_log))) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data, aes(x = weight, y = height), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = model_hight_mu_interval_log, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_hight_mu_interval_log, aes(y = mean), linetype = 3) H4 N &lt;- 25 cubic_priors &lt;- tibble(n = 1:N, alpha = rnorm( n = N, mean = -128, sd = 20 ), beta_1 = rnorm( n = N, mean = 11, sd = .1), beta_2 = rnorm( n = N, mean = -.1, sd = .01)) %&gt;% expand(nesting(n, alpha, beta_1, beta_2), weight = range(data_adults$weight)) %&gt;% mutate(height = alpha + beta_1 * (weight - mean(data_adults$weight)) + beta_2 * (weight - mean(data_adults$weight))^2) ggplot(cubic_priors, aes(x = weight, y = height, group = n)) + pmap(cubic_priors, function(alpha, beta_1, beta_2, ...){ stat_function(fun = function(x){alpha + beta_1 * x + beta_2 * x^2}, color = fll1, alpha = .1, n = 100, lwd = .2, geom = &quot;line&quot;) }) + geom_hline(data = tibble(height = c(0, 272), type = 1:2), aes(yintercept = height, linetype = factor(type)), size = .4) + scale_linetype_manual(values = c(`1` = 3, `2` = 1), guide = &quot;none&quot;) + coord_cartesian(xlim = range(data_adults$weight), ylim = c(-100, 400)) + theme(plot.title = element_markdown()) H5 cherry_blossoms_tib &lt;- cherry_blossoms %&gt;% as_tibble() %&gt;% filter(!is.na(temp) &amp; !is.na(doy)) %&gt;% mutate(temp_s = (temp - mean(temp, na.rm = TRUE))/sd(temp, na.rm = TRUE)) temp_bar &lt;- mean(cherry_blossoms_tib$temp, na.rm = TRUE) temp_sd &lt;- sd(cherry_blossoms_tib$temp, na.rm = TRUE) cherry_blossoms_tib %&gt;% as_tibble() %&gt;% ggplot(aes(x = temp, y = doy)) + geom_point(size = 1.2, color = fll2) \\[ \\begin{array}{cccr} d_i &amp; \\sim &amp; Normal( \\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta x_i &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(105, 10) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_temp &lt;- quap( flist = alist( doy ~ dnorm( mu, sigma ), mu &lt;- alpha + beta * temp_s, alpha ~ dnorm( 105, 10 ), beta ~ dnorm( 0, 10 ), sigma ~ dexp( 1 ) ), data = cherry_blossoms_tib ) temp_seq &lt;- (seq(from = 4.5, to = 8.4, by = .1) - temp_bar) / temp_sd model_temp_mu &lt;- link(model_temp, data = data.frame(temp_s = temp_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = temp_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;temp_s&quot;, values_to = &quot;doy&quot;) %&gt;% mutate(temp_s = as.numeric(temp_s), temp = temp_s * temp_sd + temp_bar) model_temp_mu_interval &lt;- model_temp_mu %&gt;% group_by(temp) %&gt;% summarise(mean = mean(doy), PI_lower = PI(doy)[1], PI_upper = PI(doy)[2]) %&gt;% ungroup() model_temp_sd &lt;- sim(model_temp, data = data.frame(temp_s = temp_seq), n = 1e4) %&gt;% as_tibble() %&gt;% set_names(nm = temp_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;temp_s&quot;, values_to = &quot;doy&quot;) %&gt;% mutate(temp_s = as.numeric(temp_s), temp = temp_s * temp_sd + temp_bar) model_temp_sd %&gt;% group_by(temp) %&gt;% summarise(mean = mean(doy), PI_lower = PI(doy)[1], PI_upper = PI(doy)[2]) %&gt;% ungroup() %&gt;% ggplot(aes(x = temp)) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = cherry_blossoms_tib, aes(y = doy), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = model_temp_mu_interval, aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(data = model_temp_mu_interval, aes(y = mean)) H6 n_knots &lt;- 15 knot_list &lt;- quantile(data_cherry$year, probs = seq(0, 1, length.out = n_knots)) b_spline_cherry &lt;- bs(data_cherry$year, knots = knot_list[-c(1, n_knots)], degree = 3, intercept = TRUE) prior_predictive &lt;- function(n = 100, prior_w = function(knots){rnorm(n = knots + 2, 0, 10)}){ tibble(.draw = 1:n, alpha = rnorm(n, 100, 10), w = purrr::map(seq_len(n), # random weighting of knots function(x, knots){ w &lt;- prior_w(knots) w }, knots = n_knots)) %&gt;% mutate(mu = purrr::map2(alpha, w, .f = function(alpha, w, b){ mu &lt;- alpha + b %*% w mu %&gt;% as_tibble(.name_repair = ~&quot;mu&quot;) %&gt;% mutate(year = data_cherry$year, .before = 1) }, b = b_spline_cherry)) %&gt;% unnest(cols = mu) } p1 &lt;- prior_predictive(n = 50) %&gt;% ggplot(aes(x = year, y = mu)) + labs(title = &quot;weight sd: 10&quot;) p2 &lt;- prior_predictive(n = 50, prior_w = function(knots){rnorm(n = knots + 2, mean = 0, sd = .3)}) %&gt;% ggplot(aes(x = year, y = mu)) + labs(title = &quot;weight sd: 0.3&quot;) p1 + p2 &amp; geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = &quot;black&quot;) &amp; geom_line(aes(group = .draw, color = .draw), alpha = .2) &amp; scale_color_gradientn(colours = c(&quot;black&quot;, clr0d, clr2), guide = &quot;none&quot;) &amp; theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) H7 This one is missing… H8 set.seed(42) model_cherry2 &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- B %*% w, w ~ dnorm(0, 10), sigma ~ dexp(1) ), data = list(D = data_cherry$doy, B = b_spline_cherry), start = list(w = rep(0, ncol(b_spline_cherry))), control = list(maxit = 5000) ) precis(model_cherry2, depth = 2) %&gt;% round(digits = 2) %&gt;% as_tibble(rownames = NA) %&gt;% knitr::kable() mean sd 5.5% 94.5% w[1] 92.84 3.22 87.69 97.98 w[2] 102.76 3.09 97.83 107.69 w[3] 100.43 2.75 96.03 104.83 w[4] 108.34 1.64 105.71 110.96 w[5] 101.56 1.68 98.88 104.24 w[6] 106.97 1.74 104.19 109.75 w[7] 97.56 1.53 95.12 99.99 w[8] 110.64 1.53 108.19 113.08 w[9] 101.68 1.68 99.00 104.36 w[10] 105.75 1.73 102.99 108.52 w[11] 107.37 1.70 104.66 110.08 w[12] 102.67 1.65 100.02 105.31 w[13] 108.15 1.69 105.44 110.86 w[14] 103.77 1.87 100.78 106.75 w[15] 101.31 2.34 97.57 105.05 w[16] 95.98 2.44 92.08 99.87 w[17] 92.12 2.30 88.45 95.80 sigma 5.95 0.15 5.71 6.18 cherry_samples &lt;- extract.samples(model_cherry2) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% set_names(nm = c(&quot;sigma&quot;, str_pad(1:17, 2,pad = 0))) cherry_samples_mu &lt;- cherry_samples %&gt;% summarise(across(everything(), mean)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;bias_function&quot;, values_to =&quot;weight&quot;) model_cherry_samples &lt;- link(model_cherry2) %&gt;% as_tibble()%&gt;% set_names(nm = data_cherry$year) %&gt;% pivot_longer(cols = everything(), names_to = &quot;year&quot;, values_to = &quot;doy&quot;) %&gt;% mutate(year = as.numeric(year)) %&gt;% arrange(year) model_cherry_stats &lt;- model_cherry_samples %&gt;% group_by(year) %&gt;% nest() %&gt;% mutate(mean = map_dbl(data, function(data){mean(data$doy)}), PI_lower = map_dbl(data, function(data){PI(data$doy)[1]}), PI_upper = map_dbl(data, function(data){PI(data$doy)[2]})) model_cherry_stats %&gt;% ggplot(aes(x = year)) + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = &quot;black&quot;) + geom_point(data = cherry_blossoms, aes(y = doy), color = clr2, alpha = .3) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr2, alpha = .35) + geom_line(aes(y = mean)) + labs(y = &quot;Day of first blossom&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) 5.8 {brms} section 5.8.1 linear model of adult height finding the posterior with {brms} brms_c4_adult_heights &lt;- brm(data = data_adults, family = gaussian, height ~ 1, prior = c(prior(normal(178, 20), class = Intercept), prior(uniform(0, 50), class = sigma)), iter = 31000, warmup = 30000, chains = 7, cores = 7, seed = 4, file = &quot;brms/brms_c4_adult_heights&quot;) posterior_summary(brms_c4_adult_heights,probs = c(.055, .945)) %&gt;% round(digits = 3) %&gt;% knitr::kable() Estimate Est.Error Q5.5 Q94.5 b_Intercept 154.604 0.418 153.947 155.275 sigma 7.742 0.304 7.276 8.250 lp__ -1226.924 1.058 -1228.772 -1225.927 brms_summary_plot &lt;- function(mod, n_chains = 4){ bayes_data &lt;- bayesplot::mcmc_areas_data(mod, prob = .95) %&gt;% filter(parameter != &quot;lp__&quot;) bayes_chains_data &lt;- bayesplot::mcmc_trace_data(mod) %&gt;% filter(parameter != &quot;lp__&quot;) p_dens &lt;- bayes_data %&gt;% filter(interval == &quot;outer&quot;) %&gt;% ggplot(aes(x = x, y = scaled_density)) + geom_area(color = clr0d, fill = fll0) + geom_area(data = bayes_data %&gt;% filter(interval == &quot;inner&quot;), color = clr2, fill = fll2) + facet_wrap(parameter ~ . , scales = &quot;free&quot;, ncol = 1) p_chains &lt;- bayes_chains_data %&gt;% ggplot(aes(x = iteration, y = value, group = chain)) + geom_line(aes(color = chain), alpha = .6) + facet_wrap(parameter ~ . , scales = &quot;free&quot;, ncol = 1) + scale_color_manual(values = scales::colour_ramp(colors = c(&quot;black&quot;,clr0d,clr2))(seq(0,1,length.out = n_chains))) p_dens + p_chains } brms_summary_plot(brms_c4_adult_heights) sampling from the posterior # equivalent to `rethinking::extract.samples()` brms_post &lt;- as_draws_df(brms_c4_adult_heights) %&gt;% as_tibble() head(brms_post) #&gt; # A tibble: 6 x 6 #&gt; b_Intercept sigma lp__ .chain .iteration .draw #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 154. 8.07 -1227. 1 1 1 #&gt; 2 154. 7.84 -1227. 1 2 2 #&gt; 3 155. 8.21 -1228. 1 3 3 #&gt; 4 154. 7.99 -1227. 1 4 4 #&gt; 5 155. 7.77 -1226. 1 5 5 #&gt; 6 154. 7.93 -1227. 1 6 6 select(brms_post, b_Intercept:sigma) %&gt;% cov() %&gt;% cov2cor() \\[\\begin{bmatrix} 1 &amp;-0.0192241582730429 \\\\-0.0192241582730429 &amp;1 \\\\ \\end{bmatrix}\\] brms_post %&gt;% dplyr::select(-(lp__:.draw)) %&gt;% pivot_longer(cols = everything()) %&gt;% group_by(name) %&gt;% summarise(quantiles = list(tibble(quant = quantile(value, probs = c(.5, .025, .75)), perc = str_c(&quot;q&quot;,names(quant)))) )%&gt;% unnest(quantiles) %&gt;% pivot_wider(names_from = perc, values_from = quant) %&gt;% mutate(across(.cols = -name, ~ round(.x, digits = 2))) %&gt;% knitr::kable() name q50% q2.5% q75% b_Intercept 154.59 153.80 154.89 sigma 7.73 7.18 7.94 posterior_summary(brms_post) \\[\\begin{bmatrix} 154.603681717344 &amp;0.417698657902289 &amp;153.797143321477 &amp;155.411872045528 &amp;7.7417412324536 &amp;0.304409842943225 \\\\7.17548730321122 &amp;8.38470302045445 &amp;-1226.92435306886 &amp;1.05809371424477 &amp;-1229.68670608237 &amp;-1225.89312495532 \\\\2.5 &amp;1.11817376920787 &amp;1 &amp;4 &amp;500.5 &amp;288.711081398222 \\\\25.975 &amp;975.025 &amp;2000.5 &amp;1154.84486692658 &amp;100.975 &amp;3900.025 \\\\ \\end{bmatrix}\\] the height model with a predictor data_adults &lt;- data_adults %&gt;% mutate(weight_centered = weight - mean(weight)) brms_c4_heights_x &lt;- brm(data = data_adults, family = gaussian, height ~ 1 + weight_centered, prior = c(prior(normal(178, 20), class = Intercept), prior(lognormal(0, 1), class = b), prior(uniform(0, 50), class = sigma)), iter = 28000, warmup = 27000, chains = 7, cores = 7, seed = 42, file = &quot;brms/brms_c4_heights_x&quot;) brms_summary_plot(brms_c4_heights_x, n_chains = 12) Logs and exps (m4.3b) brms_c4_heights_x_log &lt;- brm(data = data_adults, family = gaussian, bf(height ~ alpha + exp(logbeta) * weight_centered, alpha ~ 1, logbeta ~ 1, nl = TRUE), prior = c(prior(normal(178, 20), class = b, nlpar = alpha), prior(normal(0, 1), class = b, nlpar = logbeta), prior(uniform(0, 50), class = sigma)), iter = 31000, warmup = 30000, chains = 7, cores = 7, seed = 42, file = &quot;brms/brms_c4_heights_x_log&quot;) posterior_summary(brms_c4_heights_x)[1:3, ] %&gt;% round(digits = 2)%&gt;% as.data.frame() #&gt; Estimate Est.Error Q2.5 Q97.5 #&gt; b_Intercept 154.60 0.27 154.08 155.12 #&gt; b_weight_centered 0.90 0.04 0.82 0.99 #&gt; sigma 5.11 0.20 4.74 5.52 vcov(brms_c4_heights_x) %&gt;% cov2cor() %&gt;% round(3) %&gt;% as.data.frame() #&gt; Intercept weight_centered #&gt; Intercept 1.000 -0.003 #&gt; weight_centered -0.003 1.000 brms_posterior_samples &lt;- as_draws_df(brms_c4_heights_x) %&gt;% as_tibble() %&gt;% select(-(lp__:.draw)) brms_posterior_samples %&gt;% cov() %&gt;% cov2cor() %&gt;% round(digits = 3)%&gt;% as.data.frame() #&gt; b_Intercept b_weight_centered sigma #&gt; b_Intercept 1.000 -0.003 0.006 #&gt; b_weight_centered -0.003 1.000 0.000 #&gt; sigma 0.006 0.000 1.000 ggpairs(brms_posterior_samples, lower = list(continuous = wrap(ggally_points, colour = clr1, size = .3, alpha = .1)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll1, color = clr1, adjust = .5)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) weight_seq &lt;- tibble(weight = 25:70) %&gt;% mutate(weight_centered = weight - mean(data_adults$weight)) brms_model_hight_mu &lt;- fitted(brms_c4_heights_x, summary = FALSE, newdata = weight_seq) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq$weight_centered) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight_centered&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight_centered = as.numeric(weight_centered)) brms_model_hight_mu_interval &lt;- brms_model_hight_mu %&gt;% group_by(weight_centered) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() brms_model_hight_samples &lt;- predict(brms_c4_heights_x, summary = FALSE, newdata = weight_seq) %&gt;% as_tibble() %&gt;% set_names(nm = weight_seq$weight_centered) %&gt;% pivot_longer(cols = everything(), names_to = &quot;weight_centered&quot;, values_to = &quot;height&quot;) %&gt;% mutate(weight_centered = as.numeric(weight_centered)) brms_model_hight_samples_interval &lt;- brms_model_hight_samples %&gt;% group_by(weight_centered) %&gt;% summarise(mean = mean(height), PI_lower = PI(height)[1], PI_upper = PI(height)[2]) %&gt;% ungroup() data_adults %&gt;% ggplot(aes(x = weight_centered, y = height)) + geom_point(shape = 21, size = 2, color = clr1, fill = fll1) + geom_abline(intercept = fixef(brms_c4_heights_x)[1], slope = fixef(brms_c4_heights_x)[2]) brms_model_hight_mu_interval %&gt;% ggplot(aes(x = weight_centered)) + geom_ribbon(data = brms_model_hight_samples_interval, aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_point(data = data_adults, aes(y = height), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) + geom_line(aes(y = mean)) brms_c4_curve_x &lt;- brm(data = data_model, family = gaussian, height ~ 1 + weight_s + weight_s2, prior = c(prior(normal(178, 20), class = Intercept), prior(lognormal(0, 1), class = b, coef = &quot;weight_s&quot;), prior(normal(0, 1), class = b, coef = &quot;weight_s2&quot;), prior(uniform(0, 50), class = sigma)), iter = 30000, warmup = 29000, chains = 4, cores = 4, seed = 4, file = &quot;brms/brms_c4_curve_x&quot;) brms_summary_plot(brms_c4_curve_x, n_chains = 4) weight_seq &lt;- tibble(weight_s = seq(from = -2.5, to = 2.5, length.out = 30)) %&gt;% mutate(weight_s2 = weight_s^2) fitd_quad &lt;- fitted(brms_c4_curve_x, newdata = weight_seq, probs = c(.055, .955)) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) %&gt;% as_tibble() pred_quad &lt;- predict(brms_c4_curve_x, newdata = weight_seq, probs = c(.055, .955)) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) %&gt;% as_tibble() ggplot(data = data_model, aes(x = weight_s)) + geom_ribbon(data = pred_quad, aes(ymin = Q5.5, ymax = Q95.5), fill = clr0d, alpha = .35) + geom_smooth(data = fitd_quad, aes(y = Estimate, ymin = Q5.5, ymax = Q95.5), stat = &quot;identity&quot;, color = clr1, fill = fll1, alpha = .35, size = .2) + geom_point(aes(y = height), color = rgb(0,0,0,.5), size = .6) num_knots &lt;- 15 knot_list &lt;- quantile(data_cherry$year, probs = seq(from = 0, to = 1, length.out = num_knots)) B &lt;- bs(data_cherry$year, knots = knot_list[-c(1, num_knots)], degree = 3, intercept = TRUE) data_cherry_B &lt;- data_cherry %&gt;% mutate(B = B) data_cherry_B %&gt;% glimpse() #&gt; Rows: 827 #&gt; Columns: 6 #&gt; $ year &lt;int&gt; 812, 815, 831, 851, 853, 864, 866, 869, 889, 891, 892, 894,… #&gt; $ doy &lt;int&gt; 92, 105, 96, 108, 104, 100, 106, 95, 104, 109, 108, 106, 10… #&gt; $ temp &lt;dbl&gt; NA, NA, NA, 7.38, NA, 6.42, 6.44, NA, 6.83, 6.98, 7.11, 6.9… #&gt; $ temp_upper &lt;dbl&gt; NA, NA, NA, 12.10, NA, 8.69, 8.11, NA, 8.48, 8.96, 9.11, 8.… #&gt; $ temp_lower &lt;dbl&gt; NA, NA, NA, 2.66, NA, 4.14, 4.77, NA, 5.19, 5.00, 5.11, 5.5… #&gt; $ B &lt;bs[,17]&gt; &lt;bs[26 x 17]&gt; brms_c4_cherry_spline &lt;- brm(data = data_cherry_B, family = gaussian, doy ~ 1 + B, prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;brms/brms_c4_cherry_spline&quot;) brms_posterior_samples &lt;- as_draws_df(brms_c4_cherry_spline) years_seq &lt;- tibble(year = seq(from = min(data_cherry$year), to = max(data_cherry$year), by = 10)) fitd_quad &lt;- fitted(brms_c4_cherry_spline, probs = c(.055, .955)) %&gt;% data.frame() %&gt;% bind_cols(data_cherry_B) %&gt;% as_tibble() pred_quad &lt;- predict(brms_c4_cherry_spline, probs = c(.055, .955)) %&gt;% data.frame() %&gt;% bind_cols(data_cherry_B) %&gt;% as_tibble() ggplot(data = data_cherry, aes(x = year)) + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = rgb(0,0,0,.4)) + geom_point(aes(y = doy), color = rgb(0,0,0,.5), size = .6) + geom_ribbon(data = pred_quad, aes(ymin = Q5.5, ymax = Q95.5), fill = clr0d, alpha = .35) + geom_smooth(data = fitd_quad, aes(y = Estimate, ymin = Q5.5, ymax = Q95.5), stat = &quot;identity&quot;, color = clr1, fill = fll1, alpha = .35, size = .2) + geom_hline(yintercept = fixef(brms_c4_cherry_spline)[1, 1], color = clr1, linetype = 2) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) get_prior(data = data_cherry, family = gaussian, doy ~ 1 + s(year)) #&gt; prior class coef group resp dpar nlpar bound #&gt; (flat) b #&gt; (flat) b syear_1 #&gt; student_t(3, 105, 5.9) Intercept #&gt; student_t(3, 0, 5.9) sds #&gt; student_t(3, 0, 5.9) sds s(year) #&gt; student_t(3, 0, 5.9) sigma #&gt; source #&gt; default #&gt; (vectorized) #&gt; default #&gt; default #&gt; (vectorized) #&gt; default Using a thin plate spline brms_c4_cherry_smooth &lt;- brm(data = data_cherry, family = gaussian, doy ~ 1 + s(year), prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = b), prior(student_t(3, 0, 5.9), class = sds), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, control = list(adapt_delta = .99), file = &quot;brms/brms_c4_cherry_smooth&quot;) fitted(brms_c4_cherry_smooth, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(select(data_cherry, year, doy)) %&gt;% as_tibble() %&gt;% ggplot(aes(x = year, y = doy, ymin = Q5.5, ymax = Q94.5)) + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = rgb(0,0,0,.4)) + geom_point(aes(y = doy), color = rgb(0,0,0,.5), size = .6) + geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5), stat = &quot;identity&quot;, color = clr1, fill = fll1, alpha = .35, size = .2) + geom_hline(yintercept = fixef(brms_c4_cherry_smooth)[1, 1], color = clr1, linetype = 2) + labs(subtitle = &quot;brms smooth using s(year) (thin plate)&quot;, y = &quot;day in year&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) brms_c4_cherry_smooth2 &lt;- brm(data = data_cherry, family = gaussian, doy ~ 1 + s(year, bs = &quot;bs&quot;, k = 19), prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = b), prior(student_t(3, 0, 5.9), class = sds), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, control = list(adapt_delta = .99), file = &quot;brms/brms_c4_cherry_smooth2&quot;) fitted(brms_c4_cherry_smooth2, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(select(data_cherry, year, doy)) %&gt;% as_tibble() %&gt;% ggplot(aes(x = year, y = doy, ymin = Q5.5, ymax = Q94.5)) + geom_vline(data = tibble(year = knot_list), aes(xintercept = year), linetype = 3, color = rgb(0,0,0,.4)) + geom_point(aes(y = doy), color = rgb(0,0,0,.5), size = .6) + geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5), stat = &quot;identity&quot;, color = clr1, fill = fll1, alpha = .35, size = .2) + geom_hline(yintercept = fixef(brms_c4_cherry_smooth2)[1, 1], color = clr1, linetype = 2) + labs(subtitle = &quot;brms smooth using s(year) (B-spline)&quot;, y = &quot;day in year&quot;) + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank()) Add-on about matrix-columns (4.7 Second bonus) n &lt;- 100 # how many continuous x predictor variables would you like? k &lt;- 10 # simulate a dichotomous dummy variable for z # simulate an n by k array for X set.seed(4) data_matrix_columns &lt;- tibble(z = sample(0:1, size = n, replace = T), X = array(runif(n * k, min = 0, max = 1), dim = c(n, k))) # set the data-generating parameter values a &lt;- 1 theta &lt;- 5 b &lt;- 1:k sigma &lt;- 2 # simulate the criterion data_matrix_columns &lt;- data_matrix_columns %&gt;% mutate(y = as.vector(a + X %*% b + theta * z + rnorm(n, mean = 0, sd = sigma))) # data_matrix_columns %&gt;% glimpse() # data_matrix_columns$X[1,] brms_c4_matrix_column &lt;- brm(data = data_matrix_columns, family = gaussian, y ~ 1 + z + X, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;brms/brms_c4_matrix_column&quot;) summary(brms_c4_matrix_column) #&gt; Family: gaussian #&gt; Links: mu = identity; sigma = identity #&gt; Formula: y ~ 1 + z + X #&gt; Data: data_matrix_columns (Number of observations: 100) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; Intercept 0.95 1.19 -1.47 3.34 1.00 6928 3291 #&gt; z 4.74 0.42 3.91 5.56 1.00 7389 3032 #&gt; X1 0.57 0.75 -0.86 2.04 1.00 6798 3312 #&gt; X2 0.90 0.69 -0.47 2.26 1.00 6421 2949 #&gt; X3 3.41 0.75 1.96 4.90 1.00 7375 3214 #&gt; X4 2.81 0.73 1.36 4.25 1.00 6712 3469 #&gt; X5 5.74 0.72 4.32 7.12 1.00 6621 3588 #&gt; X6 6.40 0.73 4.97 7.82 1.00 6552 3228 #&gt; X7 8.49 0.73 7.06 9.90 1.00 7098 3335 #&gt; X8 8.40 0.69 7.04 9.76 1.00 8718 3363 #&gt; X9 8.82 0.81 7.27 10.39 1.00 8378 3273 #&gt; X10 9.32 0.73 7.88 10.78 1.00 8260 2894 #&gt; #&gt; Family Specific Parameters: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; sigma 1.99 0.16 1.71 2.32 1.00 5722 3433 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). 5.9 pymc3 section × "],
["rethinking-chapter-5.html", "6 Rethinking: Chapter 5 6.1 Directed Acyclic Graphs 6.2 Multiple Regression notion 6.3 Masked relationship 6.4 Categorical Variables 6.5 Homework 6.6 {brms} section 6.7 pymc3 section", " 6 Rethinking: Chapter 5 Spurious waffles by Richard McElreath, building on the Summaries by Solomon Kurz and Jake Thompson. library(sf) library(rethinking) library(ggfx) data(WaffleDivorce) WaffleDivorce &lt;- WaffleDivorce %&gt;% as_tibble() usa &lt;- read_sf(&quot;~/work/geo_store/USA/usa_states_albers_revised.gpkg&quot;) %&gt;% left_join(WaffleDivorce, by = c(name = &quot;Location&quot; )) p_waffle &lt;- usa %&gt;% ggplot(aes(fill = WaffleHouses / Population)) + scale_fill_gradientn(colours = c(clr0d, clr2) %&gt;% clr_lighten(.3)) p_divorce &lt;- usa %&gt;% ggplot(aes(fill = Divorce))+ scale_fill_gradientn(colours = c(clr0d, clr1) %&gt;% clr_lighten(.3)) p_age &lt;- usa %&gt;% ggplot(aes(fill = MedianAgeMarriage))+ scale_fill_gradientn(colours = c(clr_lighten(clr0d, .3), clr3)) p_waffle + p_divorce + p_age + plot_layout(guides = &quot;collect&quot;) &amp; with_shadow(geom_sf(aes(color = after_scale(clr_darken(fill)))), x_offset = 0, y_offset = 0, sigma = 3) &amp; guides(fill = guide_colorbar(title.position = &quot;top&quot;, barheight = unit(5,&quot;pt&quot;))) &amp; theme(legend.position = &quot;bottom&quot;) Age Model: first model (divorce rate depends on age at marriage) \\[ \\begin{array}{cccr} D_i &amp; {\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{A} A_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.2) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{A} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] data_waffle &lt;- WaffleDivorce %&gt;% mutate(across(.cols = c(Divorce, Marriage, MedianAgeMarriage), .fns = standardize, .names = &quot;{str_to_lower(.col)}_std&quot;), waffle_pop = WaffleHouses / Population) %&gt;% rename(median_age_std = &quot;medianagemarriage_std&quot;) sd(data_waffle$MedianAgeMarriage) #&gt; [1] 1.24363 model_age &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_A * median_age_std , alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle ) set.seed(10) age_priors &lt;- extract.prior(model_age) %&gt;% as_tibble() prior_prediction_range &lt;- c(-2, 2) age_prior_predictions &lt;- link(model_age, post = age_priors, data = list(median_age_std = prior_prediction_range)) %&gt;% as_tibble() %&gt;% set_names(nm = as.character(prior_prediction_range)) %&gt;% mutate(.draw = row_number()) age_prior_predictions %&gt;% filter(.draw &lt; 51) %&gt;% ggplot() + geom_segment(aes(x = -2, xend = 2, y = `-2`, yend = `2`, group = .draw), color = clr2, alpha = .2) + labs(x = &quot;median age of marriage (std)&quot;, y = &quot;divorce rate (std)&quot;) age_seq &lt;- seq(min(data_waffle$median_age_std), max(data_waffle$median_age_std), length.out = 101) model_age_posterior_prediction_samples &lt;- link(model_age, data = data.frame(median_age_std = age_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = age_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;median_age_std&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(median_age_std = as.numeric(median_age_std), MedianAgeMarriage = median_age_std * sd(data_waffle$MedianAgeMarriage) + mean(data_waffle$MedianAgeMarriage), Divorce = divorce_std * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) model_age_posterior_prediction_pi &lt;- model_age_posterior_prediction_samples %&gt;% group_by(median_age_std, MedianAgeMarriage) %&gt;% summarise(mean = mean(Divorce), PI_lower = PI(Divorce)[1], PI_upper = PI(Divorce)[2]) %&gt;% ungroup() model_age_posterior_prediction_simulation &lt;- sim(model_age, data = data.frame(median_age_std = age_seq), n = 5e3) %&gt;% as_tibble() %&gt;% set_names(nm = age_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;median_age_std&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(median_age_std = as.numeric(median_age_std), MedianAgeMarriage = median_age_std * sd(data_waffle$MedianAgeMarriage) + mean(data_waffle$MedianAgeMarriage), Divorce = divorce_std * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) model_age_posterior_prediction_simulation_pi &lt;- model_age_posterior_prediction_simulation %&gt;% group_by(median_age_std, MedianAgeMarriage) %&gt;% summarise(mean = mean(Divorce), PI_lower = PI(Divorce)[1], PI_upper = PI(Divorce)[2]) %&gt;% ungroup() p_age &lt;- ggplot(mapping = aes(x = MedianAgeMarriage)) + geom_ribbon(data = model_age_posterior_prediction_simulation_pi, aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_smooth(data = model_age_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr3, fill = fll3, size = .4) + geom_point(data = data_waffle, aes(y = Divorce), color = rgb(0,0,0,.5), size = .6) + labs(y = &quot; divorce&quot;) Marriage Model: alternative model (divorce rate depends on marriage rate) \\[ \\begin{array}{cccr} D_i &amp; {\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{M} M_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.2) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{M} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_marriage &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std , alpha ~ dnorm( 0, 0.2 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle ) marriage_seq &lt;- seq(min(data_waffle$marriage_std), max(data_waffle$marriage_std), length.out = 101) model_marriage_posterior_prediction_samples &lt;- link(model_marriage, data = data.frame(marriage_std = marriage_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = marriage_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;marriage_std&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(marriage_std = as.numeric(marriage_std), Marriage = marriage_std * sd(data_waffle$Marriage) + mean(data_waffle$Marriage), Divorce = divorce_std * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) model_marriage_posterior_prediction_pi &lt;- model_marriage_posterior_prediction_samples %&gt;% group_by(marriage_std, Marriage) %&gt;% summarise(mean = mean(Divorce), PI_lower = PI(Divorce)[1], PI_upper = PI(Divorce)[2]) %&gt;% ungroup() model_marriage_posterior_prediction_simulation &lt;- sim(model_marriage, data = data.frame(marriage_std = marriage_seq), n = 5e3) %&gt;% as_tibble() %&gt;% set_names(nm = marriage_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;marriage_std&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(marriage_std = as.numeric(marriage_std), Marriage = marriage_std * sd(data_waffle$Marriage) + mean(data_waffle$Marriage), Divorce = divorce_std * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) model_marriage_posterior_prediction_simulation_pi &lt;- model_marriage_posterior_prediction_simulation %&gt;% group_by(marriage_std, Marriage) %&gt;% summarise(mean = mean(Divorce), PI_lower = PI(Divorce)[1], PI_upper = PI(Divorce)[2]) %&gt;% ungroup() p_marriage &lt;- ggplot(mapping = aes(x = Marriage)) + geom_ribbon(data = model_marriage_posterior_prediction_simulation_pi, aes(ymin = PI_lower, ymax = PI_upper), fill = clr0d, alpha = .35) + geom_smooth(data = model_marriage_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr1, fill = fll1, size = .2) + geom_point(data = data_waffle, aes(y = Divorce), color = rgb(0,0,0,.5), size = .6) + labs(y = &quot; divorce&quot;) Waffle Model: model_waffle &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_W * waffle_pop , alpha ~ dnorm( 0, 0.2 ), beta_W ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle ) waffle_seq &lt;- seq(min(data_waffle$waffle_pop), max(data_waffle$waffle_pop), length.out = 101) model_waffle_posterior_prediction_samples &lt;- link(model_waffle, data = data.frame(waffle_pop = waffle_seq)) %&gt;% as_tibble() %&gt;% set_names(nm = waffle_seq) %&gt;% pivot_longer(cols = everything(), names_to = &quot;waffle_pop&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(waffle_pop = as.numeric(waffle_pop), Divorce = divorce_std * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) model_waffle_posterior_prediction_pi &lt;- model_waffle_posterior_prediction_samples %&gt;% group_by(waffle_pop) %&gt;% summarise(mean = mean(Divorce), PI_lower = PI(Divorce)[1], PI_upper = PI(Divorce)[2]) %&gt;% ungroup() p_waffle &lt;- ggplot(mapping = aes(x = waffle_pop)) + geom_smooth(data = model_waffle_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr2, fill = fll2, size = .2) + geom_point(data = data_waffle, aes(y = Divorce), color = rgb(0,0,0,.5), size = .6) + labs(y = &quot; divorce&quot;) p_waffle + p_marriage + theme(axis.title.y = element_blank(), axis.text.y = element_blank()) + p_age + theme(axis.title.y = element_blank(), axis.text.y = element_blank()) &amp; lims(y = c(4, 15)) 6.1 Directed Acyclic Graphs dag1 &lt;- dagify( D ~ A + M, M ~ A, exposure = &quot;A&quot;, outcome = &quot;M&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,1,.5), y = c(1,1, .4))) %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) dag2 &lt;- dagify( D ~ A, M ~ A, exposure = &quot;A&quot;, outcome = &quot;M&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,.5,1), y = c(1, .4, 1))) %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag1, clr_in = clr3) + plot_dag(dag2, clr_in = clr3) &amp; scale_y_continuous(limits = c(.35, 1.05)) &amp; coord_equal() DAG notation: \\(Y \\perp \\!\\!\\! \\perp X | Z\\): “\\(Y\\) is independent of \\(X\\) conditional on \\(Z\\)” \\(D \\not\\!\\perp\\!\\!\\!\\perp A\\): \"\\(D\\) is associated with \\(A\\)\" Check pair wise correlations with cor(): data_waffle %&gt;% dplyr::select(divorce_std,marriage_std, median_age_std) %&gt;% cor() %&gt;% as.data.frame(row.names = row.names(.)) %&gt;% round(digits = 2) %&gt;% knitr::kable() divorce_std marriage_std median_age_std divorce_std 1.00 0.37 -0.60 marriage_std 0.37 1.00 -0.72 median_age_std -0.60 -0.72 1.00 library(dagitty) dagitty(&#39;dag{ D &lt;- A -&gt; M -&gt; D}&#39;) %&gt;% impliedConditionalIndependencies() dagitty(&#39;dag{ D &lt;- A -&gt; M }&#39;) %&gt;% impliedConditionalIndependencies() #&gt; D _||_ M | A 6.2 Multiple Regression notion \\[ \\begin{array}{cccr} D_i &amp; {\\sim} &amp; Normal(\\mu, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{M} M_{i} + \\beta_{A} A_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.2) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{M} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_M$ prior]}\\\\ \\beta_{A} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_A$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] or compact notion \\[ \\mu_i = \\alpha + \\sum_{j = 1}^{n} \\beta_jx_{ji} \\] or even matrix notion \\[ m = Xb \\] model_multiple &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle ) precis(model_multiple) %&gt;% round(digits = 2) %&gt;% as.matrix() %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.10 -0.16 0.16 beta_A -0.61 0.15 -0.85 -0.37 beta_M -0.07 0.15 -0.31 0.18 sigma 0.79 0.08 0.66 0.91 ct &lt;- coeftab(model_age, model_marriage, model_multiple,se = TRUE) plot_coeftab(ct) beta_A doesn’t really change, it only grows more uncertain, yet beta_M is only associated with divorce, when marriage rate is missing from the model. “Once we know the median age at marriage for a State, there is little to no additional predictive power in also knowing the rate of marriage at that State.” \\(\\rightarrow\\) \\(D \\perp \\!\\!\\! \\perp M | A\\) simulating the divorcee example n &lt;- 50 data_divorce_sim &lt;- tibble(median_age_std = rnorm(n), marriage_std = rnorm(n, mean = -median_age_std), divorce_std = rnorm(n, mean = median_age_std), divorce_codep = rnorm(n, mean = median_age_std + marriage_std)) p1 &lt;- ggpairs(data_divorce_sim %&gt;% dplyr::select(-divorce_codep), lower = list(continuous = wrap(ggally_points, colour = clr1, size = .9, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll1, color = clr1, adjust = 1)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) p2 &lt;- ggpairs(data_divorce_sim %&gt;% dplyr::select(-divorce_std), lower = list(continuous = wrap(ggally_points, colour = clr2, size = .9, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll2, color = clr2, adjust = 1)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) cowplot::plot_grid(ggmatrix_gtable(p1), ggmatrix_gtable(p2)) simulating the right DAG (\\(D \\perp \\!\\!\\! \\perp M | A\\)) model_multiple_sim &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_divorce_sim ) model_age_sim &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_divorce_sim ) model_marriage_sim &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std, alpha ~ dnorm( 0, 0.2 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_divorce_sim ) ct_sim &lt;- coeftab(model_age_sim, model_marriage_sim, model_multiple_sim, se = TRUE) plot_coeftab(ct_sim) simulating the left DAG (\\(D \\not\\!\\perp\\!\\!\\!\\perp M | A\\)) model_multiple_sim_codep &lt;- quap( flist = alist( divorce_codep ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_divorce_sim ) model_age_sim_codep &lt;- quap( flist = alist( divorce_codep ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_divorce_sim ) model_marriage_sim_codep &lt;- quap( flist = alist( divorce_codep ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std, alpha ~ dnorm( 0, 0.2 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_divorce_sim ) ct_sim_codep &lt;- coeftab(model_age_sim_codep, model_marriage_sim_codep, model_multiple_sim_codep, se = TRUE) plot_coeftab(ct_sim_codep) 6.2.1 Visualizations for multivariate regressions Predictor residual plots. useful for understanding the model, but not much else Posterior prediction plots. checking fit and assessing predictions Counterfactual plots. implied predictions for imaginary experiments 6.2.1.1 Predictor residual plots predictor residual plot for marriage rate pred_res_marriage &lt;- quap( flist = alist( marriage_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_AM * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_AM ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle ) residuals_marriage &lt;- link(pred_res_marriage) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(data_waffle$median_age_std)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;fit_marriage&quot;) %&gt;% group_by(row_idx) %&gt;% summarise(mean_marriage = mean(fit_marriage), lower_pi = PI(fit_marriage)[1], upper_pi = PI(fit_marriage)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx)) %&gt;% left_join(data_waffle %&gt;% mutate(row_idx = row_number()),. ) %&gt;% mutate(residual_marriage = marriage_std - mean_marriage) p_11 &lt;- residuals_marriage %&gt;% ggplot(aes(x = median_age_std)) + geom_segment(aes(xend = median_age_std, y = mean_marriage, yend = marriage_std), color = rgb(0,0,0,.6), linetype = 3) + geom_line(aes(y = mean_marriage), color = clr1) + geom_point(aes(y = marriage_std), color = clr1, fill = clr_lighten(clr1, .35), shape = 21) + geom_text(data = residuals_marriage %&gt;% filter(Loc %in% c(&quot;DC&quot;, &quot;HI&quot;, &quot;ND&quot;, &quot;ME&quot;, &quot;WY&quot;)), aes(x = median_age_std - .1, y = marriage_std, label = Loc), hjust = 1) pred_res_marriage_mu &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta * residual_marriage, alpha ~ dnorm( 0, 0.2 ), beta ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = residuals_marriage ) seq_res &lt;- seq(min(residuals_marriage$residual_marriage), max(residuals_marriage$residual_marriage), length.out = 101) residual_lm_posterior &lt;- link(pred_res_marriage_mu, data = data.frame(residual_marriage = seq_res)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_res) %&gt;% pivot_longer(cols = everything(), names_to = &quot;residual_marriage&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(residual_marriage = as.numeric(residual_marriage)) %&gt;% group_by(residual_marriage) %&gt;% summarise(mean = mean(divorce_std), PI_lower = PI(divorce_std)[1], PI_upper = PI(divorce_std)[2]) %&gt;% ungroup() p_12 &lt;- ggplot(mapping = aes(x = residual_marriage)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_smooth(data = residual_lm_posterior, aes(y = mean, ymin = PI_lower, ymax = PI_upper), stat = &quot;identity&quot;, color = clr1, fill = fll1, size = .4) + geom_point(data = residuals_marriage, aes(y = divorce_std), color = clr1, fill = clr_lighten(clr1,.35), shape = 21) + geom_text(data = residuals_marriage %&gt;% filter(Loc %in% c(&quot;DC&quot;, &quot;HI&quot;, &quot;ND&quot;, &quot;ME&quot;, &quot;WY&quot;)), aes(y = divorce_std - .4, label = Loc)) + labs(y = &quot;divorce_rate (std)&quot;) predictor residual plot for age at marriage pred_res_age &lt;- quap( flist = alist( median_age_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_MA * marriage_std, alpha ~ dnorm( 0, 0.2 ), beta_MA ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = data_waffle ) residuals_age &lt;- link(pred_res_age) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(data_waffle$marriage_std)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;fit_age&quot;) %&gt;% group_by(row_idx) %&gt;% summarise(mean_age = mean(fit_age), lower_pi = PI(fit_age)[1], upper_pi = PI(fit_age)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx)) %&gt;% left_join(data_waffle %&gt;% mutate(row_idx = row_number()),. ) %&gt;% mutate(residual_age = median_age_std - mean_age) p_21 &lt;- residuals_age %&gt;% ggplot(aes(x = marriage_std)) + geom_segment(aes(xend = marriage_std, y = mean_age, yend = median_age_std), color = rgb(0,0,0,.6), linetype = 3) + geom_line(aes(y = mean_age), color = clr2) + geom_point(aes(y = median_age_std), color = clr2, fill = clr_lighten(clr2, .35), shape = 21) + geom_text(data = residuals_marriage %&gt;% filter(Loc %in% c(&quot;DC&quot;, &quot;HI&quot;, &quot;ID&quot;)), aes(x = marriage_std - .1, y = median_age_std, label = Loc), hjust = 1) pred_res_age_mu &lt;- quap( flist = alist( divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta * residual_age, alpha ~ dnorm( 0, 0.2 ), beta ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ) ), data = residuals_age ) seq_res_age &lt;- seq(min(residuals_age$residual_age), max(residuals_age$residual_age), length.out = 101) residual_lm_posterior_age &lt;- link(pred_res_age_mu, data = data.frame(residual_age = seq_res_age)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_res_age) %&gt;% pivot_longer(cols = everything(), names_to = &quot;residual_age&quot;, values_to = &quot;divorce_std&quot;) %&gt;% mutate(residual_age = as.numeric(residual_age)) %&gt;% group_by(residual_age) %&gt;% summarise(mean = mean(divorce_std), PI_lower = PI(divorce_std)[1], PI_upper = PI(divorce_std)[2]) %&gt;% ungroup() p_22 &lt;- ggplot(mapping = aes(x = residual_age)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_smooth(data = residual_lm_posterior_age, aes(y = mean, ymin = PI_lower, ymax = PI_upper), stat = &quot;identity&quot;, color = clr2, fill = fll2, size = .4) + geom_point(data = residuals_age, aes(y = divorce_std), color = clr2, fill = clr_lighten(clr2,.35), shape = 21) + geom_text(data = residuals_age %&gt;% filter(Loc %in% c(&quot;DC&quot;, &quot;HI&quot;, &quot;ID&quot;)), aes(y = divorce_std - .4, label = Loc)) + labs(y = &quot;divorce_rate (std)&quot;) p_11 + p_21 + p_12 + p_22 6.2.1.2 Posterior Preediction Plots posterior_prediction &lt;- link(model_multiple) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(data_waffle$divorce_std)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;divorce_predicted&quot;) %&gt;% group_by(row_idx) %&gt;% summarise(divorce_predicted_mean = mean(divorce_predicted), lower_pi = PI(divorce_predicted)[1], upper_pi = PI(divorce_predicted)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx)) %&gt;% left_join(data_waffle %&gt;% mutate(row_idx = row_number()), . ) posterior_simmulation &lt;- sim(model_multiple) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(data_waffle$divorce_std)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;divorce_predicted&quot;) %&gt;% group_by(row_idx) %&gt;% summarise(lower_pi = PI(divorce_predicted)[1], upper_pi = PI(divorce_predicted)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx)) %&gt;% left_join(data_waffle %&gt;% mutate(row_idx = row_number()), . ) ggplot(mapping = aes(x = divorce_std)) + geom_abline(slope = 1, size = .7, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(data = posterior_prediction, aes(ymin = lower_pi, ymax = upper_pi, color = Loc %in% c(&quot;ID&quot;, &quot;UT&quot;)))+ geom_point(data = posterior_prediction, aes(y = divorce_predicted_mean, color = Loc %in% c(&quot;ID&quot;, &quot;UT&quot;), fill = after_scale(clr_lighten(color ,.5))), shape = 21, size = 1.5)+ geom_text(data = posterior_prediction %&gt;% filter(Loc %in% c(&quot;ID&quot;, &quot;ME&quot;, &quot;RI&quot;, &quot;UT&quot;)), aes(x = divorce_std - .15, y = divorce_predicted_mean, label = Loc)) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) Regressions tend to under-estimate variable in the high end of the range and over-estimate in the low end of the range. This is normal, they “pull towards the mean”. The labeled States however (ID, ME, RI, UT), are not well predicted by the Model (eg. due to additional social factors). Simulating spurious association N &lt;- 100 data_spurious &lt;- tibble(x_real = rnorm(N), x_spur = rnorm(N, x_real), y = rnorm(N, x_real)) ggpairs(data_spurious, lower = list(continuous = wrap(ggally_points, colour = clr3, size = .9, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll3, color = clr3, adjust = 1)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) model_spurious &lt;- quap( flist = alist( y ~ dnorm(mu, sigma), mu &lt;- alpha + beta_r * x_real + beta_s * x_spur, alpha ~ dnorm(0, .2), beta_r ~ dnorm(0, .5), beta_s ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_spurious ) precis(model_spurious) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.09 0.09 -0.06 0.24 beta_r 0.87 0.14 0.64 1.10 beta_s 0.09 0.11 -0.09 0.27 sigma 1.06 0.07 0.94 1.17 Note, how the estimated mean for beta_s is close to 0 (0.09) – despite the correlation shown above 🤔`. 6.2.1.3 Counterfactual Plots model_counterfactual &lt;- quap( flist = alist( # A -&gt; D &lt;- M divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_M * marriage_std + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), beta_M ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ), # A -&gt; M marriage_std ~ dnorm( mu_M, sigma_M ), mu_M &lt;- alpha_M + beta_AM * median_age_std, alpha_M ~ dnorm( 0, 0.2 ), beta_AM ~ dnorm( 0, 0.5 ), sigma_M ~ dexp(1) ), data = data_waffle ) precis(model_counterfactual) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.10 -0.16 0.16 beta_A -0.61 0.15 -0.85 -0.37 beta_M -0.07 0.15 -0.31 0.18 sigma 0.79 0.08 0.66 0.91 alpha_M 0.00 0.09 -0.14 0.14 beta_AM -0.69 0.10 -0.85 -0.54 sigma_M 0.68 0.07 0.57 0.79 Note, that marriage_std and median_age_std are strongly negatively correlated (-0.69) A_seq &lt;- seq(-2, 2, length.out = 30) unpack_sim &lt;- function(x, seq = A_seq){ nms &lt;- names(x) purrr::map(.x = nms, .f = function(y, x, seq_in = seq){ x[[y]] %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(seq_in)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;value&quot;) %&gt;% mutate(parameter = y) }, x = x) %&gt;% purrr::reduce(bind_rows) } data_sim &lt;- sim(fit = model_counterfactual, data = tibble(median_age_std = A_seq), vars = c(&quot;marriage_std&quot;, &quot;divorce_std&quot;)) %&gt;% unpack_sim() data_sim_pi &lt;- data_sim %&gt;% group_by(row_idx, parameter) %&gt;% summarise(mean = mean(value), PI_lower = PI(value)[1], PI_upper = PI(value)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx), median_age_std = A_seq[row_idx]) %&gt;% arrange(parameter, median_age_std) data_sim_pi %&gt;% ggplot() + geom_smooth(aes(x = median_age_std, y = mean, ymin = PI_lower, ymax = PI_upper, color = parameter, fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(clr0d, clr3), guide = &quot;none&quot;) + labs(y = &quot;counterfactual value&quot;, title = &quot;Counterfactual effects of age at marriage on&quot;) + facet_wrap(parameter ~ .) Numerical operations (eg. simulating the causal effect of raising the median age of marriage from 20 to 30): A_seq2 &lt;- (c(20, 30) - mean(data_waffle$MedianAgeMarriage)) / sd(data_waffle$MedianAgeMarriage) data_sim_num &lt;- sim(fit = model_counterfactual, data = tibble(median_age_std = A_seq2), vars = c(&quot;marriage_std&quot;, &quot;divorce_std&quot;)) %&gt;% unpack_sim(seq = A_seq2) data_sim_num %&gt;% filter(parameter == &quot;divorce_std&quot;) %&gt;% dplyr::select(-parameter) %&gt;% mutate(pair = (row_number() + 1) %/% 2) %&gt;% pivot_wider(names_from = row_idx, values_from = value) %&gt;% mutate(effect = `2` - `1`) %&gt;% summarise(mean = mean(effect)) #&gt; # A tibble: 1 x 1 #&gt; mean #&gt; &lt;dbl&gt; #&gt; 1 -4.59 …A change of four and a half standard deviations is quite extreme! M_seq &lt;- A_seq data_sim_M &lt;- sim(fit = model_counterfactual, data = tibble(marriage_std = M_seq, median_age_std = 0), vars = c(&quot;divorce_std&quot;)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(M_seq)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;divorce_std&quot;) data_sim_M_pi &lt;- data_sim_M %&gt;% group_by(row_idx) %&gt;% summarise(mean = mean(divorce_std), PI_lower = PI(divorce_std)[1], PI_upper = PI(divorce_std)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx), marriage_std = M_seq[row_idx]) data_sim_M_pi %&gt;% ggplot() + geom_smooth(aes(x = marriage_std, y = mean, ymin = PI_lower, ymax = PI_upper), color = clr1, fill = fll1, stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(clr0d, clr3), guide = &quot;none&quot;) + labs(y = &quot;counterfactual value&quot;, title = &quot;Counterfactual effects of marriage rate on divorce rate&quot;) + lims(y = c(-2, 2)) 6.3 Masked relationship Loading the milk data data(milk) data_milk &lt;- milk %&gt;% filter(complete.cases(.)) %&gt;% as_tibble() %&gt;% mutate(`mass.log` = log(mass), across(.cols = c(`kcal.per.g`, `neocortex.perc`, `mass.log`), .fns = standardize, .names = &quot;{str_remove_all(.col, &#39;\\\\\\\\..*&#39;)}_std&quot;)) data_milk %&gt;% precis() %&gt;% as.matrix() %&gt;% as.data.frame() %&gt;% filter(!is.na(mean)) %&gt;% mutate(across(.cols = mean:`94.5%`, function(x){round(as.numeric(x), digits = 2)})) %&gt;% knitr::kable() mean sd 5.5% 94.5% histogram kcal.per.g 0.66 0.17 0.47 0.93 ▇▂▁▁▁▂▁▁▁▁▁ perc.fat 36.06 14.71 15.08 54.45 ▂▁▁▂▃▃▂▅▃▁▇▂ perc.protein 16.26 5.60 9.28 23.79 ▂▅▅▅▅▂▂▅▇▂ perc.lactose 47.68 13.59 30.35 68.31 ▂▇▅▅▂▇▅▁▅▂ mass 16.64 23.58 0.30 57.89 ▇▁▁▁▁▁▁▁ neocortex.perc 67.58 5.97 58.41 75.59 ▂▁▂▅▁▅▅▅▇▅▂▂ mass.log 1.50 1.93 -1.26 4.05 ▂▁▂▂▂▂▅▂▇▁▂▂▅▅ kcal_std 0.00 1.00 -1.09 1.55 ▃▇▁▃▁▂▂ neocortex_std 0.00 1.00 -1.54 1.34 ▁▁▂▃▁▇▃▂ mass_std 0.00 1.00 -1.43 1.32 ▁▂▂▃▃▁▇ 6.3.1 Bi-variate models Neocortex effect on caloric content of milk \\[ \\begin{array}{cccr} K_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{N} N_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.2) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{N} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_N$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] Mothers weight effect on caloric content of milk \\[ \\begin{array}{cccr} K_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{M} M_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.2) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{M} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_M$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] Model implementation (neocortex, draft) model_milk_draft &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_N * neocortex_std, alpha ~ dnorm(0, 1), beta_N ~ dnorm(0, 1), sigma ~ dexp(1) ), data = data_milk ) prior_milk_draft &lt;- extract.prior(model_milk_draft) %&gt;% as_tibble() seq_prior &lt;- c(-2, 2) prior_prediction_milk_draft &lt;- link(model_milk_draft, post = prior_milk_draft, data = tibble(neocortex_std = seq_prior)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_prior) p_draft &lt;- prior_prediction_milk_draft %&gt;% filter(row_number() &lt;= 50) %&gt;% ggplot() + geom_segment(aes(x = -2, xend = 2, y = `-2`, yend = `2`), alpha = .6, color = clr0d) Model implementation (neocortex) model_milk_cortex &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_N * neocortex_std, alpha ~ dnorm(0, .2), beta_N ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_milk ) precis(model_milk_cortex) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.15 -0.24 0.24 beta_N 0.13 0.21 -0.21 0.47 sigma 0.93 0.15 0.69 1.18 prior_milk_cortex &lt;- extract.prior(model_milk_cortex) %&gt;% as_tibble() prior_prediction_milk_cortex &lt;- link(model_milk_cortex, post = prior_milk_cortex, data = tibble(neocortex_std = seq_prior)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_prior) p_cortex &lt;- prior_prediction_milk_cortex %&gt;% filter(row_number() &lt;= 50) %&gt;% ggplot() + geom_segment(aes(x = -2, xend = 2, y = `-2`, yend = `2`), alpha = .6, color = clr0d) p_draft + p_cortex &amp; coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 2)) &amp; labs(x = &quot;neocortex_std&quot;, y = &quot;kcal_std&quot;) seq_cortex &lt;- seq(min(data_milk$neocortex_std) - .15, max(data_milk$neocortex_std) + .15, length.out = 51) model_milk_cortex_posterior_prediction_samples &lt;- link(model_milk_cortex, data = data.frame(neocortex_std = seq_cortex)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_cortex) %&gt;% pivot_longer(cols = everything(), names_to = &quot;neocortex_std&quot;, values_to = &quot;kcal_std&quot;) %&gt;% mutate(neocortex_std = as.numeric(neocortex_std)) model_milk_cortex_posterior_prediction_pi &lt;- model_milk_cortex_posterior_prediction_samples %&gt;% group_by(neocortex_std) %&gt;% summarise(mean = mean(kcal_std), PI_lower = PI(kcal_std)[1], PI_upper = PI(kcal_std)[2]) %&gt;% ungroup() p_cortex &lt;- ggplot(mapping = aes(x = neocortex_std)) + geom_smooth(data = model_milk_cortex_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr2, fill = fll2, size = .2) + geom_point(data = data_milk, aes(y = kcal_std), color = rgb(0,0,0,.5), size = 1.6) + labs(x = &quot;neocprtex_std&quot;, y = &quot;kcal_std&quot;) Model implementation (mothers weight) model_milk_weight &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_M * mass_std, alpha ~ dnorm(0, .2), beta_M ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_milk ) precis(model_milk_weight) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.15 -0.23 0.23 beta_M -0.30 0.20 -0.62 0.03 sigma 0.89 0.15 0.65 1.12 seq_weight &lt;- seq(min(data_milk$mass_std) - .15, max(data_milk$mass_std) + .15, length.out = 51) model_milk_weight_posterior_prediction_samples &lt;- link(model_milk_weight, data = data.frame(mass_std = seq_weight)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_weight) %&gt;% pivot_longer(cols = everything(), names_to = &quot;mass_std&quot;, values_to = &quot;kcal_std&quot;) %&gt;% mutate(mass_std = as.numeric(mass_std)) model_milk_weight_posterior_prediction_pi &lt;- model_milk_weight_posterior_prediction_samples %&gt;% group_by(mass_std) %&gt;% summarise(mean = mean(kcal_std), PI_lower = PI(kcal_std)[1], PI_upper = PI(kcal_std)[2]) %&gt;% ungroup() p_weight &lt;- ggplot(mapping = aes(x = mass_std)) + geom_smooth(data = model_milk_weight_posterior_prediction_pi, stat = &quot;identity&quot;, aes(y = mean, ymin = PI_lower, ymax = PI_upper), color = clr2, fill = fll2, size = .2) + geom_point(data = data_milk, aes(y = kcal_std), color = rgb(0,0,0,.5), size = 1.6) + labs(x = &quot;mass_std&quot;, y = &quot;kcal_std&quot;) p_cortex + p_weight Model implementation (necocortex and mothers weight) \\[ \\begin{array}{cccr} K_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{N} N_{i} + \\beta_{M} M_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.2) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{N} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_N$ prior]}\\\\ \\beta_{M} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_M$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_milk_multi &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_N * neocortex_std + beta_M * mass_std, alpha ~ dnorm(0, .2), beta_N ~ dnorm(0, .5), beta_M ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_milk ) precis(model_milk_multi) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.13 -0.20 0.20 beta_N 0.64 0.23 0.27 1.01 beta_M -0.75 0.23 -1.12 -0.37 sigma 0.69 0.12 0.49 0.88 ct_milk &lt;- coeftab(model_milk_cortex, model_milk_weight, model_milk_multi, se = TRUE) plot_coeftab(ct_milk) data_milk %&gt;% dplyr::select(kcal_std, neocortex_std, mass_std) %&gt;% ggpairs(lower = list(continuous = wrap(ggally_points, colour = clr2, size = .9, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll2, color = clr2, adjust = 1)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) dag1 &lt;- dagify( K ~ M + N, N ~ M, exposure = &quot;M&quot;, outcome = &quot;K&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,1,.5), y = c(1,1, .4))) %&gt;% mutate(stage = if_else(name == &quot;K&quot;, &quot;response&quot;, if_else(name %in% c(&quot;M&quot;, &quot;N&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) dag2 &lt;- dagify( K ~ M + N, M ~ N, exposure = &quot;M&quot;, outcome = &quot;K&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,1,.5), y = c(1,1, .4))) %&gt;% mutate(stage = if_else(name == &quot;K&quot;, &quot;response&quot;, if_else(name %in% c(&quot;M&quot;, &quot;N&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) dag3 &lt;- dagify( K ~ M + N, M ~ U, N ~ U, exposure = &quot;M&quot;, outcome = &quot;K&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,1,.5, .5), y = c(1,1, 1,.4))) %&gt;% mutate(stage = if_else(name == &quot;K&quot;, &quot;response&quot;, if_else(name %in% c(&quot;M&quot;, &quot;N&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag1, clr_in = clr3) + plot_dag(dag2, clr_in = clr3) + plot_dag(dag3, clr_in = clr3) + plot_layout(nrow = 1) + plot_annotation(tag_levels = &quot;a&quot;) &amp; scale_y_continuous(limits = c(.35, 1.05)) &amp; coord_equal() &amp; theme(plot.tag = element_text(family = fnt_sel)) Counterfactual plots for DAG c) data_sim_mass &lt;- link(fit = model_milk_multi, data = tibble(mass_std = 0, neocortex_std = seq_cortex), vars = c(&quot;kcal_std&quot;)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(seq_cortex)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;kcal_std&quot;) data_sim_mass_pi &lt;- data_sim_mass %&gt;% group_by(row_idx) %&gt;% summarise(mean = mean(kcal_std), PI_lower = PI(kcal_std)[1], PI_upper = PI(kcal_std)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx), neocortex_std = seq_cortex[row_idx]) p_mass &lt;- data_sim_mass_pi %&gt;% ggplot() + geom_smooth(aes(x = neocortex_std, y = mean, ymin = PI_lower, ymax = PI_upper), color = clr2, fill = fll2, stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(clr0d, clr3), guide = &quot;none&quot;) + labs(y = &quot;counterfactual kcal&quot;, title = &quot;kcal at mass_std = 0&quot;) data_sim_cortex &lt;- link(fit = model_milk_multi, data = tibble(mass_std = seq_weight, neocortex_std = 0), vars = c(&quot;kcal_std&quot;)) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(seq_weight)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;kcal_std&quot;) data_sim_cortex_pi &lt;- data_sim_cortex %&gt;% group_by(row_idx) %&gt;% summarise(mean = mean(kcal_std), PI_lower = PI(kcal_std)[1], PI_upper = PI(kcal_std)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx), mass_std = seq_weight[row_idx]) p_cortex &lt;- data_sim_cortex_pi %&gt;% ggplot() + geom_smooth(aes(x = mass_std, y = mean, ymin = PI_lower, ymax = PI_upper), color = clr2, fill = fll2, stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(clr0d, clr3), guide = &quot;none&quot;) + labs(y = &quot;counterfactual kcal&quot;, title = &quot;kcal at neocortex_std = 0&quot;) p_mass + p_cortex &amp; coord_cartesian(ylim = c(-1, 2)) 6.3.2 Simulate a masking relationship DAG a) (\\(M \\rightarrow K \\leftarrow N \\leftarrow M\\)) n &lt;- 100 data_milk_sim1 &lt;- tibble(mass_std = rnorm(n = n), neocortex_std = rnorm(n = n, mean = mass_std), kcal_std = rnorm(n = n, mean = neocortex_std - mass_std)) data_milk_sim1 %&gt;% dplyr::select(kcal_std, neocortex_std, mass_std) %&gt;% ggpairs(lower = list(continuous = wrap(ggally_points, colour = clr0d, size = .9, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll0, color = clr0d, adjust = 1)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;))) DAG b) (\\(N \\rightarrow M \\rightarrow K \\leftarrow N\\)) data_milk_sim2 &lt;- tibble(neocortex_std = rnorm(n = n), mass_std = rnorm(n = n, mean = neocortex_std), kcal_std = rnorm(n = n, mean = neocortex_std - mass_std)) DAG c) (\\(U \\rightarrow N \\rightarrow M \\rightarrow K \\leftarrow N \\leftarrow U\\)) data_milk_sim3 &lt;- tibble(unsampled = rnorm(n = n), neocortex_std = rnorm(n = n, mean = unsampled), mass_std = rnorm(n = n, mean = unsampled), kcal_std = rnorm(n = n, mean = neocortex_std - mass_std)) model_milk_cortex_sim &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_N * neocortex_std, alpha ~ dnorm(0, .2), beta_N ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_milk_sim1 ) model_milk_weight_sim &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_M * mass_std, alpha ~ dnorm(0, .2), beta_M ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_milk_sim1 ) model_milk_multi_sim &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_N * neocortex_std + beta_M * mass_std, alpha ~ dnorm(0, .2), beta_N ~ dnorm(0, .5), beta_M ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_milk_sim1 ) ct_milk_sim &lt;- coeftab(model_milk_cortex_sim, model_milk_weight_sim, model_milk_multi_sim, se = TRUE) plot_coeftab(ct_milk_sim) Computing the Marcov Equivalence Set dag_milk &lt;- dagitty(&quot;dag{ M -&gt; K &lt;- N M -&gt; N}&quot;) coordinates(dag_milk) &lt;- list( x = c( M = 0, N = 1, K = .5), y = c( M = 1, N = 1, K = .3)) dag_milk %&gt;% node_equivalent_dags() %&gt;% mutate(stage = &quot;predictor&quot;) %&gt;% plot_dag() + coord_cartesian(xlim = c(-.1, 1.1), ylim = c(.2, 1.1))+ facet_wrap(~ dag) 6.4 Categorical Variables 6.4.1 Indicator vs. Index variable (binary categories) Taking gender into account for the height model (but not caring about weight). data(Howell1) data_height &lt;- as_tibble(Howell1) %&gt;% mutate(sex = if_else(male == 1, 2, 1)) Modeling as dummy/indicator variable \\[ \\begin{array}{cccr} h_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + \\beta_{m} m_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(178, 20) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{m} &amp; \\sim &amp; Normal(0, 10) &amp; \\textrm{[$\\beta_N$ prior]}\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] Modeling as index variable \\[ \\begin{array}{ccccr} h_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; &amp;\\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha_{\\textrm{sex}[i]} &amp; &amp;\\textrm{[linear model]}\\\\ \\alpha_j &amp; \\sim &amp; Normal(178, 20) &amp; \\textrm{for}~j = 1..2 &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\sigma &amp; \\sim &amp; Uniform(0,50) &amp; &amp;\\textrm{[$\\sigma$ prior]} \\end{array} \\] Demonstrating that in the indicator variable approach, the uncertainty of estimates is higher for the male type (coded as 1), since this one is influenced by the uncertainty of two priors: indicator_prior &lt;- tibble(mu_female = rnorm(1e4, 178, 20), mu_male = rnorm(1e4, 178, 20) + rnorm(1e4, 0, 10)) indicator_prior %&gt;% precis() %&gt;% as.matrix() %&gt;% knitr::kable() mean sd 5.5% 94.5% histogram mu_female 177.7964 19.99779 145.6110 209.8597 ▁▁▁▁▂▃▇▇▇▅▃▁▁▁▁ mu_male 177.5124 22.49842 141.7337 213.0894 ▁▁▁▃▇▇▂▁▁▁ indicator_long &lt;- indicator_prior %&gt;% pivot_longer(cols = everything(), names_to = &quot;sex&quot;, values_to = &quot;height&quot;, names_transform = list(sex = function(str){str_remove(string = str, &quot;mu_&quot;)})) ggplot(indicator_long) + geom_density(data = indicator_long %&gt;% dplyr::select(-sex), aes(x = height, y = ..count..), color = clr0d, fill = fll0) + geom_density(aes(x = height, y = ..count.., color = sex, fill = after_scale(clr_alpha(color)))) + facet_wrap(sex ~ . ) + scale_color_manual(values = c(clr1, clr2), guide = &quot;none&quot;) Implementing the index variable approach: model_hight &lt;- quap( flist = alist( height ~ dnorm(mu, sigma), mu &lt;- alpha[sex], alpha[sex] ~ dnorm(178, 20), sigma ~ dunif(0,50) ), data = data_height ) precis(model_hight, depth = 2) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha[1] 134.91 1.61 132.34 137.48 alpha[2] 142.58 1.70 139.86 145.29 sigma 27.31 0.83 25.99 28.63 hight_posterior_samples &lt;- extract.samples(model_hight) %&gt;% as_tibble() %&gt;% mutate(diff_sex = alpha[ ,1] - alpha[ ,2] ) The expected difference between the considered types is called a contrast: hight_posterior_samples %&gt;% precis() %&gt;% as.matrix() %&gt;% knitr::kable() mean sd 5.5% 94.5% histogram sigma 27.307086 0.822027 25.99673 28.620850 ▁▁▁▁▁▁▃▅▇▇▃▂▁▁▁ alpha.1 134.933243 1.599303 132.39721 137.457066 ▁▁▁▂▅▇▇▅▂▁▁▁▁ alpha.2 142.592035 1.708900 139.84870 145.308164 ▁▁▁▁▁▂▃▇▇▇▃▂▁▁▁ diff_sex -7.658793 2.341238 -11.38310 -3.867645 ▁▁▁▂▇▇▃▁▁▁ p_contrast1 &lt;- hight_posterior_samples %&gt;% ggplot() + geom_density(aes(x = alpha[,1], color = &quot;female&quot;, fill = after_scale(clr_alpha(color))))+ geom_density(aes(x = alpha[,2], color = &quot;male&quot;, fill = after_scale(clr_alpha(color)))) + geom_errorbarh(data = tibble(start = median(hight_posterior_samples$alpha[,1]), end = median(hight_posterior_samples$alpha[,2])), aes(y = 0, xmin = start, xmax = end), height = .01) + scale_color_manual(values = c(clr1, clr2), guide = &quot;none&quot;) + lims(y = c(-.01,.25))+ labs(x = &quot;height&quot;) + theme(axis.title.y = element_blank(), axis.text.y = element_blank()) p_contrast2 &lt;- hight_posterior_samples %&gt;% ggplot() + geom_density(aes(x = alpha[,2] - alpha[,1]), color = clr0d, fill = fll0) + labs(x = &quot;contrast height(male-female)&quot;) + lims(y = c(-.01,.25))+ theme(axis.title.y = element_blank(), axis.text.y = element_blank()) p_contrast1 + p_contrast2 + plot_layout(widths = c(1,.66)) 6.4.2 Multiple categories Taking the broad taxonomic unit into account for the milk model (but not caring about neocortex od weight). houses &lt;- c(&quot;Gryffindor&quot;, &quot;Hufflepuff&quot;, &quot;Ravenclaw&quot;, &quot;Slytherin&quot;) set.seed(63) data_milk_clade &lt;- milk %&gt;% as_tibble() %&gt;% mutate(kcal_std = standardize(`kcal.per.g`), clade_id = as.integer(clade), house_id = sample(rep(1:4, each = 8), size = length(clade)), house = houses[house_id]) \\[ \\begin{array}{ccccr} K_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; &amp;\\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha_{\\textrm{CLADE}[i]} &amp; &amp;\\textrm{[linear model]}\\\\ \\alpha_j &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{for}~j = 1..4 &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; &amp;\\textrm{[$\\sigma$ prior]} \\end{array} \\] model_milk_clade &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha[clade_id], alpha[clade_id] ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = data_milk_clade ) precis(model_milk_clade, depth = 2, pars = &quot;alpha&quot;) %&gt;% as_tibble_rn() %&gt;% mutate(clade_id = str_remove_all(param, pattern = &quot;[a-z\\\\[\\\\]]*&quot;) %&gt;% as.integer(), clade = fct_reorder(levels(data_milk$clade)[clade_id], clade_id)) %&gt;% ggplot(aes(y = clade)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(aes(xmin = `5.5%`, xmax =`94.5%`), color = clr0d, fill = clr0) + geom_point(aes(x = mean), shape = 21, size = 3, color = clr0d, fill = clr0) + scale_y_discrete(&quot;&quot;, limits = rev(levels(data_milk$clade))) + labs(x = &quot;expected kcal_std&quot;) adding another categorical variable: \\[ \\begin{array}{ccccr} K_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; &amp;\\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha_{\\textrm{CLADE}[i]} + \\alpha_{\\textrm{HOUSE}[i]} &amp; &amp;\\textrm{[linear model]}\\\\ \\alpha_{\\textrm{CLADE},j} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{for}~j = 1..4 &amp; \\textrm{[$\\alpha_{\\textrm{CLADE}}$ prior]}\\\\ \\alpha_{\\textrm{HOUSE},j} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{for}~j = 1..4 &amp; \\textrm{[$\\alpha_{\\textrm{CLADE}}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; &amp;\\textrm{[$\\sigma$ prior]} \\end{array} \\] model_milk_house &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha_clade[clade_id] + alpha_house[house_id], alpha_clade[clade_id] ~ dnorm(0, 0.5), alpha_house[house_id] ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = data_milk_clade ) precis(model_milk_house, depth = 2, pars = &quot;alpha&quot;) %&gt;% as_tibble_rn() %&gt;% mutate(type = str_remove(param, pattern = &quot;alpha_&quot;) %&gt;% str_remove(&quot;\\\\[[0-9]\\\\]&quot;), idx = str_extract(param, &quot;[0-9]&quot;) %&gt;% as.integer(), name = if_else(type == &quot;clade&quot;, levels(data_milk$clade)[idx], houses[idx])) %&gt;% ggplot(aes(y = name, color = type)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(aes(xmin = `5.5%`, xmax =`94.5%`)) + geom_point(aes(x = mean, fill = after_scale(clr_lighten(color))), shape = 21, size = 3 ) + scale_color_manual(values = c(clade = clr0d, house = clr3), guide = &quot;none&quot;) + facet_grid(type ~ . , scales = &quot;free_y&quot;, switch = &quot;y&quot;) + labs(x = &quot;expected kcal_std&quot;) + theme(axis.title.y = element_blank(), strip.placement = &quot;outside&quot;) 6.5 Homework E1 \\[ \\begin{array}{ccclr} 1) &amp; \\mu_i &amp; = &amp; \\alpha + \\beta x_{i} &amp;\\textrm{[simple linear regression]}\\\\ 2) &amp; \\mu_i &amp; = &amp; \\beta_{x} x_{i} + \\beta_{z} z_{i} &amp;\\textrm{[multiple linear regression]}\\\\ 3) &amp; \\mu_i &amp; = &amp; \\alpha + \\beta (x_{i} - z_{i}) &amp;\\textrm{[simple linear regression]}\\\\ 4) &amp; \\mu_i &amp; = &amp; \\alpha + \\beta_{x} x_{i} + \\beta_{z} z_{i} &amp;\\textrm{[multiple linear regression]}\\\\ \\end{array} \\] E2 \\[ \\begin{array}{cclr} d_i &amp; = &amp; \\alpha + \\beta_{y} y_i + \\beta_{p} p_{i}&amp; \\textrm{[linear model]}\\\\ \\end{array} \\] E3 \\[ \\begin{array}{ccclr} 1) &amp; t_i &amp; = &amp; \\alpha_{f} + \\beta_{ff} f_i &amp; \\textrm{[linear model]}\\\\ 2) &amp; t_i &amp; = &amp; \\alpha_{s} + \\beta_{ss} s_{i} &amp; \\textrm{[linear model]}\\\\ 3) &amp; t_i &amp; = &amp; \\alpha + \\beta_{f} f_i + \\beta_{s} s_{i} &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] \\(\\beta_{f} \\ge 0\\) \\(\\beta_{ss} \\ge 0\\) \\(t \\sim f\\) (\\(\\beta_{f} \\gt \\beta_{ff}\\)) \\(t \\sim s\\) (\\(\\beta_{s} \\gt \\beta_{ss}\\)) \\(f \\sim -s\\) E4 1), 3), 4) and 5) (models should contain \\(k - 1\\) indicator variables) M1 n &lt;- 100 data_spurious2 &lt;- tibble(u = rnorm(n), x = rnorm(n, mean = u), y = rnorm(n, mean = -u), z = rnorm(n, mean = u) ) data_spurious2 %&gt;% ggpairs() model_spurious2a &lt;- quap( flist = alist( z ~ dnorm(mu, sigma), mu &lt;- alpha + beta_x * x, alpha ~ dnorm(0, .2), beta_x ~ dnorm(0, .75), sigma ~ dexp(1)), data = data_spurious2 ) model_spurious2b &lt;- quap( flist = alist( z ~ dnorm(mu, sigma), mu &lt;- alpha + beta_y * y, alpha ~ dnorm(0, .2), beta_y ~ dnorm(0, .75), sigma ~ dexp(1)), data = data_spurious2 ) model_spurious2c &lt;- quap( flist = alist( z ~ dnorm(mu, sigma), mu &lt;- alpha + beta_x * x + beta_y * y, alpha ~ dnorm(0, .2), beta_x ~ dnorm(0, .75), beta_y ~ dnorm(0, .75), sigma ~ dexp(1)), data = data_spurious2 ) ct_spur &lt;- coeftab(model_spurious2a, model_spurious2b, model_spurious2c, se = TRUE) plot_coeftab(ct_spur) M2 data_masked &lt;- tibble(u = rnorm(n), x = rnorm(n, mean = u), y = rnorm(n, mean = u), z = rnorm(n, mean = x-y) ) data_masked %&gt;% ggpairs() model_masked_a &lt;- quap( flist = alist( z ~ dnorm(mu, sigma), mu &lt;- alpha + beta_x * x, alpha ~ dnorm(0, .2), beta_x ~ dnorm(0, .75), sigma ~ dexp(1)), data = data_masked ) model_masked_b &lt;- quap( flist = alist( z ~ dnorm(mu, sigma), mu &lt;- alpha + beta_y * y, alpha ~ dnorm(0, .2), beta_y ~ dnorm(0, .75), sigma ~ dexp(1)), data = data_masked ) model_masked_c &lt;- quap( flist = alist( z ~ dnorm(mu, sigma), mu &lt;- alpha + beta_x * x + beta_y * y, alpha ~ dnorm(0, .2), beta_x ~ dnorm(0, .75), beta_y ~ dnorm(0, .75), sigma ~ dexp(1)), data = data_masked ) ct_masked &lt;- coeftab(model_masked_a, model_masked_b, model_masked_c, se = TRUE) plot_coeftab(ct_masked) M3 dag &lt;- dagify( D ~ A, M ~ A, exposure = &quot;A&quot;, outcome = &quot;M&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,.5,1), y = c(1, .4, 1))) %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag, clr_in = clr3) + scale_y_continuous(limits = c(.35, 1.05)) + coord_equal() M4 data_waffle_lds &lt;- data_waffle %&gt;% left_join(read_tsv(&quot;data/lds_by_state_2019.tsv&quot;)) %&gt;% mutate(lds_std = standardize(lds_perc), lds_perc_log10 = log10(lds_perc), lds_log10_std = standardize(lds_perc_log10)) data_waffle_lds %&gt;% dplyr::select(lds_perc, lds_perc_log10) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value)) + geom_histogram(bins = 10, color = clr0d, fill = fll0) + facet_wrap(name ~ ., scales = &quot;free&quot;) model_lds &lt;- quap( flist = alist( divorce_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_age * median_age_std + beta_marriage * marriage_std + beta_lds * lds_log10_std, alpha ~ dnorm(0, .2), beta_age ~ dnorm(0, .5), beta_marriage ~ dnorm(0, .5), beta_lds ~ dnorm(0, .5), sigma ~ dexp(1) ), data = data_waffle_lds ) precis(model_lds) #&gt; mean sd 5.5% 94.5% #&gt; alpha -1.107414e-06 0.09381855 -0.1499413 0.14993905 #&gt; beta_age -6.980063e-01 0.15085492 -0.9391016 -0.45691097 #&gt; beta_marriage 7.808701e-02 0.16279768 -0.1820951 0.33826916 #&gt; beta_lds -2.954843e-01 0.14942569 -0.5342954 -0.05667314 #&gt; sigma 7.511727e-01 0.07463009 0.6318994 0.87044597 precis(model_lds, depth = 2, pars = &quot;beta&quot;) %&gt;% as_tibble_rn() %&gt;% mutate(type = str_remove(param, pattern = &quot;beta_&quot;)) %&gt;% ggplot(aes(y = type)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(aes(xmin = `5.5%`, xmax =`94.5%`), color = clr0d) + geom_point(aes(x = mean), color = clr0d, fill = clr0, shape = 21, size = 3 ) + theme(axis.title.y = element_blank()) posterior_prediction &lt;- link(model_lds) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(data_waffle$divorce_std)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;divorce_predicted&quot;) %&gt;% group_by(row_idx) %&gt;% summarise(divorce_predicted_mean = mean(divorce_predicted), lower_pi = PI(divorce_predicted)[1], upper_pi = PI(divorce_predicted)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx)) %&gt;% left_join(data_waffle %&gt;% mutate(row_idx = row_number()), . ) posterior_simmulation &lt;- sim(model_lds) %&gt;% as_tibble() %&gt;% set_names(nm = seq_along(data_waffle$divorce_std)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;row_idx&quot;, values_to = &quot;divorce_predicted&quot;) %&gt;% group_by(row_idx) %&gt;% summarise(lower_pi = PI(divorce_predicted)[1], upper_pi = PI(divorce_predicted)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx)) %&gt;% left_join(data_waffle %&gt;% mutate(row_idx = row_number()), . ) ggplot(mapping = aes(x = divorce_std)) + geom_abline(slope = 1, size = .7, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(data = posterior_prediction, aes(ymin = lower_pi, ymax = upper_pi, color = Loc %in% c(&quot;ID&quot;, &quot;UT&quot;)))+ geom_point(data = posterior_prediction, aes(y = divorce_predicted_mean, color = Loc %in% c(&quot;ID&quot;, &quot;UT&quot;), fill = after_scale(clr_lighten(color ,.5))), shape = 21, size = 1.5)+ geom_text(data = posterior_prediction %&gt;% filter(Loc %in% c(&quot;ID&quot;, &quot;ME&quot;, &quot;RI&quot;, &quot;UT&quot;)), aes(x = divorce_std - .15, y = divorce_predicted_mean, label = Loc)) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) M5 dag1 &lt;- dagify( O ~ W + E + P, W ~ P, E ~ P, exposure = &quot;P&quot;, outcome = &quot;O&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,.5,1, .5), y = c(1,1, 1,.4))) %&gt;% mutate(stage = if_else(name == &quot;O&quot;, &quot;response&quot;, if_else(name %in% c(&quot;W&quot;, &quot;E&quot;, &quot;P&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag1, clr_in = clr3) + # plot_dag(dag2, clr_in = clr3) &amp; # scale_y_continuous(limits = c(.35, 1.05)) &amp; coord_equal() with \\(o\\) as obesity rate \\(p\\) as gasoline price \\(e\\) as money spend on eating out \\(w\\) as average distance walked \\[ \\begin{array}{cclr} o_i &amp; \\sim &amp; Normal(\\mu_i, \\sigma) &amp; \\textrm{[likelyhood]}\\\\ \\mu_i &amp; = &amp; \\alpha_{p} + \\beta_{p} p_i &amp; \\textrm{[linear model (price only)]}\\\\ \\mu_i &amp; = &amp; \\alpha_{w} + \\beta_{w} w_i &amp; \\textrm{[linear model (walking)]}\\\\ \\mu_i &amp; = &amp; \\alpha_{e} + \\beta_{e} e_i &amp; \\textrm{[linear model (eating out)]}\\\\ \\mu_i &amp; = &amp; \\alpha_{m} + \\beta_{pp} + \\beta_{ww} w_i + p_i + \\beta_{ee} e_i &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] H1 dagitty(&#39;dag{ M -&gt; A -&gt; D }&#39;) %&gt;% impliedConditionalIndependencies() #&gt; D _||_ M | A This reads as conditional on \\(A\\), \\(D\\) is independent from \\(M\\). given the results from model_multiple, this seems plausible as the multiple model greatly reduces the effect of beat_M: precis(model_multiple) %&gt;% round(digits = 2) %&gt;% as.matrix() %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.10 -0.16 0.16 beta_A -0.61 0.15 -0.85 -0.37 beta_M -0.07 0.15 -0.31 0.18 sigma 0.79 0.08 0.66 0.91 plot_coeftab(ct) + scale_color_manual(values = rep(clr0d, 3), guide = &quot;none&quot;) Actually this one is a markov equivalent of the dag investigated in the main text (and all members of that set are consistent with the model): dag_h1 &lt;- dagitty(&#39;dag{ M -&gt; A -&gt; D }&#39;) coordinates(dag_h1) &lt;- list( x = c( M = 0, A = 1, D = .5), y = c( M = 1, A = 1, D = .3)) dag_h1 %&gt;% node_equivalent_dags() %&gt;% mutate(stage = &quot;predictor&quot;) %&gt;% plot_dag() + coord_equal(xlim = c(-.1, 1.1), ylim = c(.2, 1.1))+ facet_wrap(~ dag) H2 model_counterfactual_marriage &lt;- quap( flist = alist( # A -&gt; D divorce_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_A * median_age_std, alpha ~ dnorm( 0, 0.2 ), beta_A ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ), # M -&gt; A median_age_std ~ dnorm( mu_A, sigma_A ), mu_A &lt;- alpha_A + beta_MA * marriage_std, alpha_A ~ dnorm( 0, 0.2 ), beta_MA ~ dnorm( 0, 0.5 ), sigma_A ~ dexp(1) ), data = data_waffle ) precis(model_counterfactual_marriage) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.10 -0.16 0.16 beta_A -0.57 0.11 -0.74 -0.39 sigma 0.79 0.08 0.66 0.91 alpha_A 0.00 0.09 -0.14 0.14 beta_MA -0.69 0.10 -0.85 -0.54 sigma_A 0.68 0.07 0.57 0.79 M_seq &lt;- seq(-2, 2, length.out = 30) data_sim &lt;- sim(fit = model_counterfactual_marriage, data = tibble(marriage_std = M_seq), vars = c(&quot;median_age_std&quot;, &quot;divorce_std&quot;)) %&gt;% unpack_sim() data_sim_pi &lt;- data_sim %&gt;% group_by(row_idx, parameter) %&gt;% summarise(mean = mean(value), PI_lower = PI(value)[1], PI_upper = PI(value)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx), marriage_std = M_seq[row_idx]) %&gt;% arrange(parameter, marriage_std) data_sim_pi %&gt;% ggplot() + geom_smooth(aes(x = marriage_std, y = mean, ymin = PI_lower, ymax = PI_upper, color = parameter, fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(clr0d, clr3), guide = &quot;none&quot;) + # labs(y = &quot;counterfactual value&quot;, title = &quot;Counterfactual effects of age at marriage on&quot;) + facet_wrap(parameter ~ .) M_seq2 &lt;- c(data_waffle$median_age_std, data_waffle$median_age_std/2) m_rate_california &lt;- which(data_waffle$Location == &quot;Idaho&quot;) M_seq2 &lt;- c(data_waffle$median_age_std[m_rate_california], data_waffle$median_age_std[m_rate_california]/2) data_sim2 &lt;- sim(fit = model_counterfactual_marriage, data = tibble(marriage_std = M_seq2), vars = c(&quot;median_age_std&quot;, &quot;divorce_std&quot;)) %&gt;% data.frame() %&gt;% pivot_longer(cols = everything()) %&gt;% separate(name, into = c(&quot;param&quot;, &quot;rn&quot;), sep = &#39;\\\\.&#39;, convert = TRUE) %&gt;% mutate(group = c(&quot;org&quot;, &quot;half&quot;)[1 + (rn &gt; (length(M_seq2)/2))]) %&gt;% filter(param == &quot;divorce_std&quot;) %&gt;% dplyr::select(-rn) %&gt;% # mutate(value = value * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) %&gt;% pivot_wider(names_from = group, values_from = value) %&gt;% unnest() %&gt;% mutate(diff = half - org) data_sim2 %&gt;% ggplot(aes(x = diff)) + geom_density(fill = fll0, color = clr0d) data_sim2 %&gt;% ggplot() + geom_density(aes(x = org, color = &quot;orgiginal&quot;, fill = after_scale(clr_alpha(color)))) + geom_density(aes(x = half, color = &quot;half&quot;, fill = after_scale(clr_alpha(color)))) + scale_color_manual(values = c(original = clr0d, half = clr3)) + labs(x = &quot;divorce_std&quot;) + theme(legend.position = &quot;bottom&quot;) mean(data_sim2$diff) #&gt; [1] 0.4763069 Halfing a states marriage rate would on average increase the divorce rate by ~ 0 standard deviations. H3 dag1 &lt;- dagify( K ~ M + N, N ~ M, exposure = &quot;M&quot;, outcome = &quot;K&quot;) %&gt;% tidy_dagitty(.dagitty = .,layout = tibble(x = c(0,1,.5), y = c(1,1,.4))) %&gt;% mutate(stage = if_else(name == &quot;K&quot;, &quot;response&quot;, if_else(name %in% c(&quot;M&quot;, &quot;N&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag1, clr_in = clr3) + coord_equal() model_counterfactual_milk &lt;- quap( flist = alist( # M -&gt; K &lt;- N kcal_std ~ dnorm( mu, sigma ) , mu &lt;- alpha + beta_MK * mass_std + beta_NK * neocortex_std, alpha ~ dnorm( 0, 0.2 ), beta_MK ~ dnorm( 0, 0.5 ), beta_NK ~ dnorm( 0, 0.5 ), sigma ~ dexp( 1 ), # M -&gt; N neocortex_std ~ dnorm( mu_N, sigma_N ), mu_N &lt;- alpha_N + beta_MN * mass_std, alpha_N ~ dnorm( 0, 0.2 ), beta_MN ~ dnorm( 0, 0.5 ), sigma_N ~ dexp(1) ), data = data_milk ) precis(model_counterfactual_milk) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.13 -0.20 0.20 beta_MK -0.75 0.23 -1.12 -0.37 beta_NK 0.64 0.23 0.27 1.01 sigma 0.69 0.12 0.49 0.88 alpha_N 0.00 0.12 -0.19 0.19 beta_MN 0.68 0.15 0.44 0.93 sigma_N 0.63 0.11 0.46 0.80 W_seq &lt;- seq(-2, 2, length.out = 30) data_sim &lt;- sim(fit = model_counterfactual_milk, data = tibble(mass_std = W_seq), vars = c(&quot;neocortex_std&quot;, &quot;kcal_std&quot;)) %&gt;% unpack_sim() data_sim_pi &lt;- data_sim %&gt;% group_by(row_idx, parameter) %&gt;% summarise(mean = mean(value), PI_lower = PI(value)[1], PI_upper = PI(value)[2]) %&gt;% ungroup() %&gt;% mutate(row_idx = as.numeric(row_idx), mass_std = M_seq[row_idx]) %&gt;% arrange(parameter, mass_std) data_sim_pi %&gt;% ggplot() + geom_smooth(aes(x = mass_std, y = mean, ymin = PI_lower, ymax = PI_upper, color = parameter, fill = after_scale(clr_alpha(color))), stat = &quot;identity&quot;, size = .4) + scale_color_manual(values = c(clr0d, clr3), guide = &quot;none&quot;) + # labs(y = &quot;counterfactual value&quot;, title = &quot;Counterfactual effects of age at marriage on&quot;) + facet_wrap(parameter ~ .) M_seq2 &lt;- (log(c(15, 30)) - mean(log(milk$mass))) / sd(log(milk$mass)) data_sim2 &lt;- sim(fit = model_counterfactual_milk, data = tibble(mass_std = M_seq2), vars = c(&quot;neocortex_std&quot;, &quot;kcal_std&quot;)) %&gt;% data.frame() %&gt;% pivot_longer(cols = everything()) %&gt;% separate(name, into = c(&quot;param&quot;, &quot;rn&quot;), sep = &#39;\\\\.&#39;, convert = TRUE) %&gt;% mutate(group = c(&quot;org&quot;, &quot;double&quot;)[1 + (rn &gt; (length(M_seq2)/2))]) %&gt;% filter(param == &quot;kcal_std&quot;) %&gt;% dplyr::select(-rn) %&gt;% # mutate(value = value * sd(data_waffle$Divorce) + mean(data_waffle$Divorce)) %&gt;% pivot_wider(names_from = group, values_from = value) %&gt;% unnest() %&gt;% mutate(diff = double - org) data_sim2 %&gt;% ggplot(aes(x = diff)) + geom_density(fill = fll0, color = clr0d) data_sim2 %&gt;% ggplot() + geom_density(aes(x = org, color = &quot;orgiginal&quot;, fill = after_scale(clr_alpha(color)))) + geom_density(aes(x = double, color = &quot;double&quot;, fill = after_scale(clr_alpha(color)))) + scale_color_manual(values = c(original = clr0d, double = clr3)) + labs(x = &quot;kcal_std&quot;) + theme(legend.position = &quot;bottom&quot;) quantile(data_sim2$diff, probs = c(.05, .5, .95)) #&gt; 5% 50% 95% #&gt; -2.0362310 -0.0628029 1.7214482 mean(data_sim2$diff) #&gt; [1] -0.09856929 Following the paths of the dag to get the causal effect. To then get to the magnitude of the contrast, scale by max - min. prec_out &lt;- precis(model_counterfactual_milk) # ((M -&gt; N) * (M -&gt; K) ) + (M -&gt; K) * delta_input (prec_out[&quot;beta_MN&quot;, &quot;mean&quot;] * prec_out[&quot;beta_NK&quot;, &quot;mean&quot;] + prec_out[&quot;beta_MK&quot;, &quot;mean&quot;] ) * diff(M_seq2) #&gt; [1] -0.1264984 H4 data_south &lt;- data_waffle %&gt;% dplyr::select(Location, South, ends_with(&quot;_std&quot;)) dag &lt;- dagify( D ~ M + A + S, M ~ A, A ~ S, exposure = &quot;A&quot;, outcome = &quot;M&quot;) %&gt;% tidy_dagitty(.dagitty = ., layout = tibble(x = c(0,.5, .5, 1), y = c(1, .6, 1.4, 1))) %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M&quot;, &quot;S&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) plot_dag(dag, clr_in = clr3) + scale_y_continuous(limits = c(.5, 1.5)) + coord_equal() dagitty(&#39;dag{ D &lt;- A -&gt; M; D &lt;- S -&gt; A; M -&gt; D }&#39;) %&gt;% impliedConditionalIndependencies() #&gt; M _||_ S | A model_south_multi &lt;- quap( flist = alist( marriage_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_SD * South + beta_AD * median_age_std, alpha ~ dnorm(0, .2), beta_SD ~ dnorm(0,.5), beta_AD ~ dnorm(0,.5), sigma ~ dexp(1) ), data = data_south ) precis(model_south_multi) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.04 0.10 -0.12 0.19 beta_SD -0.17 0.19 -0.48 0.14 beta_AD -0.71 0.10 -0.87 -0.56 sigma 0.68 0.07 0.57 0.78 M could be independent of S (large spread around zero) precis(model_south_multi)[&quot;beta_SD&quot;, ] %&gt;% round(digits = 2) #&gt; mean sd 5.5% 94.5% #&gt; beta_SD -0.17 0.19 -0.48 0.14 Additional scenario (from Jake Thompson) dag_coords &lt;- tibble(name = c(&quot;S&quot;, &quot;A&quot;, &quot;M&quot;, &quot;D&quot;), x = c(1, 1, 2, 3), y = c(3, 1, 2, 1)/2) dagify(D ~ A + M, M ~ A + S, A ~ S, coords = dag_coords) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;A&quot;, &quot;M&quot;, &quot;S&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(.3, 1.7)) + coord_equal() div_dag &lt;- dagitty(&quot;dag{S -&gt; M -&gt; D; S -&gt; A -&gt; D; A -&gt; M}&quot;) impliedConditionalIndependencies(div_dag) #&gt; D _||_ S | A, M model_south_multi2 &lt;- quap( flist = alist( divorce_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_S * South + beta_A * median_age_std + beta_M * marriage_std, alpha ~ dnorm(0, .2), beta_S ~ dnorm(0,.5), beta_A ~ dnorm(0,.5), beta_M ~ dnorm(0,.5), sigma ~ dexp(1) ), data = data_south ) precis(model_south_multi2) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha -0.08 0.11 -0.25 0.09 beta_S 0.35 0.22 0.01 0.69 beta_A -0.56 0.15 -0.80 -0.32 beta_M -0.04 0.15 -0.28 0.19 sigma 0.76 0.08 0.64 0.88 precis(model_south_multi2)[&quot;beta_S&quot;, ] %&gt;% round(digits = 2) #&gt; mean sd 5.5% 94.5% #&gt; beta_S 0.35 0.22 0.01 0.69 6.6 {brms} section 6.6.1 Age at marriage Model Note the sample_prior = TRUE to also sample from the prior (as well as from the posterior). Prior samples are extracted with prior_draws(). brms_c5_model_age &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + median_age_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, sample_prior = TRUE, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_age&quot;) brms_age_prior &lt;- prior_draws(brms_c5_model_age) %&gt;% as_tibble() brms_age_prior %&gt;% slice_sample(n = 50) %&gt;% rownames_to_column(&quot;draw&quot;) %&gt;% expand(nesting(draw, Intercept, b), a = c(-2, 2)) %&gt;% mutate(d = Intercept + b * a) %&gt;% ggplot(aes(a,d, group = draw)) + geom_line(color = clr0d %&gt;% clr_alpha()) + labs(x = &quot;median_age_std&quot;, y = &quot;divorce_rate_std&quot;) Getting to the posterior predictions with fitted(): nd &lt;- tibble(median_age_std = seq(from = -3, to = 3.2, length.out = 30)) # now use `fitted()` to get the model-implied trajectories fitted(object = brms_c5_model_age, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = median_age_std)) + geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, color = clr0d, fill = fll0) + geom_point(data = data_waffle, aes(y = divorce_std), color = clr_dark )+ labs(x = &quot;median_age_std&quot;, y = &quot;divorce_rate_std&quot;) \\(\\rightarrow\\) The posterior for median_age_std (\\(\\beta_{age}\\)) is reliably negative (look at Estimate and 95% quantiles )… print(brms_c5_model_age) #&gt; Family: gaussian #&gt; Links: mu = identity; sigma = identity #&gt; Formula: divorce_std ~ 1 + median_age_std #&gt; Data: data_waffle (Number of observations: 50) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; Intercept 0.00 0.10 -0.20 0.20 1.00 3936 2758 #&gt; median_age_std -0.57 0.11 -0.79 -0.34 1.00 3906 3129 #&gt; #&gt; Family Specific Parameters: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; sigma 0.82 0.08 0.68 1.00 1.00 4302 3021 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). 6.6.2 Marriage rate Model brms_c5_model_marriage &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + marriage_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;brms/brms_c5_model_marriage&quot;) … smaller magnitude for the marriage rate model: print(brms_c5_model_marriage) #&gt; Family: gaussian #&gt; Links: mu = identity; sigma = identity #&gt; Formula: divorce_std ~ 1 + marriage_std #&gt; Data: data_waffle (Number of observations: 50) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; Intercept 0.00 0.11 -0.22 0.22 1.00 4602 2813 #&gt; marriage_std 0.35 0.13 0.09 0.61 1.00 4325 3000 #&gt; #&gt; Family Specific Parameters: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; sigma 0.95 0.10 0.78 1.16 1.00 4404 3059 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). nd &lt;- tibble(marriage_std = seq(from = -2.5, to = 3.5, length.out = 30)) # now use `fitted()` to get the model-implied trajectories fitted(object = brms_c5_model_marriage, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = marriage_std)) + geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, color = clr0d, fill = fll0) + geom_point(data = data_waffle, aes(y = divorce_std), color = clr_dark )+ labs(x = &quot;marriage_rate_std&quot;, y = &quot;divorce_rate_std&quot;) 6.6.3 Multiple regression brms_c5_model_multiple &lt;- brm( data = data_waffle, family = gaussian, divorce_std ~ 1 + marriage_std + median_age_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_multiple&quot;) print(brms_c5_model_multiple) #&gt; Family: gaussian #&gt; Links: mu = identity; sigma = identity #&gt; Formula: divorce_std ~ 1 + marriage_std + median_age_std #&gt; Data: data_waffle (Number of observations: 50) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; Intercept 0.00 0.10 -0.19 0.19 1.00 3829 2943 #&gt; marriage_std -0.06 0.16 -0.37 0.25 1.00 3291 2718 #&gt; median_age_std -0.60 0.16 -0.92 -0.29 1.00 2859 2440 #&gt; #&gt; Family Specific Parameters: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; sigma 0.83 0.09 0.68 1.02 1.00 3553 2380 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). mixedup::summarise_model(brms_c5_model_multiple) #&gt; Group Effect Variance SD SD_2.5 SD_97.5 Var_prop #&gt; Residual 0.68 0.83 0.68 1.02 1.00 #&gt; Term Value SE Lower_2.5 Upper_97.5 #&gt; Intercept 0.00 0.10 -0.19 0.19 #&gt; marriage_std -0.06 0.16 -0.37 0.25 #&gt; median_age_std -0.60 0.16 -0.92 -0.29 bind_cols( as_draws_df(brms_c5_model_age) %&gt;% transmute(`brms_age-beta_age` = b_median_age_std), as_draws_df(brms_c5_model_marriage) %&gt;% transmute(`brms_marriage-beta_marriage` = b_marriage_std), as_draws_df(brms_c5_model_multiple) %&gt;% transmute(`brms_multi-beta_marriage` = b_marriage_std, `brms_multi-beta_age` = b_median_age_std) ) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% separate(col = name, into = c(&quot;fit&quot;, &quot;parameter&quot;), sep = &quot;-&quot;) %&gt;% ggplot(aes(x = mean, xmin = ll, xmax = ul, y = fit)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_pointrange(color = clr0d, fill = clr0, shape = 21) + facet_wrap(~ parameter, ncol = 1, labeller = label_parsed) + theme(axis.title = element_blank()) Simulating divorce data n &lt;- 50 sim_d &lt;- tibble(age = rnorm(n, mean = 0, sd = 1), mar = rnorm(n, mean = -age, sd = 1), div = rnorm(n, mean = age, sd = 1)) brms_c5_model_age_sim &lt;- update(brms_c5_model_age, newdata = sim_d, formula = div ~ 1 + age, seed = 42, file = &quot;brms/brms_c5_model_age_sim&quot;) brms_c5_model_marriage_sim &lt;- update(brms_c5_model_marriage, newdata = sim_d, formula = div ~ 1 + mar, seed = 42, file = &quot;brms/brms_c5_model_marriage_sim&quot;) brms_c5_model_multiple_sim &lt;- update(brms_c5_model_multiple, newdata = sim_d, formula = div ~ 1 + mar + age, seed = 42, file = &quot;brms/brms_c5_model_multiple_sim&quot;) bind_cols( as_draws_df(brms_c5_model_age_sim) %&gt;% transmute(`brms_age-beta_age` = b_age), as_draws_df(brms_c5_model_marriage_sim) %&gt;% transmute(`brms_marriage-beta_marriage` = b_mar), as_draws_df(brms_c5_model_multiple_sim) %&gt;% transmute(`brms_multi-beta_marriage` = b_mar, `brms_multi-beta_age` = b_age) ) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% separate(col = name, into = c(&quot;fit&quot;, &quot;parameter&quot;), sep = &quot;-&quot;) %&gt;% ggplot(aes(x = mean, xmin = ll, xmax = ul, y = fit)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_pointrange(color = clr0d, fill = clr0, shape = 21) + facet_wrap(~ parameter, ncol = 1, labeller = label_parsed) + theme(axis.title = element_blank()) 6.6.4 Multivariate Posteriors brms_c5_model_residuals_marriage &lt;- brm( data = data_waffle, family = gaussian, marriage_std ~ 1 + median_age_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_residuals_marriage&quot;) fitted(brms_c5_model_residuals_marriage) %&gt;% data.frame() %&gt;% bind_cols(data_waffle) %&gt;% as_tibble() %&gt;% ggplot(aes(x = median_age_std, y = marriage_std)) + geom_point(color = clr_dark) + geom_segment(aes(xend = median_age_std, yend = Estimate), size = .5, linetype = 3) + geom_line(aes(y = Estimate), color = clr0d) + ggrepel::geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;WY&quot;, &quot;ND&quot;, &quot;ME&quot;, &quot;HI&quot;, &quot;DC&quot;)), aes(label = Loc), size = 3, seed = 14, family = fnt_sel) + labs(x = &quot;median_age_std&quot;, y = &quot;marriage_std&quot;) residual_data &lt;- residuals(brms_c5_model_residuals_marriage) %&gt;% as_tibble() %&gt;% bind_cols(data_waffle) brms_c5_model_residuals_data &lt;- brm( data = residual_data, family = gaussian, divorce_std ~ 1 + Estimate, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_residuals_data&quot;) nd &lt;- tibble(Estimate = seq(from = -2, to = 2, length.out = 30)) residuals_intervals &lt;- fitted(object = brms_c5_model_residuals_data, newdata = nd) %&gt;% as_tibble() %&gt;% rename(mean = &quot;Estimate&quot;) %&gt;% bind_cols(nd) residual_data %&gt;% ggplot(aes(x = Estimate, y = divorce_std)) + geom_smooth(data = residuals_intervals, aes(y = mean, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, color = clr0d, fill = fll0) + geom_vline(xintercept = 0, linetype = 3, color = clr_dark) + geom_point(color = clr_dark) + ggrepel::geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;WY&quot;, &quot;ND&quot;, &quot;ME&quot;, &quot;HI&quot;, &quot;DC&quot;)), aes(label = Loc), size = 3, seed = 5, family = fnt_sel) Don’t use residuals as input data for another model - this ignores a ton of uncertainty: residual_data %&gt;% ggplot(aes(x = Estimate, y = divorce_std)) + geom_vline(xintercept = 0, linetype = 3, color = clr_dark) + geom_pointrange(aes(xmin = `Q2.5`, xmax = `Q97.5`), color = clr0d, fill = clr0, shape = 21) + ggrepel::geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;RI&quot;, &quot;ME&quot;, &quot;UT&quot;, &quot;ID&quot;)), aes(label = Loc), size = 3, seed = 5, family = fnt_sel) Posterior prediction plot: fitted(brms_c5_model_multiple) %&gt;% as_tibble() %&gt;% bind_cols(data_waffle) %&gt;% ggplot(aes(x = divorce_std, y = Estimate)) + geom_abline(slope = 1, linetype = 3, color = clr_dark) + geom_pointrange(aes(ymin = `Q2.5`, ymax = `Q97.5`), color = clr0d, fill = clr0, shape = 21) + ggrepel::geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;RI&quot;, &quot;ME&quot;, &quot;UT&quot;, &quot;ID&quot;)), aes(label = Loc), size = 3, seed = 5, family = fnt_sel) brms_c5_model_spurious &lt;- brm( data = data_spurious, family = gaussian, y ~ 1 + x_real + x_spur, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_spurious&quot;) mixedup::extract_fixef(brms_c5_model_spurious) #&gt; # A tibble: 3 x 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Intercept 0.05 0.096 -0.135 0.242 #&gt; 2 x_real 0.902 0.146 0.619 1.18 #&gt; 3 x_spur 0.094 0.108 -0.113 0.309 6.6.5 Counterfactual plots At this point, it’s important to recognize we have two regression models. As a first step, we might specify each model separately in a bf() function and save them as objects (Estimating multivariate models with brms). divorce_model &lt;- bf(divorce.std ~ 1 + median.age.std + marriage.std) marriage_model &lt;- bf(marriage.std ~ 1 + median.age.std) divorce_model &lt;- bf(divorcestd ~ 1 + medianagestd + marriagestd) marriage_model &lt;- bf(marriagestd ~ 1 + medianagestd) Next we will combine our bf() objects with the + operator within the brm() function. For a model like this, we also specify set_rescor(FALSE) to prevent brms from adding a residual correlation between d and m. Also, notice how each prior statement includes a resp argument. This clarifies which sub-model the prior refers to. # can&#39;t use _ or . in column names in this context data_waffle_short &lt;- data_waffle %&gt;% set_names(nm = names(data_waffle) %&gt;% str_remove_all(&quot;_&quot;)) brms_c5_model_counterfactual &lt;- brm( data = data_waffle_short, family = gaussian, divorce_model + marriage_model + set_rescor(FALSE), prior = c(prior(normal(0, 0.2), class = Intercept, resp = divorcestd), prior(normal(0, 0.5), class = b, resp = divorcestd), prior(exponential(1), class = sigma, resp = divorcestd), prior(normal(0, 0.2), class = Intercept, resp = marriagestd), prior(normal(0, 0.5), class = b, resp = marriagestd), prior(exponential(1), class = sigma, resp = marriagestd)), chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_counterfactual&quot;) print(brms_c5_model_counterfactual) #&gt; Family: MV(gaussian, gaussian) #&gt; Links: mu = identity; sigma = identity #&gt; mu = identity; sigma = identity #&gt; Formula: divorcestd ~ 1 + medianagestd + marriagestd #&gt; marriagestd ~ 1 + medianagestd #&gt; Data: data_waffle_short (Number of observations: 50) #&gt; Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #&gt; total post-warmup draws = 4000 #&gt; #&gt; Population-Level Effects: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS #&gt; divorcestd_Intercept -0.00 0.10 -0.20 0.19 1.00 5659 #&gt; marriagestd_Intercept -0.00 0.09 -0.18 0.17 1.00 5705 #&gt; divorcestd_medianagestd -0.61 0.16 -0.90 -0.29 1.00 3490 #&gt; divorcestd_marriagestd -0.06 0.15 -0.36 0.25 1.00 3354 #&gt; marriagestd_medianagestd -0.69 0.10 -0.89 -0.49 1.00 4910 #&gt; Tail_ESS #&gt; divorcestd_Intercept 2695 #&gt; marriagestd_Intercept 3063 #&gt; divorcestd_medianagestd 2968 #&gt; divorcestd_marriagestd 2948 #&gt; marriagestd_medianagestd 3278 #&gt; #&gt; Family Specific Parameters: #&gt; Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #&gt; sigma_divorcestd 0.83 0.09 0.68 1.02 1.00 5112 3245 #&gt; sigma_marriagestd 0.71 0.08 0.58 0.89 1.00 4852 2896 #&gt; #&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS #&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential #&gt; scale reduction factor on split chains (at convergence, Rhat = 1). nd &lt;- tibble(medianagestd = seq(from = -2, to = 2, length.out = 30), marriagestd = 0) predict(brms_c5_model_counterfactual, resp = &quot;divorcestd&quot;, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = medianagestd, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_smooth(stat = &quot;identity&quot;, fill = fll0, color = clr0d, size = .2) + labs(subtitle = &quot;Total counterfactual effect of A on D&quot;, x = &quot;manipulated median_age_std&quot;, y = &quot;counterfactual divorce_std&quot;) nd &lt;- tibble(marriagestd = seq(from = -2, to = 2, length.out = 30), medianagestd = 0) predict(brms_c5_model_counterfactual, resp = &quot;divorcestd&quot;, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = marriagestd, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_smooth(stat = &quot;identity&quot;, fill = fll0, color = clr0d, size = .2) + labs(subtitle = &quot;Total counterfactual effect of M on D&quot;, x = &quot;manipulated marriage_std&quot;, y = &quot;counterfactual divorce_std&quot;) 6.6.6 Masked Relationships brms_c5_model_milk_draft &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 1 + neocortex_std, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, sample_prior = TRUE, file = &quot;brms/brms_c5_model_milk_draft&quot;) set.seed(42) prior_draws(brms_c5_model_milk_draft) %&gt;% slice_sample(n = 50) %&gt;% rownames_to_column() %&gt;% expand(nesting(rowname, Intercept, b), neocortex_std = c(-2, 2)) %&gt;% mutate(kcal_std = Intercept + b * neocortex_std) %&gt;% ggplot(aes(x = neocortex_std, y = kcal_std)) + geom_line(aes(group = rowname), color = clr0d %&gt;% clr_alpha()) + coord_cartesian(ylim = c(-2, 2)) + labs(x = &quot;neocortex_std&quot;, y = &quot;kcal_std&quot;, subtitle = &quot;Intercept ~ dnorm(0, 1); b ~ dnorm(0, 1)&quot;) brms_c5_model_milk_cortex &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 1 + neocortex_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, sample_prior = TRUE, file = &quot;brms/brms_c5_model_milk_cortex&quot;) set.seed(42) prior_draws(brms_c5_model_milk_cortex) %&gt;% slice_sample(n = 50) %&gt;% rownames_to_column() %&gt;% expand(nesting(rowname, Intercept, b), neocortex_std = c(-2, 2)) %&gt;% mutate(kcal_std = Intercept + b * neocortex_std) %&gt;% ggplot(aes(x = neocortex_std, y = kcal_std)) + geom_line(aes(group = rowname), color = clr0d %&gt;% clr_alpha()) + coord_cartesian(ylim = c(-2, 2)) + labs(x = &quot;neocortex_std&quot;, y = &quot;kcal_std&quot;, subtitle = &quot;Intercept ~ dnorm(0, 0.2); b ~ dnorm(0, 0.5)&quot;) bind_rows( as_draws_df(brms_c5_model_milk_draft) %&gt;% select(b_Intercept:sigma), as_draws_df(brms_c5_model_milk_cortex) %&gt;% select(b_Intercept:sigma) ) %&gt;% mutate(fit = rep(c(&quot;milk_draft&quot;, &quot;milk_cortex&quot;), each = n() / 2)) %&gt;% pivot_longer(-fit, names_to = &quot;parameter&quot;) %&gt;% group_by(parameter, fit) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% mutate(fit = factor(fit, levels = c(&quot;milk_draft&quot;, &quot;milk_cortex&quot;))) %&gt;% ggplot(aes(x = mean, xmin = ll, xmax = ul, y = fit)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_pointrange(color = clr0d, fill = clr0, shape = 21) + facet_wrap(~ parameter, ncol = 1) + theme(axis.title = element_blank()) nd &lt;- tibble(neocortex_std = seq(from = -2.5, to = 2, length.out = 30)) fitted(brms_c5_model_milk_cortex, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = neocortex_std, y = Estimate, ymin = Q25, ymax = Q75)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = fll0) + geom_smooth(stat = &quot;identity&quot;, fill = fll0, color = clr0d, size = .2) + geom_point(data = data_milk, aes(x = neocortex_std, y = kcal_std), inherit.aes = FALSE, color = clr_dark) + labs(y = &#39;kcal_std&#39;) brms_c5_model_milk_weight &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 1 + mass_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, sample_prior = TRUE, file = &quot;brms/brms_c5_model_milk_weight&quot;) nd &lt;- tibble(mass_std = seq(from = -2.5, to = 2.5, length.out = 30)) fitted(brms_c5_model_milk_weight, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = mass_std, y = Estimate, ymin = Q25, ymax = Q75)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = fll0) + geom_smooth(stat = &quot;identity&quot;, fill = fll0, color = clr0d, size = .2) + geom_point(data = data_milk, aes(x = mass_std, y = kcal_std), inherit.aes = FALSE, color = clr_dark) + labs(y = &#39;kcal_std&#39;) brms_c5_model_milk_multi &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 1 + neocortex_std + mass_std, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_milk_multi&quot;) bind_cols( as_draws_df(brms_c5_model_milk_cortex) %&gt;% transmute(`cortex-beta_N` = b_neocortex_std), as_draws_df(brms_c5_model_milk_weight) %&gt;% transmute(`weight-beta_M` = b_mass_std), as_draws_df(brms_c5_model_milk_multi) %&gt;% transmute(`multi-beta_N` = b_neocortex_std, `multi-beta_M` = b_mass_std) ) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% separate(name, into = c(&quot;fit&quot;, &quot;parameter&quot;), sep = &quot;-&quot;) %&gt;% ggplot(aes(x = mean, y = fit, xmin = ll, xmax = ul)) + geom_pointrange(color = clr0d, fill = clr0, shape = 21) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + ylab(NULL) + facet_wrap(~ parameter, ncol = 1) nd &lt;- tibble(neocortex_std = seq(from = -2.5, to = 2, length.out = 30), mass_std = 0) fitted(brms_c5_model_milk_multi, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = neocortex_std, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = fll0) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = fll0, color = clr0d, size = .2) + labs(subtitle = &quot;Counterfactual holding M = 0&quot;, x = &quot;neocortex_std&quot;, y = &quot;kcal_std&quot;) nd &lt;- tibble(mass_std = seq(from = -2.5, to = 2.5, length.out = 30), neocortex_std = 0) fitted(brms_c5_model_milk_multi, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = mass_std, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = fll0) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = fll0, color = clr0d, size = .2) + labs(subtitle = &quot;Counterfactual holding M = 0&quot;, x = &quot;mass_std&quot;, y = &quot;kcal_std&quot;) brms_c5_model_milk_multi_sim &lt;- update( brms_c5_model_milk_multi, newdata = data_milk_sim1, formula = kcal_std ~ 1 + neocortex_std + mass_std, seed = 42, file = &quot;brms/brms_c5_model_milk_multi_sim&quot;) brms_c5_model_milk_cortex_sim &lt;- update( brms_c5_model_milk_cortex, formula = kcal_std ~ 1 + neocortex_std, seed = 42, file = &quot;brms/brms_c5_model_milk_cortex_sim&quot;) brms_c5_model_milk_weight_sim &lt;- update( brms_c5_model_milk_weight, formula = kcal_std ~ 1 + mass_std, seed = 42, file = &quot;brms/brms_c5_model_milk_weight_sim&quot;) mixedup::extract_fixef(brms_c5_model_milk_cortex_sim) #&gt; # A tibble: 2 x 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Intercept 0.003 0.162 -0.324 0.321 #&gt; 2 neocortex_std 0.123 0.231 -0.325 0.584 mixedup::extract_fixef(brms_c5_model_milk_weight_sim) #&gt; # A tibble: 2 x 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Intercept 0.005 0.152 -0.293 0.307 #&gt; 2 mass_std -0.283 0.221 -0.708 0.158 mixedup::extract_fixef(brms_c5_model_milk_multi_sim) #&gt; # A tibble: 3 x 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Intercept -0.047 0.081 -0.208 0.112 #&gt; 2 neocortex_std 0.982 0.096 0.794 1.17 #&gt; 3 mass_std -1.04 0.118 -1.26 -0.808 6.6.7 Categorical Variables 6.6.7.1 Binary Categories For an indicator variable, we need this to be a factor(): data_height &lt;- data_height %&gt;% mutate(sex = factor(sex)) brms_c5_model_height &lt;- brm( data = data_height, family = gaussian, height ~ 0 + sex, prior = c(prior(normal(178, 20), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_height&quot;) contrasts with {brms} library(tidybayes) as_draws_df(brms_c5_model_height) %&gt;% mutate(diff_fm = b_sex1 - b_sex2) %&gt;% gather(key, value, -`lp__`) %&gt;% group_by(key) %&gt;% mean_qi(value, .width = .89) %&gt;% filter(!grepl(key, pattern = &quot;^\\\\.&quot;)) %&gt;% knitr::kable() key value .lower .upper .width .point .interval b_sex1 134.901752 132.38783 137.448902 0.89 mean qi b_sex2 142.593033 139.91077 145.293809 0.89 mean qi diff_fm -7.691281 -11.49839 -3.924036 0.89 mean qi sigma 26.767597 25.51963 28.079138 0.89 mean qi 6.6.7.2 Many Categories brms_c5_model_milk_clade &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 0 + clade, prior = c(prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_milk_clade&quot;) library(bayesplot) (mcmc_intervals_data(brms_c5_model_milk_clade, prob = .5) %&gt;% filter(grepl(parameter, pattern = &quot;^b&quot;)) %&gt;% ggplot(aes(y = parameter)) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + geom_linerange(aes(xmin = ll, xmax = hh), lwd = .2, color = clr2) + geom_pointrange(aes(xmin = l, x = m, xmax = h), lwd = .7, shape = 21, color = clr2, fill = clr_lighten(clr2,.2))) + theme(axis.title = element_blank()) as_draws_df(brms_c5_model_milk_clade) %&gt;% select(starts_with(&quot;b&quot;)) %&gt;% as_tibble() %&gt;% set_names(x = . , nm = names(.) %&gt;% str_remove(&quot;b_clade&quot;)) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = reorder(name, value))) + # note how we used `reorder()` to arrange the coefficients geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_pointinterval(point_interval = mode_hdi, .width = .89, size = 2, shape = 21, color = clr2, fill = clr_lighten(clr2,.2)) + labs(title = &quot;My tidybayes-based coefficient plot&quot;, x = &quot;expected kcal (std)&quot;, y = NULL) naïve {brms} model fit: brms_c5_model_milk_house &lt;- brm( data = data_milk_clade, family = gaussian, kcal_std ~ 0 + clade + house, prior = c(prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_milk_house&quot;) \\(\\rightarrow\\) there are only three house levels 🤨. mixedup::extract_fixef(brms_c5_model_milk_house) #&gt; # A tibble: 7 x 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 cladeApe -0.431 0.261 -0.932 0.082 #&gt; 2 cladeNewWorldMonkey 0.326 0.253 -0.173 0.824 #&gt; 3 cladeOldWorldMonkey 0.497 0.286 -0.075 1.04 #&gt; 4 cladeStrepsirrhine -0.504 0.294 -1.04 0.088 #&gt; 5 houseHufflepuff -0.175 0.285 -0.742 0.378 #&gt; 6 houseRavenclaw -0.129 0.278 -0.667 0.413 #&gt; 7 houseSlytherin 0.489 0.293 -0.109 1.04 precis(model_milk_house, depth = 2) %&gt;% as.matrix() %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha_clade[1] -0.4205362 0.2603510 -0.8366273 -0.0044451 alpha_clade[2] 0.3836736 0.2596808 -0.0313464 0.7986937 alpha_clade[3] 0.5664463 0.2890333 0.1045153 1.0283773 alpha_clade[4] -0.5055652 0.2966455 -0.9796621 -0.0314684 alpha_house[1] -0.1025635 0.2617090 -0.5208251 0.3156981 alpha_house[2] -0.1996998 0.2754408 -0.6399074 0.2405079 alpha_house[3] -0.1603306 0.2690551 -0.5903326 0.2696713 alpha_house[4] 0.4866255 0.2875133 0.0271236 0.9461274 sigma 0.6631322 0.0881257 0.5222904 0.8039741 But there is no overall intercept, α, that stands for the expected value when all the predictors are set to 0. When we use the typical formula syntax with brms, we can suppress the overall intercept when for a single index variable with the &lt;criterion&gt; ~ 0 + &lt;index variable&gt; syntax. That’s exactly what we did with our b5.9 model. The catch is this approach only works with one index variable within brms. Even though we suppressed the default intercept with our formula, kcal_std ~ 0 + clade + house, we ended up loosing the first category of the second variable, house. […] The solution is the use the non-linear syntax. brms_c5_model_milk_house_correct_index &lt;- brm(data = data_milk_clade, family = gaussian, bf(kcal_std ~ 0 + a + h, a ~ 0 + clade, h ~ 0 + house, nl = TRUE), prior = c(prior(normal(0, 0.5), nlpar = a), prior(normal(0, 0.5), nlpar = h), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_milk_house_correct_index&quot;) mixedup::extract_fixef(brms_c5_model_milk_house_correct_index) #&gt; # A tibble: 8 x 5 #&gt; term value se lower_2.5 upper_97.5 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 a_cladeApe -0.395 0.28 -0.936 0.146 #&gt; 2 a_cladeNewWorldMonkey 0.363 0.28 -0.183 0.902 #&gt; 3 a_cladeOldWorldMonkey 0.527 0.307 -0.112 1.11 #&gt; 4 a_cladeStrepsirrhine -0.455 0.321 -1.10 0.167 #&gt; 5 h_houseGryffindor -0.097 0.284 -0.658 0.445 #&gt; 6 h_houseHufflepuff -0.196 0.298 -0.771 0.396 #&gt; 7 h_houseRavenclaw -0.159 0.285 -0.715 0.39 #&gt; 8 h_houseSlytherin 0.468 0.31 -0.138 1.07 as_draws_df(brms_c5_model_milk_house_correct_index) %&gt;% pivot_longer(starts_with(&quot;b_&quot;)) %&gt;% mutate(name = str_remove(name, &quot;b_&quot;) %&gt;% str_remove(., &quot;clade&quot;) %&gt;% str_remove(., &quot;house&quot;) %&gt;% str_replace(., &quot;World&quot;, &quot; World &quot;)) %&gt;% separate(name, into = c(&quot;predictor&quot;, &quot;level&quot;), sep = &quot;_&quot;) %&gt;% mutate(predictor = if_else(predictor == &quot;a&quot;, &quot;clade&quot;, &quot;house&quot;)) %&gt;% ggplot(aes(x = value, y = reorder(level, value))) + # note how we used `reorder()` to arrange the coefficients geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_pointinterval(point_interval = mode_hdi, .width = .89, size = 2, color = clr0d, fill = clr0, shape = 21 ) + labs(x = &quot;expected_kcal_std&quot;, y = NULL) + facet_wrap(~ predictor, scales = &quot;free_y&quot;) 6.6.8 Alternative ways to model multiple categories 6.6.8.1 Contrast Coding data_contrast &lt;- data_height %&gt;% mutate(sex_c = if_else(sex == &quot;1&quot;, -0.5, 0.5)) brms_c5_model_height_contrast &lt;- brm( data = data_contrast, family = gaussian, height ~ 1 + sex_c, prior = c(prior(normal(178, 20), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 42, file = &quot;brms/brms_c5_model_height_contrast&quot;) Our posterior for \\(\\alpha\\), above, is designed to capture the average_of_the_group_means_in_height, not mean_height. In cases where the sample sizes in the two groups were equal, these two would be same. Since we have different numbers of males and females in our data, the two values differ a bit as_draws_df(brms_c5_model_height_contrast) %&gt;% mutate(male = b_Intercept - b_sex_c * 0.5, female = b_Intercept + b_sex_c * 0.5, `female - male` = b_sex_c) %&gt;% pivot_longer(male:`female - male`) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(.width = .95, shape = 21, fill = fll0, color = clr0d, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;height&quot;) + facet_wrap(~ name, scales = &quot;free&quot;) 6.6.9 Multilevel ANOVA (This might make sense after reading Chapter 13…) \\[ \\begin{array}{ccccr} K_i &amp; {\\sim} &amp; Normal(\\mu_i, \\sigma) &amp; &amp;\\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; \\alpha + u_{j[i]} &amp; &amp;\\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Normal(0, 0.5) &amp; &amp;\\textrm{[$\\alpha$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; &amp;\\textrm{[$\\sigma$ prior]} \\\\ u_{j[i]} &amp; \\sim &amp; Normal(0, \\sigma_{CLADE}) &amp; \\textrm{for}~j = 1..4 &amp;\\textrm{[u prior]}\\\\ \\sigma_{CLADE} &amp; \\sim &amp; Exponential(1) &amp; &amp;\\textrm{[$\\sigma_{CLADE}$ prior]} \\\\ \\end{array} \\] the four clade-specific deviations from that mean are captured by the four levels of \\(u_j\\), which are themselves modeled as normally distributed with a mean of zero (because they are deviations, after all) and a standard deviation \\(\\sigma_{CLADE}\\) brms_c5_model_milk_anova &lt;- brm( data = data_milk, family = gaussian, kcal_std ~ 1 + (1 | clade), prior = c(prior(normal(0, 0.5), class = Intercept), prior(exponential(1), class = sigma), prior(exponential(1), class = sd)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;brms/brms_c5_model_milk_anova&quot;) as_draws_df(brms_c5_model_milk_anova) %&gt;% mutate(Ape = b_Intercept + `r_clade[Ape,Intercept]`, `New World Monkey` = b_Intercept + `r_clade[New.World.Monkey,Intercept]`, `Old World Monkey` = b_Intercept + `r_clade[Old.World.Monkey,Intercept]`, Strepsirrhine = b_Intercept + `r_clade[Strepsirrhine,Intercept]`) %&gt;% pivot_longer(Ape:Strepsirrhine) %&gt;% ggplot(aes(x = value, y = reorder(name, value))) + geom_vline(xintercept = 0, color = clr_dark, linetype = 3) + stat_pointinterval(point_interval = mode_hdi, .width = .89, size = 2, color = clr0d, fill = clr0, shape = 21 ) + labs(x = &quot;expected_kcal_std&quot;, y = NULL) 6.7 pymc3 section × "],
["rethinking-chapter-6.html", "7 Rethinking: Chapter 6 7.1 Multicolliniarity 7.2 Post-treatment bias 7.3 Collider Bias 7.4 Homework 7.5 {brms} section 7.6 pymc3 section", " 7 Rethinking: Chapter 6 The Haunted DAG &amp; Causal Terror by Richard McElreath, building on the Summaries by Solomon Kurz and Jake Thompson. Simulating section-distortion (Berkson’s paradox) n &lt;- 200 p &lt;- .1 data_sim &lt;- tibble(newsworthy = rnorm(n), trustworthy = rnorm(n), score = newsworthy + trustworthy, treshold = quantile(score, 1 - p), selected = score &gt;= treshold) data_sim %&gt;% ggplot(aes(x = newsworthy, y = trustworthy, color = selected)) + geom_smooth(data = data_sim %&gt;% filter(selected), method = &quot;lm&quot;, se = FALSE, fullrange = TRUE, size = .5) + geom_point(aes(fill = after_scale(clr_alpha(color))), shape = 21, size = 2.5) + scale_color_manual(values = c(`TRUE` = clr1, `FALSE` = clr0d))+ coord_cartesian(xlim = range(data_sim$newsworthy) * 1.05, ylim = range(data_sim$trustworthy) * 1.05, expand = 0) + coord_equal(ylim = range(data_sim$trustworthy) * 1.05) 7.1 Multicolliniarity Simulating multicollinear legs library(rethinking) n &lt;- 100 set.seed(909) data_legs &lt;- tibble( height = rnorm(n = n, mean = 10, sd = 2), leg_proportion = runif(n, min = 0.4, max = 0.5), left_leg = leg_proportion * height + rnorm(n, 0, .02), right_leg = leg_proportion * height + rnorm(n, 0, .02), ) model_legs_multicollinear &lt;- quap( flist = alist( height ~ dnorm(mu, sigma), mu &lt;- alpha + beta_left * left_leg + beta_right * right_leg, alpha ~ dnorm(10, 100), beta_left ~ dnorm(2, 10), beta_right ~ dnorm(2, 10), sigma ~ dexp(1) ), data = data_legs ) precis(model_legs_multicollinear) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.98 0.28 0.53 1.44 beta_left 0.21 2.53 -3.83 4.25 beta_right 1.78 2.53 -2.26 5.83 sigma 0.62 0.04 0.55 0.69 precis(model_legs_multicollinear, depth = 2) %&gt;% as_tibble_rn() %&gt;% ggplot(aes(y = param)) + geom_vline(xintercept = 0, lty = 3, color = rgb(0,0,0,.6)) + geom_linerange(aes(xmin = `5.5%`, xmax =`94.5%`), color = clr0d) + geom_point(aes(x = mean), shape = 21, size = 3 , color = clr0d, fill = clr0) + scale_y_discrete(limits = c(&quot;sigma&quot;, &quot;beta_right&quot;, &quot;beta_left&quot;, &quot;alpha&quot;)) + theme(axis.title.y = element_blank()) leg_posterior_samples &lt;- extract.samples(model_legs_multicollinear) %&gt;% as_tibble() p_cor &lt;- leg_posterior_samples %&gt;% ggplot(aes(x = beta_right, y = beta_left)) + geom_point(color = clr0d, fill = clr0, shape = 21, alpha = .5) p_sum &lt;- leg_posterior_samples %&gt;% ggplot(aes(x = beta_right + beta_left)) + geom_vline(xintercept = 1/mean(data_legs$leg_proportion), color = clr_dark, linetype = 3) + geom_density(color = clr0d, fill = clr0, alpha = .5, adjust = .4) p_cor + p_sum Milk example data(milk) data_milk &lt;- milk %&gt;% as_tibble() %&gt;% drop_na(kcal.per.g:perc.lactose) %&gt;% mutate(across(where(is.double), standardize, .names = &quot;{str_remove(str_remove(.col,&#39;perc.&#39;),&#39;.per.g&#39;)}_std&quot;)) Model fat only model_milk_fat &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_fat * fat_std, alpha ~ dnorm(0,.2), beta_fat ~ dnorm(0,.5), sigma ~ dexp(1) ), data = data_milk ) precis(model_milk_fat) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.08 -0.12 0.12 beta_fat 0.86 0.08 0.73 1.00 sigma 0.45 0.06 0.36 0.54 Model lactose only model_milk_lactose &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_lactose * lactose_std, alpha ~ dnorm(0,.2), beta_lactose ~ dnorm(0,.5), sigma ~ dexp(1) ), data = data_milk ) precis(model_milk_lactose) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.07 -0.11 0.11 beta_lactose -0.90 0.07 -1.02 -0.79 sigma 0.38 0.05 0.30 0.46 Multicollinear model model_milk_multicollinear &lt;- quap( flist = alist( kcal_std ~ dnorm(mu, sigma), mu &lt;- alpha + beta_fat * fat_std + beta_lactose * lactose_std, alpha ~ dnorm(0,.2), beta_fat ~ dnorm(0,.5), beta_lactose ~ dnorm(0,.5), sigma ~ dexp(1) ), data = data_milk ) precis(model_milk_multicollinear) %&gt;% as.matrix() %&gt;% round(digits = 2) %&gt;% knitr::kable() mean sd 5.5% 94.5% alpha 0.00 0.07 -0.11 0.11 beta_fat 0.24 0.18 -0.05 0.54 beta_lactose -0.68 0.18 -0.97 -0.38 sigma 0.38 0.05 0.30 0.46 data_milk %&gt;% dplyr::select(kcal_std, fat_std, lactose_std) %&gt;% ggpairs( lower = list(continuous = wrap(ggally_points, colour = clr0d, size = 1.5, alpha = .7)), diag = list(continuous = wrap(&quot;densityDiag&quot;, fill = fll0, color = clr0d, adjust = .9)), upper = list(continuous = wrap(ggally_cor, size = 5, color = &quot;black&quot;, family = &quot;Josefin sans&quot;)) ) dagify(K ~ L + F, L ~ D, F ~ D, coords = tibble(name = c(&quot;K&quot;, &quot;L&quot;, &quot;F&quot;, &quot;D&quot;), x = c(.5, 0, 1, .5), y = c(.6, 1, 1, 1))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;K&quot;, &quot;response&quot;, if_else(name %in% c(&quot;L&quot;, &quot;F&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(.55, 1.05)) + scale_x_continuous(limits = c(-.05, 1.05)) + coord_equal() Simulating Multicollinearity simluate_collinearity &lt;- function(seed = 42, r = .9, data = data_milk ){ data &lt;- data %&gt;% mutate(x = rnorm(n = nrow(cur_data()), mean = `perc.fat` * r, sd = sqrt((1 - r ^ 2) * var(`perc.fat`)))) mod &lt;- lm(kcal.per.g ~ perc.fat + x, data = data) sqrt( diag( vcov(mod) ))[2] } # reapeat_simulation &lt;- function(r = .9, n = 100){ # stddev &lt;- replicate( n, simluate_collinearity(r)) # tibble(r = r, stddev_mean = mean(stddev), stddev_sd = sd(stddev)) # } n_seed &lt;- 100 n_rho &lt;- 60 simulation_means &lt;- crossing(seed = 1:n_seed, rho = seq(from = 0, to = .99, length.out = n_rho)) %&gt;% mutate(parameter_sd = purrr::map2_dbl(seed, rho, simluate_collinearity)) %&gt;% group_by(rho) %&gt;% summarise(mean = mean(parameter_sd), ll = quantile(parameter_sd, prob = .025), ul = quantile(parameter_sd, prob = .975)) simulation_means %&gt;% ggplot(aes(x = rho, y = mean, ymin = ll, ymax = ul)) + geom_smooth(stat = &#39;identity&#39;, size = .6, color = clr0d, fill = fll0) 7.2 Post-treatment bias Simulating fungus data n &lt;- 100 set.seed(71) data_fungus &lt;- tibble( h_0 = rnorm(n, 10, 2), treatment = rep(0:1, each = n/2), fungus = rbinom(n = n, size = 1, prob = .5 - treatment * .4 ), h_1 = h_0 + rnorm(n, 5 - 3 * fungus) ) precis(data_fungus) %&gt;% knit_precis() param mean sd 5.5% 94.5% histogram h_0 9.96 2.10 6.57 13.08 ▁▂▂▂▇▃▂▃▁▁▁▁ treatment 0.50 0.50 0.00 1.00 ▇▁▁▁▁▁▁▁▁▇ fungus 0.23 0.42 0.00 1.00 ▇▁▁▁▁▁▁▁▁▂ h_1 14.40 2.69 10.62 17.93 ▁▁▃▇▇▇▁▁ \\[ \\begin{array}{rclr} h_{1,i} &amp; \\sim &amp; Normal( \\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; h_{0,i} \\times p &amp; \\textrm{[linear model]}\\\\ \\end{array} \\] selecting a prior precis(tibble(sim_p = rlnorm(1e4, 0, .25))) %&gt;% knit_precis() param mean sd 5.5% 94.5% histogram sim_p 1.04 0.26 0.67 1.5 ▁▁▃▇▇▃▁▁▁▁▁▁ \\(\\rightarrow\\) the main mass of the prior is between 40% shrinkage and 50% growth. Model without treatment model_fungus_no_treatment &lt;- quap( flist = alist( h_1 ~ dnorm( mu, sigma ), mu &lt;- h_0 * p, p ~ dlnorm( 0, .25 ), sigma ~ dexp( 1 ) ), data = data_fungus ) precis(model_fungus_no_treatment) %&gt;% knit_precis() param mean sd 5.5% 94.5% p 1.43 0.02 1.40 1.45 sigma 1.79 0.13 1.59 1.99 Model with treatment and fungus (post-treatment variable) \\[ \\begin{array}{rclr} h_{1,i} &amp; \\sim &amp; Normal( \\mu_i, \\sigma) &amp; \\textrm{[likelihood]}\\\\ \\mu_i &amp; = &amp; h_{0,i} \\times p &amp; \\textrm{[linear model]}\\\\ p &amp; = &amp; \\alpha + \\beta_{T} T_{i} + \\beta_{F} F_{i} &amp; \\textrm{[linear model]}\\\\ \\alpha &amp; \\sim &amp; Log-Normal(0, 0.25) &amp; \\textrm{[$\\alpha$ prior]}\\\\ \\beta_{T} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_{T}$ prior]}\\\\ \\beta_{F} &amp; \\sim &amp; Normal(0, 0.5) &amp; \\textrm{[$\\beta_{F}$ prior]}\\\\ \\sigma &amp; \\sim &amp; Exponential(1) &amp; \\textrm{[$\\sigma$ prior]} \\end{array} \\] model_fungus_post_treatment &lt;- quap( flist = alist( h_1 ~ dnorm( mu, sigma ), mu &lt;- h_0 * p, p &lt;- alpha + beta_treatment * treatment + beta_fungus * fungus, alpha ~ dlnorm( 0, .2 ), beta_treatment ~ dnorm(0,.5), beta_fungus ~ dnorm(0,.5), sigma ~ dexp( 1 ) ), data = data_fungus ) precis(model_fungus_post_treatment) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 1.48 0.02 1.44 1.52 beta_treatment 0.00 0.03 -0.05 0.05 beta_fungus -0.27 0.04 -0.33 -0.21 sigma 1.41 0.10 1.25 1.57 Model with treatment but without fungus model_fungus_only_treatment &lt;- quap( flist = alist( h_1 ~ dnorm( mu, sigma ), mu &lt;- h_0 * p, p &lt;- a + beta_treatment * treatment, a ~ dlnorm( 0, .25 ), beta_treatment ~ dnorm(0,.5), sigma ~ dexp( 1 ) ), data = data_fungus ) precis(model_fungus_only_treatment) %&gt;% knit_precis() param mean sd 5.5% 94.5% a 1.38 0.03 1.34 1.42 beta_treatment 0.08 0.03 0.03 0.14 sigma 1.75 0.12 1.55 1.94 d-separation dagify(&quot;H_1&quot; ~ H_0 + F, F ~ T, coords = tibble(name = c(&quot;H_0&quot;, &quot;H_1&quot;, &quot;F&quot;, &quot;T&quot;), x = c(0, .5, .75, 1), y = c(0, 0, 0, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;H_1&quot;, &quot;response&quot;, if_else(name %in% c(&quot;H_0&quot;, &quot;F&quot;, &quot;T&quot;), &quot;predictor&quot;, &quot;confounds&quot;)), name = str_replace(name, &quot;([A-Z])_([0-9])&quot;, &quot;\\\\1\\\\[\\\\2\\\\]&quot;)) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(-.05, .05)) + scale_x_continuous(limits = c(-.05, 1.05)) + coord_equal() Directional Separation of H_1 and T occurs after conditioning on F: library(dagitty) impliedConditionalIndependencies(&quot;dag{ H_0 -&gt; H_1 &lt;- F &lt;- T}&quot;) #&gt; F _||_ H_0 #&gt; H_0 _||_ T #&gt; H_1 _||_ T | F dagify(H_1 ~ H_0 + M, F ~ T, F ~ M, coords = tibble(name = c(&quot;H_0&quot;, &quot;H_1&quot;, &quot;M&quot; , &quot;F&quot;, &quot;T&quot;), x = c(0, .5, .75, 1, 1.5), y = c(1, 1, .7, 1, 1))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;H_1&quot;, &quot;response&quot;, if_else(name %in% c(&quot;H_0&quot;, &quot;F&quot;, &quot;T&quot;), &quot;predictor&quot;, &quot;confounds&quot;)), name = str_replace(name, &quot;([A-Z])_([0-9])&quot;, &quot;\\\\1\\\\[\\\\2\\\\]&quot;)) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(.65, 1.05)) + scale_x_continuous(limits = c(-.05, 1.55)) + coord_equal() n &lt;- 1e4 set.seed(71) data_moisture &lt;- tibble( h_0 = rnorm(n, 10, 2), treatment = rep(0:1, each = n/2), moisture = rbern(n), fungus = rbinom(n = n, size = 1, prob = .5 - treatment * .4 + moisture * .4), h_1 = h_0 + rnorm(n, 5 + 3 * moisture) ) precis(data_moisture) %&gt;% knit_precis() param mean sd 5.5% 94.5% histogram h_0 10.04 2.01 6.81 13.22 ▁▁▁▂▇▇▂▁▁ treatment 0.50 0.50 0.00 1.00 ▇▁▁▁▁▁▁▁▁▇ moisture 0.50 0.50 0.00 1.00 ▇▁▁▁▁▁▁▁▁▇ fungus 0.50 0.50 0.00 1.00 ▇▁▁▁▁▁▁▁▁▇ h_1 16.55 2.69 12.22 20.84 ▁▁▁▃▇▇▅▂▁▁ Moisture-Model with treatment and fungus (post-treatment variable) model_moisture_post_treatment &lt;- quap( flist = alist( h_1 ~ dnorm( mu, sigma ), mu &lt;- h_0 * p, p &lt;- alpha + beta_treatment * treatment + beta_fungus * fungus, alpha ~ dlnorm( 0, .2 ), beta_treatment ~ dnorm(0,.5), beta_fungus ~ dnorm(0,.5), sigma ~ dexp( 1 ) ), data = data_moisture ) precis(model_moisture_post_treatment) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 1.53 0.00 1.52 1.54 beta_treatment 0.05 0.00 0.05 0.06 beta_fungus 0.13 0.00 0.12 0.14 sigma 2.13 0.02 2.11 2.16 Moisture-Model with treatment but without fungus model_moisture_only_treatment &lt;- quap( flist = alist( h_1 ~ dnorm( mu, sigma ), mu &lt;- h_0 * p, p &lt;- a + beta_treatment * treatment, a ~ dlnorm( 0, .25 ), beta_treatment ~ dnorm(0,.5), sigma ~ dexp( 1 ) ), data = data_moisture ) precis(model_moisture_only_treatment) %&gt;% knit_precis() param mean sd 5.5% 94.5% a 1.62 0.00 1.62 1.63 beta_treatment 0.00 0.00 0.00 0.01 sigma 2.22 0.02 2.19 2.25 7.3 Collider Bias # data_happy &lt;- sim_happiness(seed = 1977, N_years = 66) %&gt;% # as_tibble() progress_year &lt;- function(data, year, max_age = 65, n_births = 20, aom = 18){ new_cohort &lt;- tibble( age = 1, married = as.integer(0), happiness = seq(from = -2, to = 2, length.out = n_births), year_of_birth = year) data %&gt;% mutate(age = age + 1) %&gt;% bind_rows(., new_cohort) %&gt;% mutate(married = if_else(age &gt;= aom &amp; married == 0, rbern(n(), inv_logit(happiness - 4)), married )) %&gt;% filter(age &lt;= max_age) } sim_tidy &lt;- function(seed = 1977, n_years = 1000, max_age = 65, n_births = 20, aom = 18){ set.seed(seed) empty_tibble &lt;- tibble(age = double(), married = integer(), happiness = double()) 1:n_years %&gt;% reduce(.f = progress_year, .init = empty_tibble, max_age = max_age, n_births = n_births, aom = aom) } data_married &lt;- sim_tidy(seed = 1977, n_years = 65, n_births = 21) data_married %&gt;% mutate(married = factor(married, labels = c(&quot;unmarried&quot;, &quot;married&quot;))) %&gt;% ggplot(aes(x = age, y = happiness, color = married)) + geom_point(size = 1.75, shape = 21, aes(fill = after_scale(clr_alpha(color)))) + scale_color_manual(NULL, values = c(married = clr2, unmarried = clr0d)) + scale_x_continuous(expand = c(.015, .015)) + theme(panel.grid = element_blank(), legend.position = &quot;bottom&quot;) data_married_adults &lt;- data_married %&gt;% filter(age &gt;= 18) %&gt;% mutate(age_trans = (age - 18)/ diff(c(18, 65)), married_idx = married + 1L) model_happy_married &lt;- quap( flist = alist( happiness ~ dnorm(mu, sigma), mu &lt;- alpha[married_idx] + beta_age * age_trans, alpha[married_idx] ~ dnorm( 0, 1 ), beta_age ~ dnorm( 0, 2 ), sigma ~ dexp( 1 ) ), data = data_married_adults ) precis(model_happy_married, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha[1] -0.24 0.06 -0.34 -0.14 alpha[2] 1.20 0.08 1.07 1.33 beta_age -0.70 0.11 -0.88 -0.53 sigma 1.00 0.02 0.96 1.04 model_happy &lt;- quap( flist = alist( happiness ~ dnorm(mu, sigma), mu &lt;- alpha + beta_age * age_trans, alpha ~ dnorm( 0, 1 ), beta_age ~ dnorm( 0, 2 ), sigma ~ dexp( 1 ) ), data = data_married_adults ) precis(model_happy, depth = 2) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha 0.00 0.07 -0.12 0.12 beta_age 0.00 0.13 -0.21 0.21 sigma 1.21 0.03 1.17 1.25 7.3.1 The haunted DAG Education example (including Grandparents, Parents, Children and the unobserved Neighborhood) p_dag1 &lt;- dagify(C ~ P + G, P ~ G, coords = tibble(name = c(&quot;G&quot;, &quot;P&quot;, &quot;C&quot;), x = c(0, 1.5, 1.5), y = c(1, 1, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;C&quot;, &quot;response&quot;, if_else(name %in% c(&quot;P&quot;, &quot;G&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(-.05, 1.05)) + scale_x_continuous(limits = c(-.05, 1.55)) + coord_equal() p_dag2 &lt;- dagify(C ~ P + G + U, P ~ G + U, coords = tibble(name = c(&quot;G&quot;, &quot;P&quot;, &quot;C&quot;, &quot;U&quot;), x = c(0, 1.5, 1.5, 2), y = c(1, 1, 0, .5))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;C&quot;, &quot;response&quot;, if_else(name %in% c(&quot;P&quot;, &quot;G&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + scale_y_continuous(limits = c(-.05, 1.05)) + scale_x_continuous(limits = c(-.05, 2.05)) + coord_equal() p_dag1 + p_dag2 + plot_annotation(tag_levels = &quot;a&quot;) &amp; theme(plot.tag = element_text(family = fnt_sel)) n &lt;- 200 beta_gp &lt;- 1 # direct effect of G -&gt; P beta_gc &lt;- 0 # direct effect of G -&gt; C beta_pc &lt;- 1 # direct effect of P -&gt; C beta_U &lt;- 2 # direct effect of U on both C and P set.seed(1) data_education &lt;- tibble( unobserved = 2 * rbern(n, .5) - 1, grandparents = rnorm( n ), parents = rnorm( n, beta_gp * grandparents + beta_U * unobserved), children = rnorm( n, beta_gc * grandparents + beta_pc * parents + beta_U * unobserved) ) model_education &lt;- quap( flist = alist( children ~ dnorm(mu, sigma), mu &lt;- alpha + beta_pc * parents + beta_gc * grandparents, alpha ~ dnorm( 0, 1 ), c( beta_pc, beta_gc) ~ dnorm( 0, 1 ), sigma ~ dexp(1) ), data = data_education ) precis(model_education) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha -0.12 0.10 -0.28 0.04 beta_pc 1.79 0.04 1.72 1.86 beta_gc -0.84 0.11 -1.01 -0.67 sigma 1.41 0.07 1.30 1.52 data_education_plot &lt;- data_education %&gt;% mutate(across(grandparents:children, standardize, .names = &quot;{.col}_std&quot;)) %&gt;% mutate(parents_inner = between(parents_std, left = quantile(parents_std, probs = .45), right = quantile(parents_std, probs = .60))) data_education_plot %&gt;% ggplot(aes(x = grandparents_std, y = children_std)) + geom_smooth(data = data_education_plot %&gt;% filter(parents_inner), method = &quot;lm&quot;, se = FALSE, size = .5, color = clr_dark, fullrange = TRUE) + geom_point(aes(color = factor(unobserved), fill = after_scale(clr_alpha(color,.8)), shape = parents_inner), size = 2.5) + scale_color_manual(values = c(clr0d, clr2), guide = &quot;none&quot;) + scale_shape_manual(values = c(`FALSE` = 1, `TRUE` = 21), guide = &quot;none&quot;) + coord_equal() model_education_resolved &lt;- quap( flist = alist( children ~ dnorm(mu, sigma), mu &lt;- alpha + beta_pc * parents + beta_gc * grandparents + beta_u * unobserved, alpha ~ dnorm( 0, 1 ), c( beta_pc, beta_gc, beta_u ) ~ dnorm( 0, 1 ), sigma ~ dexp(1) ), data = data_education ) precis(model_education_resolved) %&gt;% knit_precis() param mean sd 5.5% 94.5% alpha -0.12 0.07 -0.24 -0.01 beta_pc 1.01 0.07 0.91 1.12 beta_gc -0.04 0.10 -0.20 0.11 beta_u 2.00 0.15 1.76 2.23 sigma 1.02 0.05 0.94 1.10 7.3.2 Shutting the Backdoor The four elements that construct DAGs: p_dag1 &lt;- dagify( X ~ Z, Y ~ Z, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), x = c(0, 1, .5), y = c(1, 1, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + ggtitle(&quot;Fork&quot;) p_dag2 &lt;- dagify( Z ~ X, Y ~ Z, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), x = c(0, 1, .5), y = c(1, 0, .5))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + ggtitle(&quot;Pipe&quot;) p_dag3 &lt;- dagify( Z ~ X + Y, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), x = c(0, 1, .5), y = c(0, 0 , 1))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + ggtitle(&quot;Collider&quot;) p_dag4 &lt;- dagify( Z ~ X + Y, D ~ Z, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, &quot;D&quot;), x = c(0, 1, .5, .5), y = c(0, 0 , 1, 0))) %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, &quot;D&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + ggtitle(&quot;Collider&quot;) p_dag1 + p_dag2 + p_dag3 + p_dag4 + plot_annotation(tag_levels = &quot;a&quot;) + plot_layout(nrow = 1) &amp; coord_fixed(ratio = .6) &amp; scale_y_continuous(limits = c(-.1, 1.1)) &amp; scale_x_continuous(limits = c(-.1, 1.1)) &amp; theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) a. Fork: \\(X \\perp \\!\\!\\! \\perp Y | Z\\) b. Pipe: \\(X \\perp \\!\\!\\! \\perp Y | Z\\) c. Collider: \\(X \\perp \\!\\!\\! \\perp Y\\), but \\(X \\not\\!\\perp\\!\\!\\!\\perp Y | Z\\) d. Descendant: \\(X \\perp \\!\\!\\! \\perp Y\\), but \\(X \\not\\!\\perp\\!\\!\\!\\perp Y | Z\\) (to a lesser extent) 7.3.3 Two Roads dag_roads &lt;- dagify( U ~ A, X ~ U, Y ~ X + C, C ~ A, B ~ U + C, exposure = &quot;X&quot;, outcome = &quot;Y&quot;, coords = tibble(name = c(&quot;U&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;X&quot;, &quot;Y&quot;), x = c(0, .5, .5, 1, 0, 1), y = c(.7, 1, .4 , .7, 0, 0))) dag_roads %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;Y&quot;, &quot;response&quot;, if_else(name %in% c(&quot;X&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + coord_fixed(ratio = .6) &amp; scale_y_continuous(limits = c(-.1, 1.1)) &amp; scale_x_continuous(limits = c(-.1, 1.1)) &amp; theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) adjustmentSets(dag_roads,exposure = &quot;X&quot;, outcome = &quot;Y&quot;) #&gt; { C } #&gt; { A } #&gt; { U } dag_roads &lt;- dagitty( &quot;dag { U [unobserved] X -&gt; Y X &lt;- U &lt;- A -&gt; C -&gt; Y U -&gt; B &lt;- C }&quot; ) adjustmentSets(dag_roads,exposure = &quot;X&quot;, outcome = &quot;Y&quot;) #&gt; { C } #&gt; { A } dag_waffles &lt;- dagify( D ~ A + M + W, A ~ S, M ~ A + S, W ~ S, exposure = &quot;W&quot;, outcome = &quot;D&quot;, coords = tibble(name = c(&quot;S&quot;, &quot;A&quot;, &quot;M&quot;, &quot;W&quot;, &quot;D&quot;), x = c(0, 0, .5, 1, 1), y = c(1, 0, .5 , 1, 0))) dag_waffles %&gt;% fortify() %&gt;% mutate(stage = if_else(name == &quot;D&quot;, &quot;response&quot;, if_else(name %in% c(&quot;W&quot;, &quot;A&quot;, &quot;M&quot;, &quot;S&quot;), &quot;predictor&quot;, &quot;confounds&quot;))) %&gt;% plot_dag(clr_in = clr3) + coord_fixed(ratio = .6) &amp; scale_y_continuous(limits = c(-.1, 1.1)) &amp; scale_x_continuous(limits = c(-.1, 1.1)) &amp; theme(plot.title = element_text(hjust = .5, family = fnt_sel), plot.tag = element_text(family = fnt_sel)) adjustmentSets(dag_waffles) #&gt; { A, M } #&gt; { S } Test the implications of the DAG by investigating the conditional independencies implied by the DAG in the real data (by conditioning on the respective variables): impliedConditionalIndependencies(dag_waffles) #&gt; A _||_ W | S #&gt; D _||_ S | A, M, W #&gt; M _||_ W | S 7.3.4 Backdoor Waffles 7.4 Homework E1 E2 E3 E4 M1 M2 M3 H1 H2 H3 H4 H5 H6 H7 7.5 {brms} section 7.6 pymc3 section × "],
["bayesian-statistics-the-fun-way.html", "8 Bayesian Statistics the Fun Way 8.1 Conditioning Probabilities 8.2 Combining Probailities based on logic 8.3 The binomial distribution 8.4 The beta distribution 8.5 Bayes’ Theorem 8.6 Parameter Estimation (I) 8.7 The normal distribution 8.8 Cummulative Density and Quantile Function 8.9 Parameter estimation with prior probabilities 8.10 Monte CarloSimulation 8.11 Posterior Odds 8.12 Parameter Estimation (II)", " 8 Bayesian Statistics the Fun Way by Will Kurt 8.1 Conditioning Probabilities \\[ \\begin{eqnarray} D &amp; = &amp; observed~data\\\\ H_{1} &amp; = &amp; Hypothesis\\\\ X &amp; = &amp;prior~belief\\\\ \\end{eqnarray} \\] Allow us to formulate the probability of the observed data given our hypothesis and our prior belief. \\[ P(D | H_{1}, X) \\] To compare different hypothesis, use the ratio of probabilities (odds): \\[ \\frac{P(D | H_{1}, X)}{P(D | H_{2}, X)} &gt; 1 \\] 8.2 Combining Probailities based on logic Rules for \\(AND\\) (\\(\\land\\)), \\(OR\\) (\\(\\lor\\)) and \\(NOT\\) (\\(\\neg\\)). \\(NOT:\\) \\[ \\begin{eqnarray} P(X) &amp; = &amp; p\\\\ \\neg P(X) &amp; = &amp; 1 - p \\end{eqnarray} \\] \\(AND\\) \\[ \\begin{eqnarray} P(Y) &amp; = &amp; q \\\\ P(X) \\land P(Y) &amp; = &amp; P(X,Y) = p \\times q \\end{eqnarray} \\] \\(OR\\) (mutually exclusive events): \\[ P(X) \\lor P(Y) = P(X),P(Y) = p + q \\] while: \\[ P(X) \\land P(Y) = 0 \\] \\(OR\\) (non-mutually exclusive events): \\[ P(X) \\lor P(Y) = P(X) + P(Y) - P(X, Y) \\] 8.3 The binomial distribution Factorial (factorial(x)): \\[ x! = x \\times x-1 \\times x -2 ... \\] The binomial coefficient (choose(n, k)): \\[ {n \\choose k} = \\frac{n!}{k! \\times (n - k)!} \\] The binomial distribution (a Probability Mass Function, PMF): \\[ B(k;n,p) = {n \\choose k} \\times p^k \\times (1 - p) ^{n-k} \\] wdh &lt;- 5000 n &lt;- 10 p = 0.5 tibble(k = rbinom(wdh, n , p)) %&gt;% group_by(k) %&gt;% count(name = &quot;nn&quot;) %&gt;% ungroup() %&gt;% ggplot(aes(x = k, y = nn/wdh)) + geom_bar(stat = &quot;identity&quot;, color = clr1, fill = fll1) + labs(x = &quot;k&quot;, y = glue(&quot;B(k; {n}, {p})&quot;)) + coord_cartesian(xlim = c(-.5, 10.5)) n &lt;- 10 p = 1/6 tibble(k = rbinom(wdh, n , p)) %&gt;% group_by(k) %&gt;% count(name = &quot;nn&quot;) %&gt;% ungroup() %&gt;% ggplot(aes(x = k, y = nn/wdh)) + geom_bar(stat = &quot;identity&quot;, color = clr1, fill = fll1) + labs(x = &quot;k&quot;, y = glue(&quot;B(k; {n}, {round(p,2)})&quot;)) + coord_cartesian(xlim = c(-.5, 10.5)) Cumulative probability to have at least \\(x\\) successes in \\(n\\) trials (pbinom(x-1, n, p, lower.tail = FALSE)): \\[ \\sum_{k=x}^n B(k;n,p) \\] Similarly, less then \\(x\\) successes in \\(n\\) trials (pbinom(x, n, p)): \\[ \\sum_{k=0}^{x-1} B(k;n,p) \\] 8.4 The beta distribution \\[ Beta(p;\\alpha,\\beta) = \\frac{p^{\\alpha -1} \\times (1 - p)^{\\beta - 1}}{beta(\\alpha, \\beta)} \\] Example for an \\(n = 41\\), with \\(\\alpha = 14\\) (successes) and \\(\\beta = 27\\) (fails). alpha &lt;- 14 beta &lt;- 27 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr1, fill = fll1, n = 500) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) Probability, that chance of sucess is less than 0.5: \\[ \\int_{0}^{0.5} Beta(p; 14, 27) \\] x_cutoff &lt;- 0.5 integrate(function(p){ dbeta(p, 14, 27) }, 0, x_cutoff) #&gt; 0.9807613 with absolute error &lt; 5.9e-06 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(xlim = c(0, x_cutoff), fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr2, fill = fll2, n = 500) + geom_vline(xintercept = x_cutoff, linetype = 3) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) alpha &lt;- 5 beta &lt;- 1195 x_cutoff &lt;- 0.005 integrate(function(p){ dbeta(p, alpha, beta) }, x_cutoff, 1) #&gt; 0.2850559 with absolute error &lt; 1e-04 ggplot(tibble(x = seq(0, .01, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(xlim = c(x_cutoff, .01), fun = function(x){ dbeta(x, shape1 = alpha, shape2 = beta) }, geom = &quot;area&quot;, color = clr2, fill = fll2, n = 500) + geom_vline(xintercept = x_cutoff, linetype = 3) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) Exercises # 1) integrate(function(p){ dbeta(p, 4, 6) }, 0.6, 1) #&gt; 0.09935258 with absolute error &lt; 1.1e-15 # 2) integrate(function(p){ dbeta(p, 9, 11) }, 0.45, 0.55) #&gt; 0.30988 with absolute error &lt; 3.4e-15 # 3) integrate(function(p){ dbeta(p, 109, 111) }, 0.45, 0.55) #&gt; 0.8589371 with absolute error &lt; 9.5e-15 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = 9, shape2 = 11) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(fun = function(x){ -dbeta(x, shape1 = 109, shape2 = 111) }, geom = &quot;area&quot;, color = clr1, fill = fll1, n = 500) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) 8.5 Bayes’ Theorem conditional probability The probability of A given B is \\(P(A | B)\\) Dependence updates the product rule of probabilities: \\[ P(A,B) = P(A) \\times P(B | A) \\] (This also holds for independend probabilities, where \\(P(B) = P(B|A)\\)) Bayes’ Theorem (reversing the condition to calculate the probability of the event we are conditioning on \\(P(A|B) \\rightarrow P(B|A)\\)) \\[ P(A|B) = \\frac{P(A)P(B|A)}{P(B)} \\] The important parts here are the posterior probability\\(P(H|D)\\): how strongly we belief in our hypothesis given the data likelyhood\\(P(D|H)\\): the probability of data if the hypothesis were true prior probability\\(P(H)\\): how likely our hypothesis is in the first place Unnormalized posterior \\[ P(H|D) \\propto P(H) \\times P(D|H) \\] 8.6 Parameter Estimation (I) Expectation / mean \\[ \\mu = \\sum_{1}^{n}p_{i}x_{i} \\] n &lt;- 150 mn &lt;- 3 tibble( y = rnorm(n = n, mean = mn), n = 1:n, cum_y = cumsum(y), mean_y = cum_y / n) %&gt;% ggplot(aes(x = n, y = mean_y)) + geom_hline(yintercept = mn, linetype = 3) + geom_point(aes(y = y), color = clr0, size = .75, alpha = .5) + geom_line(color = clr2, size = .75) Spread, mean absolute deviation \\[ MAD(x) = \\frac{1}{n} \\times \\sum_{1}^{n} | x_{i} - \\mu| \\] Spread, variation \\[ Var(x) = \\frac{1}{n} \\times \\sum_{1}^{n} (x_{i} - \\mu) ^{2} \\] Spread, standard deviation \\[ \\sigma = \\sqrt{\\frac{1}{n} \\times \\sum_{1}^{n} (x_{i} - \\mu) ^{2}} \\] Note that in R, var() and sd() uses \\(\\frac{1}{n-1}\\) as denominator for the normalization: var_k &lt;- function(x){ (1/length(x)) * sum( (x - mean(x)) ^ 2 ) } sd_k &lt;- function(x){ sqrt(var_k(x)) } x &lt;- 1:10 n &lt;- length(x) var(x) * ((n-1)/n) == var_k(x) #&gt; [1] TRUE sd(x) * sqrt((n-1)/n) == sd_k(x) #&gt; [1] TRUE 8.7 The normal distribution The probability density function (PDF) for the normal distribution (dnorm()): \\[ N(\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\times e ^{- \\frac{(x - \\mu) ^ 2}{2\\sigma^2}} \\] mu &lt;- 20.6 sigma &lt;- 1.62 x_cutoff &lt;- 18 ggplot(tibble(x = seq(mu - 4 * sigma, mu + 4 * sigma, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dnorm(x, mu, sigma) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(fun = function(x){ dnorm(x, mu, sigma) }, xlim = c(mu - 4 * sigma, x_cutoff), geom = &quot;area&quot;, color = clr2, fill = fll2, n = 500) + geom_vline(xintercept = x_cutoff, linetype = 3) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) integrate(function(x){ dnorm(x, mu, sigma) }, 0, x_cutoff) #&gt; 0.05425369 with absolute error &lt; 3.5e-05 Known probability mass under a normal distribution in terms of ist standard deviation: distance from \\(\\mu\\) probability \\(\\sigma\\) 68 % \\(2 \\sigma\\) 95 % \\(3 \\sigma\\) 99.7 % Excercises x &lt;- c(100, 99.8, 101, 100.5,99.7) mu &lt;- mean(x) sigma &lt;- sd(x) x_cutoff &lt;- 100.4 ggplot(tibble(x = seq(mu - 4 * sigma, mu + 4 * sigma, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dnorm(x, mu, sigma) }, geom = &quot;area&quot;, color = clr0, fill = fll0, n = 500) + stat_function(fun = function(x){ dnorm(x, mu, sigma) }, xlim = c(x_cutoff, mu + 4 * sigma), geom = &quot;area&quot;, color = clr2, fill = fll2, n = 500) + geom_vline(xintercept = x_cutoff, linetype = 3) + labs(x = &quot;p&quot;, y = &quot;Density&quot;) 1 - (integrate(function(x){ dnorm(x, mu, sigma) }, mu - sigma, x_cutoff)[[1]] + (1-.68)/2) #&gt; [1] 0.3550062 8.8 Cummulative Density and Quantile Function Beta distribution example Mean of Beta distribution \\[ \\mu_{Beta} = \\frac{\\alpha}{\\alpha + \\beta} \\] alpha &lt;- 300 beta &lt;- 39700 mu &lt;- alpha / (alpha + beta) med &lt;- qbeta(.5, shape1 = alpha, shape2 = beta) bound_left &lt;- .006 bound_right &lt;- .009 8.9 Parameter estimation with prior probabilities alpha_data &lt;- 2 beta_data &lt;- 3 alpha_prior &lt;- 1 beta_prior &lt;- 41 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_data, shape2 = beta_data) }, geom = &quot;line&quot;, color = clr0,linetype = 1, n = 500) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_prior, shape2 = beta_prior) }, geom = &quot;line&quot;, color = clr0,linetype = 2, n = 500) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_data + alpha_prior, shape2 = beta_data + beta_prior) }, geom = &quot;area&quot;, color = clr1, fill = fll1, size = .2, n = 500) + labs(x = &quot;Probability&quot;, y = &quot;Density&quot;, caption = &quot;**solid:** data; **dashed:** prior; **filled:** posterior&quot;) + coord_cartesian(ylim = c(0, 15)) + theme(plot.caption = ggtext::element_markdown(halign = .5, hjust = .5)) 8.10 Monte CarloSimulation alpha_a &lt;- 36 beta_a &lt;- 114 alpha_b &lt;- 50 beta_b &lt;- 100 alpha_prior &lt;- 3 beta_prior &lt;- 7 ggplot(tibble(x = seq(0, 1, length.out = 3)), aes(x)) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_a + alpha_prior, shape2 = beta_a + beta_prior) }, geom = &quot;area&quot;, aes(color = &quot;a&quot;, fill = after_scale(clr_alpha(color))), size = .5, n = 500) + stat_function(fun = function(x){ dbeta(x, shape1 = alpha_b + alpha_prior, shape2 = beta_b + beta_prior) }, geom = &quot;area&quot;, aes(color = &quot;b&quot;, fill = after_scale(clr_alpha(color))), size = .5, n = 500) + labs(x = &quot;Probability&quot;, y = &quot;Density&quot;) + scale_color_manual(&quot;Variant&quot;, values = c(a = clr0, b = clr2)) n_trials &lt;- 10^5 mc_simulation &lt;- tibble(samples_a = rbeta(n_trials, alpha_a + alpha_prior, beta_a + beta_prior), samples_b = rbeta(n_trials, alpha_b + alpha_prior, beta_b + beta_prior), samples_ratio = samples_b / samples_a) p_b_superior &lt;- sum(mc_simulation$samples_b &gt; mc_simulation$samples_a)/n_trials p_b_superior #&gt; [1] 0.9596 p_hist &lt;- mc_simulation %&gt;% ggplot(aes(x = samples_ratio)) + geom_histogram(color = clr2, fill = fll2, size = .2, bins = 20,boundary = 1) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) p_ecdf &lt;- mc_simulation %&gt;% ggplot() + stat_function(fun = function(x){(ecdf(x = mc_simulation$samples_ratio))(x)}, xlim = c(range(mc_simulation$samples_ratio)), geom = &quot;area&quot;,color = clr2, fill = fll2, size = .2, n = 500) + labs(x = &quot;Variant Ratio = Improvement&quot;, y = &quot;Cumulative Probability&quot;) p_hist / p_ecdf &amp; coord_cartesian(xlim = c(0.2,3.3), expand = 0) 8.11 Posterior Odds For compering Hypotheses: ratio of posterior: \\[ posterior odds = \\frac{P(H_1) \\times P(D | H_{1})}{P(H_2) \\times P(D | H_{2})} = O(H_{1}) \\times \\frac{P(D | H_{1})}{P(D | H_{2})} \\] This consists of the Bayes factor: \\[ \\frac{P(D | H_{1})}{P(D | H_{2})} \\] and the ratio of prior probabilities \\[ O(H_{1}) = \\frac{P(H_1)}{P(H_2)} \\] rough guide to evaluate poterior odds: Posterior odds Strength of evidence 1 to 3 Interesting but not conclusive 3 to 20 Looks like we’re onto something 20 to 150 Strong evidence in favor of \\(H_1\\) &gt; 150 Overwhelming evidence 8.12 Parameter Estimation (II) dx &lt;- 0.01 bayes_factor &lt;- function(h_top, h_bottom, n_success = 24, n_total = 100){ (h_top ^ n_success * (1 - h_top) ^ (n_total - n_success)) / (h_bottom ^ n_success * (1 - h_bottom) ^ (n_total - n_success)) } bayes_fs &lt;- tibble(hypothesis = seq(0, 1, by = dx), bayes_factor = bayes_factor(hypothesis, h_bottom = 0.5)) %&gt;% mutate(prior = if_else(between(hypothesis, 0.2, 0.3), .001, 1), posterior = prior * bayes_factor, posterior_normalized = posterior / sum(posterior)) p_bayes_factor &lt;- bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = bayes_factor)) + geom_area(color = clr1, fill = fll1, size = .3) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) p_prior &lt;- bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = prior)) + geom_area(color = clr0, fill = fll0, size = .3) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) p_posterior &lt;- bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = posterior)) + geom_area(color = clr2, fill = fll2, size = .3) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) p_posterior_n &lt;- bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = posterior_normalized)) + geom_area(color = clr2, fill = fll2, size = .3) p_bayes_factor / p_prior / p_posterior / p_posterior_n hypothesis bayes_factor prior posterior posterior_normalized 0.24 1478776 0.001 1478.776 0.0004708 Probability of true chance is smaller “one in two”. bayes_fs %&gt;% filter(hypothesis &lt; 0.5) %&gt;% summarise(p_lower_than_half = sum(posterior_normalized)) #&gt; # A tibble: 1 x 1 #&gt; p_lower_than_half #&gt; &lt;dbl&gt; #&gt; 1 1.00 Expectation of the probability distribution (sum of expectations weighted by their value) sum(bayes_fs$posterior_normalized * bayes_fs$hypothesis) #&gt; [1] 0.2402704 Or (because of gap) choose most likely estimate: bayes_fs %&gt;% filter(posterior_normalized == max(posterior_normalized)) %&gt;% knitr::kable() hypothesis bayes_factor prior posterior posterior_normalized 0.19 688568.9 1 688568.9 0.2192415 bayes_fs %&gt;% ggplot(aes(x = hypothesis, y = bayes_factor / sum(bayes_factor))) + geom_area(color = clr2, fill = fll2, size = .3) + stat_function(fun = function(x){ dbeta(x, shape1 = 24 + 1 , shape2 = 76 + 1) * dx}, geom = &quot;point&quot;, shape = 21, color = clr2, fill = fll2, size = 1.5, n = 500) Exercises bayes_fs_e1 &lt;- tibble(hypothesis = seq(0, 1, by = dx), bayes_factor = bayes_factor(hypothesis, h_bottom = 0.24)) %&gt;% mutate(prior = if_else(between(hypothesis, 0.2, 0.3), .001, 1), posterior = prior * bayes_factor, posterior_normalized = posterior / sum(posterior)) bayes_fs_e1 %&gt;% ggplot(aes(x = hypothesis, y = bayes_factor / sum(bayes_factor))) + geom_area(color = clr2, fill = fll2, size = .3) + stat_function(fun = function(x){ dbeta(x, shape1 = 24 + 1 , shape2 = 76 + 1) * dx}, geom = &quot;point&quot;, shape = 21, color = clr2, fill = fll2, size = 1.5, n = 500) bayes_fs_e2 &lt;- tibble(hypothesis = seq(0, 1, by = dx), bayes_factor = bayes_factor(hypothesis, h_bottom = 0.24)) %&gt;% mutate(prior = 1.05 ^ (seq_along(hypothesis) - 1), posterior = prior * bayes_factor, posterior_normalized = posterior / sum(posterior)) bayes_fs_e2 %&gt;% ggplot(aes(x = hypothesis, y = prior)) + geom_area(color = clr0, fill = fll0, size = .3) + theme(axis.text.x = element_blank(), axis.title.x = element_blank()) bayes_fs_e2 %&gt;% ggplot(aes(x = hypothesis, y = posterior_normalized)) + geom_area(color = clr2, fill = fll2, size = .3) × "],
["mixed-models-with-r.html", "9 Mixed Models with R 9.1 Standard regregression model 9.2 mixed nodel 9.3 Add random slope 9.4 Cross Classified models 9.5 Hierachical structure 9.6 Residual Structure 9.7 Generalized Linear Mixed Models 9.8 Issues/ Considderations 9.9 Formula summary", " 9 Mixed Models with R by Michael Clark load(&quot;data/gpa.RData&quot;) gpa &lt;- gpa %&gt;% as_tibble() 9.1 Standard regregression model \\[ gpa = b_{intercept} + b_{occ} \\times occasion + \\epsilon \\] Coefficients \\(b\\) for intercept and effect of time. The error \\(\\epsilon\\) is assumed to be normally distributed with \\(\\mu = 0\\) and some standard deviation \\(\\sigma\\). \\[ \\epsilon \\sim \\mathscr{N}(0, \\sigma) \\] alternate notation, with emphasis on the data generating process: \\[ gpa ~ \\sim \\mathscr{N}(\\mu, \\sigma)\\\\ \\mu = b_{intercept} + b_{occ} \\times occasion \\] 9.2 mixed nodel 9.2.1 student specific effect (initial depiction) \\[ gpa = b_{intercept} + b_{occ} \\times occasion + ( \\textit{effect}_{student} + \\epsilon )\\\\ \\textit{effect}_{student} \\sim \\mathscr{N}(0, \\tau) \\] focusing on the coefficients (rather than on sources of error): \\[ gpa = ( b_{intercept} + \\textit{effect}_{student} ) + b_{occ} \\times occasion + \\epsilon \\] or (shorter) \\[ gpa = b_{int\\_student} + b_{occ} \\times occasion + \\epsilon \\] \\(\\rightarrow\\) this means student specific intercepts… \\[ b_{int\\_student} \\sim \\mathscr{N}(b_{intercept}, \\tau) \\] …that are normally distributed with the mean of the overall intercept (random intercepts model) 9.2.2 as multi-level model two-part regression model (one at observation level, one at student level) (this is the same as above, just needs ‘plugging in’) \\[ gpa = b_{int\\_student} + b_{occ} \\times occasion + \\epsilon\\\\ b_{int\\_student} = b_{intercept} + \\textit{effect}_{student} \\] ! There is no student-specific effect for \\(occasion\\) (which is termed fixed effect), and there is no random component gpa_lm &lt;- lm(gpa ~ occasion, data = gpa) gpa %&gt;% ggplot(aes(x = year - 1 + as.numeric(semester)/2, y = gpa, group = student)) + geom_line(alpha = .2) + geom_abline(slope = gpa_lm$coefficients[[2]], intercept = gpa_lm$coefficients[[1]], color = clr2, size = 1) + labs(x = &quot;semester&quot;) + coord_cartesian(ylim = c(1,4), expand = 0) pander::pander(summary(gpa_lm), round = 3)   Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.599 0.018 145.7 0 occasion 0.106 0.006 18.04 0 Fitting linear model: gpa ~ occasion Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 1200 0.3487 0.2136 0.2129 Student effect not taken into account. 9.2.3 Mixed Model gpa_mixed &lt;- lmer(gpa ~ occasion + (1 | student), data = gpa) (Test automatic equation creation) library(equatiomatic) # Give the results to extract_eq extract_eq(gpa_mixed,) \\[ \\begin{aligned} \\operatorname{gpa}_{i} &amp;\\sim N \\left(\\alpha_{j[i]} + \\beta_{1}(\\operatorname{occasion}), \\sigma^2 \\right) \\\\ \\alpha_{j} &amp;\\sim N \\left(\\mu_{\\alpha_{j}}, \\sigma^2_{\\alpha_{j}} \\right) \\text{, for student j = 1,} \\dots \\text{,J} \\end{aligned} \\] term value se t p_value lower_2.5 upper_97.5 Intercept 2.599 0.022 119.8 0 2.557 2.642 occasion 0.106 0.004 26.1 0 0.098 0.114 group effect variance sd var_prop student Intercept 0.064 0.252 0.523 Residual 0.058 0.241 0.477 Coefficients (fixed effects) for time and intercept are the same as lm() Getting confidence intervals from a mixed model (since \\(p\\) values are not given (== 0 ?)) confint(gpa_mixed) #&gt; 2.5 % 97.5 % #&gt; .sig01 0.22517423 0.2824604 #&gt; .sigma 0.23071113 0.2518510 #&gt; (Intercept) 2.55665145 2.6417771 #&gt; occasion 0.09832589 0.1143027 mm_cinf &lt;- mixedup::extract_vc(gpa_mixed) mm_cinf %&gt;% pander::pander() Table continues below   group effect variance sd sd_2.5 sd_(Intercept)|student student Intercept 0.064 0.252 0.225 sigma Residual 0.058 0.241 0.231   sd_97.5 var_prop sd_(Intercept)|student 0.282 0.523 sigma 0.252 0.477 student effect \\(\\tau\\) = 0.252 / 0.064 (sd / var) Percentage of student variation as share of the total variation (intraclass correlation): 0.064 / 0.122 = 0.5245902 9.2.4 Estimation of random effects Random effect mixedup::extract_random_effects(gpa_mixed) %&gt;% head(5) %&gt;% knitr::kable() group_var effect group value se lower_2.5 upper_97.5 student Intercept 1 -0.071 0.092 -0.251 0.109 student Intercept 2 -0.216 0.092 -0.395 -0.036 student Intercept 3 0.088 0.092 -0.091 0.268 student Intercept 4 -0.187 0.092 -0.366 -0.007 student Intercept 5 0.030 0.092 -0.149 0.210 Random intercept (intercept + random effect) mm_coefs &lt;- mixedup::extract_coef(gpa_mixed) mm_coefs %&gt;% head(5) %&gt;% knitr::kable() group_var effect group value se lower_2.5 upper_97.5 student Intercept 1 2.528 0.095 2.343 2.713 student Intercept 2 2.383 0.095 2.198 2.568 student Intercept 3 2.687 0.095 2.502 2.872 student Intercept 4 2.412 0.095 2.227 2.597 student Intercept 5 2.629 0.095 2.444 2.814 library(merTools) mm_intervals &lt;- predictInterval(gpa_mixed) %&gt;% as_tibble() mm_mean_sd &lt;- REsim(gpa_mixed) %&gt;% as_tibble() sd_level &lt;- .95 mm_mean_sd %&gt;% mutate(sd_leveled = sd * qnorm(1 - ((1 - sd_level)/2)), sig = (median + sd_leveled) &gt;= 0 &amp; (median - sd_leveled) &lt;= 0) %&gt;% arrange(median) %&gt;% mutate(rank = row_number()) %&gt;% arrange(groupID) %&gt;% ggplot(aes(x = rank)) + geom_segment(aes(xend = rank, y = median - sd_leveled, yend = median + sd_leveled, color = sig)) + geom_point(aes(y = median), size = .3, alpha = .5) + geom_hline(yintercept = 0, size = .4, linetype = 3) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;Student&quot;, y = &quot;Coefficient&quot;, caption = &quot;interval estimates of random effects&quot;) 9.2.5 Prediction gpa_predictions &lt;- tibble(lm = predict(gpa_lm), lmm_no_random_effects = predict(gpa_mixed, re.form = NA), lmm_with_random_effects = predict(gpa_mixed)) %&gt;% bind_cols(gpa, .) gpa_predictions %&gt;% ggplot(aes(x = lm)) + geom_point(aes(y = lmm_with_random_effects, color = &quot;with_re&quot;)) + geom_point(aes(y = lmm_no_random_effects, color = &quot;no_re&quot;)) + scale_color_manual(values = c(no_re = clr2, with_re = clr0d)) student_select &lt;- 1:2 gpa_predictions %&gt;% filter(student %in% student_select) %&gt;% ggplot(aes(x = occasion)) + geom_point(aes(y = gpa, color = student))+ geom_abline(data = tibble(slope = c(gpa_lm$coefficients[[2]], mm_fixed$value[c(2,2)]), intercept = c(gpa_lm$coefficients[[1]], mm_coefs$value[as.numeric(as.character(mm_coefs$group)) %in% student_select]), model = c(&quot;lm&quot;, as.character(mm_coefs$group[as.numeric(as.character(mm_coefs$group)) %in% student_select]))), aes(slope = slope, intercept = intercept, color = model), size = .6) + scale_color_manual(values = c(lm = clr2, `1` = clr1, `2` = clr0d), labels = c( &quot;lm&quot;, &quot;lmm (student1)&quot;, &quot;lmm (student2)&quot;)) 9.2.6 Cluster Level Covariates If a cluster level covariate is added (eg. sex), \\(b_{int\\_student}\\) turns into: \\[ b_{int\\_student} = b_{intercept} + b_{sex} \\times \\textit{sex} + \\textit{effect}_{student} \\] plugging this into the model will result in \\[ gpa = b_{intercept} + b_{occ} \\times \\textit{occasion} + b_{sex} \\times \\textit{sex} + ( \\textit{effect}_{student} + \\epsilon) \\] 9.3 Add random slope gpa_mixed2 &lt;- lmer(gpa ~ occasion + (1 + occasion | student), data = gpa) mixedup::extract_fixed_effects(gpa_mixed2) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 2.599 0.018 141.592 0 2.563 2.635 occasion 0.106 0.006 18.066 0 0.095 0.118 mixedup::extract_vc(gpa_mixed2, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop student Intercept 0.045 0.213 0.491 student occasion 0.005 0.067 0.049 Residual 0.042 0.206 0.460 gpa_mixed2_rc &lt;- mixedup::extract_random_coefs(gpa_mixed2) correlation of the intercepts and slopes (negative, so students with a low starting score tend to increase a little more) VarCorr(gpa_mixed2) %&gt;% as_tibble() %&gt;% knitr::kable() grp var1 var2 vcov sdcor student (Intercept) NA 0.0451934 0.2125875 student occasion NA 0.0045039 0.0671114 student (Intercept) occasion -0.0014016 -0.0982391 Residual NA NA 0.0423879 0.2058832 gpa_lm_separate &lt;- gpa %&gt;% group_by(student) %&gt;% nest() %&gt;% mutate(mod = map(data,function(data){lm(gpa ~ occasion, data = data)})) %&gt;% bind_cols(., summarise_model(.)) p_intercepts &lt;- ggplot() + geom_density(data = gpa_mixed2_rc %&gt;% filter(effect == &quot;Intercept&quot;), aes(x = value, color = &quot;mixed&quot;, fill = after_scale(clr_alpha(color)))) + geom_density(data = gpa_lm_separate, aes(x = intercept, color = &quot;separate&quot;, fill = after_scale(clr_alpha(color)))) + labs(x = &quot;intercept&quot;) + xlim(1.5, 4) p_slopes &lt;- ggplot() + geom_density(data = gpa_mixed2_rc %&gt;% filter(effect == &quot;occasion&quot;), aes(x = value, color = &quot;mixed&quot;, fill = after_scale(clr_alpha(color)))) + geom_density(data = gpa_lm_separate, aes(x = slope, color = &quot;separate&quot;, fill = after_scale(clr_alpha(color)))) + labs(x = &quot;slope&quot;) + xlim(-.2, .4) p_intercepts + p_slopes + plot_layout(guides = &quot;collect&quot;) &amp; scale_color_manual(&quot;model&quot;, values = c(separate = clr0d, mixed = clr2)) &amp; theme(legend.position = &quot;bottom&quot;) \\(\\rightarrow\\) mixed model intercepts and slopes are less extreme In both cases the mixed model shrinks what would have been the by-group estimate, which would otherwise overfit in this scenario. This regularizing effect is yet another bonus when using mixed models. gpa_predictions &lt;- tibble(lmm_with_random_slope = predict(gpa_mixed2)) %&gt;% bind_cols(gpa_predictions, .) gpa_mixed2_rc_wide &lt;- gpa_mixed2_rc %&gt;% dplyr::select(group_var, group, effect, value) %&gt;% pivot_wider(names_from = effect, values_from = value) student_select &lt;- 1:2 p_two_students &lt;- gpa_predictions %&gt;% filter(student %in% student_select) %&gt;% ggplot(aes(x = occasion)) + geom_point(aes(y = gpa, color = student))+ geom_abline(data = tibble(slope = c(gpa_lm$coefficients[[2]], gpa_mixed2_rc_wide$occasion[as.numeric(as.character(gpa_mixed2_rc_wide$group)) %in% student_select]), intercept = c(gpa_lm$coefficients[[1]], gpa_mixed2_rc_wide$Intercept[as.numeric(as.character(gpa_mixed2_rc_wide$group)) %in% student_select]), model = c(&quot;lm&quot;, as.character(gpa_mixed2_rc_wide$group[as.numeric(as.character(gpa_mixed2_rc_wide$group)) %in% student_select]))), aes(slope = slope, intercept = intercept, color = model), size = .6) + scale_color_manual(values = c(lm = clr2, `1` = clr1, `2` = clr0d), labels = c( &quot;lm&quot;, &quot;lmm (student1)&quot;, &quot;lmm (student2)&quot;)) p_all_mod &lt;- ggplot(data = gpa_predictions, aes(x = occasion, y = gpa)) + geom_abline(data = tibble(slope = c(gpa_mixed2_rc_wide$occasion, gpa_lm$coefficients[[2]]), intercept = c(gpa_mixed2_rc_wide$Intercept, gpa_lm$coefficients[[1]]), modeltype = c(rep(&quot;lmm (random slope)&quot;, length(gpa_mixed2_rc_wide$Intercept)), &quot;lm&quot;)), aes(slope = slope, intercept = intercept, color = modeltype), size = .6) + scale_color_manual(values = c(`lmm (random slope)` = clr_alpha(clr0d ,.6), lm = clr2)) p_two_students + p_all_mod + plot_layout(guides = &quot;collect&quot;) &amp; xlim(0,5) &amp; ylim(2.2, 4) &amp; theme(legend.position = &quot;bottom&quot;) 9.4 Cross Classified models Setups where data are grouped by several factors but these are not nested (all participants get to see all images). These are crossed random effects. load(&quot;data/pupils.RData&quot;) pupils %&gt;% head() %&gt;% knitr::kable() PUPIL primary_school_id secondary_school_id achievement sex ses primary_denominational secondary_denominational 1 1 2 6.6 female highest no no 2 1 1 5.7 male lowest no yes 3 1 17 4.5 male 2 no no 4 1 3 4.4 male 2 no no 5 1 4 5.8 male 3 no yes 6 1 4 5.0 female 4 no yes pupils_crossed &lt;- lmer( achievement ~ sex + ses + ( 1 | primary_school_id ) + ( 1 | secondary_school_id ), data = pupils ) mixedup::extract_fixed_effects(pupils_crossed) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 5.924 0.123 48.303 0.000 5.684 6.164 sexfemale 0.261 0.046 5.716 0.000 0.171 0.350 ses2 0.132 0.118 1.122 0.262 -0.098 0.362 ses3 0.098 0.110 0.890 0.373 -0.118 0.314 ses4 0.298 0.105 2.851 0.004 0.093 0.503 ses5 0.354 0.101 3.514 0.000 0.156 0.551 seshighest 0.616 0.110 5.602 0.000 0.401 0.832 mixedup::extract_vc(pupils_crossed, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop primary_school_id Intercept 0.173 0.416 0.243 secondary_school_id Intercept 0.066 0.257 0.093 Residual 0.473 0.688 0.664 pupils_varicance_components_random_effects &lt;- REsim(pupils_crossed) %&gt;% as_tibble() pupils_varicance_components_random_effects %&gt;% mutate(sd_leveled = sd * qnorm(1 - ((1 - sd_level)/2)), sig = (median + sd_leveled) &gt;= 0 &amp; (median - sd_leveled) &lt;= 0) %&gt;% group_by(groupFctr) %&gt;% arrange(groupFctr, median) %&gt;% mutate(rank = row_number()) %&gt;% arrange(groupFctr, groupID) %&gt;% ungroup() %&gt;% ggplot(aes(x = rank)) + geom_segment(aes(xend = rank, y = median - sd_leveled, yend = median + sd_leveled, color = sig)) + geom_point(aes(y = median), size = .3, alpha = .5) + geom_hline(yintercept = 0, size = .4, linetype = 3) + facet_wrap(groupFctr ~ ., scales = &quot;free_x&quot;) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + labs(x = &quot;Group&quot;, y = &quot;Effect Range&quot;, caption = &quot;interval estimates of random effects&quot;) Note that we have the usual extensions here if desired. As an example, we could also do random slopes for student level characteristics. 9.5 Hierachical structure These are setups, where different grouping factors are nested within each other (eg. cities, counties, states). load(&quot;data/nurses.RData&quot;) nurses %&gt;% head() %&gt;% knitr::kable() hospital ward wardid nurse age sex experience stress wardtype hospsize treatment 1 1 11 1 36 Male 11 7 general care large Training 1 1 11 2 45 Male 20 7 general care large Training 1 1 11 3 32 Male 7 7 general care large Training 1 1 11 4 57 Female 25 6 general care large Training 1 1 11 5 46 Female 22 6 general care large Training 1 1 11 6 60 Female 22 6 general care large Training nurses_hierach &lt;- lmer( stress ~ age + sex + experience + treatment + wardtype + hospsize + ( 1 | hospital) + ( 1 | hospital:ward), # together same as ( 1 | hospital / ward) data = nurses ) mixedup::extract_fixed_effects(nurses_hierach) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 5.380 0.185 29.128 0.000 5.018 5.742 age 0.022 0.002 10.053 0.000 0.018 0.026 sexFemale -0.453 0.035 -12.952 0.000 -0.522 -0.385 experience -0.062 0.004 -13.776 0.000 -0.070 -0.053 treatmentTraining -0.700 0.120 -5.843 0.000 -0.935 -0.465 wardtypespecial care 0.051 0.120 0.424 0.671 -0.184 0.286 hospsizemedium 0.489 0.202 2.428 0.015 0.094 0.884 hospsizelarge 0.902 0.275 3.280 0.001 0.363 1.440 mixedup::extract_vc(nurses_hierach, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop hospital:ward Intercept 0.337 0.580 0.500 hospital Intercept 0.119 0.345 0.177 Residual 0.217 0.466 0.323 nurses_varicance_components_random_effects &lt;- REsim(nurses_hierach) %&gt;% as_tibble() nurses_varicance_components_random_effects %&gt;% mutate(sd_leveled = sd * qnorm(1 - ((1 - sd_level)/2)), sig = (median + sd_leveled) &gt;= 0 &amp; (median - sd_leveled) &lt;= 0) %&gt;% group_by(groupFctr) %&gt;% arrange(groupFctr, median) %&gt;% mutate(rank = row_number()) %&gt;% arrange(groupFctr, groupID) %&gt;% ungroup() %&gt;% ggplot(aes(x = rank)) + geom_segment(aes(xend = rank, y = median - sd_leveled, yend = median + sd_leveled, color = sig)) + geom_point(aes(y = median), size = .3, alpha = .5) + geom_hline(yintercept = 0, size = .4, linetype = 3) + facet_wrap(groupFctr ~ ., scales = &quot;free_x&quot;) + scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = &quot;none&quot;) + ylim(-2,2) + labs(x = &quot;Group&quot;, y = &quot;Effect Range&quot;, caption = &quot;interval estimates of random effects&quot;) 9.5.1 Crossed vs. nested nurses_hierach2 &lt;- lmer( stress ~ age + sex + experience + treatment + wardtype + hospsize + ( 1 | hospital ) + ( 1 | hospital:wardid ), # needs to be wardid now because ward is duplicated over hospitals (not unique) data = nurses ) nurses_nested &lt;- lmer( stress ~ age + sex + experience + treatment + wardtype + hospsize + ( 1 | hospital ) + ( 1 | wardid ), data = nurses ) Nested: mixedup::extract_fixed_effects(nurses_hierach2) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 5.380 0.185 29.128 0.000 5.018 5.742 age 0.022 0.002 10.053 0.000 0.018 0.026 sexFemale -0.453 0.035 -12.952 0.000 -0.522 -0.385 experience -0.062 0.004 -13.776 0.000 -0.070 -0.053 treatmentTraining -0.700 0.120 -5.843 0.000 -0.935 -0.465 wardtypespecial care 0.051 0.120 0.424 0.671 -0.184 0.286 hospsizemedium 0.489 0.202 2.428 0.015 0.094 0.884 hospsizelarge 0.902 0.275 3.280 0.001 0.363 1.440 mixedup::extract_vc(nurses_hierach2, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop hospital:wardid Intercept 0.337 0.580 0.500 hospital Intercept 0.119 0.345 0.177 Residual 0.217 0.466 0.323 Crossed: mixedup::extract_fixed_effects(nurses_nested) %&gt;% knitr::kable() term value se t p_value lower_2.5 upper_97.5 Intercept 5.380 0.185 29.128 0.000 5.018 5.742 age 0.022 0.002 10.053 0.000 0.018 0.026 sexFemale -0.453 0.035 -12.952 0.000 -0.522 -0.385 experience -0.062 0.004 -13.776 0.000 -0.070 -0.053 treatmentTraining -0.700 0.120 -5.843 0.000 -0.935 -0.465 wardtypespecial care 0.051 0.120 0.424 0.671 -0.184 0.286 hospsizemedium 0.489 0.202 2.428 0.015 0.094 0.884 hospsizelarge 0.902 0.275 3.280 0.001 0.363 1.440 mixedup::extract_vc(nurses_nested, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop wardid Intercept 0.337 0.580 0.500 hospital Intercept 0.119 0.345 0.177 Residual 0.217 0.466 0.323 9.6 Residual Structure rescov &lt;- function(model, data) { var.d &lt;- crossprod(getME(model,&quot;Lambdat&quot;)) Zt &lt;- getME(model,&quot;Zt&quot;) vr &lt;- sigma(model)^2 var.b &lt;- vr*(t(Zt) %*% var.d %*% Zt) sI &lt;- vr * Diagonal(nrow(data)) var.y &lt;- var.b + sI var.y %&gt;% as.matrix() %&gt;% as_tibble() %&gt;% mutate(row = row_number()) %&gt;% pivot_longer(cols = -row, names_to = &quot;column&quot;) } rescov(gpa_mixed, gpa) %&gt;% mutate(x = as.numeric(column), y = as.numeric(row)) %&gt;% filter(between(x,0,30), between(y,0,30)) %&gt;% ggplot(aes(x = x, y = y, fill = value)) + geom_tile(aes(color = after_scale(clr_darken(fill))), size = .3, width = .9, height = .9) + scale_fill_gradientn(colours = c(clr0, clr_lighten(clr1), clr_lighten(clr2))) + scale_y_reverse() + coord_equal() covariance matrix for a cluster (compound symmetry): \\[ \\Sigma = \\left[ \\begin{array}{ccc} \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 \\\\ \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 \\\\ \\tau^2 &amp; \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 \\\\ \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 &amp; \\tau^2\\\\ \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} &amp; \\tau^2 \\\\ \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\tau^2 &amp; \\color{#B35136}{\\sigma^2 + \\tau^2} \\\\ \\end{array}\\right] \\] Types of covariance structures: in a standard linear regression model, we have constant variance and no covariance: \\[ \\Sigma = \\left[ \\begin{array}{ccc} \\sigma^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2 \\\\ \\end{array}\\right] \\] next, relax the assumption of equal variances, and estimate each separately. In this case of heterogeneous variances, we might see more or less variance over time, for example. \\[ \\Sigma = \\left[ \\begin{array}{ccc} \\sigma_1^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_3^2 \\\\ \\end{array}\\right] \\] we actually want to get at the underlying covariance/correlation. I’ll switch to the correlation representation, but you can still think of the variances as constant or separately estimated. So now we have something like this, where \\(\\rho\\) represents the residual correlation among observations. \\[ \\Sigma = \\sigma^2 \\left[ \\begin{array}{ccc} 1 &amp; \\rho_1 &amp; \\rho_2 \\\\ \\rho_1 &amp; 1 &amp; \\rho_3 \\\\ \\rho_2 &amp; \\rho_3 &amp; 1 \\\\ \\end{array}\\right] \\] \\(\\rightarrow\\) unstructured / symmetric correlation structure (compound symmetry) Autocorrelation (lag of order one for residuals): \\[ \\Sigma = \\sigma^2 \\left[ \\begin{array}{cccc} 1 &amp; \\rho &amp; \\rho^2 &amp; \\rho^3 \\\\ \\rho &amp; 1 &amp; \\rho &amp; \\rho^2 \\\\ \\rho^2 &amp; \\rho &amp; 1 &amp; \\rho \\\\ \\rho^3 &amp; \\rho^2 &amp; \\rho &amp; 1 \\\\ \\end{array}\\right] \\] 9.6.1 Heterogeneous variance library(nlme) gpa_hetero_res &lt;- lme( gpa ~ occasion, data = gpa, random = ~ 1 | student, weights = varIdent(form = ~ 1 | occasion) ) mixedup::extract_fixed_effects(gpa_hetero_res) %&gt;% knitr::kable() term value se z p_value lower_2.5 upper_97.5 Intercept 2.599 0.026 99.002 0 2.547 2.650 occasion 0.106 0.004 26.317 0 0.098 0.114 mixedup::extract_vc(gpa_hetero_res, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop student Intercept 0.094 0.306 0.404 Residual 0.138 0.372 0.596 alternative approach to heterogeneous variance models: library(glmmTMB) gpa_hetero_res2 &lt;- glmmTMB( gpa ~ occasion + ( 1 | student ) + diag( 0 + occas | student ), data = gpa ) Comparing results of {nlme} and {glmmTMB} tibble(relative_val = c(1, coef(gpa_hetero_res$modelStruct$varStruct, unconstrained = FALSE))) %&gt;% mutate(absolute_val = (relative_val * gpa_hetero_res$sigma) ^ 2, `hetero_res (nlme)` = mixedup::extract_het_var(gpa_hetero_res, scale = &#39;var&#39;, digits = 5) %&gt;% unname() %&gt;% as.vector() %&gt;% t() %&gt;% .[,1], `hetero_res (glmmTMB)` = mixedup::extract_het_var(gpa_hetero_res2, scale = &#39;var&#39;, digits = 5) %&gt;% dplyr::select(-group) %&gt;% unname() %&gt;% as.vector() %&gt;% t() %&gt;% .[,1]) %&gt;% knitr::kable() relative_val absolute_val hetero_res (nlme) hetero_res (glmmTMB) 1.0000000 0.1381504 0.13815 0.13790 0.8261186 0.0942837 0.09428 0.09415 0.6272415 0.0543528 0.05435 0.05430 0.4311126 0.0256764 0.02568 0.02568 0.3484013 0.0167692 0.01677 0.01677 0.4324628 0.0258374 0.02584 0.02580 9.6.2 Autocorrelation gpa_autocorr &lt;- lme( gpa ~ occasion, data = gpa, random = ~ 1 | student, correlation = corAR1(form = ~ occasion) ) mixedup::extract_fixed_effects(gpa_autocorr) %&gt;% knitr::kable() term value se z p_value lower_2.5 upper_97.5 Intercept 2.597 0.023 113.146 0 2.552 2.642 occasion 0.107 0.005 20.297 0 0.097 0.118 mixedup::extract_vc(gpa_autocorr, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop student Intercept 0.046 0.215 0.381 Residual 0.075 0.273 0.619 gpa_autocorr2 &lt;- glmmTMB( gpa ~ occasion + ar1( 0 + occas | student ) + ( 1 | student ), # occas is cotegorical version of occasion data = gpa ) mixedup::extract_fixed_effects(gpa_autocorr2) %&gt;% knitr::kable() term value se z p_value lower_2.5 upper_97.5 Intercept 2.598 0.023 111.077 0 2.552 2.643 occasion 0.107 0.005 19.458 0 0.096 0.118 mixedup::extract_vc(gpa_autocorr2, ci_level = 0) %&gt;% knitr::kable() group variance sd var_prop student 0.093 0.305 0.159 student.1 0.000 0.000 0.000 Residual 0.028 0.167 0.048 9.7 Generalized Linear Mixed Models load(&quot;data/speed_dating.RData&quot;) sdating &lt;- glmer( decision ~ sex + samerace + attractive_sc + sincere_sc + intelligent_sc + ( 1 | iid), data = speed_dating, family = binomial ) mixedup::extract_fixed_effects(sdating) %&gt;% knitr::kable() term value se z p_value lower_2.5 upper_97.5 Intercept -0.743 0.121 -6.130 0.00 -0.981 -0.506 sexMale 0.156 0.164 0.954 0.34 -0.165 0.478 sameraceYes 0.314 0.075 4.192 0.00 0.167 0.460 attractive_sc 0.502 0.015 33.559 0.00 0.472 0.531 sincere_sc 0.089 0.016 5.747 0.00 0.059 0.120 intelligent_sc 0.143 0.017 8.232 0.00 0.109 0.177 mixedup::extract_vc(sdating, ci_level = 0) %&gt;% knitr::kable() group effect variance sd var_prop iid Intercept 2.708 1.645 1 9.8 Issues/ Considderations small number of clusters: problematic - similar to number of samples to compute variance / mean or similar (to get to the variance component we need enough groups). This also touches whether something should be a fixed or a random effect (random is always possible when the number of clusters is large enough) small number of observations within clusters: no problem, but might prevent random slopes (for n == 1) balanced design / missing data: not really a requirement, so as long as it is not extreme likely not an issue 9.8.1 Model comparison Using AIC can help, but should not used to make the decision—reasoning about the implications of the used models should. gpa_1 &lt;- lmer(gpa ~ occasion + (1 + occasion | student), data = gpa) gpa_2 &lt;- lmer(gpa ~ occasion + sex + (1 + occasion | student), data = gpa) gpa_3 &lt;- lmer(gpa ~ occasion + (1 | student), data = gpa) list(gpa_1 = gpa_1, gpa_2 = gpa_2, gpa_3 = gpa_3) %&gt;% map_df(function(mod) data.frame(AIC = AIC(mod)), .id = &#39;model&#39;) %&gt;% arrange(AIC) %&gt;% mutate(`Δ AIC` = AIC - min(AIC)) %&gt;% knitr::kable() model AIC Δ AIC gpa_2 269.7853 0.000000 gpa_1 272.9566 3.171217 gpa_3 416.8929 147.107536 9.9 Formula summary formula meaning (1|group) random group intercept (x|group) = (1+x|group) random slope of x within group with correlated intercept (0+x|group) = (-1+x|group) random slope of x within group: no variation in intercept (1|group) + (0+x|group) uncorrelated random intercept and random slope within group (1|site/block) = (1|site)+(1|site:block) intercept varying among sites and among blocks within sites (nested random effects) site+(1|site:block) fixed effect of sites plus random variation in intercept among blocks within sites (x|site/block) = (x|site)+(x|site:block) = (1 + x|site)+(1+x|site:block) slope and intercept varying among sites and among blocks within sites (x1|site)+(x2|block) two different effects, varying at different levels x*site+(x|site:block) fixed effect variation of slope and intercept varying among sites and random variation of slope and intercept among blocks within sites (1|group1)+(1|group2) intercept varying among crossed random effects (e.g. site, year) equation formula \\(β_0 + β_{1}X_{i} + e_{si}\\) n/a (Not a mixed-effects model) \\((β_0 + b_{S,0s}) + β_{1}X_i + e_{si}\\) ∼ X + (1∣Subject) \\((β_0 + b_{S,0s}) + (β_{1} + b_{S,1s}) X_i + e_{si}\\) ~ X + (1 + X∣Subject) \\((β_0 + b_{S,0s} + b_{I,0i}) + (β_{1} + b_{S,1s}) X_i + e_{si}\\) ∼ X + (1 + X∣Subject) + (1∣Item) As above, but \\(S_{0s}\\), \\(S_{1s}\\) independent ∼ X + (1∣Subject) + (0 + X∣ Subject) + (1∣Item) \\((β_0 + b_{S,0s} + b_{I,0i}) + β_{1}X_i + e_{si}\\) ∼ X + (1∣Subject) + (1∣Item) \\((β_0 + b_{I,0i}) + (β_{1} + b_{S,1s})X_i + e_{si}\\) ∼ X + (0 + X∣Subject) + (1∣Item) × "],
["references-and-session-info.html", "10 References and Session Info 10.1 Session Info 10.2 References", " 10 References and Session Info 10.1 Session Info sessionInfo() #&gt; R version 4.0.3 (2020-10-10) #&gt; Platform: x86_64-pc-linux-gnu (64-bit) #&gt; Running under: Ubuntu 20.04.3 LTS #&gt; #&gt; Matrix products: default #&gt; BLAS: /usr/local/lib/R/lib/libRblas.so #&gt; LAPACK: /usr/local/lib/R/lib/libRlapack.so #&gt; #&gt; locale: #&gt; [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C #&gt; [3] LC_TIME=de_DE.UTF-8 LC_COLLATE=en_US.UTF-8 #&gt; [5] LC_MONETARY=de_DE.UTF-8 LC_MESSAGES=en_US.UTF-8 #&gt; [7] LC_PAPER=de_DE.UTF-8 LC_NAME=C #&gt; [9] LC_ADDRESS=C LC_TELEPHONE=C #&gt; [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C #&gt; #&gt; attached base packages: #&gt; [1] stats graphics grDevices utils datasets methods base #&gt; #&gt; other attached packages: #&gt; [1] ggdag_0.2.3.9000 GGally_1.5.0 ggtext_0.1.1 #&gt; [4] brms_2.16.1 Rcpp_1.0.6 EnvStats_2.4.0 #&gt; [7] ggraph_2.0.5.9000 tidygraph_1.2.0 glue_1.4.2 #&gt; [10] patchwork_1.1.0.9000 prismatic_1.0.0.9000 forcats_0.5.1 #&gt; [13] stringr_1.4.0 dplyr_1.0.6 purrr_0.3.4 #&gt; [16] readr_1.4.0 tidyr_1.1.3 tibble_3.1.2 #&gt; [19] ggplot2_3.3.5 tidyverse_1.3.0.9000 #&gt; #&gt; loaded via a namespace (and not attached): #&gt; [1] readxl_1.3.1 backports_1.2.1 plyr_1.8.6 #&gt; [4] igraph_1.2.6 splines_4.0.3 crosstalk_1.1.1 #&gt; [7] inline_0.3.17 rstantools_2.1.1 digest_0.6.27 #&gt; [10] htmltools_0.5.1.1 viridis_0.5.1 rsconnect_0.8.24 #&gt; [13] fansi_0.5.0 magrittr_2.0.1 checkmate_2.0.0 #&gt; [16] graphlayouts_0.7.1 modelr_0.1.8 RcppParallel_5.0.2 #&gt; [19] matrixStats_0.56.0 xts_0.12.1 prettyunits_1.1.1 #&gt; [22] colorspace_2.0-2 rvest_1.0.0 ggrepel_0.9.1 #&gt; [25] haven_2.3.1 xfun_0.24 callr_3.6.0 #&gt; [28] crayon_1.4.1 jsonlite_1.7.2 lme4_1.1-26 #&gt; [31] zoo_1.8-8 polyclip_1.10-0 gtable_0.3.0 #&gt; [34] V8_3.4.0 distributional_0.2.2 pkgbuild_1.2.0 #&gt; [37] rstan_2.21.2 abind_1.4-5 scales_1.1.1 #&gt; [40] mvtnorm_1.1-2 DBI_1.1.1 miniUI_0.1.1.1 #&gt; [43] gridtext_0.1.4 viridisLite_0.4.0 xtable_1.8-4 #&gt; [46] StanHeaders_2.21.0-7 stats4_4.0.3 DT_0.17 #&gt; [49] htmlwidgets_1.5.3 httr_1.4.2 threejs_0.3.3 #&gt; [52] RColorBrewer_1.1-2 posterior_1.1.0 ellipsis_0.3.2 #&gt; [55] reshape_0.8.8 pkgconfig_2.0.3 loo_2.4.1 #&gt; [58] farver_2.1.0 sass_0.4.0.9000 dbplyr_2.1.1 #&gt; [61] utf8_1.2.1 tidyselect_1.1.1 rlang_0.4.12 #&gt; [64] reshape2_1.4.4 later_1.1.0.1 munsell_0.5.0 #&gt; [67] cellranger_1.1.0 tools_4.0.3 cli_2.5.0 #&gt; [70] generics_0.1.0 broom_0.7.6 ggridges_0.5.2 #&gt; [73] evaluate_0.14 fastmap_1.1.0 yaml_2.2.1.99 #&gt; [76] processx_3.5.1 knitr_1.33 fs_1.5.0 #&gt; [79] nlme_3.1-149 mime_0.11 projpred_2.0.2 #&gt; [82] xml2_1.3.2 compiler_4.0.3 bayesplot_1.8.1 #&gt; [85] shinythemes_1.2.0 rstudioapi_0.13 curl_4.3 #&gt; [88] gamm4_0.2-6 reprex_2.0.0 statmod_1.4.35 #&gt; [91] tweenr_1.0.2 bslib_0.2.4 stringi_1.7.3 #&gt; [94] ps_1.6.0 Brobdingnag_1.2-6 lattice_0.20-41 #&gt; [97] Matrix_1.2-18 nloptr_1.2.2.2 markdown_1.1 #&gt; [100] shinyjs_2.0.0 tensorA_0.36.1 vctrs_0.3.8 #&gt; [103] pillar_1.6.1 lifecycle_1.0.0 jquerylib_0.1.3 #&gt; [106] bridgesampling_1.1-2 httpuv_1.5.5 R6_2.5.0 #&gt; [109] bookdown_0.19 promises_1.2.0.1 gridExtra_2.3 #&gt; [112] codetools_0.2-16 boot_1.3-25 colourpicker_1.1.1 #&gt; [115] MASS_7.3-53 gtools_3.8.2 assertthat_0.2.1 #&gt; [118] withr_2.4.2 shinystan_2.5.0 mgcv_1.8-33 #&gt; [121] parallel_4.0.3 hms_1.1.0 grid_4.0.3 #&gt; [124] coda_0.19-4 minqa_1.2.4 rmarkdown_2.9.6 #&gt; [127] ggforce_0.3.2.9000 shiny_1.6.0 lubridate_1.7.10 #&gt; [130] base64enc_0.1-3 dygraphs_1.1.1.6 10.2 References "]
]
