---
output:
  html_document:
  theme: yeti
pdf_document: default
editor_options: 
  chunk_output_type: console
---

# Rethinking: Chapter 4

**Geocentric Models**

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      comment = "#>", 
                      dev = "svg",         # for html rendering
                      # dev = "cairo_pdf", # for pdf rendering
                      fig.asp = .5, 
                      fig.align = "center",
                      message = FALSE,
                      warning = FALSE)

source("bayesian_settings.R")
source("knitr_matrix.R")
```

by [Richard McElreath](https://xcelab.net/rm/statistical-rethinking/), building on the Summary by [Solomon Kurz](https://bookdown.org/content/4857/)

## Why normal distributions are normal

### Normal by addition

```{r, fig.asp = .7}
n_people <- 1e3
position <- crossing(person = 1:n_people,
                     step   = 0:16) %>% 
  mutate(deviation = map_dbl(step, ~if_else(. == 0, 0, runif(1, -1, 1)))) %>% 
  group_by(person) %>%
  mutate(position = cumsum(deviation)) %>% 
  ungroup()

p_all_steps <- position %>% 
  ggplot(aes(x = step, y = position, group = person)) +
  geom_line(aes(color = person == n_people)) +
  geom_point(data = position %>%  filter(person == n_people), aes(color = "TRUE"), size = 1) +
  geom_vline(data = tibble(step = c(4, 8, 16)),
             aes(xintercept = step), linetype = 3, color = rgb(0,0,0,.5)) +
  scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr_alpha(clr0d, .05)), guide = "none") +
  scale_x_continuous(breaks = c(0,4,8,16)) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())

plot_steps <- function(step_nr, data = position, add_ideal = FALSE){
  data_step <- data %>% 
    filter(step == step_nr) 
  
  p <- data_step %>% 
    ggplot(aes(x = position)) +
    geom_density(adjust = .2, color = clr0d, fill = fll0) +
    scale_x_continuous(limits = c(-6, 6)) +
    labs(title = glue("{step_nr} steps"))
  
  if(add_ideal){p <- p  +
    stat_function(fun = function(x){dnorm(x, mean = 0, sd = sd(data_step$position))},
                  n = 501, color = clr2, linetype = 3)}
  
  p
}
p_all_steps /
(plot_steps(step_nr = 4) + plot_steps(step_nr = 8) + plot_steps(step_nr = 16, add_ideal = TRUE))
```

## Normal by multiplication and by log-multiplication

```{r,fig.asp = .3}
normal_by_multiplication <- function(effect_size = 0.1,
                                     x_scale = ggplot2::scale_x_continuous(),
                                     x_lab = "normal"){
  tibble(person = 1:n_people,
       growth = replicate(length(person), prod(1 + runif(12, 0, effect_size)))) %>% 
  ggplot(aes(x = growth)) +
  geom_density(color = clr0d, fill = fll0)  +
  labs(title = glue("effect size: {effect_size}"), x = glue("growth ({x_lab})")) +
  x_scale
}

normal_by_multiplication(effect_size = .01) +
  normal_by_multiplication(effect_size = .1) +
  normal_by_multiplication(effect_size = .5)+
  normal_by_multiplication(effect_size = .5, x_scale = scale_x_log10(), x_lab = "log10") +
  plot_layout(nrow = 1)
```

### using the Gaussian distribution

- part of the *exponential family*
- probability density function
- $\mu$: mean
- $\sigma$: standard deviation
- $\tau$: precision

$$
p( y | \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} exp \left( \frac{(y-\mu)^2}{2\sigma^2} \right)\\
\tau = 1 / \sigma^2 \\
p( y | \mu, \tau) = \sqrt{\frac{\tau}{2\pi}}exp(-\tfrac{1}{2}\tau(y - \mu)^2)
$$

## A language for describing models

*The first line defines the likelihood used in Bayes' theorem, the other lines describe the priors used. The tilde means that the relationships are **stochastic*.**

re-describing the globe-toss model:

> *The count $W$ is distributed binomially with a sample size $N$ and the probabiliy $p$.*
> *The prior for $p$ is assumed to be uniform between zero and one*

$$
W \sim Binomial(N, p)\\
p \sim Uniform(0, 1)
$$

Substituting in Bayes' theorem:

$$
Pr(p | w, n) = \frac{Binomial(w|n,p)~Uniform(p|0,1)}{\int Binomial(w|n,p)~Uniform(p|0,1) dp}
$$

```{r, fig.asp = .45}
w <-  6
n <-  9
grid_data <- tibble(p_grid = seq(0,1, length.out = 101),
       likelihood = dbinom(w, n, p_grid),
       prior = dunif(p_grid, 0, 1),
       posterior_unstand = likelihood * prior,
       posterior = posterior_unstand / sum(posterior_unstand))

grid_data %>%
  pivot_longer(cols = c(prior, likelihood, posterior),
               names_to = "bayes_part",
               values_to = "p") %>% 
  mutate(bayes_part = factor(bayes_part, levels = names(clr_bayes))) %>% 
  ggplot(aes(x = p_grid)) +
  geom_area(aes(y = p, color = bayes_part, fill = after_scale(clr_alpha(color)))) +
  scale_color_manual(values = clr_bayes, guide = "none") +
  facet_wrap(bayes_part ~ ., scales = "free_y")

```

### Gaussian model of height

#### The data

```{r}
library(rethinking)
data(Howell1)

(data <- as_tibble(Howell1)) %>% 
  precis() %>% as_tibble(rownames = NA) %>% knitr::kable()
```

```{r}
(data_adults <- data %>% filter(age >= 18)) %>% 
  precis() %>% as_tibble(rownames = NA) %>% knitr::kable()
```

```{r, fig.asp = .4}
data_adults %>% 
  ggplot(aes(x = height)) +
  geom_density(adjust = .5, color = clr0d, fill = fll0) +
  scale_x_continuous(limits = c(130,185))
```

#### The model

$$
\begin{array}{cccr} 
h_i & \stackrel{iid}{\sim} & Normal(\mu, \sigma) & \textrm{[likelihood]}\\
\mu & \sim & Normal(178, 20) & \textrm{[$\mu$ prior]}\\
\sigma & \sim & Uniform(0,50) & \textrm{[$\sigma$ prior]}
\end{array}
$$

where, $iid$ means *"independent and identically distributed"*.

**Prior predictive simulation**

(*'what does the model think before seeing the data?'*)

```{r, fig.asp = .7}
n_samples <- 1e4
prior_simulation <- tibble(
  sample_mu = rnorm(n_samples, 178, 20),
  sample_sigma = runif(n_samples, 0, 50),
  prior_h = rnorm(n_samples, sample_mu, sample_sigma),
  bad_mu = rnorm(n_samples, 178, 100),
  bad_prior = rnorm(n_samples, bad_mu, sample_sigma)
)

p_mu <- ggplot() +
    stat_function(fun = function(x){dnorm(x = x, mean = 178, sd = 20)},
                  xlim = c(100,250), color = clr0d, fill = fll0, geom = "area") +
    labs(title = glue("*\U03BC* {mth('\U007E')} dnorm( 178, 20 )"), 
         y = "density", x = "*\U03BC*")

p_sigma <- ggplot() +
    stat_function(fun = function(x){dunif(x = x, min = 0, max = 50)},
                  xlim = c(-5, 55),
                  color = clr1, fill = fll1, geom = "area") +
    labs(title = glue("*{mth('\U03C3')}* {mth('\U007E')} dunif( 0, 50 )"), 
         y = "density", x = glue("*{mth('\U03C3')}*"))

p_prior_sim <- prior_simulation %>% 
  ggplot(aes(x = prior_h)) +
  geom_density(color = clr2, fill = fll2, adjust = .4) +
  scale_x_continuous(limits = c(0,356), breaks = c(0,73,178,283)) +
  labs(title =  glue("*h<sub>i</sub>* {mth('\U007E')} dnorm( *\U03BC*, {mth('\U03C3')} )"),
       x = "height")

p_bad_prior <- prior_simulation %>% 
  ggplot(aes(x = bad_prior)) +
  geom_density(color = clr2, fill = fll2, adjust = .4) +
  scale_x_continuous(limits = c(-222,578),
                     breaks = c(-128,0,178,484), expand = c(0,0)) +
  geom_vline(data = tibble(h = c(0,272)), aes(xintercept = h), linetype = 3)+
  labs(title =  glue("*h<sub>i</sub>* {mth('\U007E')} dnorm( *\U03BC*, {mth('\U03C3')} )<br>*\U03BC* {mth('\U007E')} dnorm( 178, 100 )"),
       x = "height")

p_mu + p_sigma +
  p_prior_sim + p_bad_prior &
  theme(plot.title = element_markdown(),
        axis.title.x = element_markdown())

```

#### grid approximation of the posterior distribution

```{r}
n_grid <- 101

grid_data <- cross_df(list(mu = seq(from = 152, to = 157, length.out = n_grid),
                           sigma = seq(from = 6.5, to = 9, length.out = n_grid))) %>%
  mutate(log_likelihood = map2_dbl(.x = mu, .y = sigma, .f = function(x, y){
    dnorm(x = data_adults$height, mean = x, sd = y, log = TRUE) %>% sum()
  }),
  prior_mu = dnorm(mu, mean = 178, sd = 20, log = TRUE),
  prior_sigma = dunif(sigma, min = 0, max = 50, log = TRUE),
  product = log_likelihood + prior_mu + prior_sigma,
  probability = exp(product - max(product)))

grid_data %>% 
  ggplot(aes(x = mu, y = sigma, z = probability)) +
  geom_raster(aes(fill = probability)) +
  geom_contour(color = rgb(1,1,1,.1)) +
  coord_cartesian(xlim = range(grid_data$mu),
              ylim = range(grid_data$sigma)) +
  scale_fill_gradientn(colours = clr_grd5 %>% clr_alpha(alpha = .8),
                       limits = c(0,1)) +
  coord_cartesian(xlim = range(grid_data$mu),
                  ylim = range(grid_data$sigma),
                  expand = 0) +
  guides(fill = guide_colorbar(title.position = "top",
                               barwidth = unit(.9,"npc"),
                               barheight = unit(5, "pt"))) +
  labs(x = " *\U03BC*", y = glue("*{mth('\U03C3')}*"))+
  theme(legend.position = "bottom",
        axis.title.x = element_markdown(),
        axis.title.y = element_markdown())
```

**Sampling from the posterior distribution**

```{r, fig.asp = .7}
n_posterior_sample <- 1e4
samples <- grid_data %>% 
  slice_sample(n = n_posterior_sample, weight_by = probability, replace = TRUE)

p_samples <- samples %>% 
  group_by(mu, sigma) %>% 
  count() %>% 
  ungroup() %>% 
  ggplot(aes(x = mu, y = sigma, color = n)) +
  geom_point(size = .4) +
  scale_color_gradientn(colours = clr_grd5 %>% clr_alpha(alpha = .8)) +
  coord_cartesian(xlim = buffer_range(grid_data$mu),
                  ylim = buffer_range(grid_data$sigma),
                  expand = 0) +
  guides(color = guide_colorbar(title.position = "top",
                               barwidth = unit(.2,"npc"),
                               barheight = unit(5, "pt"))) +
  labs(x = " *\U03BC*", y = glue("*{mth('\U03C3')}*"))+
  theme(legend.position = "bottom",
        axis.title.x = element_markdown(),
        axis.title.y = element_markdown())
  
p_mu_dens <- samples %>% 
  ggplot(aes(x = mu)) +
  geom_density(color = clr0d, fill = fll0) +
  scale_x_continuous(limits = buffer_range(grid_data$mu), expand = c(0, 0)) +
  labs(y = "marginal<br>density") +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.title.y = element_markdown())

p_sigma_dens <- samples %>% 
  ggplot(aes(x = sigma)) +
  geom_density(color = clr0d, fill = fll0) +
  scale_x_continuous(limits = buffer_range(grid_data$sigma), expand = c(0, 0)) +
  labs(y = "marginal density") +
  coord_flip() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank())

p_mu_dens  + patchwork::guide_area() +
  p_samples + p_sigma_dens +
  plot_layout(guides = "collect", widths = c(1,.3), heights = c(.3,1))
```

Exploration of long tail for $\sigma$ when original sample size is small:

```{r, fig.asp = .7}
heights_subset <- sample(data_adults$height, size = 20)
grid_data_subset <- cross_df(list(mu = seq(from = 145, to = 165, length.out = n_grid),
                           sigma = seq(from = 4.5, to = 16, length.out = n_grid))) %>%
  mutate(log_likelihood = map2_dbl(.x = mu, .y = sigma, .f = function(x, y){
    dnorm(x = heights_subset, mean = x, sd = y, log = TRUE) %>% sum()
  }),
  prior_mu = dnorm(mu, mean = 178, sd = 20, log = TRUE),
  prior_sigma = dunif(sigma, min = 0, max = 50, log = TRUE),
  product = log_likelihood + prior_mu + prior_sigma,
  probability = exp(product - max(product)))

samples_subset <- grid_data_subset %>% 
  slice_sample(n = n_posterior_sample, weight_by = probability, replace = TRUE)

p_samples <- samples_subset %>% 
  group_by(mu, sigma) %>% 
  count() %>% 
  ungroup() %>% 
  ggplot(aes(x = mu, y = sigma, color = n)) +
  geom_point(size = .4) +
  scale_color_gradientn(colours = clr_grd4 %>% clr_alpha(alpha = .8)) +
  coord_cartesian(xlim = buffer_range(grid_data_subset$mu),
                  ylim = buffer_range(grid_data_subset$sigma),
                  expand = 0) +
  guides(color = guide_colorbar(title.position = "top",
                               barwidth = unit(.2,"npc"),
                               barheight = unit(5, "pt"))) +
  labs(x = " *\U03BC*", y = glue("*{mth('\U03C3')}*"))+
  theme(legend.position = "bottom",
        axis.title.x = element_markdown(),
        axis.title.y = element_markdown())
  
p_mu_dens <- samples_subset %>% 
  ggplot(aes(x = mu)) +
  geom_density(color = clr0d, fill = fll0) +
  scale_x_continuous(limits = buffer_range(grid_data_subset$mu), expand = c(0, 0)) +
  labs(y = "marginal<br>density") +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.title.y = element_markdown())

p_sigma_dens <- samples_subset %>% 
  ggplot(aes(x = sigma)) +
  geom_density(color = clr0d, fill = fll0) +
  scale_x_continuous(limits = buffer_range(grid_data_subset$sigma), expand = c(0, 0)) +
    labs(y = "marginal density") +
  coord_flip() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank())

p_mu_dens  + patchwork::guide_area() +
  p_samples + p_sigma_dens +
  plot_layout(guides = "collect", widths = c(1,.3), heights = c(.3,1))
```

#### Quadratic approximation of the posterior distribution

$$
\begin{array}{cccr} 
h_i & \stackrel{iid}{\sim} & Normal(\mu, \sigma) & \verb|height ~ dnorm(mu, sigma)|\\
\mu & \sim & Normal(178, 20) & \verb|mu ~ dnorm(178, 20)|\\
\sigma & \sim & Uniform(0,50) & \verb|sigma ~ dunif(0, 50)|
\end{array}
$$

```{r}
model_spec <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)

# "maximum a priori estimate"
map_starting_points <-  list(
  mu = mean(data_adults$height),
  sigma = sd(data_adults$height)
)

model_heights_quap_weak_prior <- quap(flist = model_spec,
                                      data =  data_adults,
                                      start = map_starting_points)

precis(model_heights_quap_weak_prior) %>%
  as_tibble(rownames = NA) %>%
  round(digits = 2) %>% 
  knitr::kable()
```

Comparing how a stronger prior for $\mu$ (narrower distribution) forces a larger estimate of $\sigma$ to compensate for this.

```{r}
quap(
  flist = alist(
    height ~ dnorm( mu , sigma ),
    mu ~ dnorm( 178, 0.1 ),
    sigma ~ dunif( 0, 50 )
  ),
  data =  data_adults,
  start = map_starting_points) %>%
  precis() %>% 
  as_tibble(rownames = NA) %>%
  round(digits = 2) %>% 
  knitr::kable()
```

The *variance-covariance* matrix of the quadratic aprroximation for sampling the multi-dimensional gaussian distribution:

```{r}
vcov_mod_heights <- vcov(model_heights_quap_weak_prior)
vcov_mod_heights %>%
  round(digits = 6) %>% 
  knitr::kable()

diag(vcov_mod_heights)
round(cov2cor(vcov_mod_heights), digits = 5)
```

sampling from the multi-dimensional posterior distribution

```{r}
posterior_sample <- extract.samples(model_heights_quap_weak_prior, n = 1e4) %>% 
  as_tibble()

precis(posterior_sample) %>%
  as_tibble() %>%
  knitr::kable()
```

## Linear Prediction

```{r}
ggplot(data_adults,
       aes(height, weight)) +
  geom_point(shape = 21, size = 1.5, color = clr1, fill = fll1)
```

$$
\begin{array}{cccr} 
h_i & {\sim} & Normal(\mu, \sigma) & \textrm{[likelihood]}\\
\mu_i & = & \alpha + \beta (x_i - \bar{x}) & \textrm{[linear model]}\\ % alternatively \overline{x}
\alpha & \sim & Normal(178, 20) & \textrm{[$\alpha$ prior]}\\
\beta & \sim & Normal(0, 10) & \textrm{[$\beta$ prior]}\\
\sigma & \sim & Uniform(0,50) & \textrm{[$\sigma$ prior]}
\end{array}
$$

The current prior for $\beta$ is a bad choice, because it allows *negative* as well as unreasonably high and low dependencies of $h$ (height) on $x$ (weight):

```{r, fig.asp = .4}
set.seed(2971)
N <- 100
linear_priors <- tibble(n = 1:N,
                        alpha = rnorm( n = N, mean =  178, sd = 20 ),
                        beta_1 = rnorm( n = N, mean = 0, sd = 10),
                        beta_2 = rlnorm( n = N, mean = 0, sd = 1)) %>% 
  expand(nesting(n, alpha, beta_1, beta_2), weight = range(data_adults$weight)) %>% 
  mutate(height_1 = alpha + beta_1 * (weight - mean(data_adults$weight)),
         height_2 = alpha + beta_2 * (weight - mean(data_adults$weight)))

p_lin_pr <- ggplot(linear_priors, aes(x = weight, y = height_1, group = n)) +
  labs(title = glue("{mth('*\U03B2* ~')} Normal(0, 10)"), y = "height") 

p_log_pr <- ggplot(linear_priors, aes(x = weight, y = height_2, group = n)) +
  labs(title = glue("{mth('*\U03B2* ~')} Log-Normal(0, 1)"), y = "height") 

p_lnorm <- ggplot() +
  stat_function(fun = function(x){dlnorm(x = x, meanlog = 0, sdlog = 1)},
                xlim = c(0,5), geom = "area", color = clr2, fill = fll2, n = 501) +
  labs(title = "Log-Norm(0, 0.1)", y = "density")

(p_lin_pr + p_log_pr &
    geom_hline(data = tibble(height = c(0, 272), type = 1:2),
               aes(yintercept = height, linetype = factor(type)), size = .4) &
    geom_line(color = clr2, alpha = .25) &
    scale_linetype_manual(values = c(`1` = 3, `2` = 1), guide = "none") &
    coord_cartesian(xlim = range(data_adults$weight), ylim = c(-100, 400)) &
    theme(plot.title = element_markdown())) +
  p_lnorm
```

The log-normal prior seems more sensible, so we update the model priors as such:

$$
\begin{array}{cccr} 
\beta & \sim & Log-Normal(0, 1) & \textrm{[$\beta$ prior]}\\
\end{array}
$$

### Finding the posterior Distribution

```{r}
xbar <- mean(data_adults$weight)
model_hight <- quap(
  flist = alist(
    height ~ dnorm( mu, sigma ),
    mu <- alpha + beta * ( weight - xbar ),
    alpha ~ dnorm( 178, 20 ),
    beta ~ dlnorm( 0, 1 ),
    sigma ~ dunif( 0, 50)
  ),
  data = data_adults
)
```

Table of marginal distributions of the parameters after training the model on the data

```{r}
centered_remember_hw <- precis(model_hight) %>% 
  round(digits = 3) %>% 
  as_tibble(rownames = NA)

centered_remember_hw %>% 
  knitr::kable()
```

We also need thevariance-covariance matrix to fully describe the audratic approximation completely:

```{r, fig.asp = .7}
model_hight %>% 
  vcov() %>% 
  round(digits = 2) %>% 
  as.data.frame(row.names = row.names(.)) %>% 
  knitr::kable()

model_hight_smp <- extract.samples(model_hight)  %>% 
  as_tibble() 

model_hight_smp_mean <- model_hight_smp %>% 
  summarise(across(.cols = everything(), mean))

model_hight_smp %>%
  ggpairs(
        lower = list(continuous = wrap(ggally_points, colour = clr1, size = .2, alpha = .1)),
        diag = list(continuous = wrap("densityDiag", fill = fll1, color = clr1, adjust = .5)),
        upper = list(continuous = wrap(ggally_cor, size = 5, color = "black", family = "Josefin sans")))

```

Plotting the posterior distribution against the data

```{r}
ggplot(data_adults, aes(x  = weight, y = height)) +
  geom_point(color = clr0d) +
  stat_function(fun = function(x){model_hight_smp_mean$alpha + model_hight_smp_mean$beta * (x - xbar)},
                color = clr2, n = 2)

```

A demonstration of the the effect of sample size on the uncertainty of the linear fit

```{r}
sub_model <- function(N = 10){
  data_inner <- data_adults[1:N,]
  xbar <- mean(data_inner$weight)
  
  model_hight_inner <- quap(
    flist = alist(
      height ~ dnorm( mu, sigma ),
      mu <- alpha + beta * ( weight - xbar ),
      alpha ~ dnorm( 178, 20 ),
      beta ~ dlnorm( 0, 1 ),
      sigma ~ dunif( 0, 50)
    ),
    data = data_inner
  )
  
  model_hight_smp_inner <- extract.samples(model_hight_inner)  %>% 
    as_tibble()  %>% 
    sample_n(20)
  
  ggplot(data_inner, aes(x = weight, y = height)) +
    geom_point(color = clr0d) +
    (purrr::map(1:20, function(i){stat_function(
      fun = function(x){model_hight_smp_inner$alpha[i] + model_hight_smp_inner$beta[i] * (x - xbar)},
      color = clr2, n = 2, alpha = .1)})) +
    labs(title = glue("N: {N}"))
}

sub_model(10) + sub_model(50) +
  sub_model(150)  + sub_model(352) 
```

adding intervals

```{r}
mu_at_50 <- model_hight_smp %>% 
  mutate(mu_at_50 = alpha + beta * (50 - xbar))

p_density <- mu_at_50 %>% 
  ggplot(aes(x = mu_at_50)) +
  geom_density(adjust = .5, color = clr0d, fill = fll0) +
  stat_function(fun = function(x){demp(x, obs = mu_at_50$mu_at_50, density.arg.list = list(adjust = .5))},
                xlim = mu_at_50$mu_at_50 %>% PI(), geom = "area", fill = fll2, color = clr2) +
  geom_vline(data = tibble(weights = mu_at_50$mu_at_50 %>% PI()), aes(xintercept = weights), linetype  = 3)+
  scale_x_continuous(glue("{mth('*\U03BC*')} | weight = 50"), limits = c(157.7, 160.8)) +
  theme(axis.title.x = element_markdown())

mu_at_50$mu_at_50 %>% PI()
```

```{r, fig.asp = .35}
weight_seq <- seq(from = 25, to = 70, by = 1)
model_hight_mu <- link(model_hight, data = data.frame(weight = weight_seq)) %>% 
  as_tibble() %>% 
  set_names(nm = weight_seq) %>% 
  pivot_longer(cols = everything(), names_to = "weight", values_to = "height") %>% 
  mutate(weight = as.numeric(weight)) 

p_dots <- model_hight_mu %>% 
  ggplot(aes(x = weight, y = height)) +
  geom_point(aes(color = weight == 50), alpha = .1, size = .3) +
  scale_color_manual(values = c(`TRUE` = clr2, `FALSE` = clr0d), guide = "none")

model_hight_mu_interval <- model_hight_mu %>% 
  group_by(weight) %>% 
  summarise(mean = mean(height),
            PI_lower = PI(height)[1],
            PI_upper = PI(height)[2]) %>% 
  ungroup()

p_interval <- model_hight_mu_interval %>% 
  ggplot(aes(x = weight)) +
  geom_point(data = data_adults, aes(y = height), color = clr0, size = .3) +
  geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr2, alpha = .35) +
  geom_line(aes(y = mean))

p_density + p_dots + p_interval
```

Prediction intervals

```{r}
model_hight_sd <- sim(model_hight, data = data.frame(weight = weight_seq), n = 1e4) %>% 
  as_tibble() %>% 
  set_names(nm = weight_seq) %>% 
  pivot_longer(cols = `25`:`70`, names_to = "weight", values_to = "height") %>% 
  mutate(weight = as.numeric(weight)) 

model_hight_sd %>% 
  group_by(weight) %>% 
  summarise(mean = mean(height),
            PI_lower = PI(height)[1],
            PI_upper = PI(height)[2]) %>% 
  ungroup() %>% 
  ggplot(aes(x = weight)) +
  geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35)  +
  geom_point(data = data_adults, aes(y = height), color = clr0, size = .3) +
  geom_ribbon(data = model_hight_mu_interval,
              aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) +
  geom_line(data = model_hight_mu_interval,
              aes(y = mean))

```

## Curves from lines

The full data (including kids) is clearly curved in shape:

```{r}
ggplot(data = data, aes(x = weight, y = height)) +
  geom_point(color = clr0d)
```


We will work on standardized $x$ values to prevent *"numerical glitches"* by transforming $x$ via $x_s = (\frac{x - \bar{x}}{sd(x)})$:

:::columns
:::column1st
quadratic polynomial fit
$$
\begin{array}{cccr} 
h_i & {\sim} & Normal(\mu, \sigma) & \textrm{[likelihood]}\\
\mu_i & = & \alpha + \beta_1 x_i +  \beta_2 x_i ^ 2& \textrm{[linear model]}\\ % alternatively \overline{x}
\alpha & \sim & Normal(178, 20) & \textrm{[$\alpha$ prior]}\\
\beta_1 & \sim & Log-Normal(0, 1) & \textrm{[$\beta_1$ prior]}\\
\beta_2 & \sim & Normal(0, 1) & \textrm{[$\beta_2$ prior]}\\
\sigma & \sim & Uniform(0,50) & \textrm{[$\sigma$ prior]}
\end{array}
$$
:::   
:::column2nd
cubic polynomial fit
$$
\begin{array}{cccr} 
h_i & {\sim} & Normal(\mu, \sigma) & \textrm{[likelihood]}\\
\mu_i & = & \alpha + \beta_1 x_i + \beta_2 x_i ^ 2 + \beta_3 x_i ^ 3 & \textrm{[linear model]}\\ % alternatively \overline{x}
\alpha & \sim & Normal(178, 20) & \textrm{[$\alpha$ prior]}\\
\beta_1 & \sim & Log-Normal(0, 1) & \textrm{[$\beta_1$ prior]}\\
\beta_2 & \sim & Normal(0, 1) & \textrm{[$\beta_2$ prior]}\\
\beta_3 & \sim & Normal(0, 1) & \textrm{[$\beta_3$ prior]}\\
\sigma & \sim & Uniform(0,50) & \textrm{[$\sigma$ prior]}
\end{array}
$$
:::
:::

```{r}
plot_model_intervals <- function(mod, data,
                                 weight_seq = list(weight_s = seq(from = min(data_model$weight_s),
                                                                  to = max(data_model$weight_s),
                                                                  length.out = 70))){
  model_hight_mu_interval <- link(mod, data = weight_seq) %>% 
    as_tibble() %>% 
    set_names(nm = weight_seq$weight_s ) %>% 
    pivot_longer(cols = everything(), names_to = "weight_s", values_to = "height") %>% 
    mutate(weight_s = as.numeric(weight_s)) %>% 
    group_by(weight_s) %>% 
    summarise(mean = mean(height),
              PI_lower = PI(height)[1],
              PI_upper = PI(height)[2]) %>% 
    ungroup()
  
  model_hight_sd <- sim(mod, data = weight_seq, n = 1e4) %>% 
    as_tibble() %>% 
    set_names(nm = weight_seq$weight_s) %>% 
    pivot_longer(cols = everything(), names_to = "weight_s", values_to = "height") %>% 
    mutate(weight_s = as.numeric(weight_s)) 
  
  model_hight_sd %>% 
    group_by(weight_s) %>% 
    summarise(mean = mean(height),
              PI_lower = PI(height)[1],
              PI_upper = PI(height)[2]) %>% 
    ungroup() %>% 
    ggplot(aes(x = weight_s)) +
    geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35)  +
    geom_point(data = data, aes(y = height), color = rgb(0,0,0,.3), size = .4) +
    geom_ribbon(data = model_hight_mu_interval,
                aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) +
    geom_line(data = model_hight_mu_interval,
              aes(y = mean))
}

data_model <-  data %>% 
  mutate(weight_s = (weight - mean(weight))/sd(weight),
         weight_s2 = weight_s ^ 2,
         weight_s3 = weight_s ^ 3)

model_hight_s1 <- quap(
  flist = alist(
    height ~ dnorm( mu, sigma ),
    mu <- alpha + beta *  weight_s ,
    alpha ~ dnorm( 178, 20 ),
    beta ~ dlnorm( 0, 1 ),
    sigma ~ dunif( 0, 50)
  ),
  data = data_model
)

model_hight_s2 <- quap(
  flist = alist(
    height ~ dnorm( mu, sigma ),
    mu <- alpha + beta1 *  weight_s + beta2 *  weight_s2,
    alpha ~ dnorm( 178, 20 ),
    beta1 ~ dlnorm( 0, 1 ),
    beta2 ~ dnorm( 0, 1 ),
    sigma ~ dunif( 0, 50)
  ),
  data = data_model
)

model_hight_s3 <- quap(
  flist = alist(
    height ~ dnorm( mu, sigma ),
    mu <- alpha + beta1 *  weight_s + beta2 *  weight_s2 + beta3 *  weight_s3,
    alpha ~ dnorm( 178, 20 ),
    beta1 ~ dlnorm( 0, 1 ),
    beta2 ~ dnorm( 0, 1 ),
    beta3 ~ dnorm( 0, 1 ),
    sigma ~ dunif( 0, 50)
  ),
  data = data_model
)
```


```{r, fig.asp = .35}
plot_model_intervals(model_hight_s1, data_model) +
plot_model_intervals(model_hight_s2, data_model,
                     weight_seq = tibble(weight_s = seq(from = min(data_model$weight_s),
                                                      to = max(data_model$weight_s),
                                                      length.out = 70),
                                       weight_s2 = weight_s ^ 2)) +
plot_model_intervals(model_hight_s3, data_model,
                     weight_seq = tibble(weight_s = seq(from = min(data_model$weight_s),
                                                      to = max(data_model$weight_s),
                                                      length.out = 70),
                                       weight_s2 = weight_s ^ 2,
                                       weight_s3 = weight_s ^ 3))
```


```{r}
plot_model_intervals(model_hight_s3, data_model,
                     weight_seq = tibble(weight_s = seq(from = min(data_model$weight_s),
                                                      to = max(data_model$weight_s),
                                                      length.out = 70),
                                       weight_s2 = weight_s ^ 2,
                                       weight_s3 = weight_s ^ 3)) +
  scale_x_continuous("weight [kg]", 
                     breaks = (seq(5,65, length.out = 5) - mean(data_model$weight)) /  sd(data_model$weight),
    labels = seq(5,65, length.out = 5)) +
  labs(y = "height [cm]")
```

## Splines

Loading the *Hanami* data (<span style='fontfamily:UnYetgul'>花見</span>), containing the historical dates of first annual cherry tree blossom.

```{r}
data(cherry_blossoms)
precis(cherry_blossoms) %>% as_tibble() %>% knitr::kable()
```

```{r, fig.asp = .3}
cherry_blossoms %>% 
  ggplot(aes(x = year, y = doy)) +
  geom_point(color = clr2, alpha = .3) +
  labs(y = "Day of first blossom")
```

```{r}
data_cherry <- cherry_blossoms %>% 
  filter(complete.cases(doy)) %>%
  as_tibble()

n_knots <- 15
knot_list <- quantile(data_cherry$year, probs = seq(0, 1, length.out = n_knots))
```


```{r, fig.asp = .3}
library(splines)
b_spline_cherry <- bs(data_cherry$year,
                      knots = knot_list[-c(1, n_knots)],
                      degree = 3,
                      intercept = TRUE)

b_spline_tib <- b_spline_cherry %>% 
  as_tibble() %>%
  set_names(nm = str_pad(1:17, width = 2, pad = 0)) %>% 
  bind_cols(select(data_cherry, year)) %>% 
  pivot_longer(cols = -year, names_to = "bias_function", values_to = "bias")

ggplot() +
  geom_vline(data = tibble(year = knot_list),
             aes(xintercept = year),
             linetype = 3, color = "black") +
  geom_line(data = b_spline_tib, aes(x = year, y = bias,
                                     color = as.numeric(bias_function)
                                     , group = bias_function), 
            size = 1, alpha = .75) +
  scale_color_gradientn(colours = c("black", clr0d, clr2), guide = "none")+
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank())
```

B-spline model:

$$
\begin{array}{cccr} 
D_i & \sim & Normal( \mu_i, \sigma) & \textrm{[likelihood]}\\
\mu_i & = & \alpha + \sum_{k=1}^K w_k B_{k,i} & \textrm{[linear model]}\\
\alpha & \sim & Normal(100, 10) & \textrm{[$\alpha$ prior]}\\
w_i & \sim & Normal(0, 10) & \textrm{[w prior]}\\
\sigma & \sim & Exponential(1) & \textrm{[$\sigma$ prior]}

\end{array}
$$

```{r}
model_cherry <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + B %*% w,
    a ~ dnorm(100, 10),
    w ~ dnorm(0, 10),
    sigma ~ dexp(1)
  ),
  data = list(D = data_cherry$doy, B = b_spline_cherry),
  start = list(w = rep(0, ncol(b_spline_cherry)))
)

precis(model_cherry, depth = 2) %>% round(digits = 2) %>% as_tibble(rownames = NA) %>%  knitr::kable()
```

```{r}
cherry_samples <- extract.samples(model_cherry) %>% 
  as.data.frame() %>% 
  as_tibble() %>% 
  set_names(nm = c("a", "sigma", str_pad(1:17, 2,pad = 0)))

cherry_samples_mu <- cherry_samples %>% 
  summarise(across(everything(), mean)) %>% 
  pivot_longer(cols = everything(),
               names_to = "bias_function", values_to ="weight")
```


```{r, fig.asp = .3}
ggplot() +
  geom_vline(data = tibble(year = knot_list),
             aes(xintercept = year),
             linetype = 3, color = "black") +
  geom_line(data = b_spline_tib %>% left_join(cherry_samples_mu),
            aes(x = year, y = bias * weight,
                color = as.numeric(bias_function)
                , group = bias_function), 
            size = 1, alpha = .75) +
  scale_color_gradientn(colours = c("black", clr0d, clr2), guide = "none") +
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank())
```


```{r, fig.asp = .3}
model_cherry_samples <- link(model_cherry) %>% 
  as_tibble()%>% 
  set_names(nm = data_cherry$year) %>% 
  pivot_longer(cols = everything(), names_to = "year", values_to = "doy") %>% 
  mutate(year = as.numeric(year))  %>% 
  arrange(year)

model_cherry_stats <- model_cherry_samples %>% 
  group_by(year) %>% 
  nest() %>% 
  mutate(mean = map_dbl(data, function(data){mean(data$doy)}),
         PI_lower = map_dbl(data, function(data){PI(data$doy)[1]}),
         PI_upper = map_dbl(data, function(data){PI(data$doy)[2]}))

model_cherry_stats %>% 
  ggplot(aes(x = year)) +
  geom_vline(data = tibble(year = knot_list),
             aes(xintercept = year),
             linetype = 3, color = "black") +
    geom_point(data = cherry_blossoms, aes(y = doy), color = clr2, alpha = .3) +
  geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr2, alpha = .35) +
  geom_line(aes(y = mean)) +
  labs(y = "Day of first blossom") +
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank())
```

---

## Homework

**E1**

$$
\begin{array}{cccr} 
y_i & \sim & Normal( \mu, \sigma) & \textrm{[likelihood]}\\
\mu & \sim & Normal(0, 10) & \textrm{[$\mu$ prior]}\\
\sigma & \sim & Exponential(1) & \textrm{[$\sigma$ prior]}
\end{array}
$$

**E2**

There are two parameters:

- $\mu$
- $\sigma$

**E3**

$$
\begin{array}{rcl} 
Pr( \mu, \sigma | y ) & = & \frac{Normal( y | \mu, \sigma ) Pr(y)}{Pr(\mu, \sigma)} \\
Pr( \mu, \sigma | y ) & = & \frac{\prod_i Pr( y_i | \mu, \sigma) Normal( \mu | 0, 10) Exponential(\sigma | 1) }{ \int\int\prod_i Pr( y_i | \mu, \sigma) Normal( \mu | 0, 10) Exponential(\sigma | 1)d\mu d\sigma}
\end{array}
$$

**E4**

$$
\begin{array}{cccr} 
y_i & \sim & Normal( \mu, \sigma) & \textrm{[likelihood]}\\
\mu_i & = & \alpha + \beta x_i & \textrm{[linear model]}\\
\alpha & \sim & Normal(0, 10) & \textrm{[$\alpha$ prior]}\\
\beta & \sim & Normal(0, 1) & \textrm{[$\beta$ prior]}\\
\sigma & \sim & Exponential(2) & \textrm{[$\sigma$ prior]}
\end{array}
$$

**E5**

There are three parameters

- $\alpha$
- $\beta$
- $\sigma$

**M1**

$$
\begin{array}{cccr} 
y_i & \sim & Normal( \mu, \sigma) & \textrm{[likelihood]}\\
\mu & \sim & Normal(0, 10) & \textrm{[$\mu$ prior]}\\
\sigma & \sim & Exponential(1) & \textrm{[$\sigma$ prior]}
\end{array}
$$

Using grid approximation

```{r}
n <- 5e3
sample_data <- tibble(y = rnorm(n = n, # sample size 
                                mean = rnorm(n = n, mean = 0, sd = 10), # mu prior
                                sd = rexp(n = n, rate = 1))) # sigma prior

sample_data %>% 
  ggplot(aes(x = y)) +
  geom_density(color = clr0d, fill = fll0) +
  stat_function(fun = function(x){dnorm(x = x, mean = 0, sd = 10 )}, geom = "line", linetype = 3, color = clr1) +
  scale_x_continuous(limits = c(-50, 50)) +
  labs(y = "density")
```

**M2**

```{r, fig.asp = .3}
quap_formula <- alist(
    y ~ dnorm(mu, sigma),  # likelihood
    mu ~ dnorm(0, 10),     # mu prior
    sigma ~ exp(1)         # sigma prior
  )
```

**M3**

$$
\begin{array}{cccr} 
y_i & \sim & Normal( \mu, \sigma) & \textrm{[likelihood]}\\
\mu & = & \alpha + \beta x_i & \textrm{[linear model]}\\
\alpha & \sim & Normal(0, 10) & \textrm{[$\alpha$ prior]}\\
\beta & \sim & Uniform(0, 1) & \textrm{[$\beta$ prior]}\\
\sigma & \sim & Exponential(1) & \textrm{[$\sigma$ prior]}
\end{array}
$$

**M4**

$$
\begin{array}{cccr} 
h_i & \sim & Normal( \mu, \sigma) & \textrm{[likelihood]} \\
\mu & = & \alpha + \beta h_i & \textrm{[linear model]} \\
\alpha & \sim & Normal(150, 5) & \textrm{[$\alpha$ prior, starting size]} \\
\beta & \sim & Uniform(0, 10) & \textrm{[$\beta$ prior, yearly growth]} \\
\sigma & \sim & Normal(0, 8) & \textrm{[$\sigma$ prior, size variation]}
\end{array}
$$

**M5**

No, the chosen prior for $\beta$ already covers this information: $\beta \sim Uniform(0, 10)$ is always positive, forcing a positive growth per year.

**M6**

Limiting the variance of height to 64cm could be done in different ways: by choosing a uniform prior with fixed boundaries [eg. $Uniform(0,64)$], or by limiting the variance of an unbound distribution [eg. $\sigma$ for a normal distribution. 99.7% of the mass is within $3 \sigma$, so $Normal(32, 10)$ would do as well].

**M7**

```{r}
model_uncentered <- quap(
  flist = alist(
    height ~ dnorm( mu, sigma ),
    mu <- alpha + beta * weight,
    alpha ~ dnorm( 178, 20),
    beta ~ dlnorm( 0, 1 ),
    sigma ~ dunif( 0, 50 )
  ),
  data = data_adults
)


precis(model_uncentered) %>% 
  round(digits = 3) %>% 
  as_tibble(rownames = NA) %>%
  knitr::kable()

model_uncentered %>% 
  vcov() %>% 
  round(digits = 2) %>% 
  as.data.frame(row.names = row.names(.)) %>% 
  knitr::kable()
```

compare to the centered version:

```{r}
centered_remember_hw %>%
  knitr::kable()

model_hight %>% 
  vcov() %>% 
  round(digits = 2) %>% 
  as.data.frame(row.names = row.names(.)) %>% 
  knitr::kable()
```

The un-centered model shows higher covariances between $\alpha$ and all other parameters.

```{r}
model_uncentered_mu <- link(model_uncentered, data = data.frame(weight = weight_seq)) %>% 
  as_tibble() %>% 
  set_names(nm = weight_seq) %>% 
  pivot_longer(cols = everything(), names_to = "weight", values_to = "height") %>% 
  mutate(weight = as.numeric(weight)) 

model_uncentered_mu_interval <- model_uncentered_mu %>% 
  group_by(weight) %>% 
  summarise(mean = mean(height),
            PI_lower = PI(height)[1],
            PI_upper = PI(height)[2]) %>% 
  ungroup()

model_uncentered_sd <- sim(model_uncentered, data = data.frame(weight = weight_seq), n = 5e3) %>% 
  as_tibble() %>% 
  set_names(nm = weight_seq) %>% 
  pivot_longer(cols = `25`:`70`, names_to = "weight", values_to = "height") %>% 
  mutate(weight = as.numeric(weight)) 

p_1 <- model_uncentered_sd %>% 
  group_by(weight) %>% 
  summarise(mean = mean(height),
            PI_lower = PI(height)[1],
            PI_upper = PI(height)[2]) %>% 
  ungroup() %>% 
  ggplot(aes(x = weight)) +
  geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35)  +
  geom_point(data = data_adults, aes(y = height), color = clr0, size = .3) +
  geom_ribbon(data = model_uncentered_mu_interval,
              aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) +
  geom_line(data = model_uncentered_mu_interval,
              aes(y = mean))+
  labs(title = " uncentered")

p_2 <- model_hight_sd %>% 
  group_by(weight) %>% 
  summarise(mean = mean(height),
            PI_lower = PI(height)[1],
            PI_upper = PI(height)[2]) %>% 
  ungroup() %>% 
  ggplot(aes(x = weight)) +
  geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35)  +
  geom_point(data = data_adults, aes(y = height), color = clr0, size = .3) +
  geom_ribbon(data = model_hight_mu_interval,
              aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .35) +
  geom_line(data = model_hight_mu_interval,
              aes(y = mean))+
  labs(title = " centered")

p_1 + p_2
```

Hmm `r emo::ji("thinking")``: I can't see a difference - maybe that is the point?

**M8**

```{r}
spline_check <- function(n_knots = 15, inner = TRUE){
  knot_list <- quantile(data_cherry$year, probs = seq(0, 1, length.out = n_knots))
  
  b_spline_cherry <- bs(data_cherry$year,
                        knots = knot_list[-c(1, n_knots)],
                        degree = 3,
                        intercept = TRUE)
  
  b_spline_tib <- b_spline_cherry %>% 
    as_tibble() %>%
    set_names(nm = str_pad(1:(n_knots+2), width = 2, pad = 0)) %>% 
    bind_cols(select(data_cherry, year)) %>% 
    pivot_longer(cols = -year, names_to = "bias_function", values_to = "bias")
  
  model_cherry <- quap(
    alist(
      D ~ dnorm(mu, sigma),
      mu <- a + B %*% w,
      a ~ dnorm(100, prior_sd_a),
      w ~ dnorm(0, prior_sd_w),
      sigma ~ dexp(1)
    ),
    data = list(D = data_cherry$doy, B = b_spline_cherry),
    start = list(w = rep(0, ncol(b_spline_cherry)))
  )
  
  cherry_samples <- extract.samples(model_cherry) %>% 
    as.data.frame() %>% 
    as_tibble() %>% 
    set_names(nm = c("a", "sigma", str_pad(1:(n_knots+2), 2,pad = 0)))
  
  cherry_samples_mu <- cherry_samples %>% 
    summarise(across(everything(), mean)) %>% 
    pivot_longer(cols = everything(),
                 names_to = "bias_function", values_to ="weight")
  
  model_cherry_samples <- link(model_cherry) %>% 
    as_tibble()%>% 
    set_names(nm = data_cherry$year) %>% 
    pivot_longer(cols = everything(), names_to = "year", values_to = "doy") %>% 
    mutate(year = as.numeric(year))  %>% 
    arrange(year)
  
  model_cherry_stats <- model_cherry_samples %>% 
    group_by(year) %>% 
    nest() %>% 
    mutate(mean = map_dbl(data, function(data){mean(data$doy)}),
           PI_lower = map_dbl(data, function(data){PI(data$doy)[1]}),
           PI_upper = map_dbl(data, function(data){PI(data$doy)[2]}))
  
  p_splines_pure <- ggplot() +
    geom_vline(data = tibble(year = knot_list),
               aes(xintercept = year),
               linetype = 3, color = rgb(0,0,0,.5)) +
    geom_line(data = b_spline_tib %>% left_join(cherry_samples_mu),
              aes(x = year, y = bias * weight,
                  color = as.numeric(bias_function)
                  , group = bias_function), 
              size = .3, alpha = .75) +
    scale_color_gradientn(colours = c("black", clr0d, clr1), guide = "none") +
    theme(panel.grid.minor.x = element_blank(),
          panel.grid.major.x = element_blank(), 
          axis.title.x = element_blank(),
          axis.text.x = element_blank()) +
    labs(title = glue("{n_knots} kn, sd a: {prior_sd_a}, sd w: {prior_sd_w}"))
  
  p_splines_fitted <- model_cherry_stats %>% 
    ggplot(aes(x = year)) +
    geom_vline(data = tibble(year = knot_list),
               aes(xintercept = year),
               linetype = 3, color = rgb(0,0,0,.5)) +
    geom_point(data = cherry_blossoms, aes(y = doy), color = clr1, alpha = .1, size = .2) +
    geom_ribbon(aes(ymin = PI_lower, ymax = PI_upper), fill = clr1, alpha = .65) +
    geom_line(aes(y = mean)) +
    labs(y = "Day of first blossom") +
    theme(panel.grid.minor.x = element_blank(),
          panel.grid.major.x = element_blank(),
          axis.title.x = element_blank(),
          axis.text.x = element_blank())
  
  if(inner){ 
    p_splines_pure <- p_splines_pure + theme(axis.title.y = element_blank())
    p_splines_fitted <- p_splines_fitted + theme(axis.title.y = element_blank())
  }
  
  p_splines_pure + p_splines_fitted + plot_layout(ncol = 1, heights = c(.5, 1))
}

set.seed(14)
prior_sd_a = 10
prior_sd_w <- 10
p1 <- spline_check(n_knots = 3, inner = FALSE)

prior_sd_w <-  50
p2 <- spline_check(n_knots = 3, inner = TRUE)
set.seed(42)
prior_sd_w <-  100
p3 <- spline_check(n_knots = 3, inner = TRUE)

p1 | p2 | p3
```


```{r}
set.seed(41)
prior_sd_w <- 10
p1 <- spline_check(n_knots = 10, inner = FALSE)

prior_sd_w <-  50
p2 <- spline_check(n_knots = 10, inner = TRUE)

prior_sd_w <-  100
p3 <- spline_check(n_knots = 10, inner = TRUE)

p1 | p2 | p3
```


```{r}
set.seed(42)
prior_sd_w <- 10
p1 <- spline_check(n_knots = 30, inner = FALSE)

prior_sd_w <-  50
p2 <- spline_check(n_knots = 30, inner = TRUE)

prior_sd_w <-  100
p3 <- spline_check(n_knots = 30, inner = TRUE)

p1 | p2 | p3
```

They control the division of data and the initial scale for the weighting

**H1**

**H2**

**H3**

**H4**

**H5**


## {brms} section

---

<div id="myModal" class="modal">
  <span class="close">&times;</span>
  <img class="modal-content" id="img01">
  <div id="caption"></div>
</div>

<script src="./js/zoom.js"></script>